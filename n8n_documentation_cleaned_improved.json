[
  {
    "file_path": "1-0-migration-checklist.md",
    "content": "n8n v1.0 migration guide\nThis document provides a summary of what you should be aware of before updating to version 1.0 of n8n.\nThe release of n8n 1.0 marks a milestone in n8n's journey to make n8n available for demanding production environments. Version 1.0 represents the hard work invested over the last four years to make n8n the most accessible, powerful, and versatile automation tool. n8n 1.0 is now ready for use in production.\nNew features\nPython support in the Code node\nAlthough JavaScript remains the default language, you can now also select Python as an option in the Code node and even make use of many Python modules. Note that Python is unavailable in Code nodes added to a workflow before v1.0.\nPR #4295, PR #6209\nExecution order\nn8n 1.0 introduces a new execution order for multi-branch workflows:\nIn multi-branch workflows, n8n needs to determine the order in which to execute nodes on branches. Previously, n8n executed the first node of each branch, then the second of each branch, and so on (breadth-first). The new execution order ensures that each branch executes completely before starting the next one (depth-first). Branches execute based on their position on the canvas, from top to bottom. If two branches are at the same height, the leftmost one executes first.\nn8n used to execute multi-input nodes as long as they received data on their first input. Nodes connected to the second input of multi-input nodes automatically executed regardless of whether they received data. The new execution order introduced in n8n 1.0 simplifies this behavior: Nodes are now executed only when they receive data, and multi-input nodes require data on at least one of their inputs to execute.\nYour existing workflows will use the legacy order, while new workflows will execute using the v1 order. You can configure the execution order for each workflow in workflow settings.\nPR #4238, PR #6246, PR #6507\nDeprecations\nMySQL and MariaDB\nn8n has deprecated support for MySQL and MariaDB as storage backends for n8n. These database systems are used by only a few users, yet they require continuous development and maintenance efforts. n8n recommends migrating to PostgreSQL for better compatibility and long-term support.\nPR #6189\nEXECUTIONS_PROCESS and \"own\" mode\nPreviously, you could use the EXECUTIONS_PROCESS environment variable to specify whether executions should run in the main process or in their own processes. This option and own mode are now deprecated and will be removed in a future version of n8n. This is because it led to increased code complexity while offering marginal benefits. Starting from n8n 1.0, main will be the new default.\nNote that executions start much faster in main mode than in own mode. However, if a workflow consumes more memory than is available, it might crash the entire n8n application instead of just the worker thread. To mitigate this, make sure to allocate enough system resources or configure queue mode to distribute executions among multiple workers.\nPR #6196\nBreaking changes\nDocker\nPermissions change\nWhen using Docker-based deployments, the n8n process is now run by the user node instead of root. This change increases security.\nIf permission errors appear in your n8n container logs when starting n8n, you may need to update the permissions by executing the following command on the Docker host:\nImage removal\nWe've removed the Debian and RHEL images. If you were using these you need to change the image you use. This shouldn't result in any errors unless you were making a custom image based on one of those images.\nEntrypoint change\nThe entrypoint for the container has changed and you no longer need to specify the n8n command. If you were previously running n8n worker --concurrency=5 it's now worker --concurrency=5\nPR #6365\nWorkflow failures due to expression errors\nWorkflow executions may fail due to syntax or runtime errors in expressions, such as those that reference non-existent nodes. While expressions already throw errors on the frontend, this change ensures that n8n also throws errors on the backend, where they were previously silently ignored. To receive notifications of failing workflows, n8n recommends setting up an \"error workflow\" under workflow settings.\nPR #6352\nMandatory owner account\nThis change makes User Management mandatory and removes support for other authentication methods, such as BasicAuth and External JWT. Note that the number of permitted users on n8n.cloud or custom plans still varies depending on your subscription.\nPR #6362\nDirectory for installing custom nodes\nn8n will no longer load custom nodes from its global node_modules directory. Instead, you must install (or link) them to ~/.n8n/custom (or a directory defined by N8N_CUSTOM_EXTENSIONS). Custom nodes that are npm packages will be located in ~/.n8n/nodes.\nIf you have custom nodes that were linked using npm link into the global node_modules directory, you need to link them again, into ~/.n8n/nodes instead.\nPR #6396\nWebSockets\nThe N8N_PUSH_BACKEND environment variable can be used to configure one of two available methods for pushing updates to the user interface: sse and websocket. Starting with n8n 1.0, websocket is the default method.\nPR #6196\nDate transformation functions\nn8n provides various transformation functions that operate on dates. These functions may return either a JavaScript Date or a Luxon DateTime object. With the new behavior, the return type always matches the input. If you call a date transformation function on a Date, it returns a Date. Similarly, if you call it on a DateTime object, it returns a DateTime object.\nTo identify any workflows and nodes that might be impacted by this change, you can use this utility workflow.\nFor more information about date transformation functions, please refer to the official documentation.\nPR #6435\nExecution data retention\nStarting from n8n 1.0, all successful, failed, and manual workflow executions will be saved by default. These settings can be modified for each workflow under \"Workflow Settings,\" or globally using the respective environment variables. Additionally, the EXECUTIONS_DATA_PRUNE setting will be enabled by default, with EXECUTIONS_DATA_PRUNE_MAX_COUNT set to 10,000. These default settings are designed to prevent performance degradation when using SQLite. Make sure to configure them according to your individual requirements and system capacity.\nPR #6577\nRemoved N8N_USE_DEPRECATED_REQUEST_LIB\nThe legacy request library has been deprecated for some time now. As of n8n 1.0, the ability to fall back to it in the HTTP Request node by setting the N8N_USE_DEPRECATED_REQUEST_LIB environment variable has been fully removed. The HTTP Request node will now always use the new HttpRequest interface.\nIf you build custom nodes, refer to HTTP request helpers for more information on migrating to the new interface.\nPR #6413\nRemoved WEBHOOK_TUNNEL_URL\nAs of version 0.227.0, n8n has renamed the WEBHOOK_TUNNEL_URL configuration option to WEBHOOK_URL. In n8n 1.0, WEBHOOK_TUNNEL_URL has been removed. Update your setup to reflect the new name. For more information about this configuration option, refer to the docs.\nPR #1408\nRemove Node 16 support\nn8n now requires Node 18.17.0 or above.\nUpdating to n8n 1.0\nCreate a full backup of n8n.\nn8n recommends updating to the latest n8n 0.x release before updating to n8n 1.x. This will allow you to pinpoint any potential issues to the correct release. Once you have verified that n8n 0.x starts up without any issues, proceed to the next step.\nCarefully read the Deprecations and Breaking Changes sections above to assess how they may affect your setup.\nUpdate to n8n 1.0:\nDuring beta (before July 24th 2023): If using Docker, pull the next Docker image.\nAfter July 24th 2023: If using Docker, pull the latest Docker image.\nIf you encounter any issues, redeploy the previous n8n version and restore the backup.\nReporting issues\nIf you encounter any issues during the process of updating to n8n 1.0, please seek help in the community forum.\nThank you\nWe would like to take a moment to express our gratitude to all of our users for their continued support and feedback. Your contributions are invaluable in helping us make n8n the best possible automation tool. We're excited to continue working with you as we move forward with the release of version 1.0 and beyond. Thank you for being a part of our journey!"
  },
  {
    "file_path": "choose-n8n.md",
    "content": "Choose your n8n\nThis section contains information on n8n's range of platforms, pricing plans, and licenses.\nPlatforms\nThere are different ways to set up n8n depending on how you intend to use it:\nn8n Cloud: hosted solution, no need to install anything.\nSelf-host: recommended method for production or customized use cases.\nnpm\nDocker\nServer setup guides for popular platforms\nEmbed: n8n Embed allows you to white label n8n and build it into your own product. Contact n8n on the Embed website for pricing and support.\nLicenses\nn8n's Sustainable Use License and n8n Enterprise License are based on the fair-code model.\nFor a detailed explanation of the license, refer to Sustainable Use License.\nFree versions\nn8n offers the following free options:\nA free trial of Cloud\nA free self-hosted community edition for self-hosted users\nPaid versions\nn8n has two paid versions:\nn8n Cloud: choose from a range of paid plans to suit your usage and feature needs.\nSelf-hosted: there are both free and paid versions of self-hosted.\nFor details of the Cloud plans and contact details for Enterprise Self-hosted, refer to Pricing on the n8n website."
  },
  {
    "file_path": "external-secrets.md",
    "content": "External secrets\nYou can use an external secrets store to manage credentials for n8n.\nn8n stores all credentials encrypted in its database, and restricts access to them by default. With the external secrets feature, you can store sensitive credential information in an external vault, and have n8n load it in when required. This provides an extra layer of security and allows you to manage credentials used across multiple n8n environments in one central place.\nConnect n8n to your secrets store\nIn n8n, go to Settings > External Secrets.\nSelect Set Up for your store provider.\nEnter the credentials for your provider:\nAzure Key Vault: Provide your vault name, tenant ID, client ID, and client secret. Refer to the Azure documentation to register a Microsoft Entra ID app and create a service principal. n8n supports only single-line values for secrets.\nAWS Secrets Manager: provide your access key ID, secret access key, and region. The IAM user must have the secretsmanager:ListSecrets, secretsmanager:BatchGetSecretValue, and secretsmanager:GetSecretValue permissions.\nTo give n8n access to all secrets in your AWS Secrets Manager, you can attach the following policy to the IAM user:\nYou can also be more restrictive and give n8n access to select specific AWS Secret Manager secrets. You still need to allow the secretsmanager:ListSecrets and secretsmanager:BatchGetSecretValue permissions to access all resources. These permissions allow n8n to retrieve ARN-scoped secrets, but don't provide access to the secret values.\nNext, you need set the scope for the secretsmanager:GetSecretValue permission to the specific Amazon Resource Names (ARNs) for the secrets you wish to share with n8n. Ensure you use the correct region and account ID in each resource ARNs. You can find the ARN details in the AWS dashboard for your secrets.\nFor example, the following IAM policy only allows access to secrets with a name starting with n8n in your specified AWS account and region:\nFor more IAM permission policy examples, consult the AWS documentation.\nHashiCorp Vault: provide the Vault URL for your vault instance, and select your Authentication Method.  Enter your authentication details. Optionally provide a namespace.\nRefer to the HashiCorp documentation for your authentication method:\nToken auth method\nAppRole auth method\nUserpass auth method\nIf you use vault namespaces, you can enter the namespace n8n should connect to. Refer to Vault Enterprise namespaces for more information on HashiCorp Vault namespaces.\nInfisical: provide a Service Token. Refer to Infisical's Service token documentation for information on getting your token. If you self-host Infisical, enter the Site URL.\nGoogle Cloud Platform: provide a Service Account Key (JSON) for a service account that has at least these roles: Secret Manager Secret Accessor and Secret Manager Secret Viewer. Refer to Google's service account documentation for more information.\nSave your configuration.\nEnable the provider using the Disabled / Enabled toggle.\nUse secrets in n8n credentials\nTo use a secret from your store in an n8n credential:\nCreate a new credential, or open an existing one.\nOn the field where you want to use a secret:\nHover over the field.\nSelect Expression.\nIn the field where you want to use a secret, enter an expression referencing the secret name:\nis either vault (for HashiCorp) or infisical or awsSecretsManager. Replace  with the name as it appears in your vault.\nUsing external secrets with n8n environments\nn8n's Source control and environments feature allows you to create different n8n environments, backed by Git. The feature doesn't support using different credentials in different instances. You can use an external secrets vault to provide different credentials for different environments by connecting each n8n instance to a different vault or project environment.\nFor example, you have two n8n instances, one for development and one for production. You use Infisical for your vault. In Infisical, create a project with two environments, development and production. Generate a token for each Infisical environment. Use the token for the development environment to connect your development n8n instance, and the token for your production environment to connect your production n8n instance.\nUsing external secrets in projects\nTo use external secrets in an RBAC project, you must have an instance owner or instance admin as a member of the project.\nTroubleshooting\nInfisical version changes\nInfisical version upgrades can introduce problems connecting to n8n. If your Infisical connection stops working, check if there was a recent version change. If so, report the issue to help@n8n.io.\nOnly set external secrets on credentials owned by an instance owner or admin\nDue to the permissions that instance owners and admins have, it's possible for owners and admins to update credentials owned by another user with a secrets expression. This will appear to work in preview for an instance owner or admin, but the secret won't resolve when the workflow runs in production.\nOnly use external secrets for credentials that are owned by an instance admin or owner. This ensures they resolve correctly in production."
  },
  {
    "file_path": "glossary.md",
    "content": "AI agent\nAI agents are artificial intelligence systems capable of responding to requests, making decisions, and performing real-world tasks for users. They use large language models (LLMs) to interpret user input and make decisions about how to best process requests using the information and resources they have available.\nAI chain\nAI chains allow you to interact with large language models (LLMs) and other resources in sequences of calls to components. AI chains in n8n don't use persistent memory, so you can't use them to reference previous context (use AI agents for this).\nAI embedding\nEmbeddings are numerical representations of data using vectors. They're used by AI to interpret complex data and relationships by mapping values across many dimensions. Vector databases, or vector stores, are databases designed to store and access embeddings.\nAI groundedness\nIn AI, and specifically in retrieval-augmented generation (RAG) contexts, groundedness and ungroundedness are measures of how much a model's responses accurately reflect source information. The model uses its source documents to generate grounded responses, while ungrounded responses involve speculation or hallucination unsupported by those same sources.\nAI reranking\nReranking is a technique that refines the order of a list of candidate documents to improve the relevance of search results. Retrieval-Augmented Generation (RAG) and other applications use reranking to prioritize the most relevant information for generation or downstream tasks.\nAI memory\nIn an AI context, memory allows AI tools to persist message context across interactions. This allows you to have a continuing conversations with AI agents, for example, without submitting ongoing context with each message. In n8n, AI agent nodes can use memory, but AI chains can't.\nAI retrieval-augmented generation (RAG)\nRetrieval-augmented generation, or RAG, is a technique for providing LLMs access to new information from external sources to improve AI responses. RAG systems retrieve relevant documents to ground responses in up-to-date, domain-specific, or proprietary knowledge to supplement their original training data. RAG systems often rely on vector stores to manage and search this external data efficiently.\nAI tool\nIn an AI context, a tool is an add-on resource that the AI can refer to for specific information or functionality when responding to a request. The AI model can use a tool to interact with external systems or complete specific, focused tasks.\nAI vector store\nVector stores, or vector databases, are databases designed to store numerical representations of information called embeddings.\nAPI\nAPIs, or application programming interfaces, offer programmatic access to a service's data and functionality. APIs make it easier for software to interact with external systems. They're often offered as an alternative to traditional user-focused interfaces accessed through web browsers or UI.\ncanvas (n8n)\nThe canvas is the main interface for building workflows in n8n's editor UI. You use the canvas to add and connect nodes to compose workflows.\ncluster node (n8n)\nIn n8n, cluster nodes are groups of nodes that work together to provide functionality in a workflow. They consist of a root node and one or more sub nodes that extend the node's functionality.\ncredential (n8n)\nIn n8n, credentials store authentication information to connect with specific apps and services. After creating credentials with your authentication information (username and password, API key, OAuth secrets, etc.), you can use the associated app node to interact with the service.\ndata pinning (n8n)\nData pinning allows you to temporarily freeze the output data of a node during workflow development. This allows you to develop workflows with predictable data without making repeated requests to external services. Production workflows ignore pinned data and request new data on each execution.\neditor (n8n)\nThe n8n editor UI allows you to create and manage workflows. The main area is the canvas, where you can compose workflows by adding, configuring, and connecting nodes. The side and top panels allow you to access other areas of the UI like credentials, templates, variables, executions, and more.\nentitlement (n8n)\nIn n8n, entitlements grant n8n instances access to plan-restricted features for a specific period of time.\nFloating entitlements are a pool of entitlements that you can distribute among various n8n instances. You can re-assign a floating entitlement to transfer its access to a different n8n instance.\nevaluation (n8n)\nIn n8n, evaluation allows you to tag and organize execution history and compare it against new executions. You can use this to understand how your workflow performs over time as you make changes. In particular, this is useful while developing AI-centered workflows.\nexpression (n8n)\nIn n8n, expressions allow you to populate node parameters dynamically by executing JavaScript code. Instead of providing a static value, you can use the n8n expression syntax to define the value using data from previous nodes, other workflows, or your n8n environment.\nLangChain\nLangChain is an AI-development framework used to work with large language models (LLMs). LangChain provides a standardized system for working with a wide variety of models and other resources and linking different components together to build complex applications.\nLarge language model (LLM)\nLarge language models, or LLMs, are AI machine learning models designed to excel in natural language processing (NLP) tasks. They're built by training on large amounts of data to develop probabilistic models of language and other data.\nnode (n8n)\nIn n8n, nodes are individual components that you compose to create workflows. Nodes define when the workflow should run, allow you to fetch, send, and process data, can define flow control logic, and connect with external services.\nproject (n8n)\nn8n projects allow you to separate workflows, variables, and credentials into separate groups for easier management. Projects make it easier for teams to collaborate by sharing and compartmentalizing related resources.\nroot node (n8n)\nEach n8n cluster node contains a single root nodes that defines the main functionality of the cluster. One or more sub nodes attach to the root node to extend its functionality.\nsub node (n8n)\nn8n cluster nodes consist of one or more sub nodes connected to a root node. Sub nodes extend the functionality of the root node, providing access to specific services or resources or offering specific types of dedicated processing, like calculator functionality, for example.\ntemplate (n8n)\nn8n templates are pre-built workflows designed by n8n and community members that you can import into your n8n instance. When using templates, you may need to fill in credentials and adjust the configuration to suit your needs.\ntrigger node (n8n)\nA trigger node is a special node responsible for executing the workflow in response to certain conditions. All production workflows need at least one trigger to determine when the workflow should run.\nworkflow (n8n)\nAn n8n workflow is a collection of nodes that automate a process. Workflows begin execution when a trigger condition occurs and execute sequentially to achieve complex tasks."
  },
  {
    "file_path": "index.md",
    "content": "Welcome to n8n Docs\nThis is the documentation for n8n, a fair-code licensed workflow automation tool that combines AI capabilities with business process automation.\nIt covers everything from setup to usage and development. It's a work in progress and all contributions are welcome.\nWhere to start\n-   __Quickstarts__\nJump in with n8n's quickstart guides.\n:octicons-arrow-right-24: Try it out\n-   __Choose the right n8n for you__\nCloud, npm, self-host . . .\n:octicons-arrow-right-24: Options\n-   __Explore integrations__\nBrowse n8n's integrations library.\n:octicons-arrow-right-24: Find your apps\n-   __Build AI functionality__\nn8n supports building AI functionality and tools.\n:octicons-arrow-right-24: Advanced AI\nAbout n8n\nn8n (pronounced n-eight-n) helps you to connect any app with an API with any other, and manipulate its data with little or no code.\nCustomizable: highly flexible workflows and the option to build custom nodes.\nConvenient: use the npm or Docker to try out n8n, or the Cloud hosting option if you want us to handle the infrastructure.\nPrivacy-focused: self-host n8n for privacy and security."
  },
  {
    "file_path": "insights.md",
    "content": "Insights\nInsights gives instance owners and admins visibility into how workflows perform over time. This feature consists of three parts:\nInsights summary banner: Shows key metrics about your instance from the last 7 days at the top of the overview space.\nInsights dashboard: A more detailed visual breakdown with per-workflow metrics and historical comparisons.\nTime saved (Workflow ROI): For each workflow, you can set the number of minutes of work that each production execution saves you.\nInsights summary banner\nn8n collects several metrics for both the insights summary banner and dashboard. They include:\nTotal production executions (not including sub-workflow executions or manual executions)\nTotal failed production executions\nProduction execution failure rate\nTime saved (when set on at least one or more active workflows)\nRun time average (including wait time from any wait nodes)\nInsights dashboard\nThose on the Pro and Enterprise plans can access the Insights section from the side navigation. Each metric from the summary banner is also clickable, taking you to the corresponding chart.\nThe insights dashboard also has a table showing individual insights from each workflow including total production executions, failed production executions, failure rate, time saved, and run time average.\nInsights time periods\nBy default, the insights summary banner and dashboard show a rolling 7 day window with a comparison to the previous period to identify increases or decreases for each metric. On the dashboard, paid plans also display data for other date ranges:\nPro: 7 and 14 days\nEnterprise: 24 hours, 7 days, 14 days, 30 days, 90 days, 6 months, 1 year\nSetting the time saved by a workflow\nFor each workflow, you can set the number of minutes of work a workflow saves you each time it runs. You can configure this by navigating to the workflow, selecting the three dots menu in the top right and selecting settings. There you can update the Estimated time saved value and save.\nThis setting helps you calculate how much time automating a process saves over time vs the manual effort to complete the same task or process. Once set, n8n calculates the amount of time the workflow saves you based on the number of production executions and displays it on the summary banner and dashboard.\nDisable or configure insights metrics collection\nIf you self-host n8n, you can disable or configure insights and metrics collection using environment variables.\nInsights FAQs\nWhich executions do n8n use to calculate the values in the insights banner and dashboard?\nn8n insights only collects data from production executions (for example, those from active workflows triggered on a schedule or a webhook) from the main (parent) workflow. This means that it doesn't count manual (test) executions or executions from sub-workflows or error workflows.\nDoes n8n use historic execution data when upgrading to a version with insights?\nn8n only starts collecting data for insights once you update to the first supported version (1.89.0). This means it only reports on executions from that point forward and you won't see execution data in insights from prior periods."
  },
  {
    "file_path": "keyboard-shortcuts.md",
    "content": "Keyboard shortcuts and controls\nn8n provides keyboard shortcuts for some actions.\nWorkflow controls\nCtrl + Alt + n: create new workflow\nCtrl + o: open workflow\nCtrl + s: save the current workflow\nCtrl + z: undo\nCtrl + shift + z: redo\nCtrl + Enter: execute workflow\nCanvas\nMove the canvas\nCtrl + Left Mouse Button + drag: move node view\nCtrl + Middle mouse button + drag: move node view\nSpace + drag: move node view\nMiddle mouse button + drag: move node view\nTwo fingers on a touch screen: move node view\nCanvas zoom\n+ or =: zoom in\n- or _: zoom out\n0: reset zoom level\n1: zoom to fit workflow\nCtrl + Mouse wheel: zoom in/out\nNodes on the canvas\nDouble click on a node: open the node details\nCtrl/Cmd + Double click on a sub-workflow node: open the sub-workflow in a new tab\nCtrl + a: select all nodes\nCtrl + v: paste nodes\nShift + s: add sticky note\nWith one or more nodes selected in canvas\nArrowDown: select sibling node below the current one\nArrowLeft: select node left of the current one\nArrowRight: select node right of the current one\nArrowUp: select sibling node above the current one\nCtrl + c: copy\nCtrl + x: cut\nD: deactivate\nDelete: delete\nEnter: open\nF2: rename\nP: pin data in node. Refer to Data pinning for more information.\nShift + ArrowLeft: select all nodes left of the current one\nShift + ArrowRight: select all nodes right of the current one\nCtrl/Cmd + Shift + o on a sub-workflow node: open the sub-workflow in a new tab\nNode panel\nTab: open the Node Panel\nEnter: insert selected node into workflow\nEscape: close Node panel\nNode panel categories\nEnter: insert node into workflow, collapse/expand category, open subcategory\nArrowRight: expand category, open subcategory\nArrowLeft: collapse category, close subcategory view\nWithin nodes\n=: in an empty parameter input, this switches to expressions mode."
  },
  {
    "file_path": "learning-path.md",
    "content": "This guide outlines a series of tutorials and resources designed to get you started with n8n.\nIt's not necessary to complete all items listed to start using n8n. Use this as a reference to navigate to the most relevant parts of the documentation and other resources according to your needs.\nJoin the community\nn8n has an active community where you can get and offer help. Connect, share, and learn with other n8n users:\nAsk questions and make feature requests in the Community Forum.\nReport bugs and contribute on GitHub.\nSet up your n8n\nIf you don't have an account yet, sign up to a free trial on n8n Cloud or install n8n's community edition with Docker (recommended) or npm. See Choose your n8n for more details.\nTry it out\nStart with the quickstart guides to help you get up and running with building basic workflows.\nA very quick quickstart\nA longer introduction\nBuild an AI workflow in n8n\nStructured Courses\nn8n offers two sets of courses.\nVideo courses\nLearn key concepts and n8n features, while building examples as you go.\nThe Beginner course covers the basics of n8n.\nThe Advanced course covers more complex workflows, more technical nodes, and enterprise features\nText courses\nBuild more complex workflows while learning key concepts along the way. Earn a badge and an avatar in your community profile.\nLevel 1: Beginner Course\nLevel 2: Intermediate Course\nSelf-hosting n8n\nExplore various self-hosting options in n8n. If you‚Äôre not sure where to start, these are two popular options:\nHosting n8n on DigitalOcean\nHosting n8n on Amazon Web Services\nBuild a node\nIf you can't find a node for a specific app or a service, you can build a node yourself and share with the community. See what others have built on npm website.\nBuild a declarative-style node\nLearn how to build your own n8n nodes (Youtube Video)\nStay updated\nFollow new features and bug fixes in the Release Notes\nFollow n8n on socials: Twitter/X, Discord, LinkedIn, YouTube"
  },
  {
    "file_path": "license-key.md",
    "content": "License Key\nTo enable certain licensed features, you must first activate your license. You can do this either through the UI or by setting environment variables.\nAdd a license key using the UI\nIn your n8n instance:\nLog in as Admin or Owner.\nSelect Settings > Usage and plan.\nSelect Enter activation key.\nPaste in your license key.\nSelect Activate.\nAdd a license key using an environment variables\nIn your n8n configuration, set N8N_LICENSE_ACTIVATION_KEY to your license key. If the instance already has an activated license, this variable will have no effect.\nRefer to Environment variables to learn more about configuring n8n.\nAllowlist the license server IP addresses\nn8n uses Cloudflare to host the license server. As the specific IP addresses can change, you need to allowlist the full range of Cloudflare IP addresses to ensure n8n can always reach the license server."
  },
  {
    "file_path": "log-streaming.md",
    "content": "Log streaming\nLog streaming allows you to send events from n8n to your own logging tools. This allows you to manage your n8n monitoring in your own alerting and logging processes.\nSet up log streaming\nTo use log streaming, you have to add a streaming destination.\nNavigate to Settings > Log Streaming.\nSelect Add new destination.\nChoose your destination type. n8n opens the New Event Destination modal.\nIn the New Event Destination modal, enter the configuration information for your event destination. These depend on the type of destination you're using.\nSelect Events to choose which events to stream.\nSelect Save.\nEvents\nThe following events are available. You can choose which events to stream in Settings > Log Streaming > Events.\nWorkflow\nStarted\nSuccess\nFailed\nNode executions\nStarted\nFinished\nAudit\nUser signed up\nUser updated\nUser deleted\nUser invited\nUser invitation accepted\nUser re-invited\nUser email failed\nUser reset requested\nUser reset\nUser credentials created\nUser credentials shared\nUser credentials updated\nUser credentials deleted\nUser API created\nUser API deleted\nPackage installed\nPackage updated\nPackage deleted\nWorkflow created\nWorkflow deleted\nWorkflow updated\nAI node logs\nMemory get messages\nMemory added message\nOutput parser get instructions\nOutput parser parsed\nRetriever get relevant documents\nEmbeddings embedded document\nEmbeddings embedded query\nDocument processed\nText splitter split\nTool called\nVector store searched\nLLM generated\nVector store populated\nRunner\nTask requested\nResponse received\nQueue\nJob enqueued\nJob dequeued\nJob completed\nJob failed\nJob stalled\nDestinations\nn8n supports three destination types:\nA syslog server\nA generic webhook\nA Sentry client"
  },
  {
    "file_path": "release-notes.md",
    "content": "Release notes\nNew features and bug fixes for n8n.\nYou can also view the Releases in the GitHub repository.\nSemantic versioning in n8n\nn8n uses semantic versioning. All version numbers are in the format MAJOR.MINOR.PATCH. Version numbers increment as follows:\nMAJOR version when making incompatible changes which can require user action.\nMINOR version when adding functionality in a backward-compatible manner.\nPATCH version when making backward-compatible bug fixes.\nn8n@1.104.1\nView the commits for this version.\nRelease date: 2025-07-23\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.103.2\nView the commits for this version.\nRelease date: 2025-07-22\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.104.0\nView the commits for this version.\nRelease date: 2025-07-21\n=======\nThis release contains core updates, editor improvements, a new node, node updates, and bug fixes.\nContributors\nnunulk\niaptsiauri\nKGuillaume-chaps\nFor full release details, refer to Releases on GitHub.\nn8n@1.101.3\nView the commits for this version.\nRelease date: 2025-07-18\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.102.4\nView the commits for this version.\nRelease date: 2025-07-17\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.103.1\nView the commits for this version.\nRelease date: 2025-07-17\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.102.3\nView the commits for this version.\nRelease date: 2025-07-14\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.103.0\nView the commits for this version.\nRelease date: 2025-07-14\nThis release contains core updates, editor improvements, new nodes, node improvements, and bug fixes.\nImproved instance user list with more visibility\nThe instance user list has been updated with a new table layout and additional details to help admins manage access more easily.\nYou can now:\nSee total users and filter by name or email\nView which projects each user has access to\nWhether a user has enabled 2FA and sort based on that\nSee the last active date for each user\nThis makes it easier to audit user activity, identify inactive accounts, and understand how access is distributed across your instance.\n!Improved instance user list with more visbility screenshot\nWebhook HTML responses\nStarting with this release, if your workflow sends an HTML response to a webhook, n8n automatically wraps the content in an . This is a security mechanism to protect the instance users.\nThis has the following implications:\nHTML renders in a sandboxed iframe instead of directly in the parent document.\nJavaScript code that attempts to access the top-level window or local storage will fail.\nAuthentication headers aren't available in the sandboxed iframe (for example, basic auth). You need to use an alternative approach, like embedding a short-lived access token within the HTML.\nRelative URLs (for example, ) won't work. Use absolute URLs instead.\n### Built-in Metrics for AI Evaluations\nUsing evaluations is a best practice for any AI solution, and a must if reliability and predictability are business-critical. With this release, we‚Äôve made it easier to set up evaluations in n8n by introducing a set of built-in metrics. These metrics can review AI responses and assign scores based on factors like correctness, helpfulness, and more.\nYou can run regular evaluations and review scores over time as a way to monitor your AI workflow's performance. You can also compare results across different models to help guide model selection, or run evaluations before and after a prompt change to support data-driven, iterative building.\nAs with all evaluations in n8n, you‚Äôll need a dataset that includes the inputs you want to test. For some evaluations, the dataset must also include expected outputs (ground truth) to compare against. The evaluation workflow runs each input through the portion you're testing to generate a response. The built-in metric scores each response based on the aspect you're measuring, allowing you to compare results before and after changes or track trends over time in the Evaluations tab.\nYou can still define your own custom metrics, but for common use cases, the built-in options make it much faster to implement.\nüõ†Ô∏è **How to:**\n1. Set up your evaluation as described here, using an **Evaluation** node as the trigger and another with the **Set Metrics** operation.\n2. In the **Set Metrics** node, choose a metric from the dropdown list.\n3. Define any additional parameters required for your selected metric. In most cases, this includes mapping the dataset columns to the appropriate fields.\nüìè **Available built-in metrics:**\n- **Correctness (AI-based):** Compares AI workflow-generated responses to expected answers. Another LLM acts as a judge, scoring the responses based on guidance you provide in the prompt.\n- **Helpfulness (AI-based):** Evaluates how helpful a response is in relation to a user query, using an LLM and prompt-defined scoring criteria.\n- **String Similarity:** Measures how closely the response matches the expected output by comparing strings. Useful for command generation or when output needs to follow a specific structure.\n- **Categorization:** Checks whether a response matches an expected label, such as assigning items to the correct category.\n- **Tools Used:** Verifies whether the AI agent called the tools you specified in your dataset.\nTo enable this, make sure **Return Intermediate Steps** is turned on in your agent so the evaluation can access the tools it actually called.\nüß† Keep in mind\n- Registered Community Edition enables analysis of one evaluation in the¬†**Evaluations**¬†tab which allows easy comparison of evaluation runs over time. Pro and Enterprise plans allow unlimited evaluations in the¬†**Evaluations**¬†tab.\n!Built-in Metrics\nBuilt-in Metrics\nLearn more about setting up and customizing evaluations.\nAI Agent Tool node\nWith the AI Agent Tool node we are introducing a simplified pattern for multi-agent orchestration that can be run in a single execution and stay entirely on one canvas. You can now connect multiple AI Agent Tool nodes to a primary AI Agent node, allowing it to supervise and delegate work across other specialized agents.\nThis setup is especially useful for building complex systems that function like real-world teams, where a lead agent assigns parts of a task to specialists. You can even add multiple layers of agents directing other agents, just like you would have in a real multi-tiered organizational structure.  It also helps with prompt management by letting you split long, complex instructions into smaller, focused tasks across multiple agents. While similar orchestration was already possible using sub-workflows, AI Agent Tool nodes are a good choice when you want the interaction to happen within a single execution or prefer to manage and debug everything from a single canvas.\nüõ†Ô∏è How to:\nAdd an AI Agent node to your workflow and click + to create a Tools connection.\nSearch for and select the AI Agent Tool node from the Nodes Panel.\nName the node clearly so the primary agent can reference it, then add a short description and prompt.\nConnect any LLMs, memory, and tools the agent needs to perform its role.\nInstruct the primary AI Agent on when to use the AI Agent Tool and to pass along relevant context in its prompt.\nüß† Keep in mind:\nThe orchestrating agent does not pass full execution context by default. Any necessary context must be included in the prompt.\nAI Agent Tool nodes makes it easier to build layered, agent-to-agent workflows without relying on sub-workflows, helping you move faster when building and debugging multi-agent systems.\n!\nAI Agent Tool node\nContributors\nksg97031\nisraelshenkar\nFor full release details, refer to Releases on GitHub.\nn8n@1.102.2\nView the commits for this version.\nRelease date: 2025-07-11\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.101.2\nView the commits for this version.\nRelease date: 2025-07-11\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.102.1\nView the commits for this version.\nRelease date: 2025-07-09\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.102.0\nView the commits for this version.\nRelease date: 2025-07-07\nThis release contains core updates, editor improvements, new nodes, node updates, and bug fixes.\nEnforce 2FA across your instance\nEnterprise Instance owners can now enforce two-factor authentication (2FA) for all users in their instance.\nOnce enabled, any user who hasn‚Äôt set up 2FA will be redirected to complete the setup before they can continue using n8n. This helps organizations meet internal security policies and ensures stronger protection across the workspace.\nThis feature is available only on the Enterprise plan.\nContributors\nmarty-sullivan\ncesars-gh\ndudanogueira\nFor full release details, refer to Releases on GitHub.\nn8n@1.101.1\nView the commits for this version.\nRelease date: 2025-07-03\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.101.0\nView the commits for this version.\nRelease date: 2025-06-30\nThis release contains core updates, editor improvements, node updates, and bug fixes.\nContributors\nluka-mimi\nFor full release details, refer to Releases on GitHub.\nn8n@1.100.1\nView the commits for this version.\nRelease date: 2025-06-25\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.100.0\nView the commits for this version.\nRelease date: 2025-06-23\nThis release contains core updates, editor improvements, a new node, node updates, and bug fixes.\nModel Selector node\nThe Model Selector node gives you more control when working with multiple LLMs in your workflows.\nUse it to determine which connected model should handle a given input, based on conditions like expressions or global variables. This makes it easier to implement model routing strategies, such as switching models based on performance, task type, cost, or availability.\nüõ†Ô∏è How to: Connect multiple model nodes to the Model Selector node, then configure routing conditions in the node‚Äôs settings.\nüß† Keep in mind:\nRules are evaluated in order. The first matching rule determines which model is used even if others would also match.\nAs a sub-node, expressions behave differently here: they always resolves to the first item rather than resolving for each item in turn.\nThe Model Selector node is especially useful in evaluation or production scenarios where routing logic between models needs to adapt based on performance, cost, availability, or dataset-specific needs.\n!Model Selector node\nModel Selector node\nSupport for OIDC (OpenID Connect) authentication\nYou can now use OIDC (OpenID Connect) as an authentication method for Single Sign-On (SSO).\nThis gives enterprise teams more flexibility to integrate n8n with their existing identity providers using a widely adopted and easy-to-manage standard. OIDC is now available alongside SAML, giving Enterprises the choice to select what best fits their internal needs.\nProject admins can now commit to Git within environments\nProject admins now have the ability to commit workflow and credential changes directly to Git through the environments feature. This update streamlines the workflow deployment process by giving project-level admins direct control over committing their changes. It also ensures that the those who know their workflows best can review and commit updates themselves, without needing to involve instance-level admins.\nLearn more about source control environments\nContributors\naliou\nFor full release details, refer to Releases on GitHub.\nn8n@1.99.1\nView the commits for this version.\nRelease date: 2025-06-19\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.98.2\nView the commits for this version.\nRelease date: 2025-06-18\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.99.0\nView the commits for this version.\nRelease date: 2025-06-16\nThis release contains performance improvements, core updates, editor changes, node updates, and bug fixes.\nAutomatically name nodes\nDefault node names now update automatically based on the resource and operation selected, so you‚Äôll always know what a node does at a glance.\nThis adds clarity to your canvas and saves time renaming nodes manually.\nDon‚Äôt worry, automatic naming won‚Äôt break references. And, and if you‚Äôve renamed a node yourself, we‚Äôll leave it just the way you wrote it.\nSupport for RAG extended with built-in templates\nRetrieval-Augmented Generation (RAG) can improve AI responses by providing language models access to data sources with up-to-date, domain-specific, or proprietary knowledge. RAG workflows typically rely on vector stores to manage and search this data efficiently.\nTo get the benefits of using vector stores, such as returning results based on semantic meaning rather than just keyword matches, you need a way to upload your data to the vector store and a way to query it.\nIn n8n, uploading and querying vectors stores happens in two workflows. Now, you have an example to get your started and make implementation easier with the RAG starter template.\nThe Load Data workflow shows how to add data with the appropriate embedding model, split it into chunks with the Default Data Loader, and add metadata as desired.\nThe Retriever workflow for querying data, shows how  agents and vector stores work together to help you define highly relevant results and save tokens using the Question and Answer tool.\nEnable semantic search and the retrieval of unstructured data  for increased quality and relevance of AI responses.\nüõ†Ô∏è How to:\nSearch for RAG starter template in the search bar of the Nodes panel to insert it into your workflow.\nLearn more about implementing RAG in n8n here.\n!RAG starter template\nRAG starter template\nFor full release details, refer to Releases on GitHub.\nn8n@1.98.1\nView the commits for this version.\nRelease date: 2025-06-12\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.98.0\nView the commits for this version.\nRelease date: 2025-06-11\nThis release contains performance improvements, core updates, editor changes, node updates, a new node, and bug fixes.\nContributors\nluka-mimi\nAlexandero89\nkhoazero123\nFor full release details, refer to Releases on GitHub.\nn8n@1.97.1\nView the commits for this version.\nRelease date: 2025-06-04\nThis release contains backports.\nFor full release details, refer to Releases on GitHub.\nn8n@1.95.3\nView the commits for this version.\nRelease date: 2025-06-03\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.97.0\nView the commits for this version.\nRelease date: 2025-06-02\nThis release contains new features, performance improvements and bug fixes.\nConvert to sub-workflow\nLarge, monolithic workflows can slow things down. They‚Äôre harder to maintain, tougher to debug, and more difficult to scale. With sub-workflows, you can take a more modular approach, breaking up big workflows into smaller, manageable parts that are easier to reuse, test, understand, and explain.\nUntil now, creating sub-workflows required copying and pasting nodes manually, setting up a new workflow from scratch, and reconnecting everything by hand. Convert to sub-workflow allows you to simplify this process into a single action, so you can spend more time building and less time restructuring.\nHow it works\nHighlight the nodes you want to convert to a sub-workflow. These must:\nBe fully connected, meaning no missing steps in between them\nStart from a single starting node\nEnd with a single node\nRight-click to open the context menu and select Convert to sub-workflow\nOr use the shortcut: Alt + X\nn8n will:\nOpen a new tab containing the selected nodes\nPreserve all node parameters as-is\nReplace the selected nodes in the original workflow with a Call My Sub-workflow node\nNote: You will need to manually adjust the field types in the Start and Return nodes in the new sub-workflow.\nThis makes it easier to keep workflows modular, performant, and easier to maintain.\nLearn more about sub-workflows.\nThis release contains performance improvements and bug fixes.\nContributors\nmaatthc\nFor full release details, refer to Releases on GitHub.\nn8n@1.96.0\nView the commits for this version.\nRelease date: 2025-06-02\nThis release contains API updates, core changes, editor improvements, node updates, and bug fixes.\nAPI support for assigning users to projects\nYou can now use the API to add and update users within projects. This includes:\nAssigning existing or pending users to a project with a specific role\nUpdating a user‚Äôs role within a project\nRemoving users from one or more projects\nThis update now allows you to use the API to add users to both the instance and specific projects, removing the need to manually assign them in the UI.\nAdd pending users to project member assignment\nYou can now add pending users, those who have been invited but haven't completed sign-up, to projects as members.\nThis change lets you configure a user's project access upfront, without waiting for them to finish setting up their account. It eliminates the back-and-forth of managing access post-sign-up, ensuring users have the right project roles immediately upon joining.\nContributors\nmatthabermehl\nStamsy\nFor full release details, refer to Releases on GitHub.\nn8n@1.95.2\nView the commits for this version.\nRelease date: 2025-05-29\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.95.1\nView the commits for this version.\nRelease date: 2025-05-27\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.94.1\nView the commits for this version.\nRelease date: 2025-05-27\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.95.0\nView the commits for this version.\nRelease date: 2025-05-26\nThis release contains core updates, editor improvements, node updates, and bug fixes.\n### Evaluations for AI workflows\nWe‚Äôve added a feature to help you iterate, test, and compare changes to your AI automations before pushing them to production so you can achieve more predictability and make better decisions.\nWhen you're building with AI, a small prompt tweak or model swap might improve results with some inputs, while quietly degrading performance with others. But without a way to evaluate performance across many inputs, you‚Äôre left guessing whether your AI is actually getting better when you make a change.\nBy implementing **Evaluations for AI workflows** in n8n, you can assess how your AI performs across a range of inputs by adding a dedicated path in your workflow for running test cases and applying custom metrics to track results. This helps you build viable proof-of-concepts quickly, iterate more effectively, catch regressions early, and make more confident decisions when your AI is in production.\n#### Evaluation node and tab\nThe **Evaluation node** includes several operations that, when used together, enable end-to-end AI evaluation.\n!Evaluation node\nEvaluation node\nUse this node to:\n- Run your AI logic against a wide range of test cases in the same execution\n- Capture the outputs of those test cases\n- Score the results using your own metrics or LLM-as-judge logic\n- Isolate a testing path to only include the nodes and logic you want to evaluate\nThe **Evaluations tab** enables you to review test results in the n8n UI, perfect for comparing runs, spotting regressions, and viewing performance over time.\n#### üõ† How evaluations work\nThe evaluation path runs alongside your normal execution logic and only activates when you want‚Äîmaking it ideal for testing and iteration.\nGet started by selecting an AI workflow you want to evaluate that includes one or more LLM or Agent nodes.\n1. Add an **Evaluation** node with the **On new Evaluation event** operation. This node will act as an additional trigger you‚Äôll run only when testing. Configure it to read your dataset from Google Sheets, with each row representing a test input.\n> üí°  Better datasets mean better evaluations. Craft your dataset from a variety of test cases, including edge cases and typical inputs, to get meaningful feedback on how your AI performs. Learn more and access sample datasets here.\n2. Add a second **Evaluation** node using the **Set Outputs** operation after the part of the workflow you're testing‚Äîtypically after an LLM or Agent node. This captures the response and writes it back to your dataset in Google Sheets.\n3. To evaluate output quality, add a third **Evaluation** node with the **Set Metrics** operation at a point after you‚Äôve generated the outputs. You can develop workflow logic, custom calculations, or add an LLM-as-Judge to score the outputs. Map these metrics to your dataset in the node‚Äôs parameters.\n> üí° Well-defined metrics = smarter decisions. Scoring your outputs based on similarity, correctness, or categorization can help you track whether changes are actually improving performance. Learn more and get links to example templates here.\n!Evaluation workflow\nEvaluation workflow\nWhen the Evaluation trigger node is executed, it runs each input in our dataset through your AI logic. This continues until all test cases are processed, a limit is reached, or you manually stop the execution. Once your evaluation path is set up, you can update your prompt, model, or workflow logic‚Äîand re-run the Evaluation trigger node to compare results. If you‚Äôve added metrics, they‚Äôll appear in the Evaluations tab.\nIn some instances, you may want to isolate your testing path to make iteration faster or to avoid executing downstream logic.  In this case, you can add an Evaluation node with the Check If Evaluating operation to ensure only the expected nodes run when performing evaluations.\n#### Things to keep in mind\nEvaluations for AI Workflows are designed to fit  into your development flow, with more enhancements on the way. For now, here are a few things to note:\n- Test datasets are currently managed through Google Sheets. You‚Äôll need a Google Sheets credential to run evaluations.\n- Each workflow supports one evaluation at a time. If you‚Äôd like to test multiple segments, consider splitting them into sub-workflows for more flexibility.\n- Community Edition supports one single evaluation. Pro and Enterprise plans allow unlimited evaluations.\n- AI Evaluations are not enabled for instances in scaling mode at this time.\nYou can find details, tips, and common troubleshooting info here.\nüëâ Learn more about the AI evaluation strategies and practical implementation techniques. Watch now.\nContributors\nPhiph\ncesars-gh\nFor full release details, refer to Releases on GitHub.\nn8n@1.94.0\nView the commits for this version.\nRelease date: 2025-05-19\nThis release contains editor improvements, an API update, node updates, new nodes, and bug fixes.\n### Verified community nodes on Cloud\nWe‚Äôve expanded the n8n ecosystem and unlocked a new level of flexibility for all users including those on n8n Cloud! Now you can access a select set of community nodes and partner integrations without leaving the canvas. This means you install and automate with a wider range of integrations without leaving your workspace. The power of the community is now built-in.\nThis update focuses on three major improvements:\n- **Cloud availability**: Community nodes are no longer just for self-hosted users. A select set of nodes is now available on n8n Cloud.\n- **Built-in discovery**: You can find and explore these nodes right from the Nodes panel without leaving the editor or searching on npm.\n- **Trust and verification**: Nodes that appear in the editor have been manually vetted for quality and security. These verified nodes are marked with a checkmark.\nWe‚Äôre starting with a selection of around 25 nodes, including some of the most-used community-built packages and partner-supported integrations. For this phase, we focused on nodes that don‚Äôt include external package dependencies - helping streamline the review process and ensure a smooth rollout.\nThis is just the start. We plan to expand the library gradually, bringing even more verified nodes into the editor along with the powerful and creative use cases they unlock. In time, our criteria will evolve, opening the door to a wider range of contributions while keeping quality and security in focus.\nLearn more about this update and find out which nodes are already installable from the editor in our blog post.\nüíª **Use a verified node**\nMake sure you're on **n8n version 1.94.0** or later and the instance Owner has enabled verified community nodes. On Cloud, this can be done from the Admin Panel. For self-hosted instances, please refer to documentation. In both cases, verified nodes are enabled by default.\n- Open the **Nodes panel** from the editor\n- Search for the Node. Verified nodes are indicated by a shield üõ°Ô∏è\n- Select the node and click **Install**\nOnce an Owner installs a node, everyone on the instance can start using it‚Äîjust drag, drop, and connect like any other node in your workflow.\nüõ†Ô∏è **Build a node and get it verified**\nWant your node to be verified and discoverable from the editor? Here‚Äôs how to get involved:\n1. Review the community node verification guidelines.\n2. If you‚Äôre building something new, follow the recommendations for creating nodes.\n3. Check your design against the UX guidelines.\n4. Submit your node to npm.\n5. Request verification by filling out this form.\n**Already built a node? Raise your hand!**\nIf you‚Äôve already published a community node and want it considered for verification, make sure it meets the requirements noted above, then let us know by submitting the interest form. We‚Äôre actively curating the next batch and would love to include your work.\nExtended logs view\nWhen workflows get complex, debugging can get... clicky. That‚Äôs where an extended Logs View comes in. Now you can get a clearer path to trace executions, troubleshoot issues, and understand the behavior of a complete workflow ‚Äî without bouncing between node detail views.\nThis update brings a unified, always-accessible panel to the bottom of the canvas, showing you each step of the execution as it happens. Whether you're working with loops, sub-workflows, or AI agents, you‚Äôll see a structured view of everything that ran, in the order it ran‚Äîwith input, output, and status info right where you need it.\nYou can jump into node details when you want to dig deeper, or follow a single item through every step it touched. Real-time highlighting shows you which nodes are currently running or have failed, and you‚Äôll see total execution time for any workflow‚Äîplus token usage for AI workflows to help monitor performance. And if you're debugging across multiple screens? Just pop the logs out and drag them wherever you‚Äôd like.\n‚öôÔ∏èWhat it does\nAdds a Logs view to the bottom of the canvas that can be opened or collapsed. (Chat also appears here if your workflow uses it).\nDisplays a hierarchical list of nodes in the order they were executed‚Äîincluding expanded views of sub-workflows.\nAllows you to click a node in hierarchy to preview inputs and outputs directly, or jump into the full Node Details view with a link.\nProvides ability to toggle input and output data on and off.\nHighlights each node live as it runs, showing when it starts, completes, or fails.\nIncludes execution history view to explore past execution data in a similar way.\nShows roll-up stats like total execution time and total AI tokens used (for AI-enabled workflows).\nIncludes a  ‚Äúpop out‚Äù button to open the logs as a floating window‚Äîperfect for dragging to another screen while debugging.\nüõ†Ô∏èHow to\nTo access the expanded logs view, click on the Logs bar at the bottom of the canvas. The view is also opens up when you open the chat window on the bottom of the page.\nContributors\nStamsy\nfeelgood-interface\nFor full release details, refer to Releases on GitHub.\nn8n@1.93.0\nView the commits for this version.\nRelease date: 2025-05-12\nThis release contains core updates, editor improvements, new nodes, node updates, and bug fixes.\nFaster ways to open sub-workflows\nWe‚Äôve added several new ways to navigate your multi-workflow automations faster.\nFrom any workflow with a sub-workflow node:\nüñ±Ô∏è Right-click on a sub-workflow node and select Open sub-workflow from the context menu\n‚å®Ô∏è Keyboard shortcuts\nWindows: CTRL + SHIFT + O or CTRL + Double Click\nMac: CMD + SHIFT + O or CMD + Double Click\nThese options will bring your sub-workflow up in a new tab.\nArchive workflows\nIf you‚Äôve ever accidentally removed a workflow, you‚Äôll appreciate the new archiving feature. Instead of permanently deleting workflows with the Remove action, workflows are now archived by default. This allows you to recover them if needed.\nHow to:\nArchive a workflow - Select Archive from the Editor UI menu. It has replaced the Remove action.\nFind archived workflows - Archived workflows are hidden by default. To find your archived workflows, select the option for Show archived workflows in the workflow filter menu.\nPermanently delete a workflow - Once a workflow is archived, you can Delete it from the  options menu.\nRecover a workflow - Select Unarchive from the options menu.\nKeep in mind:\nWorkflows archival requires the same permissions as required previously for removal.\nYou cannot select archived workflows as sub-workflows to execute\nActive workflows are deactivated when they are archived\nArchived workflows can not be edited\nContributors\nLeaDevelop\nayhandoslu\nvalentina98\nFor full release details, refer to Releases on GitHub.\nn8n@1.92.2\nView the commits for this version.\nRelease date: 2025-05-08\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.91.3\nView the commits for this version.\nRelease date: 2025-05-08\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.92.1\nView the commits for this version.\nRelease date: 2025-05-06\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.92.0\nView the commits for this version.\nRelease date: 2025-05-05\nThis release contains core updates, editor improvements, node updates, and bug fixes.\nPartial Execution for AI Tools\nWe‚Äôve made it easier to build and iterate on AI agents in n8n. You can now run and test specific tools without having to execute the entire agent workflow.\nPartial execution is especially useful when refining or troubleshooting parts of your agent logic. It allows you to test changes incrementally, without triggering full agent runs, reducing unnecessary AI calls, token usage, and downstream activity. This makes iteration faster, more cost-efficient, and more precise when working with complex or multi-step AI workflows.\nPartial execution for AI tools is available now for all tools - making it even easier to build, test, and fine-tune AI agents in n8n.\nHow to:\nTo use this feature you can either:\nClick the Play button on the tool you want to execute directly from the canvas view.\nOpen the tool‚Äôs Node Details View and select \"Execute Step\" to run it from there.\nIf you have previously run the workflow, the input and output will be prefilled with data from the last execution. A pop-up form will open where you can manually fill in the parameters before executing your test.\nExtended logs view\nWhen workflows get complex, debugging can get... clicky. That‚Äôs where an extended Logs View comes in. Now you can get a clearer path to trace executions, troubleshoot issues, and understand the behavior of a complete workflow ‚Äî without bouncing between node detail views.\nThis update brings a unified, always-accessible panel to the bottom of the canvas, showing you each step of the execution as it happens. Whether you're working with loops, sub-workflows, or AI agents, you‚Äôll see a structured view of everything that ran, in the order it ran‚Äîwith input, output, and status info right where you need it.\nYou can jump into node details when you want to dig deeper, or follow a single item through every step it touched. Real-time highlighting shows you which nodes are currently running or have failed, and you‚Äôll see total execution time for any workflow‚Äîplus token usage for AI workflows to help monitor performance. And if you're debugging across multiple screens? Just pop the logs out and drag them wherever you‚Äôd like.\n‚öôÔ∏èWhat it does\nAdds a Logs view to the bottom of the canvas that can be opened or collapsed. (Chat also appears here if your workflow uses it).\nDisplays a hierarchical list of nodes in the order they were executed‚Äîincluding expanded views of sub-workflows.\nAllows you to click a node in hierarchy to preview inputs and outputs directly, or jump into the full Node Details view with a link.\nProvides ability to toggle input and output data on and off.\nHighlights each node live as it runs, showing when it starts, completes, or fails.\nIncludes execution history view to explore past execution data in a similar way.\nShows roll-up stats like total execution time and total AI tokens used (for AI-enabled workflows).\nIncludes a  ‚Äúpop out‚Äù button to open the logs as a floating window‚Äîperfect for dragging to another screen while debugging.\nüõ†Ô∏èHow to\nTo access the expanded logs view, click on the Logs bar at the bottom of the canvas. The view is also opens up when you open the chat window on the bottom of the page.\nInsights enhancements for Enterprise\nTwo weeks after the launch of Insights, we‚Äôre releasing some enhancements designed for enterprise users.\nExpanded time ranges. You can now filter insights over a variety of time periods, from the last 24 hours up to 1 year. Pro users are limited to 7 day and 14 day views.\nHourly granularity. Drill down into the last 24 hours of production executions with hourly granularity, making it easier to analyze workflows and quickly identify issues.\nThese updates provide deeper visibility into workflow history, helping you uncover trends over longer periods and detect problems sooner with more precise reporting.\n!Filter insights\nFilter insights\nContributors\nStamsy\nFor full release details, refer to Releases on GitHub.\nn8n@1.91.2\nView the commits for this version.\nRelease date: 2025-05-05\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.90.3\nView the commits for this version.\nRelease date: 2025-05-05\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.91.1\nView the commits for this version.\nRelease date: 2025-05-01\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.91.0\nView the commits for this version.\nRelease date: 2025-04-28\nThis release contains core updates, editor improvements, node updates, and bug fixes.\nBreadcrumb view from the canvas\nWe‚Äôve added breadcrumb navigation directly on the canvas, so you can quickly navigate to any of a workflow‚Äôs parent folders right from the canvas.\nFor full release details, refer to Releases on GitHub.\nn8n@1.90.2\nView the commits for this version.\nRelease date: 2025-04-25\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.90.1\nView the commits for this version.\nRelease date: 2025-04-22\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.90.0\nView the commits for this version.\nRelease date: 2025-04-22\nThis release contains core updates, editor updates, node updates, performance improvements, and bug fixes.\nExtended HTTP Request tool functionality\nWe‚Äôve brought the full power of the HTTP Request node to the HTTP Request tool in AI workflows. That means your AI Agents now have access to all the advanced configuration options‚Äîlike Pagination, Batching, Timeout, Redirects, Proxy support, and even cURL import.\nThis update also includes support for the $fromAI function to dynamically generate the right parameters based on the context of your prompt ‚Äî making API calls smarter, faster, and more flexible than ever.\nHow to:\nOpen your AI Agent node in the canvas.\nClick the ‚Äò+‚Äô icon to add a new tool connection.\nIn the Tools panel, select HTTP Request Tool.\nConfigure it just like you would a regular HTTP Request node ‚Äî including advanced options\nüëâ Learn more about configuring the HTTP Request tool.\nScoped API keys\nUsers on the Enterprise plan can now create API keys with specific scopes to control exactly what each key can access.\n!Scoped API keys\nScoped API keys\nPreviously, API keys had full read/write access across all endpoints. While sometimes necessary, this level of access can be excessive and too powerful for most use cases.  Scoped API keys allow you to limit access to only the resources and actions a service or user actually needs.\nWhat‚Äôs new\nWhen creating a new API key, you can now:\nSelect whether the key has read, write, or both types of access.\nSpecify which resources the key can interact with.\nSupported scopes include:\nVariables ‚Äî list, create, delete\nSecurity audit ‚Äî generate reports\nProjects ‚Äî list, create, update, delete\nExecutions ‚Äî list, read, delete\nCredentials ‚Äî list, create, update, delete, move\nWorkflows ‚Äî list, create, update, delete, move, add/remove tags\nScoped API keys give you more control and security. You can limit access to only what‚Äôs needed, making it safer to work with third parties and easier to manage internal API usage.\nDrag and Drop in Folders\nFolders just got friendlier. With this release, you can now drag and drop workflows and folders ‚Äî making it even easier to keep things tidy.\nNeed to reorganize? Just select a workflow or folder and drag it into another folder or breadcrumb location. It‚Äôs a small change that makes a big difference when managing a growing collection of workflows.\nüìÅ Folders are available to all registered users‚Äîjump in and get your workspace in order!\nContributors\nZordrak\nFor full release details, refer to Releases on GitHub.\nn8n@1.89.2\nView the commits for this version.\nRelease date: 2025-04-16\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.89.1\nView the commits for this version.\nRelease date: 2025-04-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.89.0\nView the commits for this version.\nRelease date: 2025-04-14\nThis release contains API updates, core updates, editor updates, a new node, node updates, and bug fixes.\n### Insights\nWe're rolling out Insights, a new dashboard to monitor how your workflows are performing over time. It's designed to give admins (and owners) better visibility of their most important workflow metrics and help troubleshoot potential issues and improvements.\nIn this first release, we‚Äôre introducing a summary banner, the insights dashboard, and time saved per execution.\n#### 1. Summary banner\nA new banner on the overview page that gives instance admins and owners a birds eye view of key metrics over the last 7 days.\n!Summary banner\nInsights summary banner\nAvailable metrics:\n- Total production executions\n- Total failed executions\n- Failure rate\n- Average runtime of all workflows\n- Estimated time saved\nThis overview is designed to help you stay on top of workflow activity at a glance. It is available for all plans and editions.\n#### 2. Insights dashboard\nOn Pro and Enterprise plans, a new dashboard offers a deeper view into workflow performance and activity.\n!Insights dashboard\nInsights dashboard\nThe dashboard includes:\n- Total production executions over time, including a comparison of successful and failed executions\n- Per-workflow breakdowns of key metrics\n- Comparisons with previous periods to help spot changes in usage or behavior\n- Runtime average and failure rate over time\n#### 3. Time saved per execution\nWithin workflow settings, you can now assign a ‚Äútime saved per execution‚Äù value to any workflow. This makes it possible to track the impact of your workflows and make it easier to share this visually with other teams and stakeholders.\nThis is just the beginning for Insights: the next phase will introduce more advanced filtering and comparisons, custom date ranges, and additional monitoring capabilities.\nNode updates\nWe added a credential check for the Salesforce node\nWe added SearXNG as a tool for AI agents\nYou can now search within subfolders, making it easier to find workflows across all folder levels. Just type in the search bar and go.\nFor full release details, refer to Releases on GitHub.\nn8n@1.88.0\nView the commits for this version.\nRelease date: 2025-04-10\nThis release contains new features, new nodes, performance improvements, and bug fixes.\n### Model Context Protocol (MCP) nodes\nMCP aims to standardise how LLMs like Claude, ChatGPT, or Cursor can interact with tools or integrate data for their agents. Many providers - both established or new - are adopting MCP as a standard way to build agentic systems. It is an easy way to either expose your own app as a server, making capabilities available to a model as tools, or as a client that can call on tools outside of your own system.\nWhile it‚Äôs still early in the development process, we want to give you access to our new MCP nodes. This will help us understand your requirements better and will also let us converge on a great general solution quicker.\nWe are adding two new nodes:\n- a MCP Server Trigger for any workflow\n- a MCP Client Tool for the AI Agent\nThe MCP Server Trigger turns n8n into an MCP server, providing n8n tools to models running outside of n8n. You can run multiple MCP servers from your n8n instance. The MCP Client Tool connects LLMs - and other intelligent agents - to any MCP-enabled service through a single interface.\nMax from our DevRel team created an official walkthrough for you to get started:\n![Studio]()\nStudio Update #04\n### MCP Server Trigger\nThe MCP Server Trigger turns n8n into an MCP server, providing n8n tools to models running outside of n8n. The node acts as an entry point into n8n for MCP clients. It operates by exposing a URL that MCP clients can interact with to access n8n tools. This means your n8n workflows and integrations are now available to models run elsewhere. Pretty neat.\n!MCP Server Trigger\nMCP Server Trigger\nExplore the MCP Server Trigger docs\n### MCP Client Tool\nThe MCP Client Tool node is a MCP client, allowing you to use the tools exposed by an external MCP server. You can connect the MCP Client Tool node to your models to call external tools with n8n agents. In this regard it is similar to using a n8n tool with your AI agent. One advantage is that the MCP Client Tool can access multiple tools on the MCP server at once, keeping your canvas cleaner and easier to understand.\n!MCP Client Tool\nMCP Client Tools\nExplore the MCP Client Tool docs\nNode updates\nAdded a node for Azure Cosmos DB\nAdded a node for Milvus Vector Store\nUpdated the Email Trigger (IMAP) node\nContributors\nadina-hub\numanamente\nFor full release details, refer to Releases on GitHub.\nn8n@1.87.2\nView the commits for this version.\nRelease date: 2025-04-09\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.86.1\nView the commits for this version.\nRelease date: 2025-04-09\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.87.1\nView the commits for this version.\nRelease date: 2025-04-08\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.87.0\nView the commits for this version.\nRelease date: 2025-04-07\nThis release contains new nodes, node updates, API updates, core updates, editor updates, and bug fixes.\nContributors\ncesars-gh\nStamsy\nPash10g\nFor full release details, refer to Releases on GitHub.\nn8n@1.86.0\nView the commits for this version.\nRelease date: 2025-03-31\nThis release contains API updates, core updates, editor improvements, node updates, and bug fixes.\nContributors\nAijeyomah\nownerer\nulevitsky\nFor full release details, refer to Releases on GitHub.\nn8n@1.85.4\nView the commits for this version.\nRelease date: 2025-03-27\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.84.3\nView the commits for this version.\nRelease date: 2025-03-27\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.84.2\nView the commits for this version.\nRelease date: 2025-03-26\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.85.3\nView the commits for this version.\nRelease date: 2025-03-26\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.85.2\nView the commits for this version.\nRelease date: 2025-03-25\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.85.1\nView the commits for this version.\nRelease date: 2025-03-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.85.0\nView the commits for this version.\nRelease date: 2025-03-24\nThis release contains a new node, a new credential, core updates, editor updates, node updates, and bug fixes.\nFolders\nWhat can we say about folders? Well, they‚Äôre super handy for categorizing just about everything and they‚Äôre finally available for your n8n workflows. Tidy up your workspace with unlimited folders and nested folders. Search for workflows within folders. It‚Äôs one of the ways we‚Äôre making it easier to organize your n8n instances more effectively.\nHow to use it:\nCreate and manage folders within your personal space or within projects. You can also create workflows from within a folder. You may need to restart your instance in order to activate folders.\n!Folders\nIt's a folder alright\nFolders are available for all registered users so get started with decluttering your workspace now and look for more features (like drag and drop) to organize your instances soon.\nEnhancements to Form Trigger Node\nRecent updates to the Form Trigger node have made it a more powerful tool for building business solutions. These enhancements provide more flexibility and customization, enabling teams to create visually engaging and highly functional workflows with forms.\nHTML customization: Add custom HTML to forms, including embedded images and videos, for richer user experiences.\nCustom CSS support: Apply custom styles to user-facing components to align forms with your brand‚Äôs look and feel. Adjust fonts, colors, and spacing for a seamless visual identity.\nForm previews: Your form‚Äôs description and title will pull into previews of your form when sharing on social media or messaging apps, providing a more polished look.\nHidden fields: Use query parameters to add hidden fields, allowing you to pass data‚Äîsuch as a referral source‚Äîwithout exposing it to the user.\nNew responses options: Respond to user submissions in multiple ways including text, HTML, or a downloadable file (binary format). This enables forms to display rich webpages or deliver digital assets such as dynamically generated invoices or personalized certificates.\n!Form with custom CSS applied\nForm with custom CSS applied\nThese improvements elevate the Form Trigger node beyond a simple workflow trigger, transforming it into a powerful tool for addressing use cases from data collection and order processing to custom content creation.\nContributors\nFank\nFor full release details, refer to Releases on GitHub.\nn8n@1.84.1\nView the commits for this version.\nRelease date: 2025-03-18\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.84.0\nView the commits for this version.\nRelease date: 2025-03-17\nThis release contains a new node, node updates, editor updates, and bug fixes.\nContributors\nPash10g\nFor full release details, refer to Releases on GitHub.\nn8n@1.83.2\nView the commits for this version.\nRelease date: 2025-03-14\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.82.4\nView the commits for this version.\nRelease date: 2025-03-14\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.82.3\nView the commits for this version.\nRelease date: 2025-03-13\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.83.1\nView the commits for this version.\nRelease date: 2025-03-12\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.83.0\nView the commits for this version.\nRelease date: 2025-03-12\nThis release contains bug fixes and an editor update.\nSchema Preview\nSchema Preview lets you view and work with a node‚Äôs expected output without executing it or adding credentials, keeping you in flow while building.\nSee expected node outputs instantly. View schemas for over 100+ nodes to help you design workflows efficiently without extra steps.\nDefine workflow logic first, take care of credentials later. Build your end-to-end workflow without getting sidetracked by credential setup.\nAvoid unwanted executions when building. Prevent unnecessary API calls, unwanted data changes, or potential third-party service costs by viewing outputs without executing nodes.\nHow to use it:\nAdd a node with Schema Preview support to your workflow.\nOpen the next node in the sequence - Schema Preview data appears in the Node Editor where you would typically find it in the Schema View.\nUse Schema Preview fields just like other schema data - drag and drop them into parameters and settings as needed.\nDon‚Äôt forget to add the required credentials before putting your workflow into production.\nContributors\npemontto\nHaru922\nFor full release details, refer to Releases on GitHub.\nn8n@1.82.2\nView the commits for this version.\nRelease date: 2025-03-12\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.82.1\nView the commits for this version.\nRelease date: 2025-03-04\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.82.0\nView the commits for this version.\nRelease date: 2025-03-03\nThis release contains core updates, editor updates, new nodes, node updates, new credentials, credential updates, and bug fixes.\nTidy up\nTidy up instantly aligns nodes, centers stickies, untangles connections, and brings structure to your workflows. Whether you're preparing to share a workflow or just want to improve readability, this feature saves you time and makes your logic easier to follow. Clean, well-organized workflows aren't just nicer to look at‚Äîthey‚Äôre also quicker to understand.\nHow to:\nOpen the workflow you want to tidy, then choose one of these options:\nClick the Tidy up button in the bottom-left corner of the canvas (it looks like a broom üßπ)\nPress Shift + Alt + T on your keyboard\nRight-click anywhere on the canvas and select Tidy up workflow\nWant to tidy up just part of your workflow? Select the specific nodes you want to clean up first - Tidy up will only adjust those, along with any stickies behind them.\nMultiple API keys\nn8n now supports multiple API keys, allowing users to generate and manage separate keys for different workflows or integrations. This improves security by enabling easier key rotation and isolation of credentials. Future updates will introduce more granular controls.\n!Multiple API keys\nMultiple API keys\nContributors\nRostammahabadi\nLanhild\nmatthiez\nfeelgood-interface\nadina-hub\nFor full release details, refer to Releases on GitHub.\nn8n@1.81.4\nView the commits for this version.\nRelease date: 2025-03-03\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.81.3\nView the commits for this version.\nRelease date: 2025-03-03\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.81.2\nView the commits for this version.\nRelease date: 2025-02-28\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.80.5\nView the commits for this version.\nRelease date: 2025-02-28\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.80.4\nView the commits for this version.\nRelease date: 2025-02-27\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.81.1\nView the commits for this version.\nRelease date: 2025-02-27\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.81.0\nView the commits for this version.\nRelease date: 2025-02-24\nThis release contains bug fixes, a core update, editor improvements, and a node update.\nImproved partial executions\nThe new execution engine for partial executions ensures that testing parts of a workflow in the builder closely mirrors production behaviour. This makes iterating with updated run-data faster and more reliable, particularly for complex workflows.\nBefore, user would test parts of a workflow in the builder that didn't consistently reflect production behaviour, leading to unexpected results during development.\nThis update aligns workflow execution in the builder with production behavior.\nHere is an example for loops:\nBefore\nAfter\nFor full release details, refer to Releases on GitHub.\nn8n@1.80.3\nView the commits for this version.\nRelease date: 2025-02-21\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.79.4\nView the commits for this version.\nRelease date: 2025-02-21\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.80.2\nView the commits for this version.\nRelease date: 2025-02-21\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.79.3\nView the commits for this version.\nRelease date: 2025-02-21\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.80.1\nView the commits for this version.\nRelease date: 2025-02-20\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.79.2\nView the commits for this version.\nRelease date: 2025-02-20\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.80.0\nView the commits for this version.\nRelease date: 2025-02-17\nThis release contains bug fixes and an editor improvement.\nFor full release details, refer to Releases on GitHub.\nn8n@1.75.3\nView the commits for this version.\nRelease date: 2025-02-17\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.74.4\nView the commits for this version.\nRelease date: 2025-02-17\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.79.1\nView the commits for this version.\nRelease date: 2025-02-15\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.78.1\nView the commits for this version.\nRelease date: 2025-02-15\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.77.4\nView the commits for this version.\nRelease date: 2025-02-15\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.76.4\nView the commits for this version.\nRelease date: 2025-02-15\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.79.0\nView the commits for this version.\nRelease date: 2025-02-12\nThis release contains new features, node updates, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.77.3\nView the commits for this version.\nRelease date: 2025-02-06\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.78.0\nView the commits for this version.\nRelease date: 2025-02-05\nThis release contains new features, node updates, and bug fixes.\nContributors\nmocanew\nTimtendo12\nFor full release details, refer to Releases on GitHub.\nn8n@1.77.2\nView the commits for this version.\nRelease date: 2025-02-04\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.76.3\nView the commits for this version.\nRelease date: 2025-02-04\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.77.1\nView the commits for this version.\nRelease date: 2025-02-03\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.76.2\nView the commits for this version.\nRelease date: 2025-02-03\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.77.0\nView the commits for this version.\nRelease date: 2025-01-29\nThis release contains new features, editor updates, new nodes, new credentials, node updates, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.76.1\nView the commits for this version.\nRelease date: 2025-01-23\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.76.0\nView the commits for this version.\nRelease date: 2025-01-22\nThis release contains new features, editor updates, new credentials, node improvements, and bug fixes.\nContributors\nStamsy\nGKdeVries\nFor full release details, refer to Releases on GitHub.\nn8n@1.75.2\nView the commits for this version.\nRelease date: 2025-01-17\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.74.3\nView the commits for this version.\nRelease date: 2025-01-17\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.75.1\nView the commits for this version.\nRelease date: 2025-01-17\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.74.2\nView the commits for this version.\nRelease date: 2025-01-17\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.75.0\nView the commits for this version.\nRelease date: 2025-01-15\nThis release contains bug fixes and editor updates.\nImproved consistency across environments\nWe added new UX and automatic changes improvements resulting in a better consistency between your staging and production instances.\nPreviously, users faced issues like:\nLack of visibility into required credential updates when pulling changes\nIncomplete synchronization, where changes ‚Äî such as deletions ‚Äî weren‚Äôt always applied across environments\nConfusing commit process, making it unclear what was being pushed or pulled\nWe addressed these by:\nClearly indicating required credential updates when pulling changes\nEnsuring deletions and other modifications sync correctly across environments\nImproving commit selection to provide better visibility into what‚Äôs being pushed\n!Commit modal\nCommit modal\n!Pull notification\nPull notification\nFor full release details, refer to Releases on GitHub.\nn8n@1.74.1\nView the commits for this version.\nRelease date: 2025-01-09\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.74.0\nView the commits for this version.\nRelease date: 2025-01-08\nThis release contains new features, a new node, node updates, performance improvements and bug fixes.\n### Overhauled Code node editing experience\nWe added a ton of new helpers to the Code node, making edits of your code much faster and more comfortable. You get:\n- TypeScript autocomplete\n- TypeScript linting\n- TypeScript hover tips\n- Search and replace\n- New keyboard shortcuts based on the VSCode keymap\n- Auto-formatting using prettier (Alt+Shift+F)\n- Remember folded regions and history after refresh\n- Multi cursor\n- Type function in the Code node using JSDoc types\n- Drag and drop for all Code node modes\n- Indentation markers\nWe build this on a web worker architecture so you won't have to suffer from performance degradation while typing.\nTo get the full picture, check out our Studio update with Max and Elias, where they discuss and demo the new editing experience. üëá\n![Studio]()\nStudio Update #04\nNew node: Microsoft Entra ID\nMicrosoft Entra ID (formerly known as Microsoft Azure Active Directory or Azure AD) is used for cloud-based identity and access management. The new node supports a wide range of Microsoft Entra ID features, which includes creating, getting, updating, and deleting users and groups, as well as adding users to and removing them from groups.\nNode updates\nAI Agent: Vector stores can now be directly used as tools for the agent\nCode: Tons of new speed and convenience features, see above for details\nGoogle Vertex Chat: Added option to specify the GCP region for the Google API credentials\nHighLevel: Added support for calendar items\nWe also added a custom projects icon selector on top of the available emojis. Pretty!\nContributors\nigatanasov\nStamsy\nfeelgood-interface\nFor full release details, refer to Releases on GitHub.\nn8n@1.73.1\nView the commits for this version.\nRelease date: 2024-12-19\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.73.0\nView the commits for this version.\nRelease date: 2024-12-19\nThis release contains node updates, performance improvements, and bug fixes.\nNode updates\nAI Agent: Updated descriptions for Chat Trigger options\nFacebook Graph API: Updated for API v21.0\nGmail: Added two new options for the Send and wait operation, free text and custom form\nLinear Trigger: Added support for admin scope\nMailerLite: Now supports the new API\nSlack:  Added two new options for the Send and wait operation, free text and custom form\nWe also added credential support for SolarWinds IPAM and SolarWinds Observability.\nLast, but not least, we improved the schema view performance in the node details view by 90% and added drag and drop re-ordering to parameters. This comes in very handy in the If or Edit Fields nodes.\nContributors\nCodeShakingSheep\nmickaelandrieu\nStamsy\npbdco\nFor full release details, refer to Releases on GitHub.\nn8n@1.72.1\nView the commits for this version.\nRelease date: 2024-12-12\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.71.3\nView the commits for this version.\nRelease date: 2024-12-12\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.72.0\nView the commits for this version.\nRelease date: 2024-12-11\nThis release contains node updates, usability improvements, and bug fixes.\nNode updates\nAI Transform: The maximum context length error now retries with reduced payload size\nRedis: Added support for continue on fail\nImproved commit modal\nWe added filters and text search to the commit modal when working with Environments. This will make committing easier as we provide more information and better visibility. Environments are available on the Enterprise plan.\nFor full release details, refer to Releases on GitHub.\nn8n@1.71.2\nView the commits for this version.\nRelease date: 2024-12-10\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.70.4\nView the commits for this version.\nRelease date: 2024-12-10\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.71.1\nView the commits for this version.\nRelease date: 2024-12-06\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.70.3\nView the commits for this version.\nRelease date: 2024-12-05\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.71.0\nView the commits for this version.\nRelease date: 2024-12-04\nThis release contains node updates, performance improvements, and bug fixes.\n### Task runners for the Code node in public beta\nWe're introducing a significant performance upgrade to the Code node with our new Task runner system. This enhancement moves JavaScript code execution to a separate process, improving your workflow execution speed while adding better isolation.\n!Task runners overview\nTask runners overview\nOur benchmarks show up to 6x improvement in workflow executions using Code nodes - from approximately 6 to 35 executions per second. All these improvements happen under the hood, keeping your Code node experience exactly the same.\nThe Task runner comes in two modes:\n- Internal mode (default): Perfect for getting started, automatically managing task runners as child processes\n- External mode: For advanced hosting scenarios requiring maximum isolation and security\nCurrently, this feature is opt-in and can be enabled using environment variables. Once stable, it will become the default execution method for Code nodes.\nTo start using Task runners today, check out the docs.\nNode updates\nAI Transform node: We improved the prompt for code generation to transform data\nCode node: We added a warning if pairedItem is absent or could not be auto mapped\nFor full release details, refer to Releases on GitHub.\nn8n@1.70.2\nView the commits for this version.\nRelease date: 2024-12-04\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.70.1\nView the commits for this version.\nRelease date: 2024-11-29\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.70.0\nView the commits for this version.\nRelease date: 2024-11-27\nThis release contains node updates, performance improvements and bug fixes.\nNew canvas in beta\nThe new canvas is now the default setting for all users. It should bring significant performance improvements and adds a handy minimap. As it is still a beta version you can still revert to the previous version with the three dot menu.\nWe're looking forward to your feedback. Should you encounter a bug, you will find a handy button to create an issue at the bottom of the new canvas as well.\nNode updates\nWe added credential support for Zabbix to the HTTP request node\nWe added new OAuth2 credentials for Microsoft SharePoint\nThe Slack node now uses markdown for the approval message when using the Send and Wait for Approval operation\nContributors\nfeelgood-interface\nadina-hub\nFor full release details, refer to Releases on GitHub.\nn8n@1.68.1\nView the commits for this version.\nRelease date: 2024-11-26\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.69.2\nView the commits for this version.\nRelease date: 2024-11-26\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.69.1\nView the commits for this version.\nRelease date: 2024-11-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.69.0\nView the commits for this version.\nRelease date: 2024-11-20\nThis release contains a new feature, node improvements and bug fixes.\nSub-workflow debugging\nWe made it much easier to debug sub-workflows by improving their accessibility from the parent workflow.\nFor full release details, refer to Releases on GitHub.\nn8n@1.68.0\nView the commits for this version.\nRelease date: 2024-11-13\nThis release contains node updates, performance improvements and many bug fixes.\n#### New AI agent canvas chat\nWe revamped the chat experience for AI agents on the canvas. A neatly organized view instead of a modal hiding the nodes. You can now see the canvas, chat and logs at the same time when testing your workflow.\nFor full release details, refer to Releases on GitHub.\nn8n@1.67.1\nView the commits for this version.\nRelease date: 2024-11-07\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.67.0\nView the commits for this version.\nRelease date: 2024-11-06\nThis release contains node updates and bug fixes.\nNode updates\nAI Transform: Improved usability\nAnthropic Chat Model Node: Added Haiku 3.5 support\nConvert to File: Added delimiter option for writing to CSV\nGmail Trigger: Added option to filter for draft messages\nIntercom: Credential can now be used in the HTTP Request node\nRapid7 InsightVM: Added credential support\nFor full release details, refer to Releases on GitHub.\nn8n@1.66.0\nView the commits for this version.\nRelease date: 2024-10-31\nThis release contains performance improvements, a node update and bug fixes.\nNode update\nAnthropic Chat Model: Added support for claude-3-5-sonnet-20241022\nWe made updates to how projects and workflow ownership are displayed making them easier to understand and navigate.\nWe further improved the performance logic of partial executions, leading to a smoother and more enjoyable building experience.\nNew n8n canvas alpha\nWe have enabled the alpha version of our new canvas. The canvas is the ‚Äòdrawing board‚Äô of the n8n editor, and we‚Äôre working on a full rewrite. Your feedback and testing will help us improve it.\nRead all about it on our community forum.\nFor full release details, refer to Releases on GitHub.\nn8n@1.65.2\nView the commits for this version.\nRelease date: 2024-10-28\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.64.3\nView the commits for this version.\nRelease date: 2024-10-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.65.1\nView the commits for this version.\nRelease date: 2024-10-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.65.0\nView the commits for this version.\nRelease date: 2024-10-24\nThis release contains a new features, new nodes, node enhancements, and bug fixes.\n### New node: n8n Form\nUse the n8n Form node to create user-facing forms with multiple pages. You can add other nodes with custom logic between to process user input. Start the workflow with a n8n Form Trigger.\n!A multi-page form with branching\nA multi-page form with branching\nAdditionally you can:\n- Set default selections with query parameters\n- Define the form with a JSON array of objects\n- Show a completion screen and redirect to another URL\nNode updates\nNew nodes:\nGoogle Business Profile and Google Business Profile Trigger: Use these to integrate Google Business Profile reviews and posts with your workflows\nEnhanced nodes:\nAI Agent: Removed the requirement to add at least one tool\nGitHub: Added workflows as a resource operation\nStructured Output Parser: Added more user-friendly error messages\nFor additional security, we improved how we handle multi-factor authentication, hardened config file permissions and introduced JWT for the public API.\nFor better performance, we improved how partial executions are handled in loops.\nFor full release details, refer to Releases on GitHub.\nContributors\nIdan Fishman\nn8n@1.64.2\nView the commits for this version.\nRelease date: 2024-10-24\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.64.1\nView the commits for this version.\nRelease date: 2024-10-21\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.64.0\nView the commits for this version.\nRelease date: 2024-10-16\nThis release contains a new node, node enhancements, performance improvements and bug fixes.\n### Enhanced node: Remove Duplicates\nThe Remove Duplicates node got a major makeover with the addition of two new operations:\n- Remove Items Processed in Previous Executions: Compare items in the current input to items from previous executions and remove duplicates\n- Clear Deduplication History: Wipe the memory of items from previous executions.\nThis makes it easier to only process new items from any data source. For example, you can now more easily poll a Google sheet for new entries by id or remove duplicate orders from the same customer by comparing their order date. The great thing is, you can now do this within **and across** workflow runs.\nNew node: Gong\nThe new node for Gong allows you to get users and calls to process them further in n8n. Very useful for sales related workflows.\nFor full release details, refer to Releases on GitHub.\nContributors\nS√∂ren Uhrbach\nn8n@1.63.4\nView the commits for this version.\nRelease date: 2024-10-15\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.62.6\nView the commits for this version.\nRelease date: 2024-10-15\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.63.3\nView the commits for this version.\nRelease date: 2024-10-15\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.63.2\nView the commits for this version.\nRelease date: 2024-10-11\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.62.5\nView the commits for this version.\nRelease date: 2024-10-11\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.63.1\nView the commits for this version.\nRelease date: 2024-10-11\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.62.4\nView the commits for this version.\nRelease date: 2024-10-11\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.63.0\nView the commits for this version.\nRelease date: 2024-10-09\nThis release contains new features, node enhancements and bug fixes.\nNode updates\nOpenAI: Added the option to choose between the default memory connector to provide memory to the assistant or to specify a thread ID\nGmail and Slack: Added custom approval operations to have a human in the loop of a workflow\nWe have also optimized the worker health checks (see breaking change above).\nEach credential now has a seperate url you can link to. This makes sharing much easier.\nFor full release details, refer to Releases on GitHub.\nContributors\nPemontto\nn8n@1.62.3\nView the commits for this version.\nRelease date: 2024-10-08\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.62.2\nView the commits for this version.\nRelease date: 2024-10-07\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.62.1\nView the commits for this version.\nRelease date: 2024-10-02\nThis release contains new features, node enhancements and bug fixes.\n#### Additional nodes as tools\nWe have made additional nodes usable with the Tools AI Agent node.\nAdditionally, we have added a $fromAI() placeholder function to use with tools, allowing you to dynamically pass information from the models to the connected tools. This function works similarly to placeholders used elsewhere in n8n.\nBoth of these new features enable you to build even more powerful AI agents by drawing directly from the apps your business uses. This makes integrating LLMs into your business processes even easier than before.\nNode updates\nGoogle BigQuery: Added option to return numeric values as integers and not strings\nHTTP Request: Added credential support for Sysdig\nInvoice Ninja: Additional query params for getAll requests\nQuestion and Answer Chain: Added the option to use a custom prompt\nDrag and drop insertion on cursor position from schema view is now also enabled for code, SQL and Html fields in nodes.\nCustomers with an enterprise license can now rate, tag and highlight execution data in the executions view. To use highlighting, add an Execution Data Node (or Code node) to the workflow to set custom executions data.\nFor full release details, refer to Releases on GitHub.\nContributors\nBenjamin Roedell\nCodeShakingSheep\nmanuelbcd\nMiguel Prytoluk\nn8n@1.61.0\nView the commits for this version.\nRelease date: 2024-09-25\nThis release contains new features, node enhancements and bug fixes.\nNode updates\nBrandfetch: Updated to use the new API\nSlack: Made adding or removing the workflow link to a message easier\nBig datasets now render faster thanks to virtual scrolling and execution annotations are harder to delete.\nFor full release details, refer to Releases on GitHub.\nn8n@1.59.4\nView the commits for this version.\nRelease date: 2024-09-20\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.60.1\nView the commits for this version.\nRelease date: 2024-09-20\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.60.0\nView the commits for this version.\nRelease date: 2024-09-18\nThis release contains new features, node enhancements and bug fixes.\n#### Queue metrics for workers\nYou can now expose and consume metrics from your workers. The worker instances have the same metrics available as the main instance(s) and can be configured with environment variables.\nYou can now customize the maximum file size when uploading files within forms to webhooks. The environment variable to set for this is N8N_FORMDATA_FILE_SIZE_MAX. The default setting is 200MiB.\nNode updates\nEnhanced nodes:\nInvoice Ninja: Added actions for bank transactions\nOpenAI: Added O1 models to the model select\nFor full release details, refer to Releases on GitHub.\nContributors\nCodeShakingSheep\nn8n@1.59.3\nView the commits for this version.\nRelease date: 2024-09-18\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.59.2\nView the commits for this version.\nRelease date: 2024-09-17\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.59.1\nView the commits for this version.\nRelease date: 2024-09-16\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.58.2\nView the commits for this version.\nRelease date: 2024-09-12\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.59.0\nView the commits for this version.\nRelease date: 2024-09-11\nThis release contains bug fixes and feature enhancements.\nFor full release details, refer to Releases on GitHub.\nContributors\noscarpedrero\nn8n@1.58.1\nView the commits for this version.\nRelease date: 2024-09-06\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.58.0\nView the commits for this version.\nRelease date: 2024-09-05\nThis release contains new features, bug fixes and feature enhancements.\n#### New node: PGVector Vector Store\nThis release adds the PGVector Vector Store node. Use this node to interact with the PGVector tables in your PostgreSQL database. You can insert, get, and retrieve documents from a vector table to provide them to a retriever connected to a chain.\n#### See active collaborators on workflows\nWe added collaborator avatars back to the workflow canvas. You will see other users who are active on the workflow, preventing you from overriding each other's work.\n!Collaboration avatars\nCollaboration avatars\nFor full release details, refer to Releases on GitHub.\nn8n@1.57.0\nView the commits for this version.\nRelease date: 2024-08-28\nThis release contains new features and bug fixes.\n#### Improved execution queue handling\nWe are exposing new execution queue metrics to give users more visibility of the queue length. This helps to inform decisions on horizontal scaling, based on queue status. We have also made querying executions faster.\n#### New credentials for the HTTP Request node\nWe added credential support for Datadog, Dynatrace, Elastic Security, Filescan, Iris, and Malcore to the HTTP Request node making it easier to use existing credentials.\nWe also made it easier to select workflows as tools when working with AI agents by implementing a new workflow selector parameter type.\nFor full release details, refer to Releases on GitHub.\nContributors\nBram Kn\nn8n@1.56.2\nView the commits for this version.\nRelease date: 2024-08-26\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.56.1\nView the commits for this version.\nRelease date: 2024-08-23\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.56.0\nView the commits for this version.\nRelease date: 2024-08-21\nThis release contains node updates, security and bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nCodeShakingSheep\nOz Weiss\nFor full release details, refer to Releases on GitHub.\nn8n@1.55.3\nView the commits for this version.\nRelease date: 2024-08-16\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.55.2\nView the commits for this version.\nRelease date: 2024-08-16\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.55.1\nView the commits for this version.\nRelease date: 2024-08-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.54.4\nView the commits for this version.\nRelease date: 2024-08-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.54.3\nView the commits for this version.\nRelease date: 2024-08-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.54.2\nView the commits for this version.\nRelease date: 2024-08-14\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.55.0\nView the commits for this version.\nRelease date: 2024-08-14\nThis release contains a new feature, a new node, a node update and bug fixes.\n#### Override the npm registry\nThis release adds the option to override the npm registry for installing community packages. This is a paid feature.\nWe now also prevent npm downloading community packages from a compromised npm registry by explicitly using --registry in all npm install commands.\n#### New node: AI Transform\nThis release adds the AI Transform node. Use the AI Transform node to generate code snippets based on your prompt. The AI is context-aware, understanding the workflow‚Äôs nodes and their data types. The node is only available on Cloud plans.\n#### New node: Okta\nThis release adds the Okta node. Use the Okta node to automate work in Okta and integrate Okta with other applications. n8n has built-in support for a wide range of Okta features, which includes creating, updating, and deleting users.\nNode updates\nEnhanced node:\nMySQL\nThis release also adds the new schema view for the expression editor modal.\nFor full release details, refer to Releases on GitHub.\nn8n@1.54.1\nView the commits for this version.\nRelease date: 2024-08-13\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.53.2\nView the commits for this version.\nRelease date: 2024-08-08\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.54.0\nView the commits for this version.\nRelease date: 2024-08-07\nThis release contains new features, node enhancements, bug fixes and updates to our API.\nAPI update\nOur public REST API now supports additional operations:\nCreate, delete, and edit roles for users\nCreate, read, update and delete projects\nFind the details in the API reference.\nContributors\nCodeShakingSheep\nJavier Ferrer Gonz√°lez\nMicka√´l Andrieu\nOz Weiss\nPemontto\nFor full release details, refer to Releases on GitHub.\nn8n@1.45.2\nView the commits for this version.\nRelease date: 2024-08-06\nThis release contains a bug fix.\nFor full release details, refer to Releases on GitHub.\nn8n@1.53.1\nView the commits for this version.\nRelease date: 2024-08-02\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.53.0\nView the commits for this version.\nRelease date: 2024-07-31\nThis release contains new features, new nodes, node enhancements, bug fixes and updates to our API.\n#### Added Google Cloud Platform Secrets Manager support\nThis release adds Google Cloud Platform Secrets Manager to the list of external secret stores. We already support AWS secrets, Azure Key Vault, Infisical and HashiCorp Vault. External secret stores are available under an enterprise license.\n#### New node: Information Extractor\nThis release adds the Information Extractor node. The node is specifically tailored for information extraction tasks. It uses Structured Output Parser under the hood, but provides a simpler way to extract information from text in a structured JSON form.\n#### New node: Sentiment Analysis\nThis release adds the Sentiment Analysis node. The node leverages LLMs to analyze and categorize the sentiment of input text. Users can easily integrate this node into their workflows to perform sentiment analysis on text data. The node is flexible enough to handle various use cases, from basic positive/negative classification to more nuanced sentiment categories.\nNode updates\nEnhanced nodes:\nCalendly Trigger\nHTTP Request\nn8n Form Trigger\nShopify\nAPI update\nOur public REST API now supports additional operations:\nCreate, read, and delete for variables\nFiltering workflows by project\nTransferring workflows\nFind the details in the API reference.\nContributors\nfeelgood-interface\nOz Weiss\nFor full release details, refer to Releases on GitHub.\nn8n@1.52.2\nView the commits for this version.\nRelease date: 2024-07-31\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.52.1\nView the commits for this version.\nRelease date: 2024-07-26\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.51.2\nView the commits for this version.\nRelease date: 2024-07-26\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.52.0\nView the commits for this version.\nRelease date: 2024-07-25\nThis release contains new features, node enhancements and bug fixes.\n#### Added Azure Key Vault support\nThis release adds Azure Key Vault to the list of external secret stores. We already support AWS secrets, Infisical and HashiCorp Vault and are working on Google Secrets Manager. External secret stores are available under an enterprise license.\nNode updates\nEnhanced nodes:\nPinecone Vector Store\nSupabase Vector Store\nSend Email\nDeprecated nodes:\nOpenAI Model: You can use the OpenAI Chat Model instead\nGoogle Palm Chat Model: You can use Google Vertex or Gemini instead\nGoogle Palm Model: You can use Google Vertex or Gemini instead\nn8n@1.51.1\nView the commits for this version.\nRelease date: 2024-07-23\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.50.2\nView the commits for this version.\nRelease date: 2024-07-23\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.51.0\nView the commits for this version.\nRelease date: 2024-07-18\nThis release contains new nodes, node enhancements and bug fixes.\n#### New node: Text Classifier\nThis release adds the Text Classifier node.\n#### New node: Postgres Chat Memory\nThis release adds the Postgres Chat Memory node.\n#### New node: Google Vertex Chat Model\nThis release adds the Google Vertex Chat Model node.\nFor full release details, refer to Releases on GitHub.\nNode updates\nEnhanced nodes: Asana\nn8n@1.50.1\nView the commits for this version.\nRelease date: 2024-07-16\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.50.0\nView the commits for this version.\nRelease date: 2024-07-10\nThis release contains node enhancements and bug fixes.\nNode updates\nEnhanced nodes: Chat Trigger, Google Cloud Firestore, Qdrant Vector Store, Splunk, Telegram\nDeprecated node: Orbit (product shut down)\nBeta Feature Removal\nThe Ask AI beta feature for the HTTP Request node has been removed from this version\nContributors\nStanley Yoshinori Takamatsu\nCodeShakingSheep\njeanpaul\nadrian-martinez-onestic\nMalki Davis\nn8n@1.49.0\nView the commits for this version.\nRelease date: 2024-07-03\nThis release contains a new node, node enhancements, and bug fixes.\nNode updates\nNew node added: Vector Store Tool for the AI Agent\nEnhanced nodes: Zep Cloud Memory, Copper, Embeddings Cohere, GitHub, Merge, Zammad\nFor full release details, refer to Releases on GitHub.\nContributors\nJochem\nKhDu\nNico Weichbrodt\nPavlo Paliychuk\nn8n@1.48.3\nView the commits for this version.\nRelease date: 2024-07-03\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.47.3\nView the commits for this version.\nRelease date: 2024-07-03\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.48.2\nView the commits for this version.\nRelease date: 2024-07-01\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.47.2\nView the commits for this version.\nRelease date: 2024-07-01\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.48.1\nView the commits for this version.\nRelease date: 2024-06-27\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.48.0\nView the commits for this version.\nRelease date: 2024-06-27\nThis release contains bug fixes and feature enhancements.\nFor full release details, refer to Releases on GitHub.\nContributors\nKubeAl\nn8n@1.47.1\nView the commits for this version.\nRelease date: 2024-06-26\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.47.0\nView the commits for this version.\nRelease date: 2024-06-20\nThis release contains bug fixes, feature enhancements, a new node, node enhancements and performance improvements.\nFor full release details, refer to Releases on GitHub.\n#### New node: HTTP request tool\nThis release adds the HTTP request tool. You can use it with an AI agent as a tool to collect information from a website or API. Refer to the HTTP request tool for details.\nContributors\nDaniel\nekadin-mtc\nEric Francis\nJosh Sorenson\nMohammad Alsmadi\nNikolai T. Jensen\nn8n-ninja\npebosi\nTaylor Hoffmann\nn8n@1.45.1\nView the commits for this version.\nRelease date: 2024-06-12\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.46.0\nView the commits for this version.\nRelease date: 2024-06-12\nThis release contains feature enhancements, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nJean Khawand\npemontto\nValentin Coppin\nn8n@1.44.2\nView the commits for this version.\nRelease date: 2024-06-12\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.42.2\nView the commits for this version.\nRelease date: 2024-06-10\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.45.0\nView the commits for this version.\nRelease date: 2024-06-06\nThis release contains new features, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.44.1\nView the commits for this version.\nRelease date: 2024-06-03\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.44.0\nView the commits for this version.\nRelease date: 2024-05-30\nThis release contains new features, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.43.1\nView the commits for this version.\nRelease date: 2024-05-28\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.43.0\nView the commits for this version.\nRelease date: 2024-05-22\nThis release contains new features, node enhancements, and bug fixes.\n#### New feature: Projects\nWith projects and roles, you can give your team access to collections of workflows and credentials, rather than having to share each workflow and credential individually. Simultaneously, you tighten security by limiting access to people on the relevant team.\nRefer to the RBAC documentation for information on creating projects and using roles.\nThe number of projects and role types vary depending on your plan. Refer to Pricing for details.\n#### New node: Slack Trigger\nThis release adds a trigger node for Slack. Refer to the Slack Trigger documentation for details.\nOther highlights\nImproved memory support for OpenAI assistants.\nRolling back to a previous version\nIf you update to this version, then decide you need to role back:\nSelf-hosted n8n:\nDelete any RBAC projects you created.\nRevert the database migrations using n8n db:revert.\nCloud: contact help@n8n.io.\nContributors\nAyato Hayashi\nDaniil Zobov\nGuilherme Barile\nRomain MARTINEAU\nn8n@1.42.1\nView the commits for this version.\nRelease date: 2024-05-20\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.41.1\nView the commits for this version.\nRelease date: 2024-05-16\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.42.0\nView the commits for this version.\nRelease date: 2024-05-15\nThis release contains new features, node enhancements, and bug fixes.\nNote that this release removes the AI error debugger. We're working on a new and improved version.\n#### New feature: Tools Agent\nThis release adds a new option to the Agent node: the Tools Agent.\nThis agent has an enhanced ability to work with tools, and can ensure a standard output format. This is now the recommended default agent.\nFor full release details, refer to Releases on GitHub.\nContributors\nMike Quinlan\nguangwu\nn8n@1.41.0\nView the commits for this version.\nRelease date: 2024-05-08\nThis release contains new features, node enhancements, and bug fixes.\nNote that this release temporarily disables the AI error helper.\nFor full release details, refer to Releases on GitHub.\nContributors\nFlorin Lungu\nn8n@1.40.0\nView the commits for this version.\nRelease date: 2024-05-02\nThis release contains new features, new nodes, node enhancements, and bug fixes.\n#### New feature: Ask AI in the HTTP node\nYou can now ask AI to help create API requests in the HTTP Request node:\n1. In the HTTP Request node, select **Ask AI**.\n1. Enter the **Service** and **Request** you want to use. For example, to use the NASA API to get their picture of the day, enter NASA in **Service** and get picture of the day in **Request**.\n1. Check the parameters: the AI tries to fill them out, but you may still need to adjust or correct the configuration.\nSelf-hosted users need to enable AI features and provide their own API keys\n#### New node: Groq Chat Model\nThis release adds the Groq Chat Model node.\nFor full release details, refer to Releases on GitHub.\nContributors\nAlberto Pasqualetto\nBram Kn\nCodeShakingSheep\nNicolas-nwb\npemontto\npengqiseven\nwebk\nYoshino-s\nn8n@1.39.1\nView the commits for this version.\nRelease date: 2024-04-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.38.2\nView the commits for this version.\nRelease date: 2024-04-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.37.4\nView the commits for this version.\nRelease date: 2024-04-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.39.0\nView the commits for this version.\nRelease date: 2024-04-24\nThis release contains new nodes, node enhancements, and bug fixes.\n#### New node: WhatsApp Trigger\nThis release adds the WhatsApp Trigger node.\n#### Node enhancement: Multiple methods, one Webhook node\nThe Webhook Trigger node can now handle calls to multiple HTTP methods. Refer to the Webhook node documentation for information on enabling this.\nFor full release details, refer to Releases on GitHub.\nContributors\nBram Kn\nn8n@1.38.1\nView the commits for this version.\nRelease date: 2024-04-18\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.37.3\nView the commits for this version.\nRelease date: 2024-04-18\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.38.0\nView the commits for this version.\nRelease date: 2024-04-17\nThis release contains new nodes, bug fixes, and node enhancements.\n#### New node: Google Gemini Chat Model\nThis release adds the Google Gemini Chat Model sub-node.\n#### New node: Embeddings Google Gemini\nThis release adds the Google Gemini Embeddings sub-node.\nFor full release details, refer to Releases on GitHub.\nContributors\nChengyou Liu\nFrancesco Mannino\nn8n@1.37.2\nView the commits for this version.\nRelease date: 2024-04-17\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.36.4\nView the commits for this version.\nRelease date: 2024-04-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.36.3\nView the commits for this version.\nRelease date: 2024-04-12\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.37.1\nView the commits for this version.\nRelease date: 2024-04-11\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.36.2\nView the commits for this version.\nRelease date: 2024-04-11\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.37.0\nView the commits for this version.\nRelease date: 2024-04-10\nThis release contains a new node, improvements to error handling and messaging, node enhancements, and bug fixes.\n#### New node: JWT\nThis release adds the JWT core node.\nFor full release details, refer to Releases on GitHub.\nContributors\nMiguel Prytoluk\nn8n@1.36.1\nView the commits for this version.\nRelease date: 2024-04-04\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.36.0\nView the commits for this version.\nRelease date: 2024-04-03\nThis release contains new nodes, enhancements and bug fixes.\n#### New node: Salesforce Trigger node\nThis release adds the Salesforce Trigger node.\n#### New node: Twilio Trigger node\nThis release adds the Twilio Trigger node.\nFor full release details, refer to Releases on GitHub.\nn8n@1.35.0\nView the commits for this version.\nRelease date: 2024-03-28\nThis release contains enhancements and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.34.2\nView the commits for this version.\nRelease date: 2024-03-26\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.34.1\nView the commits for this version.\nRelease date: 2024-03-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.34.0\nView the commits for this version.\nRelease date: 2024-03-20\nThis release contains new features, new nodes, and bug fixes.\n#### New node: Microsoft OneDrive Trigger node\nThis release adds the Microsoft OneDrive Trigger node. You can now trigger workflows on file and folder creation and update events.\n#### New data transformation functions\nThis release introduces new data transformation functions:\n**String**\n**Number**\n**Object**\n**Array**\n**Date & DateTime**\n**Boolean**\nContributors\nBram Kn\npemontto\nn8n@1.33.1\nView the commits for this version.\nRelease date: 2024-03-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.32.2\nView the commits for this version.\nRelease date: 2024-03-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.33.0\nView the commits for this version.\nRelease date: 2024-03-13\nThis release contains new features, node enhancements, and bug fixes.\n#### Support for Claude 3\nThis release adds support for Claude 3 to the Anthropic Chat Model node.\nFor full release details, refer to Releases on GitHub.\nContributors\ngumida\nAyato Hayashi\nJordan\nMC Naveen\nn8n@1.32.1\nView the commits for this version.\nRelease date: 2024-03-07\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.31.2\nView the commits for this version.\nRelease date: 2024-03-07\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.32.0\nView the commits for this version.\nRelease date: 2024-03-06\nThis release contains new features, node enhancements, performance improvements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.31.1\nView the commits for this version.\nRelease date: 2024-03-06\nThis is a bug fix release and it contains a breaking change.\nFor full release details, refer to Releases on GitHub.\nn8n@1.31.0\nView the commits for this version.\nRelease date: 2024-02-28\nThis release contains new features, new nodes, node enhancements and bug fixes.\n#### New nodes: Microsoft Outlook trigger and Ollama embeddings\nThis release adds two new nodes.\n* Microsoft Outlook Trigger\n* Ollama Embeddings\nFor full release details, refer to Releases on GitHub.\nn8n@1.30.1\nView the commits for this version.\nRelease date: 2024-02-23\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.30.0\nView the commits for this version.\nRelease date: 2024-02-21\nThis release contains new features, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.29.1\nView the commits for this version.\nRelease date: 2024-02-16\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.29.0\nView the commits for this version.\nRelease date: 2024-02-15\nThis release contains new features, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nNew features\n#### OpenAI node overhaul\nThis release includes a new version of the OpenAI node, adding more operations, including support for working with assistants.\nOther highlights:\nSupport for AI events in log streaming.\nAdded support for workflow tags in the public API.\nContributors\nBruno Inec\nJes√∫s Burgers\nn8n@1.27.3\nView the commits for this version.\nRelease date: 2024-02-15\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.28.0\nView the commits for this version.\nRelease date: 2024-02-07\nThis release contains new features, new nodes, node enhancements and bug fixes.\n#### New nodes: Azure OpenAI chat model and embeddings\nThis release adds two new nodes to work with Azure OpenAI in your advanced AI workflows:\n* Embeddings Azure OpenAI\n* Azure OpenAI Chat Model\nFor full release details, refer to Releases on GitHub.\nContributors\nAndrea Ascari\nn8n@1.27.2\nView the commits for this version.\nRelease date: 2024-02-02\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.27.1\nView the commits for this version.\nRelease date: 2024-01-31\nThis release contains new features, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.27.0\nView the commits for this version.\nRelease date: 2024-01-31\nThis release contains node enhancements and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.26.0\nView the commits for this version.\nRelease date: 2024-01-24\nThis release contains new features, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nDaniel Schr√∂der\nNihaal Sangha\nn8n@1.25.1\nView the commits for this version.\nRelease date: 2024-01-22\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nContributors\nNihaal Sangha\nn8n@1.25.0\nView the commits for this version.\nRelease date: 2024-01-17\nThis release contains a new node, feature improvements, and bug fixes.\n#### New node: Chat Memory Manager\nThe Chat Memory Manager node replaces the Chat Messages Retriever node. It manages chat message memories within your AI workflows.\nFor full release details, refer to Releases on GitHub.\nn8n@1.24.1\nView the commits for this version.\nRelease date: 2024-01-16\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.22.6\nView the commits for this version.\nRelease date: 2024-01-10\nThis is a bug fix release. It includes important fixes for the HTTP Request and monday.com nodes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.24.0\nView the commits for this version.\nRelease date: 2024-01-10\nThis release contains new nodes for advanced AI, node enhancements, new features, performance enhancements, and bug fixes.\n#### Chat trigger\nn8n has created a new Chat Trigger node. The new node provides a chat interface that you can make publicly available, with customization and authentication options.\n#### Mistral Cloud Chat and Embeddings\nThis release introduces two new nodes to support Mistral AI:\n* Mistral Cloud Chat Model\n* Embeddings Mistral Cloud\nContributors\nAnush\nEric Koleda\nMason Geloso\nvacitbaydarman\nn8n@1.22.5\nView the commits for this version.\nRelease date: 2024-01-09\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.23.0\nView the commits for this version.\nRelease date: 2024-01-03\nThis release contains new nodes, node enhancements, new features, and bug fixes.\n#### New nodes and improved experience for working with files\nThis release includes a major overhaul of nodes relating to files (binary data).\nThere are now three key nodes dedicated to handling binary data files:\n- Read/Write Files from Disk to read and write files from/to the machine where n8n is running.\n- Convert to File to take input data and output it as a file.\n- Extract From File to get data from a binary format and convert it to JSON.\nn8n has moved support for iCalendar, PDF, and spreadsheet formats into these nodes, and removed the iCalendar, Read PDF, and Spreadsheet File nodes. There are still standalone nodes for HTML and XML.\n#### New node: Qdrant vector store\nThis release adds support for Qdrant with the Qdrant vector store node.\nRead n8n's Qdrant vector store node documentation\nContributors\nAaron Gutierrez\nAdvaith Gundu\nAnush\nBin\nNihaal Sangha\nn8n@1.22.4\nView the commits for this version.\nRelease date: 2024-01-03\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.22.3\nView the commits for this version.\nRelease date: 2023-12-27\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.22.2\nView the commits for this version.\nRelease date: 2023-12-27\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.22.1\nView the commits for this version.\nRelease date: 2023-12-21\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.22.0\nView the commits for this version.\nRelease date: 2023-12-21\nThis release contains node enhancements, new features, performance improvements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.18.4\nView the commits for this version.\nRelease date: 2023-12-19\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.21.1\nView the commits for this version.\nRelease date: 2023-12-15\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.18.3\nView the commits for this version.\nRelease date: 2023-12-15\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.21.0\nView the commits for this version.\nRelease date: 2023-12-13\nThis release contains new features and nodes, node enhancements, and bug fixes.\n#### New user role: Admin\nThis release introduces a third account type: admin. This role is available on pro and enterprise plans. Admins have similar permissions to instance owners.\nRead more about user roles\n#### New data transformation nodes\nThis release replaces the Item Lists node with a collection of nodes for data transformation tasks:\n* Aggregate: take separate items, or portions of them, and group them together into individual items.\n* Limit: remove items beyond a defined maximum number.\n* Remove Duplicates: identify and delete items that are identical across all fields or a subset of fields.\n* Sort: organize lists of in a desired ordering, or generate a random selection.\n* Split Out: separate a single data item containing a list into multiple items.\n* Summarize: aggregate items together, in a manner similar to Excel pivot tables.\n#### Increased sharing permissions for owners and admins\nInstance owners and users with the admin role can now see and share all workflows and credentials. They can't view sensitive credential information.\nFor full release details, refer to Releases on GitHub.\nn8n@1.20.0\nView the commits for this version.\nRelease date: 2023-12-06\nThis release contains bug fixes, node enhancements, and ongoing new feature work.\nFor full release details, refer to Releases on GitHub.\nContributors\nAndrey Starostin\nn8n@1.19.5\nView the commits for this version.\nRelease date: 2023-12-05\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.18.2\nView the commits for this version.\nRelease date: 2023-12-05\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.19.4\nView the commits for this version.\nRelease date: 2023-12-01\nFor full release details, refer to Releases on GitHub.\nn8n@1.19.0\nView the commits for this version.\nRelease date: 2023-11-29\nThis release contains new features, node enhancements, and bug fixes.\n#### LangChain general availability\nThis release adds LangChain support to the main n8n version. Refer to LangChain for more information on how to build AI tools in n8n, the new nodes n8n has introduced, and related learning resources.\n#### Show avatars of users working on the same workflow\nThis release improves the experience of users collaborating on workflows. You can now see who else is editing at the same time as you.\nn8n@1.18.1\nView the commits for this version.\nRelease date: 2023-11-30\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.18.0\nView the commits for this version.\nRelease date: 2023-11-22\nThis release contains new features and bug fixes.\n#### Template creator hub\nBuilt a template you want to share? This release introduces the n8n Creator hub. Refer to the creator hub Notion doc for more information on this project.\n#### Node input and output search filter\nCloud Pro and Enterprise users can now search and filter the input and output data in nodes. Refer to Data filtering for more information.\nFor full release details, refer to Releases on GitHub.\nn8n@1.17.1\nView the commits for this version.\nRelease date: 2023-11-17\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.17.0\nView the commits for this version.\nRelease date: 2023-11-15\nThis release contains node enhancements and bug fixes.\n#### Sticky Note Colors\nYou can now select background colors for sticky notes.\n#### Discord Node Overhaul\nAn overhaul of the Discord node, improving the UI making it easier to configure, improving error handling, and fixing issues.\nFor full release details, refer to Releases on GitHub.\nContributors\nantondollmaier\nteomane\nn8n@1.16.0\nView the commits for this version.\nRelease date: 2023-11-08\nThis release contains node enhancements and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.15.2\nView the commits for this version.\nRelease date: 2023-11-07\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.15.1\nView the commits for this version.\nRelease date: 2023-11-02\nThis release contains new features, node enhancements, and bug fixes.\n#### Workflow history\nThis release introduces workflow history: view and load previous versions of your workflows.\nWorkflow history is available in Enterprise n8n, and with limited history for Cloud Pro.\nLearn more in the Workflow history documentation.\n#### Dark mode\n_Almost_ in time for Halloween: this release introduces dark mode.\nTo enable dark mode:\n1. Select **Settings** > **Personal**.\n1. Under **Personalisation**, change **Theme** to **Dark theme**.\n#### Optional error output for nodes\nAll nodes apart from sub-nodes and trigger nodes have a new optional output: **Error**. Use this to add steps to handle node errors.\n#### Pagination support added to HTTP Request node\nThe HTTP Request node now supports an pagination. Read the node docs for information and examples.\nFor full release details, refer to Releases on GitHub.\nContributors\nYoshino-s\nn8n@1.14.2\nView the commits for this version.\nRelease date: 2023-10-26\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.14.1\nView the commits for this version.\nRelease date: 2023-10-26\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.14.0\nView the commits for this version.\nRelease date: 2023-10-25\nThis release contains node enhancements and bug fixes.\n#### Switch node supports more outputs\nThe Switch node now supports an unlimited number of outputs.\nFor full release details, refer to Releases on GitHub.\nn8n@1.13.0\nView the commits for this version.\nRelease date: 2023-10-25\nThis release contains new features, feature enhancements, and bug fixes.\n#### RSS Feed Trigger node\nThis releases introduces a new node, the RSS Feed Trigger. Use this node to start a workflow when a new RSS feed item is published.\n#### Facebook Lead Ads Trigger node\nThis releases add another new node, the Facebook Lead Ads Trigger. Use this node to trigger a workflow when you get a new lead.\nFor full release details, refer to Releases on GitHub.\nn8n@1.12.2\nView the commits for this version.\nRelease date: 2023-10-24\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nBurak Akg√ºn\nn8n@1.12.1\nView the commits for this version.\nRelease date: 2023-10-23\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nL√©o Martinez\nn8n@1.11.2\nView the commits for this version.\nRelease date: 2023-10-23\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nInga\npemontto\nn8n@1.12.0\nView the commits for this version.\nRelease date: 2023-10-18\nThis release contains new features, node enhancements, and bug fixes.\n#### Form Trigger node\nThis releases introduces a new node, the n8n Form Trigger. Use this node to start a workflow based on a user submitting a form. It provides a configurable form interface.\nFor full release details, refer to Releases on GitHub.\nContributors\nDamian Karzon\nInga\npemontto\nn8n@1.11.1\nView the commits for this version.\nRelease date: 2023-10-13\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.11.0\nView the commits for this version.\nRelease date: 2023-10-11\nThis release contains new features and bug fixes.\n#### External storage for binary files\nSelf-hosted users can now use an external service to store binary data. Learn more in External storage.\nIf you're using n8n Cloud and are interested in this feature, please contact n8n.\n#### Item Lists node supports binary data\nThe Item Lists node now supports splitting and concatenating binary data inputs. This means you no longer need to use code to split a collection of files into multiple items.\nFor full release details, refer to Releases on GitHub.\nn8n@1.10.1\nView the commits for this version.\nRelease date: 2023-10-11\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.9.3\nView the commits for this version.\nRelease date: 2023-10-10\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.9.2\nView the commits for this version.\nRelease date: 2023-10-09\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.10.0\nView the commits for this version.\nRelease date: 2023-10-05\nThis release contains bug fixes and preparatory work for new features.\nFor full release details, refer to Releases on GitHub.\nn8n@1.9.1\nView the commits for this version.\nRelease date: 2023-10-04\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nLangChain in n8n (beta)\nRelease date: 2023-10-04\nThis release introduces support for building with LangChain in n8n.\nWith n8n's LangChain nodes you can build AI-powered functionality within your workflows. The LangChain nodes are configurable, meaning you can choose your preferred agent, LLM, memory, and other components. Alongside the LangChain nodes, you can connect any n8n node as normal: this means you can integrate your LangChain logic with other data sources and services.\nRead more:\nThis is a beta release, and not yet available in the main product. Follow the instructions in Access LangChain in n8n to try it out. Self-hosted and Cloud options are available.\nLearn how LangChain concepts map to n8n nodes in LangChain concepts in n8n.\nBrowse n8n's new Cluster nodes. This is a new set of node types that allows for multiple nodes to work together to configure each other.\nn8n@1.9.0\nView the commits for this version.\nRelease date: 2023-09-28\nThis release contains new features, performance improvements, and bug fixes.\n#### Tournament\nThis releases replaces RiotTmpl, the templating language used in expressions, with n8n's own templating language, Tournament. You can now use arrow functions in expressions.\n#### N8N_BINARY_DATA_TTL and EXECUTIONS_DATA_PRUNE_TIMEOUT removed\nThe environment variables N8N_BINARY_DATA_TTL and EXECUTIONS_DATA_PRUNE_TIMEOUT no longer have any effect and can be removed. Instead of relying on a TTL system for binary data, n8n cleans up binary data together with executions during pruning.\nFor full release details, refer to Releases on GitHub.\nn8n@1.8.2\nView the commits for this version.\nRelease date: 2023-09-25\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.8.1\nView the commits for this version.\nRelease date: 2023-09-21\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.8.0\nView the commits for this version.\nRelease date: 2023-09-20\nThis release contains node enhancements and bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.7.1\nView the commits for this version.\nRelease date: 2023-09-14\nThis release contains bug fixes.\nFor full release details, refer to Releases on GitHub.\nn8n@1.7.0\nView the commits for this version.\nRelease date: 2023-09-13\nThis release contains node enhancements and bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nQuang-Linh LE\nMC Naveen\nn8n@1.6.1\nView the commits for this version.\nRelease date: 2023-09-06\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.6.0\nView the commits for this version.\nRelease date: 2023-09-06\nThis release contains bug fixes, new features, and node enhancements.\n#### TheHive 5\nThis release introduces support for TheHive API version 5. This uses a new node and credentials:\n* TheHive 5 node\n* TheHive 5 Trigger node\n* TheHive 5 credentials\n#### N8N_PERSISTED_BINARY_DATA_TTL removed\nThe environment variables N8N_PERSISTED_BINARY_DATA_TTL no longer has any effect and can be removed. This legacy flag was originally introduced to support ephemeral executions (see details), which are no longer supported.\nFor full release details, refer to Releases on GitHub.\nn8n@1.5.1\nView the commits for this version.\nRelease date: 2023-08-31\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.5.0\nView the commits for this version.\nRelease date: 2023-08-31\nThis release contains new features, node enhancements, and bug fixes.\nHighlights\n#### External secrets storage for credentials\nEnterprise-tier accounts can now use external secrets vaults to manage credentials in n8n. This allows you to store credential information securely outside your n8n instance. n8n supports Infisical and HashiCorp Vault.\nRefer to External secrets for guidance on enabling and using this feature.\n#### Two-factor authentication\nn8n now supports two-factor authentication (2FA) for self-hosted instances. n8n is working on bringing support to Cloud. Refer to Two-factor authentication for guidance on enabling and using it.\n#### Debug executions\nUsers on a paid n8n plan can now load data from previous executions into their current workflow. This is useful when debugging a failed execution.\nRefer to Debug executions for guidance on using this feature.\nFor full release details, refer to Releases on GitHub.\nn8n@1.4.1\nView the commits for this version.\nRelease date: 2023-08-29\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.4.0\nView the commits for this version.\nRelease date: 2023-08-23\nThis release contains new features, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\npemontto\nn8n@1.3.1\nView the commits for this version.\nRelease date: 2023-08-18\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.3.0\nView the commits for this version.\nRelease date: 2023-08-16\nThis release contains new features and bug fixes.\nHighlights\n#### Trial feature: AI support in the Code node\nThis release introduces limited support for using AI to generate code in the Code node. Initially this feature is only available on Cloud, and will gradually be rolled out, starting with about 20% of users.\nLearn how to use the feature, including guidance on writing prompts, in Generate code with ChatGPT.\nFor full release details, refer to Releases on GitHub.\nContributors\nIan Gallagher\nXavier Calland\nn8n@1.2.2\nView the commits for this version.\nRelease date: 2023-08-14\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.2.1\nView the commits for this version.\nRelease date: 2023-08-09\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.2.0\nView the commits for this version.\nRelease date: 2023-08-09\nThis release contains new features, node enhancements, bug fixes, and performance improvements.\nHighlights\n#### Credential support for SecOps services\nThis release introduces support for setting up credentials in n8n for the following services:\n* AlienVault\n* Auth0 Management\n* Carbon Black API\n* Cisco Meraki API\n* Cisco Secure Endpoint\n* Cisco Umbrella API\n* CrowdStrike\n* F5 Big-IP\n* Fortinet FortiGate\n* Hybrid Analysis\n* Imperva WAF\n* Kibana\n* Microsoft Entra ID\n* Mist\n* Okta\n* OpenCTI\n* QRadar\n* Qualys\n* Recorded Future\n* Sekoia\n* Shuffler\n* Trellix ePO\n* VirusTotal\n* Zscaler ZIA\nThis makes it easier to do Custom operations with these services, using the HTTP Request node.\nFor full release details, refer to Releases on GitHub.\nn8n@1.1.1\nView the commits for this version.\nRelease date: 2023-07-27\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.1.0\nView the commits for this version.\nRelease date: 2023-07-26\nThis release contains new features, bug fixes, and node enhancements.\nHighlights\n#### Source control and environments\nThis release introduces source control and environments for enterprise users.\nn8n uses Git-based source control to support environments. Linking your n8n instances to a Git repository lets you create multiple n8n environments, backed by Git branches.\nRefer to Source control and environments to learn more about the features and set up your environments.\nFor full release details, refer to Releases on GitHub.\nContributors\nAdri√°n Mart√≠nez\nAlberto Pasqualetto\nMarten Steketee\nperseus-algol\nSandra Ashipala\nZergRael\nn8n@1.0.5\nView the commits for this version.\nRelease date: 2023-07-24\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@1.0.4\nView the commits for this version.\nRelease date: 2023-07-19\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nContributors\nRomain Dunand\nnoctarius aka Christoph Engelbert\nn8n@1.0.3\nView the commits for this version.\nRelease date: 2023-07-13\nThis release contains API enhancements and adds support for sending messages to forum threads in the Telegram node.\nFor full release details, refer to Releases on GitHub.\nContributors\nKirill\nn8n@1.0.2\nView the commits for this version.\nRelease date: 2023-07-05\nThis is a bug fix release.\nContributors\nRomain Dunand\nn8n@1.0.1\nView the commits for this version.\nRelease date: 2023-07-05\nThis is n8n's version one release.\nFor full details, refer to the n8n v1.0 migration guide.\nHighlights\n#### Python support\nAlthough JavaScript remains the default language, you can now also select Python as an option in the Code node and even make use of many Python modules. Note that Python is unavailable in Code nodes added to a workflow before v1.0.\nContributors\nMarten Steketee"
  },
  {
    "file_path": "sustainable-use-license.md",
    "content": "Sustainable Use License\nn8n's Sustainable Use License and n8n Enterprise License are based on the fair-code model.\nLicense FAQs\nWhat license do you use?\nn8n uses the Sustainable Use License and n8n Enterprise License. These licenses are based on the fair-code model.\nWhat source code is covered by the Sustainable Use License?\nThe Sustainable Use License applies to all our source code hosted in our main GitHub repository except:\nContent of branches other than master.\nSource code files that contain .ee. in their file name. These are licensed under the n8n Enterprise License.\nWhat is the Sustainable Use License?\nThe Sustainable Use License is a fair-code software license created by n8n in 2022. You can read more about why we did this here. The license allows you the free right to use, modify, create derivative works, and redistribute, with three limitations:\nYou may use or modify the software only for your own internal business purposes or for non-commercial or personal use.\nYou may distribute the software or provide it to others only if you do so free of charge for non-commercial purposes.\nYou may not alter, remove, or obscure any licensing, copyright, or other notices of the licensor in the software. Any use of the licensor's trademarks is subject to applicable law.\nWe encourage anyone who wants to use the Sustainable Use License. If you are building something out in the open, it makes sense to think about licensing earlier in order to avoid problems later. Contact us at license@n8n.io if you would like to ask any questions about it.\nWhat is and isn't allowed under the license in the context of n8n's product?\nOur license restricts use to \"internal business purposes\". In practice this means all use is allowed unless you are selling a product, service, or module in which the value derives entirely or substantially from n8n functionality. Here are some examples that wouldn't be allowed:\nWhite-labeling n8n and offering it to your customers for money.\nHosting n8n and charging people money to access it.\nAll of the following examples are allowed under our license:\nUsing n8n to sync the data you control as a company, for example from a CRM to an internal database.\nCreating an n8n node for your product or any other integration between your product and n8n.\nProviding consulting services related to n8n, for example building workflows, custom features closely connect to n8n, or code that gets executed by n8n.\nSupporting n8n, for example by setting it up or maintaining it on an internal company server.\nCan I use n8n to act as the back-end to power a feature in my app?\nUsually yes, as long as the back-end process doesn't use users' own credentials to access their data.\nHere are two examples to clarify:\nExample 1: Sync ACME app with HubSpot\nBob sets up n8n to collect a user's HubSpot credentials to sync data in the ACME app with data in HubSpot.\nNOT ALLOWED under the Sustainable Use License. This use case collects the user's own HubSpot credentials to pull information to feed into the ACME app.\nExample 2: Embed AI chatbot in ACME app\nBob sets up n8n to embed an AI chatbot within the ACME app. The AI chatbot's credentials in n8n use Bob's company credentials. ACME app end-users only enter their questions or queries to the chatbot.\nALLOWED under the Sustainable Use License. No user credentials are being collected.\nWhat if I want to use n8n for something that's not permitted by the license?\nYou must sign a separate commercial agreement with us. We actively encourage software creators to embed n8n within their products; we just ask them to sign an agreement laying out the terms of use, and the fees owed to n8n for using the product in this way. We call this mode of use n8n Embed. You can learn more, and contact us about it here.\nIf you are unsure whether the use case you have in mind constitutes an internal business purpose or not, take a look at the examples, and if you're still unclear, email us at license@n8n.io.\nWhy don't you use an open source license?\nn8n's mission is to give everyone who uses a computer technical superpowers. We've decided the best way for us to achieve this mission is to make n8n as widely and freely available as possible for users, while ensuring we can build a sustainable, viable business. By making our product free to use, easy to distribute, and source-available we help everyone access the product. By operating as a business, we can continue to release features, fix bugs, and provide reliable software at scale long-term.\nWhy did you create a license?\nCreating a license was our least favorite option. We only went down this path after reviewing the possible existing licenses and deciding nothing fit our specific needs. There are two ways in which we try to mitigate the pain and friction of using a proprietary license:\nBy using plain English, and keeping it as short as possible.\nBy promoting fair-code with the goal of making it a well-known umbrella term to describe software models like ours.\nOur goals when we created the Sustainable Use License were:\nTo be as permissive as possible.\nSafeguarding our ability to build a business.\nBeing as clear as possible what use was permitted or not.\nMy company has a policy against using code that restricts commercial use ‚Äì can I still use n8n?\nProvided you are using n8n for internal business purposes, and not making n8n available to your customers for them to connect their accounts and build workflows, you should be able to use n8n. If you are unsure whether the use case you have in mind constitutes an internal business purpose or not, take a look at the examples, and if you're still unclear, email us at license@n8n.io.\nWhat happens to the code I contribute to n8n in light of the Sustainable Use License?\nAny code you contribute on GitHub is subject to GitHub's terms of use. In simple terms, this means you own, and are responsible for, anything you contribute, but that you grant other GitHub users certain rights to use this code. When you contribute code to a repository containing notice of a license, you license the code under the same terms.\nn8n asks every contributor to sign our Contributor License Agreement. In addition to the above, this gives n8n the ability to change its license without seeking additional permission. It also means you aren't liable for your contributions (e.g. in case they cause damage to someone else's business).\nIt's easy to get started contributing code to n8n here, and we've listed broader ways of participating in our community here.\nWhy did you switch to the Sustainable Use License from your previous license arrangement (Apache 2.0 with Commons Clause)?\nn8n was licensed under Apache 2.0 with Commons Clause until 17 March 2022. Commons Clause was initiated by various software companies wanting to protect their rights against cloud providers. The concept involved adding a commercial restriction on top of an existing open source license.\nHowever, the use of the Commons Clause as an additional condition to an open source license, as well as the use of wording that's open to interpretation, created some confusion and uncertainty regarding the terms of use. The Commons Clause also restricted people's ability to offer consulting and support services: we realized these services are critical in enabling people to get value from n8n, so we wanted to remove this restriction.\nWe created the Sustainable Use License to be more permissive and more clear about what use is allowed, while continuing to ensure n8n gets the funding needed to build and improve our product.\nWhat are the main differences between the Sustainable Use License and your previous license arrangement (Apache 2.0 with Commons Clause)?\nThere are two main differences between the Sustainable Use License and our previous license arrangement. The first is that we have tightened the definition of how you can use the software. Previously the Commons Clause restricted users ability to \"sell\" the software; we have redefined this to restrict use to internal business purposes. The second difference is that our previous license restricted people's ability to charge fees for consulting or support services related to the software: we have lifted that restriction altogether.\nThat means you are now free to offer commercial consulting or support services (e.g. building n8n workflows) without the need for a separate license agreement with us. If you are interested in joining our community of n8n experts providing these services, you can learn more here.\nIs n8n open source?\nAlthough n8n's source code is available under the Sustainable Use License, according to the Open Source Initiative (OSI), open source licenses can't include limitations on use, so we do not call ourselves open source. In practice, n8n offers most users many of the same benefits as OSI-approved open source.\nWe coined the term 'fair-code' as a way of describing our licensing model, and the model of other companies who are source-available, but restrict commercial use of their source code.\nWhat is fair-code, and how does the Sustainable Use License relate to it?\nFair-code isn't a software license. It describes a software model where software:\nIs generally free to use and can be distributed by anybody.\nHas its source code openly available.\nCan be extended by anybody in public and private communities.\nIs commercially restricted by its authors.\nThe Sustainable Use License is a fair-code license. You can read more about it and see other examples of fair-code licenses here.\nWe're always excited to talk about software licenses, fair-code, and other principles around sharing code with interested parties. To get in touch to chat, email license@n8n.io.\nCan I use n8n's Sustainable Use License for my own project?\nYes! We're excited to see more software use the Sustainable Use License. We'd love to hear about your project if you're using our license: license@n8n.io."
  },
  {
    "file_path": "video-courses.md",
    "content": "Video courses\nn8n provides two video courses on YouTube.\nFor support, join the Forum.\nBeginner\nThe Beginner course covers the basics of n8n:\nIntroduction and workflows\nAPIs and Webhooks\nNodes\nData in n8n\nCore workflow concepts\nUseful nodes\nError handling\nDebugging\nCollaboration\nAdvanced\nThe Advanced course covers more complex workflows, more technical nodes, and enterprise features:\nIntroduction and complex data flows\nAdvanced technical nodes\nPinning and editing output data\nSub-workflows\nError workflows\nBuilding a full example\nHandling files\nEnterprise features"
  },
  {
    "file_path": "advanced-ai\\index.md",
    "content": "Advanced AI\nBuild AI functionality using n8n: from creating your own chat bot, to using AI to process documents and data from other sources.\n-   __Get started__\nWork through the short tutorial to learn the basics of building AI workflows in n8n.\n:octicons-arrow-right-24: Tutorial\n-   __Use a Starter Kit__\nTry n8n's Self-hosted AI Starter Kit to quickly start building AI workflows.\n:octicons-arrow-right-24: Self-hosted AI Starter Kit\n-   __Explore examples and concepts__\nBrowse examples and workflow templates to help you build. Includes explanations of important AI concepts.\n:octicons-arrow-right-24: Examples\n-   __How n8n uses LangChain__\nLearn more about how n8n builds on LangChain.\n:octicons-arrow-right-24: LangChain in n8n\n-   __Browse AI templates__\nExplore a wide range of AI workflow templates on the n8n website.\n:octicons-arrow-right-24: AI workflows on n8n.io\nRelated resources\nRelated documentation and tools.\nNode types\nThis feature uses Cluster nodes: groups of root and sub nodes that work together.\nWorkflow templates\nYou can browse workflow templates in-app or on the n8n website Workflows page.\nRefer to Templates for information on accessing templates in-app.\nChat trigger\nUse the n8n Chat Trigger to trigger a workflow based on chat interactions.\nChatbot widget\nn8n provides a chatbot widget that you can use as a frontend for AI-powered chat workflows. Refer to the @n8n/chat npm page for usage information."
  },
  {
    "file_path": "advanced-ai\\intro-tutorial.md",
    "content": "Build an AI chat agent with n8n\nWelcome to the introductory tutorial for building AI workflows with n8n. Whether you have used n8n before, or this is your first time, we will show you how the building blocks of AI workflows fit together and construct a working AI-powered chat agent which you can easily customize for your own purposes.\n!\"Screenshot of the completed workflow\"\nMany people find it easier to take in new information in video format. This tutorial is based on one of n8n's popular videos, linked below. Watch the video or read the steps here, or both!\nWhat you will need\nn8n: For this tutorial we recommend using the n8n cloud service - there is a free trial for new users! For a self hosted service, refer to the installation pages.\nCredentials for a chat model: This tutorial uses OpenAI, but you can easily use DeepSeek, Google Gemini, Groq, Azure, and others (see the sub-nodes documentation for more).\nWhat you will learn\nAI concepts in n8n\nHow to use the AI Agent node\nWorking with Chat input\nConnecting with AI models\nCustomising input\nObserving the conversation\nAdding persistence\nAI concepts in n8n\nIf you're already familiar with AI, feel free to skip this section. This is a basic introduction to AI concepts and how they can be used in n8n workflows.\nAn AI agent builds on Large Language Models (LLMs), which generate text based\non input by predicting the next word. While LLMs only process input to produce\noutput, AI agents add goal-oriented functionality. They can use tools, process\ntheir outputs, and make decisions to complete tasks and solve problems.\nIn n8n, the AI agent is represented as a node with some extra connections.\nTABLE_PLACEHOLDER_0\nBy incorporating the AI agent as a node, n8n can combine AI-driven steps with traditional programming for efficient, real-world workflows. For instance, simpler tasks, like validating an email address, do not require AI, whereas a complex tasks, like processing the content of an email or dealing with multimodal inputs (e.g., images, audio), are excellent uses of an AI agent.\n1. Create a new workflow\n2. Add a trigger node\nEvery workflow needs somewhere to start. In n8n these are called 'trigger nodes'. For this workflow, we want to start with a chat node.\nSelect Add first step or press ++tab++ to open the node menu.\nSearch for Chat Trigger. n8n shows a list of nodes that match the search.\nSelect Chat Trigger to add the node to the canvas. n8n opens the node.\nClose the node details view (Select Back to canvas) to return to the canvas.\n??? explanation \"More about the Chat Trigger node...\"\nThe trigger node generates output when there is an event causing it to trigger. In this case we want to be able to type in text to cause the workflow to run. In production, this trigger can be hooked up to a public chat interface as provided by n8n or embedded into another website. To start this simple workflow we will just use the built-in local chat interface to communicate, so no further setup is required.\n3. Add an AI Agent Node\nThe AI Agent node is the core of adding AI to your workflows.\nSelect the Add node !Add node icon{.off-glb} connector on the trigger node to bring up the node search.\nStart typing \"AI\" and choose the AI agent node to add it.\nThe editing view of the AI agent will now be displayed.\nThere are some fields which can be changed. As we're using the Chat Trigger node, the default setting for the source and specification of the prompt don't need to be changed.\n4. Configure the node\nAI agents require a chat model to be attached to process the incoming prompts.\nAdd a chat model by clicking the plus !Add node icon{.off-glb} button underneath the Chat Model connection on the AI Agent node (it's the first connection along the bottom of the node).\nThe search dialog will appear, filtered on 'Language Models'. These are the models with built-in support in n8n. For this tutorial we will use OpenAI Chat Model.\nSelecting the OpenAI Chat model from the list will attach it to the AI Agent node and open the node editor. One of the parameters which can be changed is the 'Model'. Note that for the basic OpenAI accounts, only the 'gpt-4o-mini' model is allowed.\n??? explanation \"Which chat model?\"\nAs mentioned earlier, the LLM is the component which generates the text according to a prompt it is given. LLMs have to be created and trained, usually an intensive process. Different LLMS may have different capabilities or specialties, depending on the data they were trained with.\n5. Add credentials (if needed)\nIn order for n8n to communicate with the chat model, it will need some credentials (login data giving it access to an account on a different online service). If you already have credentials set up for OpenAI, these should appear by default in the credentials selector. Otherwise you can use the Credentials selector to help you add a new credential.\n!image showing the credentials dialog for OpenAI\nTo add a new credential, click on the text which says 'Select credential'. An option to add a new credential will appear\n!Screenshot showing create a new credential button\nThis credential just needs an API key. When adding credentials of any type, check the text to the right-hand side. In this case it has a handy link to take you straight to your OpenAI account to retrieve the API key.\nThe API key is just one long string. That's all you need for this particular credential. Copy it from the OpenAI website and paste it into the API key section.\n??? explanation \"Keeping your credentials safe\"\nCredentials are private pieces of information issued by apps and services to authenticate you as a user and allow you to connect and share information between the app or service and the n8n node. The type of information required varies depending on the app/service concerned. You should be careful about sharing or revealing the credentials outside of n8n.\n6. Test the node\nNow that the node is connected to the Chat Trigger and a chat model, we can test this part of the workflow.\nClick on the 'Chat' button near the bottom of the canvas. This opens up a local chat window on the left and the AI agent logs on the right.\nType in a message and press ++enter++. You will now see the response from the chat model appear below your message.\nThe log window displays the inputs to and outputs from the AI Agent.\n!image showing a chat session in progress\n??? explanation \"Accessing the logs...\"\nYou can access the logs for the AI node even when you aren't using the chat interface. Open up the AI Agent node and click on the Logs tab in the right hand panel.\n!screenshot showing the Logs tab in the AIAgent\n7. Changing the prompt\nThe logs in the previous step reveal some extra data - the system prompt. This is the default message that the AI Agent primes the chat model with. From the log you can see this is set to \"You are a helpful assistant\". We can however change this prompt to alter the behavior of the chat model.\nOpen the AI Agent node. In the bottom of the panel is a section labeled 'Options' and a selector labeled 'Add Option'. Use this to select 'System message'\nThe system message is now displayed. This is the same priming prompt we noticed before in the logs. Change the prompt to something else to prime the chat model in a different way. You could try something like \"You are a brilliant poet who always replies in rhyming couplets\" for example.\nClose the node and return to the chat window. Repeat your message and notice how the output has changed.\n!image showing changed text for chat, now it rhymes; if you can believe that\n8. Adding persistence\nThe chat model is now giving us useful output, but there is something wrong with it which will become apparent when you try to have a conversation.\nUse the chat and tell the chat model your name, for example \"Hi there, my name is Nick\".\nWait for the response, then type the message \"What's my name?\". The AI will not be able to tell you, however apologetic it may seem. The reason for this is we are not saving the context. The AI Agent has no memory.\n!image showing a conversation illustrating the above\nIn order to remember what has happened in the conversation, the AI Agent needs to preserve context. We can do this by adding memory to the AI Agent node. On the canvas click on the !Add node icon{.off-glb} on the bottom of the AI Agent node labeled \"Memory\".\nFrom the panel which appears, select \"Simple Memory\". This will use the memory from the instance running n8n, and is usually sufficient for simple usage. The default value of 5 interactions should be sufficient here, but remember where this option is if you may want to change it later.\nRepeat the exercise of having a conversation above, and see that the AI Agent now remembers your name.\n9. Saving the workflow\nBefore we leave the workflow editor, remember to save the workflow or all your changes will be lost.\nClick on the \"Save\" button in the top right of the editor window. Your workflow will now be saved and you can return to it later to chat again or add new features.\nCongratulations!\nYou have taken your first steps in building useful and effective workflows with AI. In this tutorial we have investigated the basic building blocks of an AI workflow, added an AI Agent and a chat model, and adjusted the prompt to get the kind of output we wanted. We also added memory so the chat could retain context between messages.\nNext steps\nNow you have seen how to create a basic AI workflow, there are plenty of resources to build on that knowledge and plenty of examples to give you ideas of where to go next:\nLearn more about AI concepts and view examples in Examples and concepts.\nBrowse AI Workflow templates.\nFind out how to enhance the AI agent with tools."
  },
  {
    "file_path": "advanced-ai\\rag-in-n8n.md",
    "content": "RAG in n8n\nWhat is RAG\nRetrieval-Augmented Generation (RAG) is a technique that improves AI responses by combining language models with external data sources. Instead of relying solely on the model's internal training data, RAG systems retrieve relevant documents to ground responses in up-to-date, domain-specific, or proprietary knowledge. RAG workflows typically rely on vector stores to manage and search this external data efficiently.\nWhat is a vector store?\nA vector store is a special database designed to store and search high-dimensional vectors: numerical representations of text, images, or other data. When you upload a document, the vector store splits it into chunks and converts each chunk into a vector using an embedding model.\nYou can query these vectors using similarity searches, which construct results based on semantic meaning, rather than keyword matches. This makes vector stores a powerful foundation for RAG and other AI systems that need to retrieve and reason over large sets of knowledge.\nHow to use RAG in n8n\nInserting data into your vector store\nBefore your agent can access custom knowledge, you need to upload that data to a vector store:\nAdd the nodes needed to fetch your source data.\nInsert a Vector Store node (e.g. the Simple Vector Store) and choose the Insert Documents operation.\nSelect an embedding model, which converts your text into vector embeddings. Consult the FAQ for more information on choosing the right embedding model.\nAdd a Default Data Loader node, which splits your content into chunks. You can use the default settings or define your own chunking strategy:\nCharacter Text Splitter: splits by character length.\nRecursive Character Text Splitter: recursively splits by Markdown, HTML, code blocks or simple characters (recommended for most use cases).\nToken Text Splitter: splits by token count.\n(Optional) Add metadata to each chunk to enrich the context and allow better filtering later.\nQuerying your data\nYou can query the data in two main ways: using an agent or directly through a node.\nUsing agents\nAdd an agent to your workflow.\nAdd the vector store as a tool and give it a description to help the agent understand when to use it:\nSet the limit to define how many chunks to return.\nEnable Include Metadata to provide extra context for each chunk.\nAdd the same embedding model you used when inserting the data.\nUsing the node directly\nAdd your vector store node to the canvas and choose the Get Many operation.\nEnter a query or prompt:\nSet a limit for how many chunks to return.\nEnable Include Metadata if needed.\nFAQs\nHow do I choose the right embedding model?\nThe right embedding model differs from case to case.\nIn general, smaller models (for example, text-embedding-ada-002) are faster and cheaper and thus ideal for short, general-purpose documents or lightweight RAG workflows. Larger models (for example, text-embedding-3-large) offer better semantic understanding. These are best for long documents, complex topics, or when accuracy is critical.\nWhat is the best text splitting for my use case?\nThis again depends a lot on your data:\nSmall chunks (for example, 200 to 500 tokens) are good for fine-grained retrieval.\nLarge chunks may carry more context but can become diluted or noisy.\nUsing the right overlap size is important for the AI to understand the context of the chunk. That's also why using the Markdown or Code Block splitting can often help to make chunks better.\nAnother good approach is to add more context to it (for example, about the document where the chunk came from). If you want you can read more about this, you can check out this great article from Anthropic."
  },
  {
    "file_path": "advanced-ai\\evaluations\\light-evaluations.md",
    "content": "Light evaluations\nWhat are light evaluations?\nWhen building your workflow, you often want to test it with a handful of examples to get a sense of how it performs and make improvements. At this stage of workflow development, looking over workflow outputs for each example is often enough. The benefits of setting up more formal scoring or metrics don't yet justify the effort.\nLight evaluation allows you to run the examples in a test dataset through your workflow one-by-one, writing the outputs back to your dataset. You can then examine those outputs next to each other, and visually compare them to the expected outputs (if you have them).\nHow it works\nLight evaluations take place in the 'Editor' tab of your workflow, although you‚Äôll find instructions on how to set it up in the 'Evaluations' tab.\nSteps:\nCreate a dataset\nWire the dataset up to the workflow\nWrite workflow outputs back to dataset\nRun evaluation\nThe following explanation will use a sample workflow that assigns a category and priority to incoming support tickets.\n!Example AI workflow\n1. Create a dataset\nCreate a Google Sheet with a handful of examples for your workflow. Your sheet should contain column(s) for:\nThe workflow input\n(Optional) The expected or correct workflow output\nThe actual output\nLeave the actual output column or columns blank, since you'll be filling them during the evaluation.\n!Sample dataset for a support ticket classification workflow\nA\n!Connecting the evaluation trigger\nThe support ticket classification workflow with the evaluation trigger added in and wired up.\n3. Write workflow outputs back to dataset\nTo populate the output column(s) of your dataset when the evaluation runs:\nInsert the 'Set outputs' action of the evaluation node\nWire it up to your workflow at a point after it has produced the outputs you're evaluating\nIn the node's parameters, map the workflow outputs into the correct dataset column\n!Connecting the set outputs node\nThe support ticket classification workflow with the 'set outputs' node added in and wired up.\n4. Run evaluation\nClick on the Execute workflow button to the left of the evaluation trigger. The workflow will execute multiple times, once for each row of the dataset:\n!Execute workflow button\nReview the outputs of each execution in the Google Sheet, and examine the execution details using the workflow's 'executions' tab if you need to.\nOnce your dataset grows past a handful of examples, consider metric-based evaluation to get a numerical view of performance. See also tips and common issues."
  },
  {
    "file_path": "advanced-ai\\evaluations\\metric-based-evaluations.md",
    "content": "Metric-based evaluations\nWhat are metric-based evaluations?\nOnce your workflow is ready for deployment, you often want to test it on more examples than when you were building it.\nFor example, when production executions start to turn up edge cases, you want to add them to your test dataset so that you can make sure they're covered.\nFor large datasets like the ones built from production data, it can be hard to get a sense of performance just by eyeballing the results. Instead, you must measure performance. Metric-based evaluations can assign one or more scores to each test run, which you can compare to previous runs. Individual scores get rolled up to measure performance on the whole dataset.\nThis feature allows you to run evaluations that calculate metrics, track how those metrics change between runs and drill down into the reasons for those changes.\nMetrics can be deterministic functions (such as the distance between two strings) or you can calculate them using AI. Metrics often involve checking how far away the output is from a reference output (also called ground truth). To do so, the dataset must contain that reference output. Some evaluations don't need this reference output though (for example, checking text for sentiment or toxicity).\nHow it works\nSet up light evaluation\nCalculate metrics\nWrite metrics back to evaluation\nRun evaluation and view results\n1. Set up light evaluation\nFollow the setup instructions to create a dataset and wire it up to your workflow, writing outputs back to the dataset.\nThe following steps use the same support ticket classification workflow from the light evaluation docs:\n!Light evaluation workflow\n2. Calculate metrics\nMetrics are dimensions used to score the output of your workflow. They often compare the actual workflow output with a reference output. It's common to use AI to calculate metrics, although it's sometimes possible to just use code. In n8n, metrics are always numbers.\nYou need to add the logic to calculate the metrics for your workflow, at a point after it has produced the outputs. You can add any reference outputs your metric uses as a column in your dataset. This makes sure they it will be available in the workflow, since they will be output by the evaluation trigger.\nExamples:\nCorrectness: whether the output's meaning is consistent with a reference output.\nCategorization: whether the output exactly matches the expected output.\nHelpfulness: whether the answer addresses the question.\nString similarity: how close the output is to a reference output, measured character-by-character.\nTool calling: whether the agent called the right tool.\nRAG document relevance: when working with a vector database, whether the documents retrieved are relevant to the question.\nRAG answer groundedness: when working with a vector database, whether the answer is \"grounded\" in the documents retrieved.\nCalculating metrics can add latency and cost, so you may only want to do it when running an evaluation and avoid it when making a production execution. You can do this by putting the metric logic after a 'check if evaluating' operation.\n!Check if evaluating node\n3. Write metrics back to evaluation\nn8n needs to know how to extract the metrics you calculated in step 2. Do this by adding an evaluation node with the 'Set metrics' operation and mapping your metrics into it.\n!Add set metrics node\nThis support ticket classification workflow shows the 'Set outputs' operation added and wired up. Since the metrics in this workflow just check whether the actual output is an exact match of the expected one, the workflow calculates them in an expression in the 'Set metrics' node rather than adding any further nodes to the workflow.\n4. Run evaluation and view results\nSwitch to the Evaluations tab on your workflow and click the Run evaluation button. An evaluation will start. Once the evaluation has finished, it will display a summary score for each metric.\nYou can see the results for each test case by clicking on the test run row. Clicking on an individual test case will open the execution that produced it (in a new tab)."
  },
  {
    "file_path": "advanced-ai\\evaluations\\overview.md",
    "content": "Overview\nWhat are evaluations?\nEvaluation is a crucial technique for checking that your AI workflow is reliable. It can be the difference between a flaky proof of concept and a solid production workflow. It's important both in the building phase and after deploying to production.\nThe foundation of evaluation is running a test dataset through your workflow. This dataset contains multiple test cases. Each test case contains a sample input for your workflow, and often includes the expected output(s) too.\nEvaluation allows you to:\nTest your workflow over a range of inputs so you know how it performs on edge cases\nMake changes with confidence without inadvertently making things worse elsewhere\nCompare performance across different models or prompts\nThe following video explains what evaluations are, why they're useful, and how they work:\nWhy is evaluation needed?\nAI models are fundamentally different than code. Code is deterministic and you can reason about it. This is difficult to do with LLMs, since they're black boxes. Instead, you must measure LLM output by running data through them and observing the output.\nYou can only build confidence that your model performs reliably after you have run it over multiple inputs that accurately reflect all the edge cases that it will have to deal with in production.\nTwo types of evaluation\nLight evaluation (pre-deployment)\nBuilding a clean, comprehensive dataset is hard. In the initial building phase, it often makes sense to generate just a handful of examples. These can be enough to iterate the workflow to a releasable state (or a proof of concept). You can visually compare the results to get a sense of the workflow's quality, without setting up formal metrics.\nMetric-based evaluation (post-deployment)\nOnce you deploy your workflow, it's easier to build a bigger, more representative dataset from production executions. When you discover a bug, you can add the input that caused it to the dataset. When fixing the bug, it's important to run the whole dataset over the workflow again as a regression test to check that the fix hasn't inadvertently made something else worse.\nSince there are too many test cases to check individually, evaluations measure the quality of the outputs using a metric, a numeric value representing a particular characteristic. This also allows you to track quality changes between runs.\nComparison of evaluation types\nTABLE_PLACEHOLDER_0\nLearn more\nLight evaluations: Perfect for evaluating your AI workflows against hand-selected test cases during development.\nMetric-based evaluations: Advanced evaluations to maintain performance and correctness in production by using scoring and metrics with large datasets.\nTips and common issues: Learn how to set up specific evaluation use cases and work around common issues."
  },
  {
    "file_path": "advanced-ai\\evaluations\\tips-and-common-issues.md",
    "content": "Tips and common issues\nCombining multiple triggers\nIf you have another trigger in the workflow already, you have two potential starting points: that trigger and the evaluation trigger. To make sure your workflow works as expected no matter which trigger executes, you will need to merge these branches together.\n!Merging trigger branches\nLogic to merge two trigger branches together so that they have the same data format and can be referenced from a single node.\nTo do so:\nGet the data format of the other trigger:\nExecute the other trigger.\nOpen it and navigate to the JSON view of its output pane.\nClick the copy button on the right.\nRe-shape the evaluation trigger data to match:\nInsert an Edit Fields (Set) node after the evaluation trigger and connect them together.\nChange its mode to JSON.\nPaste your data into the 'JSON' field, removing the [ and ] on the first and last lines.\nSwitch the field type to Expression.\nMap in the data from the trigger by dragging it from the input pane.\nFor strings, make sure to replace the entire value (including the quotes) and add .toJsonString() to the end of the expression.\nMerge the branches using a 'No-op' node: Insert a No-op node and wire both the other trigger and the Set node up to it. The 'No-op' node just outputs whatever input it receives.\nReference the 'No-op' node outputs in the rest of the workflow: Since both paths will flow through this node with the same format, you can be sure that your input data will always be there.\nAvoiding evaluation breaking the chat\nn8n's internal chat reads the output data of the last executed node in the workflow. After adding an evaluation node with the 'set outputs' operation, this data may not be in the expected format, or even contain the chat response.\n!Add second output branch\nThe solution is to add an extra branch coming out of your agent. Lower branches execute later in n8n, which means any node you attach to this branch will execute last. You can use a no-op node here since it only needs to pass the agent output through.\nAccessing tool data when calculating metrics\nSometimes you need to know what happened in executed sub-nodes of an agent, for example to check whether it executed a tool. You can't reference these nodes directly with expressions, but you can enable the Return intermediate steps option in the agent. This will add an extra output field called intermediateSteps which you can use in later nodes:\n!Enable return intermediate steps\nMultiple evaluations in the same workflow\nYou can only have one evaluation set up per workflow. In other words, you can only have one evaluation trigger per workflow.\nEven so, you can still test different parts of your workflow with different evaluations by putting those parts in sub-workflows and evaluating each sub-workflow.\nDealing with inconsistent results\nMetrics can often have noise: they may be different across evaluation runs of the exact same workflow. This is because the workflow itself may return different results, or any LLM-based metrics might have natural variation in them.\nYou can compensate for this by duplicating the rows of your dataset, so that each row appears more than once in the dataset. Since this means that each input will effectively be running multiple times, it will smooth out any variations."
  },
  {
    "file_path": "advanced-ai\\examples\\agent-chain-comparison.md",
    "content": "Demonstration of key differences between agents and chains\nIn this workflow you can choose whether your chat query goes to an agent or chain. It shows some of the ways that agents are more powerful than chains.\nKey features\nThis workflow uses:\nChat Trigger: start your workflow and respond to user chat interactions. The node provides a customizable chat interface.\nSwitch node: directs your query to either the agent or chain, depending on which you specify in your query. If you say \"agent\" it sends it to the agent. If you say \"chain\" it sends it to the chain.\nAgent: the Agent node interacts with other components of the workflow and makes decisions about what tools to use.\nBasic LLM Chain: the Basic LLM Chain node supports chatting with a connected LLM, but doesn't support memory or tools.\nUsing the example"
  },
  {
    "file_path": "advanced-ai\\examples\\api-workflow-tool.md",
    "content": "Call an API to fetch data\nUse n8n to bring data from any API to your AI. This workflow uses the Chat Trigger to provide the chat interface, and the Call n8n Workflow Tool to call a second workflow that calls the API. The second workflow uses AI functionality to refine the API request based on the user's query.\nKey features\nThis workflow uses:\nChat Trigger: start your workflow and respond to user chat interactions. The node provides a customizable chat interface.\nAgent: the key piece of the AI workflow. The Agent interacts with other components of the workflow and makes decisions about what tools to use.\nCall n8n Workflow Tool: plug in n8n workflows as custom tools. In AI, a tool is an interface the AI can use to interact with the world (in this case, the data provided by your workflow). The AI model uses the tool to access information beyond its built-in dataset.\nA Basic LLM Chain with an Auto-fixing Output Parser and Structured Output Parser to read the user's query and set parameters for the API call based on the user input.\nUsing the example"
  },
  {
    "file_path": "advanced-ai\\examples\\data-google-sheets.md",
    "content": "Chat with a Google Sheet using AI\nUse n8n to bring your own data to AI. This workflow uses the Chat Trigger to provide the chat interface, and the Call n8n Workflow Tool to call a second workflow that queries Google Sheets.\nKey features\nThis workflow uses:\nChat Trigger: start your workflow and respond to user chat interactions. The node provides a customizable chat interface.\nAgent: the key piece of the AI workflow. The Agent interacts with other components of the workflow and makes decisions about what tools to use.\nCall n8n Workflow Tool: plug in n8n workflows as custom tools. In AI, a tool is an interface the AI can use to interact with the world (in this case, the data provided by your workflow). The AI model uses the tool to access information beyond its built-in dataset.\nUsing the example"
  },
  {
    "file_path": "advanced-ai\\examples\\human-fallback.md",
    "content": "Have a human fallback for AI workflows\nThis is a workflow that tries to answer user queries using the standard GPT-4 model. If it can't answer, it sends a message to Slack to ask for human help. It prompts the user to supply an email address.\nThis workflow uses the Chat Trigger to provide the chat interface, and the Call n8n Workflow Tool to call a second workflow that handles checking for email addresses and sending the Slack message.\nKey features\nThis workflow uses:\nChat Trigger: start your workflow and respond to user chat interactions. The node provides a customizable chat interface.\nAgent: the key piece of the AI workflow. The Agent interacts with other components of the workflow and makes decisions about what tools to use.\nCall n8n Workflow Tool: plug in n8n workflows as custom tools. In AI, a tool is an interface the AI can use to interact with the world (in this case, the data provided by your workflow). It allows the AI model to access information beyond its built-in dataset.\nUsing the example"
  },
  {
    "file_path": "advanced-ai\\examples\\introduction.md",
    "content": "Advanced AI examples and concepts\nThis section provides explanations of important AI concepts, and workflow templates that highlight those concepts, with explanations and configuration guides. The examples cover common use cases and highlight different features of advanced AI in n8n.\n-   __Agents and chains__\nLearn about agents and chains in AI, including exploring key differences using the example workflow.\n:octicons-arrow-right-24: What's a chain in AI?\n:octicons-arrow-right-24: What's an agent in AI?\n:octicons-arrow-right-24: Demonstration of key differences between agents and chains\n-   __Call n8n Workflow Tool__\nLearn about tools in AI, then explore examples that use n8n workflows as custom tools to give your AI workflow access to more data.\n:octicons-arrow-right-24: What's a tool in AI?\n:octicons-arrow-right-24: Chat with Google Sheets\n:octicons-arrow-right-24: Call an API to fetch data\n:octicons-arrow-right-24: Set up a human fallback\n:octicons-arrow-right-24: Let AI specify tool parameters with $fromAI()\n-   __Vector databases__\nLearn about vector databases in AI, along with related concepts including embeddings and retrievers.\n:octicons-arrow-right-24: What's a vector database?\n:octicons-arrow-right-24: Populate a Pinecone vector database from a website\n-   __Memory__\nLearn about memory in AI.\n:octicons-arrow-right-24: What's memory in AI?\n-   __AI workflow templates__\nYou can browse AI templates, included community contributions, on the n8n website.\n:octicons-arrow-right-24: Browse all AI templates"
  },
  {
    "file_path": "advanced-ai\\examples\\understand-agents.md",
    "content": "What's an agent in AI?\nOne way to think of an agent is as a chain that knows how to make decisions. Where a chain follows a predetermined sequence of calls to different AI components, an agent uses a language model to determine which actions to take.\nAgents are the part of AI that act as decision-makers. They can interact with other agents and tools. When you send a query to an agent, it tries to choose the best tools to use to answer. Agents adapt to your specific queries, as well as the prompts that configure their behavior.\nAgents in n8n\nn8n provides one Agent node, which can act as different types of agent depending on the settings you choose. Refer to the Agent node documentation for details on the available agent types.\nWhen execute a workflow containing an agent, the agent runs multiple times. For example, it may do an initial setup, followed by a run to call a tool, then another run to evaluate the tool response and respond to the user."
  },
  {
    "file_path": "advanced-ai\\examples\\understand-chains.md",
    "content": "What's a chain in AI?\nChains bring together different components of AI to create a cohesive system. They set up a sequence of calls between the components. These components can include models and memory (though note that in n8n chains can't use memory).\nChains in n8n\nn8n provides three chain nodes:\nBasic LLM Chain: use to interact with an LLM, without any additional components.\nQuestion and Answer Chain: can connect to a vector store using a retriever, or to an n8n workflow using the Workflow Retriever node. Use this if you want to create a workflow that supports asking questions about specific documents.\nSummarization Chain: takes an input and returns a summary.\nThere's an important difference between chains in n8n and in other tools such as LangChain: none of the chain nodes support memory. This means they can't remember previous user queries. If you use LangChain to code an AI application, you can give your application memory. In n8n, if you need your workflow to support memory, use an agent. This is essential if you want users to be able to have a natural ongoing conversation with your app."
  },
  {
    "file_path": "advanced-ai\\examples\\understand-memory.md",
    "content": "What's memory in AI?\nMemory is a key part of AI chat services. The memory keeps a history of previous messages, allowing for an ongoing conversation with the AI, rather than every interaction starting fresh.\nAI memory in n8n\nTo add memory to your AI workflow you can use either:\nSimple Memory: stores a customizable length of chat history for the current session. This is the easiest to get started with.\nOne of the memory services that n8n provides nodes for. These include:\nMotorhead\nRedis Chat Memory\nPostgres Chat Memory\nXata\nZep\nIf you need to do advanced AI memory management in your workflows, use the Chat Memory Manager node."
  },
  {
    "file_path": "advanced-ai\\examples\\understand-tools.md",
    "content": "What's a tool in AI?\nIn AI, 'tools' has a specific meaning. Tools act like addons that your AI can use to access extra context or resources.\nHere are a couple of other ways of expressing it:\nTools are interfaces that an agent can use to interact with the world (source)\nWe can think of these tools as being almost like functions that your AI model can call (source)\nAI tools in n8n\nn8n provides tool sub-nodes that you can connect to your AI agent. As well as providing some popular tools, such as Wikipedia and SerpAPI, n8n provides three especially powerful tools:\nCall n8n Workflow Tool: use this to load any n8n workflow as a tool.\nCustom Code Tool: write code that your agent can run.\nHTTP Request Tool: make calls to fetch a website or data from an API.\nThe next three examples highlight the Call n8n Workflow Tool:\nChat with Google Sheets\nCall an API to fetch data\nSet up a human fallback\nYou can also learn how to let AI dynamically specify parameters for tools with the $fromAI() function."
  },
  {
    "file_path": "advanced-ai\\examples\\understand-vector-databases.md",
    "content": "What are vector databases?\nVector databases store information as numbers:\nA vector database is a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. (source)\nThis enables fast and accurate similarity searches. With a vector database, instead of using conventional database queries, you can search for relevant data based on semantic and contextual meaning.\nA simplified example\nA vector database could store the sentence \"n8n is a source-available automation tool that you can self-host\", but instead of storing it as text, the vector database stores an array of dimensions (numbers between 0 and 1) that represent its features. This doesn't mean turning each letter in the sentence into a number. Instead, the vectors in the vector database describe the sentence.\nSuppose that in a vector store 0.1 represents automation tool, 0.2 represents source available, and 0.3 represents can be self-hosted. You could end up with the following vectors:\nTABLE_PLACEHOLDER_0\nDemonstrating the power of similarity search\nQdrant provides vector search demos to help users understand the power of vector databases. The food discovery demo shows how a vector store can help match pictures based on visual similarities.\nThis demo uses data from Delivery Service. Users may like or dislike the photo of a dish, and the app will recommend more similar meals based on how they look. It's also possible to choose to view results from the restaurants within the delivery radius. (source)\nFor full technical details, refer to the Qdrant demo-food-discovery GitHub repository.\nEmbeddings, retrievers, text splitters, and document loaders\nVector databases require other tools to function:\nDocument loaders and text splitters: document loaders pull in documents and data, and prepare them for embedding. Document loaders can use text splitters to break documents into chunks.\nEmbeddings: these are the tools that turn the data (text, images, and so on) into vectors, and back into raw data. Note that n8n only supports text embeddings.\nRetrievers: retrievers fetch documents from vector databases. You need to pair them with an embedding to translate the vectors back into data."
  },
  {
    "file_path": "advanced-ai\\examples\\using-the-fromai-function.md",
    "content": "Let AI specify the tool parameters\nWhen configuring tools connected to the Tools Agent, many parameters can be filled in by the AI model itself. The AI model will use the context from the task and information from other connected tools to fill in the appropriate details.\nThere are two ways to do this, and you can switch between them.\nLet the model fill in the parameter\nEach appropriate parameter field in the tool's editing dialog has an extra button at the end:\n!image showing stars icon to the right of parameter field\nOn activating this button, the AI Agent will fill in the expression for you, with no need for any further user input.\nThe field itself is filled in with a message indicating that the parameter has been defined automatically by the model.\nIf you want to define the parameter yourself, click on the 'X' in this box to revert to user-defined values. Note that the 'expression' field will now contain the expression generated by this feature, though you can now edit it further to add extra details as described in the following section.\nUse the $fromAI() function\nThe $fromAI() function uses AI to dynamically fill in parameters for tools connected to the Tools AI agent.\nTo use the $fromAI() function, call it with the required key parameter:\nThe key parameter and other arguments to the $fromAI() function aren't references to existing values. Instead, think of these arguments as hints that the AI model will use to populate the right data.\nFor instance, if you choose a key called email, the AI Model will look for an email address in its context, other tools, and input data. In chat workflows, it may ask the user for an email address if it can't find one elsewhere. You can optionally pass other parameters like description to give extra context to the AI model.\nParameters\nThe $fromAI() function accepts the following parameters:\nTABLE_PLACEHOLDER_0\nExamples\nAs an example, you could use the following $fromAI() expression to dynamically populate a field with a name:\nIf you don't need the optional parameters, you could simplify this as:\nTo dynamically populate the number of items you have in stock, you could use a $fromAI() expression like this:\nIf you only want to fill in parts of a field with a dynamic value from the model, you can use it in a normal expression as well. For example, if you want the model to fill out the subject parameter for an e-mail, but always pre-fix the generated value with the string 'Generated by AI:', you could use the following expression:\nTemplates\nYou can see the $fromAI() function in action in the following templates:\nAngie, Personal AI Assistant with Telegram Voice and Text\nAutomate Customer Support Issue Resolution using AI Text Classifier\nScale Deal Flow with a Pitch Deck AI Vision, Chatbot and QDrant Vector Store"
  },
  {
    "file_path": "advanced-ai\\examples\\vector-store-website.md",
    "content": "Populate a Pinecone vector database from a website\nUse n8n to scrape a website, load the data into Pinecone, then query it using a chat workflow. This workflow uses the HTTP node to get website data, extracts the relevant content using the HTML node, then uses the Pinecone Vector Store node to send it to Pinecone.\nKey features\nThis workflow uses:\nHTTP node: fetches website data.\nHTML node: simplifies the data by extracting the main content from the page.\nPinecone Vector Store node and Embeddings OpenAI: transform the data into vectors and store it in Pinecone.\nChat Trigger and Question and Answer Chain to query the vector database.\nUsing the example"
  },
  {
    "file_path": "advanced-ai\\langchain\\langchain-learning-resources.md",
    "content": "LangChain learning resources\nYou don't need to know details about LangChain to use n8n, but it can be helpful to learn a few concepts. This pages lists some learning resources that people at n8n have found helpful.\nThe LangChain documentation includes introductions to key concepts and possible use cases. Choose the LangChain TABLE_PLACEHOLDER_0\nWhat Product People Need To Know About LangChain provides a list of terminology and concepts, explained with helpful metaphors. Aimed at a wide audience.\nIf you prefer video, this YouTube series by Greg Kamradt works through the LangChain documentation, providing code examples as it goes.\nn8n offers space to discuss LangChain on the Discord. Join to share your projects and discuss ideas with the community."
  },
  {
    "file_path": "advanced-ai\\langchain\\langchain-n8n.md",
    "content": "LangChain concepts in n8n\nThis page explains how LangChain concepts and features map to n8n nodes.\nThis page includes lists of the LangChain-focused nodes in n8n. You can use any n8n node in a workflow where you interact with LangChain, to link LangChain to other services. The LangChain features uses n8n's Cluster nodes.\nTrigger nodes\nChat Trigger\nCluster nodes\nRoot nodes\nEach cluster starts with one root node.\nChains\nA chain is a series of LLMs, and related tools, linked together to support functionality that can't be provided by a single LLM alone.\nAvailable nodes:\nBasic LLM Chain\nRetrieval Q&A Chain\nSummarization Chain\nSentiment Analysis\nText Classifier\nLearn more about chaining in LangChain.\nAgents\nAn agent{ data-preview} has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next. Source\nAvailable nodes:\nAgent\nLearn more about Agents in LangChain.\nVector stores\nVector stores store embedded data, and perform vector searches on it.\nSimple Vector Store\nPGVector Vector Store\nPinecone Vector Store\nQdrant Vector Store\nSupabase Vector Store\nZep Vector Store\nLearn more about Vector stores in LangChain.\nMiscellaneous\nUtility nodes.\nLangChain Code: import LangChain. This means if there is functionality you need that n8n hasn't created a node for, you can still use it.\nSub-nodes\nEach root node can have one or more sub-nodes attached to it.\nDocument loaders\nDocument loaders add data to your chain as documents. The data source can be a file or web service.\nAvailable nodes:\nDefault Document Loader\nGitHub Document Loader\nLearn more about Document loaders in LangChain.\nLanguage models\nLLMs (large language models) are programs that analyze datasets. They're the key element of working with AI.\nAvailable nodes:\nAnthropic Chat Model\nAWS Bedrock Chat Model\nCohere Model\nHugging Face Inference Model\nMistral Cloud Chat Model\nOllama Chat Model\nOllama Model\nOpenAI Chat Model\nLearn more about Language models in LangChain.\nMemory\nMemory retains information about previous queries in a series of queries. For example, when a user interacts with a chat model, it's useful if your application can remember and call on the full conversation, not just the most recent query entered by the user.\nAvailable nodes:\nMotorhead\nRedis Chat Memory\nPostgres Chat Memory\nSimple Memory\nXata\nZep\nLearn more about Memory in LangChain.\nOutput parsers\nOutput parsers take the text generated by an LLM and format it to match the structure you require.\nAvailable nodes:\nAuto-fixing Output Parser\nItem List Output Parser\nStructured Output Parser\nLearn more about Output parsers in LangChain.\nRetrievers\nContextual Compression Retriever\nMultiQuery Retriever\nVector Store Retriever\nWorkflow Retriever\nText splitters\nText splitters break down data (documents), making it easier for the LLM to process the information and return accurate results.\nAvailable nodes:\nCharacter Text Splitter\nRecursive Character Text Splitter\nToken Splitter\nn8n's text splitter nodes implements parts of LangChain's text_splitter API.\nTools\nUtility tools.\nCalculator\nCode Tool\nSerpAPI\nThink Tool\nVector Store Tool\nWikipedia\nWolfram|Alpha\nWorkflow Tool\nEmbeddings\nEmbeddings capture the \"relatedness\" of text, images, video, or other types of information. (source)\nAvailable nodes:\nEmbeddings AWS Bedrock\nEmbeddings Cohere\nEmbeddings Google PaLM\nEmbeddings Hugging Face Inference\nEmbeddings Mistral Cloud\nEmbeddings Ollama\nEmbeddings OpenAI\nLearn more about Text embeddings in LangChain.\nMiscellaneous\nChat Memory Manager"
  },
  {
    "file_path": "advanced-ai\\langchain\\langsmith.md",
    "content": "Use LangSmith with n8n\nLangSmith is a developer platform created by the LangChain team. You can connect your n8n instance to LangSmith to record and monitor runs in n8n, just as you can in a LangChain application.\nConnect your n8n instance to LangSmith\nLog in to LangSmith and get your API key.\nSet the LangSmith environment variables:\nTABLE_PLACEHOLDER_0\nSet the variables so that they're available globally in the environment where you host your n8n instance. You can do this in the same way as the rest of your general configuration. These aren't n8n environment variables, so don't try to set them using the n8n configuration file.\nRestart n8n.\nFor information on using LangSmith, refer to LangSmith's documentation."
  },
  {
    "file_path": "advanced-ai\\langchain\\overview.md",
    "content": "LangChain in n8n\nn8n provides a collection of nodes that implement LangChain's functionality. The LangChain nodes are configurable, meaning you can choose your preferred agent, LLM, memory, and so on. Alongside the LangChain nodes, you can connect any n8n node as normal: this means you can integrate your LangChain logic with other data sources and services.\nLearning resources: n8n's documentation for LangChain assumes you're familiar with AI and LangChain concepts. This page provides links to learning resources.\nLangChain concepts and features in n8n: how n8n represents LangChain concepts and features."
  },
  {
    "file_path": "api\\authentication.md",
    "content": "API authentication\nn8n uses API keys to authenticate API calls.\nAPI Scopes\nUsers of enterprise instances can limit which resources and actions a key can access with scopes. API key scopes allow you specify the exact level of access a key needs for its intended purpose.\nNon-enterprise API keys have full access to all the account's resources and capabilities.\nCreate an API key\nLog in to n8n.\nGo to Settings > n8n API.\nSelect Create an API key.\nChoose a Label and set an Expiration time for the key.\nIf on an enterprise plan, choose the Scopes to give the key.\nCopy My API Key and use this key to authenticate your calls.\nCall the API using your key\nSend the API key in your API call as a header named X-N8N-API-KEY.\nFor example, say you want to get all active workflows. Your curl request will look like this:\nDelete an API key\nLog in to n8n.\nGo to Settings > n8n API.\nSelect Delete next to the key you want to delete.\nConfirm the delete by selecting Delete Forever."
  },
  {
    "file_path": "api\\index.md",
    "content": "n8n public REST API\nUsing n8n's public API, you can programmatically perform many of the same tasks as you can in the GUI. This section introduces n8n's REST API, including:\nHow to authenticate\nPaginating results\nUsing the built-in API playground (self-hosted n8n only)\nEndpoint reference\nn8n provides an n8n API node to access the API in your workflows.\nLearn about REST APIs\nThe API documentation assumes you are familiar with REST APIs. If you're not, these resources may be helpful:\nKnowledgeOwl's guide to working with APIs: a basic introduction, including examples of how to call REST APIs.\nIBM Cloud Learn Hub - What is an Application Programming Interface (API): this gives a general, but technical, introduction to APIs.\nIBM Cloud Learn Hub - What is a REST API?: more detailed information about REST APIs.\nMDN web docs - An overview of HTTP: REST APIs work over HTTP and use HTTP verbs, or methods, to specify the action to perform."
  },
  {
    "file_path": "api\\pagination.md",
    "content": "API pagination\nThe default page size is 100 results. You can change the page size limit. The maximum permitted size is 250.\nWhen a response contains more than one page, it includes a cursor, which you can use to request the next pages.\nFor example, say you want to get all active workflows, 150 at a time.\nGet the first page:\nThe response is in JSON format, and includes a nextCursor value. This is an example response.\nThen to request the next page:"
  },
  {
    "file_path": "api\\using-api-playground.md",
    "content": "Using the API playground\nThe n8n API comes with a built-in Swagger UI playground in self-hosted versions. This provides interactive documentation, where you can try out requests. The path to access the playground depends on your hosting.\nn8n constructs the path from values set in your environment variables:\nThe API version number is 1. There may be multiple versions available in the future.\nThe API includes built-in documentation about credential formats. This is available using the credentials endpoint:"
  },
  {
    "file_path": "code\\ai-code.md",
    "content": "AI coding with GPT\nNot available on self-hosted.\nPython isn't supported.\n///\nUse AI in the Code node\nUsage limits\nDuring the trial phase there are no usage limits. If n8n makes the feature permanent, there may be usage limits as part of your pricing tier.\nFeature limits\nThe ChatGPT implementation in n8n has the following limitations:\nThe AI writes code that manipulates data from the n8n workflow. You can't ask it to pull in data from other sources.\nThe AI doesn't know your data, just the schema, so you need to tell it things like how to find the data you want to extract, or how to check for null.\nNodes before the Code node must execute and deliver data to the Code node before you run your AI query.\nDoesn't work with large incoming data schemas.\nMay have issues if there are a lot of nodes before the code node.\nWriting good prompts\nWriting good prompts increases the chance of getting useful code back.\nSome general tips:\nProvide examples: if possible, give a sample expected output. This helps the AI to better understand the transformation or logic you‚Äôre aiming for.\nDescribe the processing steps: if there are specific processing steps or logic that should apply to the data, list them in sequence. For example: \"First, filter out all users under 18. Then, sort the remaining users by their last name.\"\nAvoid ambiguities: while the AI understands various instructions, being clear and direct ensures you get the most accurate code. Instead of saying \"Get the older users,\" you might say \"Filter users who are 60 years and above.\"\nBe clear about what you expect as the output. Do you want the data transformed, filtered, aggregated, or sorted? Provide as much detail as possible.\nAnd some n8n-specific guidance:\nThink about the input data: make sure ChatGPT knows which pieces of the data you want to access, and what the incoming data represents. You may need to tell ChatGPT about the availability of n8n's built-in methods and variables.\nDeclare interactions between nodes: if your logic involves data from multiple nodes, specify how they should interact. \"Merge the output of 'Node A' with 'Node B' based on the 'userID' property\". if you prefer data to come from certain nodes or to ignore others, be clear: \"Only consider data from the 'Purchases' node and ignore the 'Refunds' node.\"\nEnsure the output is compatible with n8n. Refer to Data structure for more information on the data structure n8n requires.\nExample prompts\nThese examples show a range of possible prompts and tasks.\nExample 1: Find a piece of data inside a second dataset\nTo try the example yourself, download the example workflow and import it into n8n.\nIn the third Code node, enter this prompt:\nThe slack data contains only one item. The input data represents all Notion users. Sometimes the person property that holds the email can be null. I want to find the notionId of the Slack user and return it.\nTake a look at the code the AI generates.\nThis is the JavaScript you need:\nExample 2: Data transformation\nTo try the example yourself, download the example workflow and import it into n8n.\nIn the Join items Code node, enter this prompt:\nReturn a single line of text that has all usernames listed with a comma. Each username should be enquoted with a double quotation mark.\nTake a look at the code the AI generates.\nThis is the JavaScript you need:\nExample 3: Summarize data and create a Slack message\nTo try the example yourself, download the example workflow and import it into n8n.\nIn the Summarize Code node, enter this prompt:\nCreate a markdown text for Slack that counts how many ideas, features and bugs have been submitted. The type of submission is saved in the property_type field. A feature has the property \"Feature\", a bug has the property \"Bug\" and an idea has the property \"Bug\". Also, list the five top submissions by vote in that message. Use \"\" as markdown for links.\nTake a look at the code the AI generates.\nThis is the JavaScript you need:\nReference incoming node data explicitly\nIf your incoming data contains nested fields, using dot notation to reference them can help the AI understand what data you want.\n!\"Screenshot of an n8n code node, highlighting how to reference data with dot notation in an AI query\"\nTo try the example yourself, download the example workflow and import it into n8n.\nIn the second Code node, enter this prompt:\nThe data in \"Mock data\" represents a list of people. For each person, return a new item containing personal_info.first_name and work_info.job_title.\nThis is the JavaScript you need:\nRelated resources\nPluralsight offer a short guide on How to use ChatGPT to write code, which includes example prompts.\nFixing the code\nThe AI-generated code may work without any changes, but you may have to edit it. You need to be aware of n8n's Data structure. You may also find n8n's built-in methods and variables useful."
  },
  {
    "file_path": "code\\expressions.md",
    "content": "Expressions\nExpressions are a powerful feature implemented in all n8n nodes. They allow node parameters to be set dynamically based on data from:\nPrevious node executions\nThe workflow\nYour n8n environment\nYou can also execute JavaScript within an expression, making this a convenient and easy way to manipulate data into useful parameter values without writing extensive extra code.\nn8n created and uses a templating language called Tournament, and extends it with custom methods and variables and data transformation functions. These features make it easier to perform common tasks like getting data from other nodes or accessing workflow metadata.\nn8n additionally supports two libraries:\nLuxon, for working with dates and time.\nJMESPath, for querying JSON.\nWriting expressions\nTo use an expression to set a parameter value:\nHover over the parameter where you want to use an expression.\nSelect Expressions in the Fixed/Expression toggle.\nWrite your expression in the parameter, or select Open expression editor !Open expressions editor icon{.off-glb} to open the expressions editor. If you use the expressions editor, you can browse the available data in the Variable selector. All expressions have the format {{ your expression here }}.\nExample: Get data from webhook body\nConsider the following scenario: you have a webhook trigger that receives data through the webhook body. You want to extract some of that data for use in the workflow.\nYour webhook data looks similar to this:\nIn the next node in the workflow, you want to get just the value of city. You can use the following expression:\nThis expression:\nAccesses the incoming JSON-formatted data using n8n's custom $json variable.\nFinds the value of city (in this example, \"New York\"). Note that this example uses JMESPath syntax to query the JSON data. You can also write this expression as {{$json'body'}}.\nExample: Writing longer JavaScript\nAn expression contains one line of JavaScript. This means you cannot do things like variable assignments or multiple standalone operations.\nTo understand the limitations of JavaScript in expressions, and start thinking about workarounds, look at the following two pieces of code. Both code examples use the Luxon date and time library to find the time between two dates in months, and encloses the code in handlebar brackets, like an expression.\nHowever, the first example isn't a valid n8n expression:\nWhile the second example is valid:\nCommon issues\nFor common errors or issues with expressions and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "code\\index.md",
    "content": "Code in n8n\nn8n is a low-code tool. This means you can do a lot without code, then add code when needed.\nCode in your workflows\nThere are two places in your workflows where you can use code:\n- __Expressions__\nUse expressions to transform data in your nodes. You can use JavaScript in expressions, as well as n8n's Built-in methods and variables and Data transformation functions.\n:octicons-arrow-right-24: Expressions\n- __Code node__\nUse the Code node to add JavaScript or Python to your workflow.\n:octicons-arrow-right-24: Code node\nOther technical resources\nThese are features that are relevant to technical users.\nTechnical nodes\nn8n provides core nodes, which simplify adding key functionality such as API requests, webhooks, scheduling, and file handling.\n- __Write a backend__\nThe HTTP Request, Webhook, and Code nodes help you make API calls, respond to webhooks, and write any JavaScript in your workflow.\nUse this do things like Create an API endpoint.\n:octicons-arrow-right-24: Core nodes\n- __Represent complex logic__\nYou can build complex flows, using nodes like If, Switch, and Merge nodes.\n:octicons-arrow-right-24: Flow logic\nOther developer resources\n- __The n8n API__\nn8n provides an API, where you can programmatically perform many of the same tasks as you can in the GUI. There's an n8n API node to access the API in your workflows.\n:octicons-arrow-right-24: API\n- __Self-host__\nYou can self-host n8n. This keeps your data on your own infrastructure.\n:octicons-arrow-right-24: Hosting\n- __Build your own nodes__\nYou can build custom nodes, install them on your n8n instance, and publish them to npm.\n:octicons-arrow-right-24: Creating nodes"
  },
  {
    "file_path": "code\\variables.md",
    "content": "Custom variables\nCustom variables are read-only variables that you can use to store and reuse values in n8n workflows.\nCreate variables\nTo create a new variable:\nOn the¬†Variables¬†page, select¬†Add Variable.\nEnter a Key and Value. The maximum key length is 50 characters, and the maximum value length is 220 characters. n8n limits the characters you can use in the key and value to lowercase and uppercase letters, numbers, and underscores (A-Z, a-z, 0-9, _).\nSelect Save. The variable is now available for use in all workflows in the n8n instance.\nEdit and delete variables\nTo edit or delete a variable:\nOn the¬†Variables¬†page, hover over the variable you want to change.\nSelect Edit or Delete.\nUse variables in workflows\nYou can access variables in the Code node and in expressions:\nAll variables are strings.\nDuring workflow execution, n8n replaces the variables with the variable value. If the variable has no value, n8n treats its value as undefined. Workflows don't automatically fail in this case.\nVariables are read-only. You must use the UI to change the values. If you need to set and access custom data within your workflow, use Workflow static data."
  },
  {
    "file_path": "code\\builtin\\convenience.md",
    "content": "Convenience methods\nn8n provides these methods to make it easier to perform common tasks in expressions.\n=== \"JavaScript\"\nTABLE_PLACEHOLDER_0 | _ifEmpty(value, defaultValue) | The _ifEmpty() function takes two parameters, tests the first to check if it's empty, then returns either the parameter (if not empty) or the second parameter (if the first is empty). The first parameter is empty if it's:undefinednullAn empty string ''An array where value.length returns falseAn object where Object.keys(value).length returns false | :white_check_mark: |"
  },
  {
    "file_path": "code\\builtin\\current-node-input.md",
    "content": "Current node input\nMethods for working with the input of the current node. Some methods and variables aren't available in the Code node.\n=== \"JavaScript\"\nTABLE_PLACEHOLDER_0 | _input.context.noItemsLeft | Boolean. Only available when working with the Loop Over Items node. Provides information about what's happening in the node. Use this to determine whether the node is still processing items. |"
  },
  {
    "file_path": "code\\builtin\\date-time.md",
    "content": "Built-in date and time methods\nMethods for working with date and time.\n=== \"JavaScript\"\nTABLE_PLACEHOLDER_0\nn8n passes dates between nodes as strings, so you need to parse them. Luxon helps you do this. Refer to Date and time with Luxon for more information.\nn8n provides built-in convenience functions to support data transformation in expressions for dates. Refer to Data transformation functions | Dates for more information."
  },
  {
    "file_path": "code\\builtin\\http-node-variables.md",
    "content": "HTTP node variables\nVariables for working with HTTP node requests and responses when using pagination.\nRefer to HTTP Request for guidance on using the HTTP node, including configuring pagination.\nRefer to HTTP Request node cookbook | Pagination for example pagination configurations."
  },
  {
    "file_path": "code\\builtin\\jmespath.md",
    "content": "JMESPath method\nThis is an n8n-provided method for working with the JMESPath library.\n=== \"JavaScript\"\nTABLE_PLACEHOLDER_0 | _jmespath() | Perform a search on a JSON object using JMESPath. |"
  },
  {
    "file_path": "code\\builtin\\langchain-methods.md",
    "content": "LangChain Code node methods\nn8n provides these methods to make it easier to perform common tasks in the LangChain Code node."
  },
  {
    "file_path": "code\\builtin\\n8n-metadata.md",
    "content": "n8n metadata\nMethods for working with n8n metadata.\nThis includes:\nAccess to n8n environment variables for self-hosted n8n.\nMetadata about workflows, executions, and nodes.\nInformation about instance Variables and External secrets.\n=== \"JavaScript\"\nTABLE_PLACEHOLDER_0 | _workflow.name | The workflow name. |"
  },
  {
    "file_path": "code\\builtin\\output-other-nodes.md",
    "content": "Output of other nodes\nMethods for working with the output of other nodes. Some methods and variables aren't available in the Code node.\n=== \"JavaScript\"\nTABLE_PLACEHOLDER_0 | (\"\").itemMatching(currentNodeInputIndex) | Use instead of (\"\").item in the Code node if you need to trace back from an input item. Refer to Retrieve linked items from earlier in the workflow for an example. | :white_check_mark: |"
  },
  {
    "file_path": "code\\builtin\\overview.md",
    "content": "Built-in methods and variables\nn8n provides built-in methods and variables for working with data and accessing n8n data. This section provides a reference of available methods and variables for use in expressions, with a short description.\nThe Cookbook contains examples for some common tasks, including some Code node only functions."
  },
  {
    "file_path": "code\\builtin\\data-transformation-functions\\arrays.md",
    "content": "Arrays\nA reference document listing built-in convenience functions to support data transformation in expressions for arrays."
  },
  {
    "file_path": "code\\builtin\\data-transformation-functions\\booleans.md",
    "content": "Booleans\nA reference document listing built-in convenience functions to support data transformation in expressions for arrays."
  },
  {
    "file_path": "code\\builtin\\data-transformation-functions\\dates.md",
    "content": "Dates\nA reference document listing built-in convenience functions to support data transformation in expressions for dates."
  },
  {
    "file_path": "code\\builtin\\data-transformation-functions\\index.md",
    "content": "Data transformation functions\nData transformation functions are helper functions to make data transformation easier in expressions.\nFor a list of available functions, refer to the page for your data type:\nArrays\nDates\nNumbers\nObjects\nStrings\nUsage\nData transformation functions are available in the expressions editor.\nThe syntax is:\nFor example, to check if a string is an email:"
  },
  {
    "file_path": "code\\builtin\\data-transformation-functions\\numbers.md",
    "content": "Numbers\nA reference document listing built-in convenience functions to support data transformation in expressions for numbers."
  },
  {
    "file_path": "code\\builtin\\data-transformation-functions\\objects.md",
    "content": "Objects\nA reference document listing built-in convenience functions to support data transformation in expressions for objects."
  },
  {
    "file_path": "code\\builtin\\data-transformation-functions\\strings.md",
    "content": "Strings\nA reference document listing built-in convenience functions to support data transformation in expressions for strings."
  },
  {
    "file_path": "code\\cookbook\\jmespath.md",
    "content": "Query JSON with JMESPath\nJMESPath is a query language for JSON that you can use to extract and transform elements from a JSON document. For full details of how to use JMESPath, refer to the JMESPath documentation.\nThe jmespath() method\nn8n provides a custom method, jmespath(). Use this method to perform a search on a JSON object using the JMESPath query language.\nThe basic syntax is:\n=== \"JavaScript\"\n=== \"Python\"\nTo help understand what the method does, here is the equivalent longer JavaScript:\nobject is a JSON object, such as the output of a previous node. searchString is an expression written in the JMESPath query language. The JMESPath Specification provides a list of supported expressions, while their Tutorial and Examples provide interactive examples.\nCommon tasks\nThis section provides examples for some common operations. More examples, and detailed guidance, are available in JMESPath's own documentation.\nWhen trying out these examples, you need to set the Code node Mode to Run Once for Each Item.\nApply a JMESPath expression to a collection of elements with projections\nFrom the JMESPath projections documentation:\nProjections are one of the key features of JMESPath. Use it to apply an expression to a collection of elements. JMESPath supports five kinds of projections:\nList Projections\nSlice Projections\nObject Projections\nFlatten Projections\nFilter Projections\nThe following example shows basic usage of list, slice, and object projections. Refer to the JMESPath projections documentation for detailed explanations of each projection type, and more examples.\nGiven this JSON from a webhook node:\nRetrieve a list of all the people's first names:\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\n=== \"Code node (Python)\"\nGet a slice of the first names:\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\n=== \"Code node (Python)\"\nGet a list of the dogs' ages using object projections:\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\n=== \"Code node (Python)\"\nSelect multiple elements and create a new list or object\nUse Multiselect to select elements from a JSON object and combine them into a new list or object.\nGiven this JSON from a webhook node:\nUse multiselect list to get the first and last names and create new lists containing both names:\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\n=== \"Code node (Python)\"\nAn alternative to arrow functions in expressions\nFor example, generate some input data by returning the below code from the Code node:\nYou could do a search like \"find the item with the name Lenovo and tell me their category ID.\""
  },
  {
    "file_path": "code\\cookbook\\luxon.md",
    "content": "Date and time with Luxon\nLuxon is a JavaScript library that makes it easier to work with date and time. For full details of how to use Luxon, refer to Luxon's documentation.\nn8n passes dates between nodes as strings, so you need to parse them. Luxon makes this easier.\nVariables\nn8n uses Luxon to provide two custom variables:\nnow: a Luxon object containing the current timestamp. Equivalent to DateTime.now().\ntoday: a Luxon object containing the current timestamp, rounded down to the day. Equivalent to DateTime.now().set({ hour: 0, minute: 0, second: 0, millisecond: 0 }).\nNote that these variables can return different time formats when cast as a string. This is the same behavior as Luxon's DateTime.now().\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\n=== \"Code node (Python)\"\nn8n provides built-in convenience functions to support data transformation in expressions for dates. Refer to Data transformation functions | Dates for more information.\nDate and time behavior in n8n\nBe aware of the following:\nIn a workflow, n8n converts dates and times to strings between nodes. Keep this in mind when doing arithmetic on dates and times from other nodes.\nWith vanilla JavaScript, you can convert a string to a date with new Date('2019-06-23'). In Luxon, you must use a function explicitly stating the format, such as DateTime.fromISO('2019-06-23') or DateTime.fromFormat(\"23-06-2019\", \"dd-MM-yyyy\").\nSetting the timezone in n8n\nLuxon uses the n8n timezone. This value is either:\nDefault: America/New York\nA custom timezone for your n8n instance, set using the GENERIC_TIMEZONE environment variable.\nA custom timezone for an individual workflow, configured in workflow settings.\nCommon tasks\nThis section provides examples for some common operations. More examples, and detailed guidance, are available in Luxon's own documentation.\nConvert date string to Luxon\nYou can convert date strings and other date formats to a Luxon DateTime object. You can convert from standard formats and from arbitrary strings.\nIf you have a date in a supported standard technical format:\nMost dates use fromISO(). This creates a Luxon DateTime from an ISO 8601 string. For example:\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\nLuxon's API documentation has more information on fromISO.\nLuxon provides functions to handle conversions for a range of formats. Refer to Luxon's guide to Parsing technical formats for details.\nIf you have a date as a string that doesn't use a standard format:\nUse Luxon's Ad-hoc parsing. To do this, use the fromFormat() function, providing the string and a set of tokens that describe the format.\nFor example, you have n8n's founding date, 23rd June 2019, formatted as 23-06-2019. You want to turn this into a Luxon object:\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\nWhen using ad-hoc parsing, note Luxon's warning about Limitations. If you see unexpected results, try their Debugging guide.\nGet n days from today\nGet a number of days before or after today.\n=== \"Expressions (JavaScript)\"\nFor example, you want to set a field to always show the date seven days before the current date.\nIn the expressions editor, enter:\nOn the 23rd June 2019, this returns [Object: \"2019-06-16T00:00:00.000+00:00\"].\nThis example uses n8n's custom variable $today for convenience. It's the equivalent of DateTime.now().set({ hour: 0, minute: 0, second: 0, millisecond: 0 }).minus({days: 7}).\n=== \"Code node (JavaScript)\"\nFor example, you want a variable containing the date seven days before the current date.\nIn the code editor, enter:\nOn the 23rd June 2019, this returns [Object: \"2019-06-16T00:00:00.000+00:00\"].\nThis example uses n8n's custom variable $today for convenience. It's the equivalent of DateTime.now().set({ hour: 0, minute: 0, second: 0, millisecond: 0 }).minus({days: 7}).\nFor more detailed information and examples, refer to:\nLuxon's guide to math\nTheir API documentation on DateTime plus and DateTime minus\nCreate human-readable dates\nIn Get n days from today, the example gets the date seven days before the current date, and returns it as [Object: \"yyyy-mm-dd-T00:00:00.000+00:00\"] (for expressions) or yyyy-mm-dd-T00:00:00.000+00:00 (in the Code node). To make this more readable, you can use Luxon's formatting functions.\nFor example, you want the field containing the date to be formatted as DD/MM/YYYY, so that on the 23rd June 2019, it returns 23/06/2019.\nThis expression gets the date seven days before today, and converts it to the DD/MM/YYYY format.\n=== \"Expressions (JavaScript)\"\n=== \"Code node (JavaScript)\"\nYou can alter the format. For example:\n=== \"Expressions (JavaScript)\"\nOn 23rd June 2019, this returns \"16 June 2019\".\n=== \"Code node (JavaScript)\"\nOn 23rd June 2019, this returns \"16 June 2019\".\nRefer to Luxon's guide on toLocaleString (strings for humans) for more information.\nGet the time between two dates\nTo get the time between two dates, use Luxon's diffs feature. This subtracts one date from another and returns a duration.\nFor example, get the number of months between two dates:\n=== \"Expressions (JavaScript)\"\nThis returns [Object: {\"months\":1}].\n=== \"Code node (JavaScript)\"\nThis returns {\"months\":1}.\nRefer to Luxon's Diffs for more information.\nA longer example: How many days to Christmas?\nThis example brings together several Luxon features, uses JMESPath, and does some basic string manipulation.\nThe scenario: you want a countdown to 25th December. Every day, it should tell you the number of days remaining to Christmas. You don't want to update it for next year - it needs to seamlessly work for every year.\n=== \"Expressions (JavaScript)\"\nThis outputs \"There are <number of days> days to Christmas!\". For example, on 9th March, it outputs \"There are 291 days to Christmas!\".\nA detailed explanation of what the expression does:\n* {{: indicates the start of the expression.\n* \"There are \": a string.\n* +: used to join two strings.\n* $today.diff(): This is similar to the example in Get the time between two dates, but it uses n8n's custom $today variable.\n* DateTime.fromISO($today.year + '-12-25'), 'days': this part gets the current year using $today.year, turns it into an ISO string along with the month and date, and then takes the whole ISO string and converts it to a Luxon DateTime data structure. It also tells Luxon that you want the duration in days.\n* toObject() turns the result of diff() into a more usable object. At this point, the expression returns [Object: {\"days\":-<number-of-days>}]. For example, on 9th March, [Object: {\"days\":-291}].\n* .days uses JMESPath syntax to retrieve just the number of days from the object. For more information on using JMESPath with n8n, refer to our JMESpath documentation. This gives you the number of days to Christmas, as a negative number.\n* .toString().substring(1) turns the number into a string and removes the -.\n* + \" days to Christmas!\": another string, with a + to join it to the previous string.\n* }}: indicates the end of the expression.\n=== \"Code node (JavaScript)\"\nThis outputs \"There are <number of days> days to Christmas!\". For example, on 9th March, it outputs \"There are 291 days to Christmas!\".\nA detailed explanation of what the code does:\n* \"There are \": a string.\n* +: used to join two strings.\n* $today.diff(): This is similar to the example in Get the time between two dates, but it uses n8n's custom $today variable.\n* DateTime.fromISO($today.year + '-12-25'), 'days': this part gets the current year using $today.year, turns it into an ISO string along with the month and date, and then takes the whole ISO string and converts it to a Luxon DateTime data structure. It also tells Luxon that you want the duration in days.\n* toObject() turns the result of diff() into a more usable object. At this point, the expression returns [Object: {\"days\":-<number-of-days>}]. For example, on 9th March, [Object: {\"days\":-291}].\n* .days uses JMESPath syntax to retrieve just the number of days from the object. For more information on using JMESPath with n8n, refer to our JMESpath documentation. This gives you the number of days to Christmas, as a negative number.\n* .toString().substring(1) turns the number into a string and removes the -.\n* + \" days to Christmas!\": another string, with a + to join it to the previous string."
  },
  {
    "file_path": "code\\cookbook\\builtin\\all.md",
    "content": "(\"\").all(branchIndex?: number, runIndex?: number)\nThis gives access to all the items of the current or parent nodes. If you don't supply any parameters, it returns all the items of the current node.\nGetting items\n=== \"JavaScript\"\n=== \"Python\"\nAccessing item data\nGet all items output by a previous node, and log out the data they contain:\n=== \"JavaScript\"\n=== \"Python\""
  },
  {
    "file_path": "code\\cookbook\\builtin\\execution.md",
    "content": "execution\nexecution.id\nContains the unique ID of the current workflow execution.\n=== \"JavaScript\"\n=== \"Python\"\nexecution.resumeUrl\nThe webhook URL to call to resume a waiting workflow.\nSee the Wait > On webhook call documentation to learn more.\nexecution.customData\nThis is only available in the Code node.\n=== \"JavaScript\"\n=== \"Python\"\nRefer to Custom executions data for more information."
  },
  {
    "file_path": "code\\cookbook\\builtin\\get-workflow-static-data.md",
    "content": "getWorkflowStaticData(type)\nThis gives access to the static workflow data.\nYou can save data directly in the workflow. This data should be small.\nAs an example: you can save a timestamp of the last item processed from\nan RSS feed or database. It will always return an object. Properties can then read, delete or\nset on that object. When the workflow execution succeeds, n8n checks automatically if the data\nhas changed and saves it, if necessary.\nThere are two types of static data, global and node. Global static data is the\nsame in the whole workflow. Every node in the workflow can access it. The node static data is unique to the node. Only the node that set it can retrieve it again.\nExample with global data:\n=== \"JavaScript\"\n=== \"Python\"\nExample with node data:\n=== \"JavaScript\"\n=== \"Python\"\nTemplates and examples"
  },
  {
    "file_path": "code\\cookbook\\builtin\\index.md",
    "content": "Examples using n8n's built-in methods and variables\nn8n provides built-in methods and variables for working with data and accessing n8n data. This section provides usage examples.\nRelated resources\nBuilt-in methods and variables reference\nExpressions\nCode node"
  },
  {
    "file_path": "code\\cookbook\\builtin\\itemmatching.md",
    "content": "Retrieve linked items from earlier in the workflow\nEvery item in a node's input data links back to the items used in previous nodes to generate it. This is useful if you need to retrieve linked items from further back than the immediate previous node.\nTo access the linked items from earlier in the workflow, use (\"\").itemMatching(currentNodeinputIndex).\nFor example, consider a workflow that does the following:\nThe Customer Datastore node generates example data:\nThe Edit Fields node simplifies this data:\nThe Code node restore the email address to the correct person:\nThe Code node does this using the following code:\n=== \"JavaScript\"\n=== \"Python\"\nYou can view and download the example workflow from n8n website | itemMatchin usage example ."
  },
  {
    "file_path": "code\\cookbook\\builtin\\vars.md",
    "content": "vars\nvars contains all Variables for the active environment. It's read-only: you can access variables using vars, but must set them using the UI.\n=== \"JavaScript\"\n=== \"Python\""
  },
  {
    "file_path": "code\\cookbook\\code-node\\console-log.md",
    "content": "Output to the browser console with console.log() or print() in the Code node\nYou can use console.log() or print() in the Code node to help when writing and debugging your code.\nFor help opening your browser console, refer to this guide by Balsamiq.\nconsole.log (JavaScript)\nFor technical information on console.log(), refer to the MDN developer docs.\nFor example, copy the following code into a Code node, then open your console and run the node:\nprint (Python)\nFor technical information on print(), refer to the Real Python's guide.\nFor example, set your Code node Language to Python, copy the following code into the node, then open your console and run the node:\nHandling an output of [object Object]\nIf the console displays [object Object] when you print, check the data type, then convert it as needed.\nTo check the data type:\nJsProxy\nIf type() outputs , you need to convert the JsProxy to a native Python object using to_py(). This occurs when working with data in the n8n node data structure, such as node inputs and outputs. For example, if you want to print the data from a previous node in the workflow:\nRefer to the Pyodide documentation on JsProxy for more information on this class."
  },
  {
    "file_path": "code\\cookbook\\code-node\\get-binary-data-buffer.md",
    "content": "Get the binary data buffer\nThe binary data buffer contains all the binary file data processed by a workflow. You need to access it if you want to perform operations on the binary data, such as:\nManipulating the data: for example, adding column headers to a CSV file.\nUsing the data in calculations: for example, calculating a hash value based on it.\nComplex HTTP requests: for example, combining file upload with sending other data formats.\nYou can access the buffer using n8n's getBinaryDataBuffer() function:\nFor example:\nYou should always use the getBinaryDataBuffer() function, and avoid using older methods of directly accessing the buffer, such as targeting it with expressions like items[0].binary.data.data."
  },
  {
    "file_path": "code\\cookbook\\code-node\\index.md",
    "content": "Code node cookbook\nThis section contains examples and recipes for tasks you can do with the Code node.\nRelated resources\nBuilt-in methods and variables reference\nCode node"
  },
  {
    "file_path": "code\\cookbook\\code-node\\number-items-last-node.md",
    "content": "Get number of items returned by the previous node\nTo get the number of items returned by the previous node:\n=== \"JavaScript\"\nThe output will be similar to the following.\n=== \"Python\"\nThe output will be similar to the following."
  },
  {
    "file_path": "code\\cookbook\\expressions\\check-incoming-data.md",
    "content": "Check incoming data\nAt times, you may want to check the incoming data. If the incoming data doesn't match a condition, you may want to return a different value. For example, you want to check if a variable from the previous node is empty and return a string if it's empty. Use the following code snippet to return not found if the variable is empty.\nThe above expression uses the ternary operator. You can learn more about the ternary operator here.\nAs an alternative, you can use the nullish coalescing operator (??) or the logical or operator (TABLE_PLACEHOLDER_0\nIn either of the above two cases, the value of $x will be used if it's set to a non-null, non-false value. The string default value is the fallback value."
  },
  {
    "file_path": "code\\cookbook\\expressions\\common-issues.md",
    "content": "Expressions common issues\nHere are some common errors and issues related to expressions and steps to resolve or troubleshoot them.\nThe 'JSON Output' in item 0 contains invalid JSON\nThis error occurs when you use JSON mode but don't provide a valid JSON object. Depending on the problem with the JSON object, the error sometimes display as The 'JSON Output' in item 0 does not contain a valid JSON object.\nTo resolve this, make sure that the code you provide is valid JSON:\nCheck the JSON with a JSON validator.\nCheck that your JSON object doesn't reference undefined input data. This may occur if the incoming data doesn't always include the same fields.\nCan't get data for expression\nThis error occurs when n8n can't retrieve the data referenced by an expression. Often, this happens when the preceding node hasn't been run yet.\nAnother variation of this may appear as Referenced node is unexecuted.  In that case, the full text of this error will tell you the exact node that isn't executing in this format:\nAn expression references the node '<node-name>', but it hasn‚Äôt been executed yet. Either change the expression, or re-wire your workflow to make sure that node executes first.\nTo begin troubleshooting, test the workflow up to the named node.\nFor nodes that use JavaScript or other custom code, you can check if a previous node has executed before trying to use its value by checking the following:\nAs an example, this JSON references the parameters of the input data. This error will display if you test this step without connecting it to another node:\nInvalid syntax\nThis error occurs when you use an expression that has a syntax error.\nFor example, the expression in this JSON includes a trailing period, which results in an invalid syntax error:\nTo resolve this error, check your expression syntax to make sure they follow the expected format."
  },
  {
    "file_path": "code\\cookbook\\expressions\\index.md",
    "content": "Expressions cookbook\nThis section contains examples and recipes for tasks you can do with expressions.\nRelated resources\nBuilt-in methods and variables reference\nExpressions"
  },
  {
    "file_path": "code\\cookbook\\http-node\\index.md",
    "content": "Examples using n8n's HTTP Request node\nThe HTTP Request node is one of the most versatile nodes in n8n. Use this node to make HTTP requests to query data from any app or service with a REST API.\nRefer to HTTP Request for information on node settings.\nRelated resources\nHTTP Request\nBuilt-in methods and variables reference\nExpressions"
  },
  {
    "file_path": "code\\cookbook\\http-node\\pagination.md",
    "content": "Pagination in the HTTP Request node\nThe HTTP Request node supports pagination. This page provides some example configurations, including using the HTTP node variables.\nRefer to HTTP Request for more information on the node.\nEnable pagination\nIn the HTTP Request node, select Add Option > Pagination.\nUse a URL from the response to get the next page using $response\nIf the API returns the URL of the next page in its response:\nSet Pagination Mode to Response Contains Next URL. n8n displays the parameters for this option.\nIn Next URL, use an expression to set the URL. The exact expression depends on the data returned by your API. For example, if the API includes a parameter called next-page in the response body:\nGet the next page by number using $pageCount\nIf the API you're using supports targeting a specific page by number:\nSet Pagination Mode to Update a Parameter in Each Request.\nSet Type to Query.\nEnter the Name of the query parameter. This depends on your API and is usually described in its documentation. For example, some APIs use a query parameter named page to set the page. So Name would be page.\nHover over Value and toggle Expression on.\nEnter {{ $pageCount + 1 }}\n$pageCount is the number of pages the HTTP Request node has fetched. It starts at zero. Most API pagination counts from one (the first page is page one). This means that adding +1 to $pageCount means the node fetches page one on its first loop, page two on its second, and so on.\nNavigate pagination through body parameters\nIf the API you're using allows you to paginate through the body parameters:\nSet the HTTP Request Method to POST\nSet Pagination Mode to Update a Parameter in Each Request.\nSelect Body in the Type parameter.\nEnter the Name of the body parameter. This depends on the API you're using. page is a common key name.\nHover over Value and toggle Expression on.\nEnter {{ $pageCount + 1 }}\nSet the page size in the query\nIf the API you're using supports choosing the page size in the query:\nSelect Send Query Parameters in main node parameters (this is the parameters you see when you first open the node, not the settings within options).\nEnter the Name of the query parameter. This depends on your API. For example, a lot of APIs use a query parameter named limit to set page size. So Name would be limit.\nIn Value, enter your page size."
  },
  {
    "file_path": "courses\\index.md",
    "content": "Text courses\nIf you've found your way here, it means you're serious about your interest in automation. Maybe you're tired of manually entering data into the same spreadsheet every day, of clicking through a series of tabs and buttons for that one piece of information you need, of managing tens of different tools and systems.\nWhatever the reason, one thing is clear: you shouldn't spend precious time doing things that don't spark joy or contribute to your personal and professional growth.\nThese tasks can and should be automated! And you don't need advanced technical knowledge or excellent coding skills to do this‚Äìwith no-code tools like n8n, automation is for everyone.\nAvailable courses\nLevel 1: Beginner course\nLevel 2: Intermediate course"
  },
  {
    "file_path": "courses\\level-one\\chapter-1.md",
    "content": "Navigating the Editor UI\nIn this lesson you will learn how to navigate the Editor UI. We will walk through the canvas and show you what each icon means and where to find things you will need while building workflows in n8n.\nGetting started\nBegin by setting up n8n.\nWe recommend starting with n8n Cloud, a hosted solution that doesn't require installation and includes a free trial.\nFor more details on the different ways to set up n8n, see our platforms documentation.\nOnce you have n8n running, open the Editor UI in a browser window. Log in to your n8n instance. Select Overview and then Create Workflow to view the main canvas.\nIt should look like this:\nEditor UI\nEditor UI settings\nThe editor UI is the web interface where you build workflows. You can access all your workflows and credentials, as well as support pages, from the Editor UI.\nLeft-side panel\nOn the left side of the Editor UI, there is a panel which contains the core functionalities and settings for managing your workflows. Expand and collapse it by selecting the small arrow icon.\nThe panel contains the following sections:\nOverview: Contains all the workflows and credentials you have access to. During this course, create new workflows here.\nProjects: (Not available on Community edition) Projects group workflows and credentials. You can assign roles to users in a project to control what they can do in a project. A Personal project is available by default.\nAdmin Panel: n8n Cloud only. Access your n8n instance usage, billing, and version settings.\nTemplates: A collection of pre-made workflows. Great place to get started with common use cases.\nVariables: Used to store and access fixed data across your workflows. This feature is available on the Pro and Enterprise Plans.\nAll executions: Contains information about your workflow executions.\nHelp: Contains resources around n8n product and community.\nUpdate: (When updates are available) Indicator for any recent product updates.\nSettings: Under the ellipsis (...) menu by your username. Manage users and access settings for a variety of features.\nEditor UI left-side menu\nTop bar\nThe top bar of the Editor UI contains the following information:\nWorkflow Name: By default, n8n names a new workflow as \"My workflow\", but you can edit the name at any time.\n+ Add Tag: Tags help you organise your workflows by category, use case, or whatever is relevant for you. Tags are optional.\nInactive/active toggle: This button activates or deactivates the current workflow. By default, workflows are deactivated.\nShare: You can share and collaborate with others on workflows on the Starter, Pro, and Enterprise plans.\nSave: This button saves the current workflow.\nHistory: Once you save your workflow, you can view previous versions here.\nEditor UI top bar\nCanvas\nThe canvas is the gray dotted grid background in the Editor UI. It displays several icons and a node with different functionalities:\nButtons to zoom the canvas to fit the screen, zoom in or out of the canvas, and tidy up the nodes on screen.\nA button to Execute workflow once you add your first node. When you click on it, n8n executes all nodes on the canvas in sequence.\nA button with a + sign inside. This button opens the nodes panel.\nA button with a note icon inside. This button adds a sticky note to the canvas (visible when hovering on the top right + icon).\nA dotted square with the text \"Add first step.\" This is where you add your first node.\nWorkflow canvas\nDon't worry about workflow execution and activation for now; we'll explain these concepts later on in the course.\nNodes\nYou can think of nodes as building blocks that serve different functions that, when put together, make up a functioning machine: an automated workflow.\nBased on their function, n8n classifies nodes into four types:\nApp or Action Nodes add, remove, and edit data; request and send external data; and trigger events in other systems. Refer to the Action nodes library for a full list of these nodes.\nTrigger Nodes start a workflow and supply the initial data. Refer to the Trigger nodes library for a list of trigger nodes.\nCore Nodes can be trigger or app nodes. Whereas most nodes connect to a specific external service, core nodes provide functionality such as logic, scheduling, or generic API calls. Refer to the Core Nodes library for a full list of core nodes.\nCluster Nodes are node groups that work together to provide functionality in a workflow, primarily for AI workflows. Refer to Cluster nodes for more information.\nFinding nodes\nYou can find all available nodes in the nodes panel on the right side of the Editor UI. There are three ways in which you can open the nodes panel:\nClick the + icon in the top right corner of the canvas.\nClick the + icon on the right side of an existing node on the canvas (the node to which you want to add another one).\nClick the ++tab++ key on your keyboard.\nNodes panel\nIn the nodes panel, notice that when adding your first node, you will see the different trigger node categories. After you have added your trigger node, you'll see that the nodes panel changes to show Advanced AI, Actions in an App, Data transformation, Flow, Core, and Human in the loop nodes.\nIf you want to find a specific node, use the search input at the top of the nodes panel.\nAdding nodes\nThere are two ways to add nodes to your canvas:\nSelect the node you want in the nodes panel. The new node will automatically connect to the selected node on the canvas.\nDrag and drop the node from the nodes panel to the canvas.\nNode buttons\nIf you hover on a node, you'll notice that three icons appear on top:\nExecute the node (Play icon)\nDeactivate/Activate the node (Power icon)\nDelete the node (Trash icon)\nThere will also be an ellipsis icon, which opens a context menu containing other node options.\nSummary\nIn this lesson you learned how to navigate the Editor UI, what the icons mean, how to access the left-side and node panels, and how to add nodes to the canvas.\nIn the next lesson, you will build a mini-workflow to put into practice what you've learned so far."
  },
  {
    "file_path": "courses\\level-one\\chapter-2.md",
    "content": "Building a Mini-workflow\nIn this lesson, you will build a small workflow that gets 10 articles about automation from Hacker News. The process consists of five steps:\nAdd a Manual Trigger node\nAdd the Hacker News node\nConfigure the Hacker News node\nExecute the node\nSave the workflow\nThe finished workflow will look like this:\n1. Add a Manual Trigger node\nOpen the nodes panel (reminder: you can open this by selecting the + icon in the top right corner of the canvas or selecting ++tab++ on your keyboard).\nThen:\nSearch for the Manual Trigger node.\nSelect it when it appears in the search.\nThis will add the Manual Trigger node to your canvas, which allows you to run the workflow at any time by selecting the Execute workflow button.\n2. Add the Hacker News node\nSelect the + icon to the right of the Manual Trigger node to open the nodes panel.\nThen:\nSearch for the Hacker News node.\nSelect it when it appears in the search.\nIn the Actions section, select Get many items.\nn8n adds the node to your canvas and the node window opens to display its configuration details.\n3. Configure the Hacker News node\nWhen you add a new node to the Editor UI, the node is automatically activated. The node details will open in a window with several options:\nParameters: Adjust parameters to refine and control the node's functionality.\nSettings: Adjust settings to control the node's design and executions.\nDocs: Open the n8n documentation for this node in a new window.\nParameters\nWe need to configure several parameters for the Hacker News node to make it work:\nResource: All\nThis resource selects all data records (articles).\nOperation: Get Many\nThis operation fetches all the selected articles.\nLimit: 10\nThis parameter sets a limit to the number of results the Get Many operation returns.\nAdditional Fields > Add Field > Keyword: automation\nAdditional fields are options that you can add to certain nodes to make your request more specific or filter the results. For this example, we want to get only articles that include the keyword \"automation.\"\nThe configuration of the parameters for the Hacker News node should now look like this:\nHacker News node parameters\nSettings\nThe Settings section includes several options for node design and executions. In this case, we'll configure only the final two settings, which set the node's appearance in the Editor UI canvas.\nIn the Hacker News node Settings, edit:\nNotes: Get the 10 latest articles.\nDisplay note in flow?: toggle to true\nThis option will display the Note under the node in the canvas.\nThe configuration of the settings for the Hacker News node should now look like this:\nHacker News node settings\n4. Execute the node\nSelect the Execute step button in the node details window. You should see 10 results in the Output Table view.\nResults in Table view for the Hacker News node\nNode executions\nIf a node executes successfully, a small green checkmark appears on top of the node in the canvas\nSuccessfully executed workflow\nIf there are no problems with the parameters and everything works fine, the requested data displays in the node window in Table, JSON, and Schema format. You can switch between these views by selecting the one you want from the **Table TABLE_PLACEHOLDER_0\nHere's our Hacker News output in JSON view:\nResults in JSON view for the Hacker News node\nThe node window displays more information about the node execution:\nNext to the Output title, notice a small icon (this will be a green checkmark if the node execution succeeded). Beside it, there is an info icon. If you hover on it, you'll get two more pieces of information that can provide insights into the performance of each individual node in a workflow:\nStart Time: When the node execution started.\nExecution Time: How long it took for the node to return the results from the moment it started executing.\nJust below the Output title, you'll notice another piece of information: 10 items. This field displays the number of items (records) that the node request returned. In this example, it's expected to be 10, since this is the limit we set in step 2. But if you don't set a limit, it's useful to see how many records are actually returned.\nError in nodes\n5. Save the workflow\nOnce you're finished editing the node, select Back to canvas to return to the main canvas.\nBy default, your workflow is automatically saved as \"My workflow.\"\nFor this lesson, rename the workflow to be \"Hacker News workflow.\"\nOnce you've renamed the workflow, be sure to save it.\nThere are two ways in which you can save a workflow:\nFrom the Canvas in Editor UI, click Ctrl + S or Cmd + S on your keyboard.\nSelect the Save button in the top right corner of the Editor UI. You may need to leave the node editor first by clicking outside the dialog.\nIf you see a grey Saved text instead of the Save button, your workflow was automatically saved.\nSummary\nCongratulations, you just built your first workflow! In this lesson, you learned how to use actions in app nodes, configure their parameters and settings, and save and execute your workflow.\nIn the next lesson, you'll meet your new client, Nathan, who needs to automate his sales reporting work. You will build a more complex workflow for his use case, helping him become more productive at work."
  },
  {
    "file_path": "courses\\level-one\\chapter-3.md",
    "content": "Automating a (Real-world) Use Case\nMeet Nathan üôã. Nathan works as an Analytics Manager at ABCorp. His job is to support the ABCorp team with reporting and analytics. Being a true jack of all trades, he also handles several miscellaneous initiatives.\nSome things that Nathan does are repetitive and mind-numbing. He wants to automate some of these tasks so that he doesn't burn out. As an Automation Expert, you are meeting with Nathan today to help him understand how he can offload some of his responsibilities to n8n.\nUnderstanding the scenario\nYou üë©‚Äçüîß: Nice to meet you, Nathan. Glad to be doing this! What's a repetitive task that's error-prone and that you'd like to get off your plate first?\nNathan üôã: Thanks for coming in! The most annoying one's gotta be the weekly sales reporting.\nI have to collect sales data from our legacy data warehouse, which manages data from the main business processes of an organization, such as sales or production. Now, each sales order can have the status Processing or Booked. I have to calculate the sum of all the Booked orders and announce them in the company Discord every Monday. Then I have to create a spreadsheet of all the Processing sales so that the Sales Managers can review them and check if they need to follow up with customers.\nThis manual work is tough and requires high attention to detail to make sure that all the numbers are right. Inevitably, I lose my focus and mistype a number or I don't get it done on time. I've been criticized once by my manager for miscalculating the data.\nYou üë©‚Äçüîß: Oh no! Doesn't the data warehouse have a way to export the data?\nNathan üôã: The data warehouse was written in-house ages ago. It doesn't have a CSV export but they recently added a couple of API endpoints that expose this data, if that helps.\nYou üë©‚Äçüîß: Perfect! That's a good start. If you have a generic API, we can add some custom code and a couple of services to make an automated workflow. This gig has n8n written all over it. Let's get started!"
  },
  {
    "file_path": "courses\\level-one\\chapter-4.md",
    "content": "Designing the Workflow\nNow that we know what Nathan wants to automate, let's consider the steps he needs to take to achieve his goals:\nGet the relevant data (order id, order status, order value, employee name) from the data warehouse\nFilter the orders by their status (Processing or Booked)\nCalculate the total value of all the Booked orders\nNotify the team members about the Booked orders in the company's Discord channel\nInsert the details about the Processing orders in Airtable for follow-up\nSchedule this workflow to run every Monday morning\nNathan's workflow involves sending data from the company's data warehouse to two external services:\nDiscord\nAirtable\nBefore that, the data has to be wrangled with general functions (conditional filtering, calculation, scheduling).\nn8n provides integrations for all these steps, so Nathan's workflow in n8n would look like this:\nYou will build this workflow in eight steps:\nGetting data from the data warehouse\nInserting data into Airtable\nFiltering orders\nSetting values for processing orders\nCalculating booked orders\nNotifying the team\nScheduling the workflow\nActivating and examining the workflow\nTo build this workflow, you will need the credentials found in the email you received from n8n when you signed up for this course. If you haven't signed up already, you can do it here. If you haven't received a confirmation email after signing up, contact us.\nStart building!{ .md-button }"
  },
  {
    "file_path": "courses\\level-one\\chapter-6.md",
    "content": "Exporting and importing workflows\nIn this chapter, you will learn how to export and import workflows.\nExporting and importing workflows\nYou can save n8n workflows locally as JSON files. This is useful if you want to share your workflow with someone else or import a workflow from someone else.\nImport & Export workflows menu\nYou can export and import workflows in three ways:\nFrom the Editor UI menu:\nExport: From the top navigation bar, select the three dots in the upper right, then select Download. This will download your current workflow as a JSON file on your computer.\nImport: From the top navigation bar, select the three dots in the upper right, then select¬†Import from URL (to import a published workflow) or¬†Import from File (to import a workflow as a JSON file).\nFrom the Editor UI canvas:\nExport: Select all the nodes on the canvas and use ++ctrl+c++ to copy the workflow JSON. You can paste this into a file or share it directly with other people.\nImport: You can paste a copied workflow JSON directly into the canvas with ++ctrl+v++.\nFrom the command line:\nExport: See the full list of commands  for exporting workflows or credentials.\nImport: See the full list of commands  for importing workflows or credentials."
  },
  {
    "file_path": "courses\\level-one\\chapter-7.md",
    "content": "Test your knowledge\nCongratulations, you finished the n8n Course Level 1!\nYou've learned a lot about workflow automation and built your first business workflow. Why not showcase your skills?\nYou can test your knowledge by taking a quiz, which consists of questions about the theoretical concepts and workflows covered in this course.\nYou need to have at least 80% correct answers in each part to pass the quiz.\nYou can take the quiz as many times as you want.\nThere's no time limit on answering the quiz questions.\nTake the quiz!{ .md-button }\nWhat's next?\nCreate new workflows for your work or personal use and share them with us. Don't have any ideas? Find inspiration on our blog, YouTube channel, community forum, and Discord server.\nTake the n8n Course Level 2."
  },
  {
    "file_path": "courses\\level-one\\index.md",
    "content": "Level one: Introduction\nWelcome to the n8n Course Level 1!\nIs this course right for me?\nThis course introduces you to the fundamental concepts within n8n and develops your low-code automation expertise.\nThis course is for you if you:\nAre starting to use n8n for the first time.\nAre looking for some extra help creating your first workflow.\nWant to automate processes in your personal or working life.\nThis course introduces n8n concepts and demonstrates practical workflow building without assuming any prior familiarity with n8n. If you'd like to get a feel for the basics without as much explanation, consult our quickstart guide.\nWhat will I learn in this course?\nWe believe in learning by doing. You can expect some theoretical information about the basic concepts and components of n8n, followed by practice of building workflows step by step.\nBy the end of this course you will know:\nHow to set up n8n and navigate the Editor UI.\nHow n8n structures data.\nHow to configure different node parameters and add credentials.\nWhen and how to use conditional logic in workflows.\nHow to schedule and control workflows.\nHow to import, download, and share workflows with others.\nYou will build two workflows:\nA two-node workflow to get articles from Hacker News\nA seven-node workflow to help your client get records from a data warehouse, filter them, make calculations, and notify team members about the results\nWhat do I need to get started?\nn8n set up: You can use n8n Cloud (or the self-hosted version if you have experience hosting services).\nA course user ID: Sign up here to get your unique ID and other credentials you will need in the course.\nBasic knowledge of JavaScript and APIs would be helpful, but isn't necessary.\nAn account on the n8n community forum if you wish to receive a profile badge and avatar upon successful completion.\nHow long does the course take?\nCompleting the course should take around two hours. You don't have to complete it in one go; feel free to take breaks and resume whenever you are ready.\nHow do I complete the course?\nThere are two milestones in this course that test your knowledge of what you have learned in the lessons:\n[x] Building the main workflow\n[x] Passing the quiz at the end of the course\nIf you complete the milestones above, you will get a badge and an avatar in your forum profile. You can then share your profile and course verification ID to showcase your n8n skills to others.\nLet's get started!{ .md-button }"
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.1.md",
    "content": "1. Getting data from the data warehouse\nIn this part of the workflow, you will learn how to get data by making HTTP requests with the HTTP Request node.\nAfter completing this section, your workflow will look like this:\nFirst, let's set the scene for building Nathan's workflow.\nCreate new workflow\nOpen your Editor UI and create a new workflow with one of the two possible commands:\nSelect ++ctrl+alt+n++ or ++cmd+option+n++ on your keyboard.\nOpen the left menu, navigate to Workflows, and select Add workflow.\nName this new workflow \"Nathan's workflow.\"\nThe first thing you need to do is get data from ABCorp's old data warehouse.\nIn a previous chapter, you used an action node designed for a specific service (Hacker News). But not all apps or services have dedicated nodes, like the legacy data warehouse from Nathan's company.\nThough we can't directly export the data, Nathan told us that the data warehouse has a couple of API endpoints. That's all we need to access the data using the HTTP Request node in n8n.\nAdd an HTTP Request node\nNow, in your Editor UI, add an HTTP Request node like you learned in the lesson Adding nodes. The node window will open, where you need to configure some parameters.\nHTTP Request node\nThis node will use credentials.\nIn this case, you'll need the credentials for the ABCorp data warehouse API included in the email from n8n you received when you signed up for this course. If you haven't signed up yet, sign up here.\nIn the Parameters of the HTTP Request node, make the following adjustments:\nMethod: This should default to GET. Make sure it's set to GET.\nURL: Add the Dataset URL you received in the email when you signed up for this course.\nSend Headers: Toggle this control to true. In Specify Headers, ensure Using Fields Below is selected.\nHeader Parameters > Name: Enter unique_id.\nHeader Parameters > Value: The Unique ID you received in the email when you signed up for this course.\nAuthentication: Select Generic Credential Type. This option requires credentials before allowing you to access the data.\nGeneric Auth Type: Select Header Auth. (This field will appear after you select the Generic Credential Type for the Authentication.)\nCredential for Header Auth: To add your credentials, select + Create new credential. This will open the Credentials window.\nIn the Credentials window, set Name to be the Header Auth name you received in the email when you signed up for this course.\nIn the Credentials window, set Value to be the Header Auth value you received in the email when you signed up for this course.\nSelect the Save button in the Credentials window to save your credentials. Your Credentials Connection window should look like this:\nHTTP Request node credentials\nOnce you save, exit out of the Credentials window to return to the HTTP Request node.\nGet the data\nSelect the Execute step button in the HTTP Request node window. The table view of the HTTP request results should look like this:\nHTTP Request node output\nThis view should be familiar to you from the Building a mini-workflow page.\nThis is the data from ABCorp's data warehouse that Nathan needs to work with. This data set includes sales information from 30 customers with five columns:\norderID: The unique id of each order.\ncustomerID: The unique id of each customer.\nemployeeName: The name of Nathan's colleague responsible for the customer.\norderPrice: The total price of the customer's order.\norderStatus: Whether the customer's order status is booked or still in processing.\nWhat's next?\nNathan üôã: This is great! You already automated an important part of my job with only one node. Now instead of manually accessing the data every time I need it, I can use the HTTP Request Node to automatically get the information.\nYou üë©‚Äçüîß: Exactly! In the next step, I'll help you one step further and insert the data you retrieved into Airtable."
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.2.md",
    "content": "2. Inserting data into Airtable\nIn this step of the workflow, you will learn how to insert the data received from the HTTP Request node into Airtable using the Airtable node.\nAfter this step, your workflow should look like this:\nConfigure your table\nIf we're going to insert data into Airtable, we first need to set up a table there. To do this:\nCreate an Airtable account.\nIn your Airtable workspace add a new base from scratch and name it, for example, beginner course.\nCreate an Airtable base\nIn the beginner course base, by default, you have a table called Table 1 with four fields: Name, Notes, Assignee, and Status.  These fields aren't relevant for us since they aren't in our \"orders\" data set. This brings us to the next point: the names of the fields in Airtable have to match the names of the columns in the node result. Prepare the table by doing the following:\nRename the table from Table 1 to orders to make it easier to identify.\nDelete the 3 blank records created by default.\nDelete the Notes, Assignee, and Status fields.\nEdit the Name field (the primary field) to read orderID, with the Number field type.\nAdd the rest of the fields, and their field types, using the table below as a reference:\nTABLE_PLACEHOLDER_0\nNow your table should look like this:\nOrders table in Airtable\nNow that the table is ready, let's return to the workflow in the n8n Editor UI.\nAdd an Airtable node to the HTTP Request node\nAdd an Airtable node connected to the HTTP Request node.\nIn the node panel:\nSearch for Airtable.\nSelect Create a record from the Record Actions search results.\nThis will add the Airtable node to your canvas and open the node details window.\nIn the Airtable node window, configure the following parameters:\nCredential to connect with:\nSelect Create new credential.\nKeep the default option Connect using: Access Token selected.\nAccess token: Follow the instructions from the Airtable credential page to create your token. Use the recommended scopes and add access to your beginners course base. Save the credential and close the Credential window when you're finished.\nResource: Record.\nOperation: Create. This operation will create new records in the table.\nBase: You can pick your base from a list (for example, beginner course).\nTable: orders.\nMapping Column Mode: Map automatically. In this mode, the incoming data fields must have the same as the columns in Airtable.\nTest the Airtable node\nOnce you've finished configuring the Airtable node, execute it by selecting Execute step. This might take a moment to process, but you can follow the progress by viewing the base in Airtable.\nYour results should look like this:\nAirtable node results\nAll 30 data records will now appear in the orders table in Airtable:\nImported records in the orders table\nWhat's next?\nNathan üôã: Wow, this automation is already so useful! But this inserts all collected data from the HTTP Request node into Airtable. Remember that I actually need to insert only processing orders in the table and calculate the price of booked orders?\nYou üë©‚Äçüîß: Sure, no problem. As a next step, I'll use a new node to filter the orders based on their status."
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.3.md",
    "content": "3. Filtering Orders\nIn this step of the workflow, you will learn how to filter data using conditional logic and how to use expressions in nodes using the If node.\nAfter this step, your workflow should look like this:\nTo insert only processing orders into Airtable we need to filter our data by orderStatus. Basically, we want to tell the program that if the orderStatus is processing, then insert all records with this status into Airtable; else, for example, if the orderStatus isn't processing, calculate the sum of all orders with the other orderStatus (booked).\nThis if-then-else command is conditional logic. In n8n workflows, you can add conditional logic with the If node, which splits a workflow conditionally based on comparison operations.\nAdd If node before the Airtable node\nFirst, let's add an If node between the connection from the HTTP Request node to the Airtable node:\nHover over the arrow connection the HTTP Request node and the Airtable node.\nSelect the + sign between the HTTP Request node and the Airtable node.\nConfigure the If node\nSelecting the plus removes the connection to the Airtable node to the HTTP request. Now, let's add an If node connected to the HTTP Request node:\nSearch for the If node.\nSelect it when it appears in the search.\nFor the If node, we'll use an expression.\nIn the If node window, configure the parameters:\nSet the value1 placeholder to {{ $json.orderStatus }} with the following steps:\nHover over the value1 field.\nSelect the Expression tab on the right side of the value1 field.\nNext, open the expression editor by selecting the link icon:\nOpening the Expression Editor\nUse the left-side panel to select HTTP Request >  orderStatus and drag it into the Expression field in the center of the window.\nExpression Editor in the If node\nOnce you add the expression, close the Edit Expression dialog.\nOperation: Select String > is equal to\nSet the value2 placeholder to processing.\nSelect Execute step to test the If node.\nYour results should look like this:\nIf node output\nNote that the orders with a processing order status should show for the True Branch output, while the orders with a booked order status should show in the False Branch output.\nClose the If node detail view when you're finished.\nInsert data into Airtable\nNext, we want to insert this data into Airtable. Remember what Nathan said at the end of the Inserting data into Airtable lesson?\nI actually need to insert only processing orders in the table...\nSince Nathan only needs the processing orders in the table, we'll connect the Airtable node to the If node's true connector.\nIn this case, since the Airtable node is already on our canvas, select the If node true connector and drag it to the Airtable node.\nIt's a good idea at this point to retest the Airtable node. Before you do, open your table in Airtable and delete all existing rows. Then open the Airtable node window in n8n and select Execute step.\nReview your data in Airtable to be sure your workflow only added the correct orders (those with orderStatus of processing). There should be 14 records now instead of 30.\nAt this stage, your workflow should look like this:\nWhat's next?\nNathan üôã: This If node is so useful for filtering data! Now I have all the information about processing orders. I actually only need the employeeName and orderID, but I guess I can keep all the other fields just in case.\nYou üë©‚Äçüîß: Actually, I wouldn't recommend doing that. Inserting more data requires more computational power, the data transfer is slower and takes longer, and takes up more storage resources in your table. In this particular case, 14 records with 5 fields might not seem like it'd make a significant difference, but if your business grows to thousands of records and dozens of fields, things add up and even one extra column can affect performance.\nNathan üôã: Oh, that's good to know. Can you select only two fields from the processing orders?\nYou üë©‚Äçüîß: Sure, I'll do that in the next step."
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.4.md",
    "content": "4. Setting Values for Processing Orders\nIn this step of the workflow, you will learn how to select and set data before transferring it to Airtable using the Edit Fields (Set) node. After this step, your workflow should look like this:\nThe next step in Nathan's workflow is to filter the data to only insert the employeeName and orderID of all processing orders into Airtable.\nFor this, you need to use the Edit Fields (Set) node, which allows you to select and set the data you want to transfer from one node to another.\nAdd another node before the Airtable node\nIn your workflow, add another node before the Airtable node from the If node in the same way we did it in the Filtering Orders lesson on the If node's true connector. Feel free to drag the Airtable node further away if your canvas feels crowded.\nConfigure the Edit Fields node\nNow search for the Edit Fields (Set) node after you've selected the + sign coming off the If node's true connector.\nWith the Edit Fields node window open, configure these parameters:\nEnsure Mode is set to Manual Mapping.\nWhile you can use the Expression editor we used in the Filtering Orders lesson, this time, let's drag the fields from the Input into the Fields to Set:\nDrag If > orderID as the first field.\nDrag If > employeeName as the second field.\nEnsure that Include Other Input Fields is set to false.\nSelect Execute step. You should see the following results:\nEdit Fields (Set) node\nAdd data to Airtable\nNext, let's insert these values into Airtable:\nGo to your Airtable base.\nAdd a new table called processingOrders.\nReplace the existing columns with two new columns:\norderID (primary field): Number\nemployeeName: Single line text\nDelete the three empty rows in the new table.\nIn n8n, connect the Edit Fields node connector to the Airtable node**.\nUpdate the Airtable node configuration to point to the new processingOrders table instead of the orders table.\nTest your Airtable node to be sure it inserts records into the new processingOrders table.\nAt this stage, your workflow should now look like this:\nWhat's next?\nNathan üôã: You've already automated half of my work! Now I still need to calculate the booked orders for my colleagues. Can we automate that as well?\nYou üë©‚Äçüîß: Yes! In the next step, I'll use some JavaScript code in a node to calculate the booked orders."
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.5.md",
    "content": "5. Calculating Booked Orders\nIn this step of the workflow you will learn how n8n structures data and how to add custom JavaScript code to perform calculations using the Code node. After this step, your workflow should look like this:\nThe next step in Nathan's workflow is to calculate two values from the booked orders:\nThe total number of booked orders\nThe total value of all booked orders\nTo calculate data and add more functionality to your workflows you can use the Code node, which lets you write custom JavaScript code.\nAbout the Code node\nIn n8n, the data that's passed between nodes is an array of objects with the following JSON structure:\n(required) n8n stores the actual data within a nested json key. This property is required, but can be set to anything from an empty object (like {}) to arrays and deeply nested data. The code node automatically wraps the data in a json object and parent array ([]) if it's missing.\n(optional) Binary data of item. Most items in n8n don't contain binary data.\n(required) Arbitrary key name for the binary data.\n(required) Base64-encoded binary data.\n(optional) Should set if possible.\n(optional) Should set if possible.\n(optional) Should set if possible.\nYou can learn more about the expected format on the n8n data structure page.\nConfigure the Code node\nNow let's see how to accomplish Nathan's task using the Code node.\nIn your workflow, add a Code node connected to the false branch of the If node.\nWith the Code node window open, configure these parameters:\nMode: Select Run Once for All Items.\nLanguage: Select JavaScript.\nCopy the Code below and paste it into the Code box to replace the existing code:\nNotice the format in which we return the results of the calculation:\nNow select Execute step and you should see the following results:\nCode node output\nWhat's next?\nNathan üôã: Wow, the Code node is powerful! This means that if I have some basic JavaScript skills I can power up my workflows.\nYou üë©‚Äçüîß: Yes! You can progress from no-code to low-code!\nNathan üôã: Now, how do I send the calculations for the booked orders to my team's Discord channel?\nYou üë©‚Äçüîß: There's an n8n node for that. I'll set it up in the next step."
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.6.md",
    "content": "6. Notifying the Team\nIn this step of the workflow, you will learn how to send messages to a Discord channel using the Discord node. After this step, your workflow should look like this:\nNow that you have a calculated summary of the booked orders, you need to notify Nathan's team in their Discord channel. For this workflow, you will send messages to the n8n server on Discord.\nBefore you begin the steps below, use the link above to connect to the n8n server on Discord. Be sure you can access the #course-level-1 channel.\nIn your workflow, add a Discord node connected to the Code node.\nWhen you search for the Discord node, look for Message Actions and select Send a message to add the node.\nIn the Discord node window, configure these parameters:\nConnection Type: Select Webhook.\nCredential for Discord Webhook: Select - Create New Credential -.\nCopy the Webhook URL from the email you received when you signed up for this course and paste it into the Webhook URL field of the credentials.\nSelect Save and then close the credentials dialog.\nOperation: Select Send a Message.\nMessage:\nSelect the Expression tab on the right side of the Message field.\nCopy the text below and paste it into the Expression window, or construct it manually using the Expression Editor.\nNow select Execute step in the Discord node. If all works well, you should see this output in n8n:\nDiscord node output\nAnd your message should appear in the Discord channel #course-level-1:\nDiscord message\nWhat's next?\nNathan üôã: Incredible, you've saved me hours of tedious work already! Now I can execute this workflow when I need it. I just need to remember to run it every Monday morning at 9 AM.\nYou üë©‚Äçüîß: Don't worry about that, you can actually schedule the workflow to run on a specific day, time, or interval. I'll set this up in the next step."
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.7.md",
    "content": "7. Scheduling the Workflow\nIn this step of the workflow, you will learn how to schedule your workflow so that it runs automatically at a set time/interval using the Schedule Trigger node. After this step, your workflow should look like this:\nThe workflow you've built so far executes only when you click on Execute Workflow. But Nathan needs it to run automatically every Monday morning. You can do this with the Schedule Trigger, which allows you to schedule workflows to run periodically at fixed dates, times, or intervals.\nTo achieve this, we'll remove the Manual Trigger node we started with and replace it with a Schedule Trigger node instead.\nRemove the Manual Trigger node\nFirst, let's remove the Manual Trigger node:\nSelect the Manual Trigger node connected to your HTTP Request node.\nSelect the trash can icon to delete.\nThis removes the Manual Trigger node and you'll see an \"Add first step\" option.\nAdd the Schedule Trigger node\nOpen the nodes panel and search for Schedule Trigger.\nSelect it when it appears in the search results.\nIn the Schedule Trigger node window, configure these parameters:\nTrigger Interval: Select Weeks.\nWeeks Between Triggers: Enter 1.\nTrigger on weekdays: Select Monday (and remove Sunday if added by default).\nTrigger at Hour: Select 9am.\nTrigger at Minute: Enter¬†0.\nYour Schedule Trigger node should look like this:\nSchedule Trigger Node\nConnect the Schedule Trigger node\nReturn to the canvas and connect your Schedule Trigger node to the HTTP Request node by dragging the arrow from it to the HTTP Request node.\nYour full workflow should look like this:\nWhat's next?\nYou üë©‚Äçüîß: That was it for the workflow! I've added and configured all necessary nodes. Now every time you click on Execute workflow, n8n will execute all the nodes: getting, filtering, calculating, and transferring the sales data.\nNathan üôã: This is just what I needed! My workflow will run automatically every Monday morning, correct?\nYou üë©‚Äçüîß: Not so fast. To do that, you need to activate your workflow. I'll do this in the next step and show you how to interpret the execution log."
  },
  {
    "file_path": "courses\\level-one\\chapter-5\\chapter-5.8.md",
    "content": "8. Activating and Examining the Workflow\nIn this step of the workflow, you will learn how to activate your workflow and change the default workflow settings.\nActivating a workflow means that it will run automatically every time a trigger node receives input or meets a condition. By default, all newly created workflows start deactivated.\nTo activate your workflow, set the Inactive toggle in the top navigation of the Editor UI to be Activated. Nathan's workflow will now be executed automatically every Monday at 9 AM:\nActivated workflow\nWorkflow Executions\nAn execution represents a completed run of a workflow, from the first to the last node. n8n logs workflow executions, allowing you to see if the workflow succeeded or not. The execution log is useful for debugging your workflow and seeing at what stage it runs into issues.\nTo view the executions for a specific workflow, you can switch to the Executions tab when the workflow is open on the canvas. Use the Editor tab to swap back to the node editor.\nTo see the execution log for the entire n8n instance, in your Editor UI, select Overview and then select the Executions tab in the main panel.\nExecution List\nThe Executions window displays a table with the following information:\nName: The name of the workflow\nStarted At: The date and time when the workflow started\nStatus: The status of the workflow (Waiting, Running, Succeeded, Cancelled, or Failed) and the amount of time it took the workflow to execute\nExecution ID: The ID of this workflow execution\nWorkflow Settings\nYou can customize your workflows and executions, or overwrite some global default settings in¬†Workflow Settings.\nAccess these settings by selecting the three dots in the upper right corner of the Editor UI when the workflow is open on the canvas, then select Settings.\nWorkflow Settings\nIn the Workflow Settings window you can configure the following settings:\nExecution Order: Choose the execution logic for multi-branch workflows. You should leave this set to v1 if you don't have workflows that rely on the legacy execution ordering.\nError Workflow:¬†A workflow to run if the execution of the current workflow fails.\nThis workflow can be called by: Workflows allowed to call this workflow using the Execute Sub-workflow node.\nTimezone:¬†The timezone to use in the current workflow. If not set, the global timezone. In particular, this setting is important for the Schedule Trigger node, as you want to make sure that the workflow gets executed at the right time.\nSave failed production executions: If n8n should save the Execution data of the workflow when it fails. Default is to save.\nSave successful production executions: If n8n should save the Execution data of the workflow when it succeeds. Default is to save.\nSave manual executions: If n8n should save executions started from the Editor UI. Default is to save.\nSave execution progress: If n8n should save the execution data of each node. If set to Save, you can resume the workflow from where it stopped in case of an error, though keep in mind that this might make the execution slower. Default is to not save.\nTimeout Workflow: Whether to cancel a workflow execution after a specific period of time. Default is to not timeout.\nWhat's next?\nYou üë©‚Äçüîß: That was it! Now you have a 7-node workflow that will run automatically every Monday morning. You don't have to worry about remembering to wrangle the data. Instead, you can start your week with more meaningful or exciting work.\nNathan üôã: This workflow is incredibly helpful, thank you! Now, what's next for you?\nYou üë©‚Äçüîß: I'd like to build more workflows, share them with others, and use some workflows built by other people."
  },
  {
    "file_path": "courses\\level-two\\chapter-1.md",
    "content": "Understanding the data structure\nIn this chapter, you will learn about the data structure of n8n and how to use the Code node to transform data and simulate node outputs.\nData structure of n8n\nIn a basic sense, n8n nodes function as an Extract, Transform, Load (ETL) tool. The nodes allow you to access (extract) data from multiple disparate sources, modify (transform) that data in a particular way, and pass (load) it along to where it needs to be.\nThe data that moves along from node to node in your workflow must be in a format (structure) that can be recognized and interpreted by each node. In n8n, this required structure is an array of objects.\nData sent from one node to another is sent as an array of JSON objects. The elements in this collection are called items.\nItems\nAn n8n node performs its action on each item of incoming data.\nItems in the Customer Datastore node\nCreating data sets with the Code node\nNow that you are familiar with the n8n data structure, you can use it to create your own data sets or simulate node outputs. To do this, use the Code node to write JavaScript code defining your array of objects with the following structure:\nFor example, the array of objects representing the Ninja turtles would look like this in the Code node:\nArray of objects in the Code node\nYou can also have nested pairs, for example if you want to define a primary and a secondary color. In this case, you need to further wrap the key-value pairs in curly braces {}.\nExercise\nIn a Code node, create an array of objects named myContacts that contains the properties name and email, and the email property is further split into personal and work.\n??? note \"Show me the solution\"\nIn the **Code node**, in the JavaScript Code field you have to write the following code:\nWhen you execute the **Code node**, the result should look like this:\n<figure><img src=\"/_images/courses/level-two/chapter-one/exercise_function.png\" alt=\"\" style=\"width:100%\"><figcaption align = \"center\"><i>Result of Code node</i></figcaption></figure>\nReferencing node data with the Code node\nJust like you can use expressions to reference data from other nodes, you can also use some methods and variables in the Code node.\nPlease make sure you read these pages before continuing to the next exercise.\nExercise\nLet's build on the previous exercise, in which you used the Code node to create a data set of two contacts with their names and emails. Now, connect a second Code node to the first one. In the new node, write code to create a new column named workEmail that references the work email of the first contact.\n??? note \"Show me the solution\"\nIn the Code node, in the JavaScript Code field you have to write the following code:\nWhen you execute the **Code node**, the result should look like this:\n<figure><img src=\"/_images/courses/level-two/chapter-one/exercise_function_reference.png\" alt=\"\" style=\"width:100%\"><figcaption align = \"center\"><i>Code node reference</i></figcaption></figure>\nTransforming data\nThe incoming data from some nodes may have a different data structure than the one used in n8n. In this case, you need to transform the data, so that each item can be processed individually.\nThe two most common operations for data transformation are:\nCreating multiple items from one item\nCreating a single item from multiple items\nThere are several ways to transform data for the purposes mentioned above:\nUse n8n's data transformation nodes. Use these nodes to modify the structure of incoming data that contain lists (arrays) without needing to use JavaScript code in the Code node:\nUse the Split Out node to separate a single data item containing a list into multiple items.\nUse the Aggregate node to take separate items, or portions of them, and group them together into individual items.\nUse the Code node to write JavaScript functions to modify the data structure of incoming data using the Run Once for All Items mode:\nTo create multiple items from a single item, you can use JavaScript code like this. This example assumes that the item has a key named data set to an array of items in the form of: [{ \"data\": [{}, {}, ...] }]:\nTo create a single item from multiple items, you can use this JavaScript code:\nThese JavaScript examples assume your entire input is what you want to transform. As in the exercise above, you can also execute either operation on a specific field by identifying that in the items list, for example, if our workEmail example had multiple emails in a single field, we could run some code like this:\nExercise\nUse the HTTP Request node to make a GET request to the Pok√©API  (This API requires no authentication).\nTransform the data in the results field with the Split Out node.\nTransform the data in the results field with the Code node.\n??? note \"Show me the solution\"\n1. To get the pokemon from the Pok√©API, execute the **HTTP Request node** with the following parameters:\n- **Authentication**: None\n- **Request Method**: GET\n- **URL**:\n2. To transform the data with the **Split Out node**, connect this node to the **HTTP Request node** and set the following parameters:\n- **Field To Split Out**: results\n- **Include**: No Other Fields\n3. To transform the data with the **Code node**, connect this node to the **HTTP Request node** and write the following code in the JavaScript Code field:"
  },
  {
    "file_path": "courses\\level-two\\chapter-2.md",
    "content": "Processing different data types\nIn this chapter, you will learn how to process different types of data using n8n core nodes.\nHTML and XML data\nYou're most likely familiar with HTML and XML.\nIf you need to process HTML or XML data in your n8n workflows, use the HTML node or the XML node.\nUse the HTML node to extract HTML content of a webpage by referencing CSS selectors. This is useful if you want to collect structured information from a website (web-scraping).\nHTML Exercise\nLet's get the title of the latest n8n blog post:\nUse the HTTP Request node to make a GET request to the URL  (this endpoint requires no authentication).\nConnect an HTML node and configure it to extract the title of the first blog post on the page.\nHint: If you're not familiar with CSS selectors or reading HTML, the CSS selector .post .item-title  a should help!\n??? note \"Show me the solution\"\n1. Configure the HTTP Request node with the following parameters:\n- **Authentication**: None\n- **Request Method**: GET\n- **URL**:\nThe result should look like this:\n<figure><img src=\"/_images/courses/level-two/chapter-two/exercise_html_httprequestnode.png\" alt=\"Result of HTTP Request node\" style=\"width:100%\"><figcaption align = \"center\"><i>Result of HTTP Request node</i></figcaption></figure>\n2. Connect an **HTML node** to the **HTTP Request node** and configure the former's parameters:\n- **Operation**: Extract HTML Content\n- **Source Data**: JSON\n- **JSON Property**: data\n- **Extraction Values**:\n- **Key**: title\n- **CSS Selector**: .post .item-title  a\n- **Return Value**: HTML\nYou can add more values to extract more data.\nThe result should look like this:\n<figure><img src=\"/_images/courses/level-two/chapter-two/exercise_html_htmlextractnode.png\" alt=\"Result of HTML Extract node\" style=\"width:100%\"><figcaption align = \"center\"><i>Result of HTML Extract node</i></figcaption></figure>\nUse the XML node to convert XML to JSON and JSON to XML. This operation is useful if you work with different web services that use either XML or JSON and need to get and submit data between them in the two formats.\nXML Exercise\nIn the final exercise of Chapter 1, you used an HTTP Request node to make a request to the Pok√©API. In this exercise, we'll return to that same API but we'll convert the output to XML:\nAdd an HTTP Request node that makes the same request to the Pok√©API at\nUse the XML node to convert the JSON output to XML.\n??? note \"Show me the solution\"\n1. To get the pokemon from the Pok√©API, execute the **HTTP Request node** with the following parameters:\n- **Authentication**: None\n- **Request Method**: GET\n- **URL**:\n2. Connect an **XML node** to it with the following parameters:\n- **Mode**: JSON to XML\n- **Property name**: data\nThe result should look like this:\n<figure><img src=\"/_images/courses/level-two/chapter-two/exercise_html_xmlnode_table.png\" alt=\"Table view of XML Node (JSON to XML)\" style=\"width:100%\"><figcaption align = \"center\"><i>XML node (JSON to XML) ‚Äì Table View</i></figcaption></figure>\nTo transform data the other way around, select the mode **XML to JSON**.\nDate, time, and interval data\nDate and time data types include DATE, TIME, DATETIME, TIMESTAMP, and YEAR. The dates and times can be passed in different formats, for example:\nDATE: March 29 2022, 29-03-2022, 2022/03/29\nTIME: 08:30:00, 8:30, 20:30\nDATETIME: 2022/03/29 08:30:00\nTIMESTAMP: 1616108400 (Unix timestamp), 1616108400000 (Unix ms timestamp)\nYEAR: 2022, 22\nThere are a few ways you can work with dates and times:\nUse the Date & Time node to convert date and time data to different formats and calculate dates.\nUse Schedule Trigger node to schedule workflows to run at a specific time, interval, or duration.\nSometimes, you might need to pause the workflow execution. This might be necessary if you know that a service doesn't process the data instantly or it's slow to return all the results. In these cases, you don't want n8n to pass incomplete data to the next node.\nIf you run into situations like this, use the Wait node after the node that you want to delay. The Wait node pauses the workflow execution and will resume execution:\nAt a specific time.\nAfter a specified time interval.\nOn a webhook call.\nDate Exercise\nBuild a workflow that adds five days to an input date from the Customer Datastore node that you used before. Then, if the calculated date occurred after 1959, the workflow waits 1 minute before setting the calculated date as a value. The workflow should be triggered every 30 minutes.\nTo begin:\nAdd the Customer Datastore (n8n training) node with the Get All People action selected. Return All.\nAdd the Date & Time node to Round Up the created Date from the datastore to End of Month. Output this to field new-date. Include all input fields.\nAdd the If node to check if that new rounded date is after 1960-01-01 00:00:00.\nAdd the Wait node to the True output of that node and set it to wait for one minute.\nAdd the Edit Fields (Set) node to set a new field called outputValue to a String containing new-date. Include all input fields.\nAdd the Schedule Trigger node at the beginning of the workflow to trigger it every 30 minutes. (You can keep the Manual Trigger node for testing!)\n??? note \"Show me the solution\"\n1. Add the **Customer Datastore (n8n training) node** with the **Get All People** action selected.\n- Select the option to **Return All**.\n2. Add a **Date & Time node** connected to the Customer Datastore node. Select the option to **Round a Date**.\n- Add the created date as the **Date** to round.\n- Select Round Up as the **Mode** and End of Month as the **To**.\n- Set the **Output Field Name** as new-date.\n- In **Options**, select **Add Option** and use the control to **Include Input Fields**\n3. Add an **If node** connected to the **Date & Time node**.\n- Add the new-date field as the first part of the condition.\n- Set the comparison to **Date &Time > is after**\n- Add 1960-01-01 00:00:00 as the second part of the expression. (This should produce 3 items in the True Branch and 2 items in the False Branch)\n4. Add a **Wait node** to the True output of the **If node**.\n- Set **Resume** to After Time interval.\n- Set **Wait Amount** to 1.00.\n- Set **Wait Unit** to Minutes.\n5. Add an **Edit Fields (Set) node** to the **Wait node**.\n- Use either JSON or Manual Mapping **Mode**.\n- Set a new field called outputValue to be the value of the new-date field.\n- Select the option to **Include Other Input Fields** and include **All** fields.\n6. Add a **Schedule Trigger node** at the beginning of the workflow.\n- Set the **Trigger Interval** to use Minutes.\n- Set the **Minutes Between Triggers** to 30.\n- To test your schedule, be sure to activate the workflow.\n- Be sure to connect this node to the **Customer Datastore (n8n training) node** you began with!\nThe workflow should look like this:\n<figure><img src=\"/_images/courses/level-two/chapter-two/exercise_datetime.png\" alt=\"Workflow for transforming dates\" style=\"width:100%\"><figcaption align = \"center\"><i>Workflow for transforming dates</i></figcaption></figure>\nTo check the configuration of each node, you can copy the JSON code of this workflow and either paste it into the Editor UI or save it as a file and import from file into a new workflow. See Export and import workflows for more information.\nBinary data\nUp to now, you have mainly worked with text data. But what if you want to process data that's not text, like images or PDF files? These types of files are represented in the binary numeral system, so they're considered binary data. In this form, binary data doesn't offer you useful information, so you'll need to convert it into a readable form.\nIn n8n, you can process binary data with the following nodes:\nHTTP Request to request and send files from/to web resources and APIs.\nRead/Write Files from Disk to read and write files from/to the machine where n8n is running.\nConvert to File to take input data and output it as a file.\nExtract From File to get data from a binary format and convert it to JSON.\nTo read or write a binary file, you need to write the path (location) of the file in the node's File(s) Selector parameter (for the Read operation) or in the node's File Path and Name parameter (for the Write operation).\nBinary Exercise 1\nFor our first binary exercise, let's convert a PDF file to JSON:\nMake an HTTP request to get this PDF file:\nUse the Extract From File node to convert the file from binary to JSON.\n??? note \"Show me the solution\"\nIn the **HTTP Request node**, you should see the PDF file, like this:\n<figure><img src=\"/_images/courses/level-two/chapter-two/exercise_binarydata_httprequest_file.png\" alt=\"HTTP Request node to get PDF\" style=\"width:100%\"><figcaption align = \"center\"><i>HTTP Request node to get PDF</i></figcaption></figure>\nWhen you convert the PDF from binary to JSON using the **Extract From File node**, the result should look like this:\n<figure><img src=\"/_images/courses/level-two/chapter-two/exercise_binarydata_movedata_btoj.png\" alt=\"Extract From File node\" style=\"width:100%\"><figcaption align = \"center\"><i>Extract From File node</i></figcaption></figure>\nTo check the configuration of the nodes, you can copy the JSON workflow code below and paste it into your Editor UI:\nBinary Exercise 2\nFor our second binary exercise, let's convert some JSON data to binary:\nMake an HTTP request to the Poetry DB API\nConvert the returned data from JSON to binary using the Convert to File node.\nWrite the new binary file data to the machine where n8n is running using the Read/Write Files From Disk node.\nTo check that it worked out, use the Read/Write Files From Disk node to read the generated binary file.\n??? note \"Show me the solution\"\nThe workflow for this exercise looks like this:\n<figure><img src=\"/_images/courses/level-two/chapter-two/exercise_binarydata.png\" alt=\"Workflow for moving JSON to binary data\" style=\"width:100%\"><figcaption align = \"center\"><i>Workflow for moving JSON to binary data</i></figcaption></figure>\nTo check the configuration of the nodes, you can copy the JSON workflow code below and paste it into your Editor UI:"
  },
  {
    "file_path": "courses\\level-two\\chapter-3.md",
    "content": "Merging and splitting data\nIn this chapter, you will learn how to merge and split data, and in what cases it might be useful to perform these operations.\nMerging data\nIn some cases, you might need to merge (combine) and process data from different sources.\nMerging data can involve:\nCreating one data set from multiple sources.\nSynchronizing data between multiple systems. This could include removing duplicate data or updating data in one system when it changes in another.\nIn n8n, you can merge data from two different nodes using the Merge node, which provides several merging options:\nAppend\nCombine\nMerge by Fields: requires input fields to match on\nMerge by Position\nCombine all possible combinations\nChoose Branch\nNotice that Combine > Merge by Fields requires you enter input fields to match on. These fields should contain identical values between the data sources so n8n can properly match data together. In the Merge node, they're called Input 1 Field and Input 2 Field.\nProperty Input fields in the Merge node\nMerge Exercise\nBuild a workflow that merges data from the Customer Datastore node and Code node.\nAdd a Merge node that takes Input 1 from a Customer Datastore node and Input 2 from a Code node.\nIn the Customer Datastore node, run the operation Get All People.\nIn the Code node, create an array of two objects with three properties: name, language, and country, where the property country has two sub-properties code and name.\nFill out the values of these properties with the information of two characters from the Customer Database.\nFor example, Jay Gatsby's language is English and country name is United States.\nIn the Merge node, try out different merge options.\n??? note \"Show me the solution\"\nThe workflow for this exercise looks like this:\n<figure><img src=\"/_images/courses/level-two/chapter-three/exercise_merge.png\" alt=\"Workflow exercise for merging data\" style=\"width:100%\"><figcaption align = \"center\"><i>Workflow exercise for merging data</i></figcaption></figure>\nIf you merge data with the option **Keep Matches** using the name as the input fields to match, the result should look like this (note this example only contains Jay Gatsby; yours might look different depending on which characters you selected):\n<figure><img src=\"/_images/courses/level-two/chapter-three/exercise_merge_kkm.png\" alt=\"Output of Merge node with option to keep matches\" style=\"width:100%\"><figcaption align = \"center\"><i>Output of Merge node with option to keep matches</i></figcaption></figure>\nTo check the configuration of the nodes, you can copy the JSON workflow code below and paste it into your Editor UI:\nLooping\nIn some cases, you might need to perform the same operation on each element of an array or each data item (for example sending a message to every contact in your address book). In technical terms, you need to iterate through the data (with loops).\nn8n generally handles this repetitive processing automatically, as the nodes run once for each item, so you don't need to build loops into your workflows.\nHowever, there are some exceptions of nodes and operations that will require you to build a loop into your workflow.\nTo create a loop in an n8n workflow, you need to connect the output of one node to the input of a previous node, and add an If node to check when to stop the loop.\nSplitting data in batches\nIf you need to process large volumes of incoming data, execute the Code node multiple times, or avoid API rate limits, it's best to split the data into batches (groups) and process these batches.\nFor these processes, use the Loop Over Items node. This node splits input data into a specified batch size and, with each iteration, returns a predefined amount of data.\nLoop/Batch Exercise\nBuild a workflow that reads the RSS feed from Medium and dev.to. The workflow should consist of three nodes:\nA Code node that returns the URLs of the RSS feeds of Medium () and dev.to ().\nA Loop Over Items node with Batch Size: 1, that takes in the inputs from the Code node and RSS Read node and iterates over the items.\nAn RSS Read node that gets the URL of the Medium RSS feed, passed as an expression: {{ $json.url }}.\nThe RSS Read node is one of the exception nodes which processes only the first item it receives, so the Loop Over Items node is necessary for iterating over multiple items.\n??? note \"Show me the solution\"\n1. Add a **Code Node**. You can format the code in several ways, one way is:\n- Set **Mode** to Run Once for All Items.\n- Set **Language** to JavaScript.\n- Copy the code below and paste it into the JavaScript Code editor:\n2. Add a **Loop Over Items node** connected to the **Code node**.\n- Set **Batch Size** to 1.\n3. The **Loop Over Items node** automatically adds a node called \"Replace Me\". Replace that node with an **RSS Read node**.\n- Set the **URL** to use the url from the Code Node: {{ $json.url }}.\nThe workflow for this exercise looks like this:\n<figure><img src=\"/_images/courses/level-two/chapter-three/exercise_splitinbatches.png\" alt=\"Workflow for getting RSS feeds from two blogs\" style=\"width:100%\"><figcaption align = \"center\"><i>Workflow for getting RSS feeds from two blogs</i></figcaption></figure>\nTo check the configuration of the nodes, you can copy the JSON workflow code below and paste it into your Editor UI:"
  },
  {
    "file_path": "courses\\level-two\\chapter-4.md",
    "content": "Dealing with errors in workflows\nSometimes you build a nice workflow, but it fails when you try to execute it. Workflow executions may fail for a variety of reasons, ranging from straightforward problems with incorrectly configuring a node or a failure in a third-party service to more mysterious errors.\nBut don't panic. In this lesson, you'll learn how you can troubleshoot errors so you can get your workflow up and running as soon as possible.\nChecking failed workflows\nn8n tracks executions of your workflows.\nWhen one of your workflows fails, you can check the Executions log to see what went wrong. The Executions log shows you a list of the latest execution time, status, mode, and running time of your saved workflows.\nOpen the Executions log by selecting Executions in the left-side panel.\nTo investigate a specific failed execution from the list, select the name or the View button that appears when you hover over the row of the respective execution.\nExecutions log\nThis will open the workflow in read-only mode, where you can see the execution of each node. This representation can help you identify at what point the workflow ran into issues.\nTo toggle between viewing the execution and the editor, select the Editor | Executions button at the top of the page.\nWorkflow execution view\nCatching erroring workflows\nTo catch failed workflows, create a separate Error Workflow with the Error Trigger node. This workflow will only execute if the main workflow execution fails.\nUse additional nodes in your Error Workflow that make sense, like sending notifications about the failed workflow and its errors using email or Slack.\nTo receive error messages for a failed workflow, set the Error Workflow in the Workflow Settings to an Error Workflow that uses an Error Trigger node.\nThe only difference between a regular workflow and an Error Workflow is that the latter contains an Error Trigger node. Make sure to create this node before you set this as another workflow's designated Error Workflow.\nExercise\nIn the previous chapters, you've built several small workflows. Now, pick one of them that you want to monitor and create an Error Workflow for it:\nCreate a new Error Workflow.\nAdd the Error Trigger node.\nConnect a node for the communication platform of your choice to the Error Trigger node, like Slack, Discord, Telegram, or even Gmail or a more generic Send Email.\nIn the workflow you want to monitor, open the Workflow Settings and select the new Error Workflow you just created. Note that this workflow needs to run automatically to trigger the error workflow.\n??? note \"Show me the solution\"\nThe workflow for this exercise looks like this:\n<figure><img src=\"/_images/courses/level-two/chapter-four/exercise_errors_errortriggernode_workflow.png\" alt=\"\" style=\"width:100%\"><figcaption align = \"center\"><i>Error workflow</i></figcaption></figure>\nTo check the configuration of the nodes, you can copy the JSON workflow code below and paste it into your Editor UI:\nThrowing exceptions in workflows\nAnother way of troubleshooting workflows is to include a Stop and Error node in your workflow. This node throws an error. You can specify the error type:\nError Message: returns a custom message about the error\nError Object: returns the type of error\nYou can only use the Stop and Error node as the last node in a workflow."
  },
  {
    "file_path": "courses\\level-two\\chapter-6.md",
    "content": "Test your knowledge\nCongratulations, you finished the n8n Course Level 2!\nYou've learned a lot about workflow automation and built quite a complex business workflow. Why not showcase your skills?\nYou can test your knowledge by taking a quiz, which consists of questions about the theoretical concepts and workflows covered in this course.\nYou need to have at least 80% correct answers to pass the quiz.\nYou can take the quiz as many times as you want.\nThere's no time limit on answering the quiz questions.\nTake the quiz!{ .md-button }\nWhat's next?\nCreate new workflows for your work or personal use and share them with us. Don't have any ideas? Find inspiration on the workflows page and on our blog.\nDive deeper into n8n's features by reading the docs."
  },
  {
    "file_path": "courses\\level-two\\index.md",
    "content": "Level two: Introduction\nWelcome to the n8n Course Level 2!\nIs this course right for me?\nThis course is for you if you:\nWant to automate somewhat complex business processes.\nWant to dive deeper into n8n after taking the Level 1 course.\nWhat will I learn in this course?\nThe focus in this course is on working with data. You will learn how to:\nUse the data structure of n8n correctly.\nProcess different data types (for example, XML, HTML, date, time, and binary data).\nMerge data from different sources (for example, a database, spreadsheet, or CRM).\nUse functions and JavaScript code in the Code node.\nDeal with error workflows and workflow errors.\nYou will learn all this by completing short practical exercises after the theoretical explanations and building a business workflow following instructions.\nWhat do I need to get started?\nTo follow along this course (at a comfortable pace) you will need the following:\nn8n set up: You can use the self-hosted version or n8n Cloud.\nA user ID: Sign up here to get your unique ID and other credentials you will need in the course.\nBasic n8n skills: We strongly recommend taking the Level 1 course before this one.\nBasic JavaScript understanding\nHow long does the course take?\nCompleting the course should take around two hours. You don't have to complete it in one go; feel free to take breaks and resume whenever you are ready.\nHow do I complete the course?\nThere are two milestones in this course that test your knowledge of what you have learned in the lessons:\n[x] Building the main workflow\n[x] Passing the quiz at the end of the course\nYou can always check your progress throughout the course by entering your unique ID here.\nIf you successfully complete the milestones above, you will get a badge and an avatar in your forum profile. You can then share your profile and course verification ID to showcase your n8n skills to others.\nLet's get started!{ .md-button }"
  },
  {
    "file_path": "courses\\level-two\\chapter-5\\chapter-5.0.md",
    "content": "Automating a business workflow\nRemember our friend Nathan?\nNathan üôã: Hello, it's me again. My manager was so impressed with my first workflow automation solution that she entrusted me with more responsibility.\nYou üë©‚Äçüîß: More work and responsibility. Congratulations, I guess. What do you need to do now?\nNathan üôã: I got access to all our sales data and I'm now responsible for creating two reports: one for regional sales and one for orders prices. They're based on data from different sources and come in different formats.\nYou üë©‚Äçüîß: Sounds like a lot of manual work, but the kind that can be automated. Let's do it!\nWorkflow design\nNow that we know what Nathan wants to automate, let's list the steps he needs to take to achieve this:\nGet and combine data from all necessary sources.\nSort the data and format the dates.\nWrite binary files.\nSend notifications using email and Discord.\nn8n provides core nodes for all these steps. This use case is somewhat complex. We should build it from three separate workflows:\nA workflow that merges the company data with external information.\nA workflow that generates the reports.\nA workflow that monitors errors in the second workflow.\nWorkflow prerequisites\nTo build the workflows, you will need the following:\nAn Airtable account and credentials.\nA Google account and credentials to access Gmail.\nA Discord account and webhook URL (you receive this using email when you sign up for this course).\nNext, you will build these three workflows with step-by-step instructions."
  },
  {
    "file_path": "courses\\level-two\\chapter-5\\chapter-5.1.md",
    "content": "Workflow 1: Merging data\nNathan's company stores its customer data in Airtable. This data contains information about the customers' ID, country, email, and join date, but lacks data about their respective region and subregion. You need to fill in these last two fields in order to create the reports for regional sales.\nTo accomplish this task, you first need to make a copy of this table in your Airtable account:\nNext, build a small workflow that merges data from Airtable and a REST Countries API:\nUse the Airtable node to list the data in the Airtable table named customers.\nUse the HTTP Request node to get data from the REST Countries API:  and send the query parameter name fields with the value name,region,subregion. This will return data about world countries, split out into separate items.\nUse the Merge node to merge data from Airtable and the Countries API by country name, represented as customerCountry in Airtable and name.common in the Countries API, respectively.\nUse another Airtable node to update the fields region and subregion in Airtable with the data from the Countries API.\nThe workflow should look like this:\nWorkflow 1 for merging data from Airtable and the Countries API"
  },
  {
    "file_path": "courses\\level-two\\chapter-5\\chapter-5.2.md",
    "content": "Workflow 2: Generating reports\nIn this workflow, you will merge data from different sources, transform binary data, generate files, and send notifications about them. The final workflow should look like this:\nWorkflow 2 for aggregating data and generating files\nTo make things easier, let's split the workflow into three parts.\nPart 1: Getting data from different sources\nThe first part of the workflow consists of five nodes:\nWorkflow 1: Getting data from different sources\nUse the HTTP Request node to get data from the API endpoint that stores company data. Configure the following node parameters:\nMethod: Get\nURL: The Dataset URL you received in the email when you signed up for this course.\nAuthentication: Generic Credential Type\nGeneric Auth Type: Header Auth\nCredentials for Header Auth: The Header Auth name and Header Auth value you received in the email when you signed up for this course.\nSend Headers: Toggle to true\nSpecify Headers: Select Using Fields Below\nName: unique_id\nValue: The unique ID you received in the email when you signed up for this course.\nUse the Airtable node to list data from the customers table (where you updated the fields region and subregion).\nUse the Merge node to merge data from the Airtable and HTTP Request node, based on matching the input fields for customerID.\nUse the Sort node to sort data by orderPrice in descending order.\nPart 2: Generating file for regional sales\nThe second part of the workflow consists of four nodes:\nWorkflow 2: Generating file for regional sales\nUse the If node to filter to only display orders from the region Americas.\nUse the Convert to File to transform the incoming data from JSON to binary format. Convert each item to a separate file. (Bonus points if you can figure out how to name each report based on the orderID!)\nUse the Gmail node (or another email node) to send the files using email to an address you have access to. Note that you need to add an attachment with the data property.\nUse the Discord node to send a message in the n8n Discord channel #course-level-two. In the node, configure the following parameters:\nWebhook URL: The Discord URL you received in the email when you signed up for this course.\nText: \"I sent the file using email with the label ID {label ID}. My ID: \" followed by the unique ID emailed to you when you registered for this course.  Note that you need to replace the text in curly braces {} with expressions that reference the data from the nodes.\nPart 3: Generating files for total sales\nThe third part of the workflow consists of five nodes:\nWorkflow 3: Generating files for total sales\nUse the Loop Over Items node to split data from the Item Lists node into batches of 5.\nUse the Set node to set four values, referenced with expressions from the previous node: customerEmail, customerRegion, customerSince, and orderPrice.\nUse the Date & Time node to change the date format of the field customerSince to the format MM/DD/YYYY.\nSet the Include Input Fields option to keep all the data together.\nUse the Convert to File node to create a CSV spreadsheet with the file name set as the expression: {{$runIndex > 0 ? 'file_low_orders':'file_high_orders'}}.\nUse the Discord node to send a message in the n8n Discord channel #course-level-two. In the node, configure the following parameters:\nWebhook URL: The Discord URL you received in the email when you signed up for this course.\nText: \"I created the spreadsheet {file name}. My ID:\" followed by the unique ID emailed to you when you registered for this course.  Note that you need to replace {file name} with an expression that references data from the previous Convert to File node.\n??? note \"Show me the solution\"\nTo check the configuration of the nodes, you can copy the JSON workflow code below and paste it into your Editor UI:"
  },
  {
    "file_path": "courses\\level-two\\chapter-5\\chapter-5.3.md",
    "content": "Workflow 3: Monitoring workflow errors\nLast but not least, let's help Nathan know if there are any errors running the workflow.\nTo accomplish this task, create an Error workflow that monitors the main workflow:\nCreate a new workflow.\nAdd an Error Trigger node (and execute it as a test).\nConnect a Discord node to the Error Trigger node and configure these fields:\nWebhook URL: The Discord URL that you received in the email from n8n when you signed up for this course.\nText: \"The workflow {workflow name} failed, with the error message: {execution error message}. Last node executed: {name of the last executed node}. Check this workflow execution here: {execution URL} My Unique ID: \" followed by the unique ID emailed to you when you registered for this course.\nNote that you need to replace the text in curly brackets {} with expressions that take the respective information from the Error Trigger node.\nExecute the Discord node.\nSet the newly created workflow as the Error Workflow for the main workflow you created in the previous lesson.\nThe workflow should look like this:\nWorkflow 3 for monitoring workflow errors"
  },
  {
    "file_path": "credentials\\add-edit-credentials.md",
    "content": "Create and edit credentials\nCredentials are securely stored authentication information used to connect n8n workflows to external services such as APIs, or databases.\nCreate a credential\nSelect the¬†!universal create resource icon{.off-glb}¬†button¬†in the upper-left corner of the side menu. Select credential.\nIf your n8n instance supports projects, you'll also need to choose whether to create the credential inside your personal space or a specific project you have access to. If you're using the community version, you'll create the credential inside your personal space.\nSelect the app or service you wish to connect to.\nOr:\nUsing the !universal create resource icon{.off-glb} Create¬†button in the upper-right corner from either the Overview page or a specific project. Select¬†Credential.\nIf you're doing this from the Overview page, you'll create the credential inside your personal space. If you're doing this from inside a project, you'll create the credential inside that specific project.\nSelect the app or service you wish to connect to.\nYou can also create new credential in the credential drop down when editing a node on the workflow editor.\nOnce in the credential modal, enter the details required by your service. Refer to your service's page in the credentials library for guidance.\nWhen you save a credential, n8n tests it to confirm it works.\nExpressions in credentials\nYou can use expressions to set credentials dynamically as your workflow runs:\nIn your workflow, find the data path containing the credential. This varies depending on the exact parameter names in your data. Make sure that the data containing the credential is available in the workflow when you get to the node that needs it.\nWhen creating your credential, hover over the field where you want to use an expression.\nToggle Expression on.\nEnter your expression.\nExample workflow\nUsing the example"
  },
  {
    "file_path": "credentials\\credential-sharing.md",
    "content": "Credential sharing\nYou can share a credential directly with other users to use in their own workflows. Or share a credential in a project for all members of that project to use. Any users using a shared credential won't be able to view or edit the credential details.\nUsers can share credentials they created and own. Only project admins can share credentials created in and owned by a project. Instance owners and instance admins can view and share all credentials on an instance.\nRefer to Account types for more information about owners and admins.\nIn projects, a user's role controls how they can interact with the workflows and credentials associated to the projects they're a member of.\nShare a credential\nTo share a credential:\nFrom the left menu, select either Overview or a project.\nSelect Credentials to see a list of your credentials.\nSelect the credential you want to share.\nSelect Sharing.\nIn the Share with projects or users dropdown, browse or search for the user or project with which you want to share your credentials.\nSelect a user or project.\nSelect Save to apply the changes.\nRemove access to a credential\nTo unshare a credential:\nFrom the left menu, select either Overview or a project.\nSelect Credentials to see a list of your credentials.\nSelect the credential you want to unshare.\nSelect Sharing.\nSelect trash icon!Trash icon{.off-glb} on the user or project you want to remove from the list of shared users and projects.\nSelect Save to apply the changes."
  },
  {
    "file_path": "credentials\\index.md",
    "content": "Credentials\nCredentials are private pieces of information issued by apps and services to authenticate you as a user and allow you to connect and share information between the app or service and the n8n node.\nAccess the credentials UI by opening the left menu and selecting Credentials. n8n lists credentials you created on the My credentials tab. The All credentials tab shows all credentials you can use, included credentials shared with you by other users.\nCreate and edit credentials.\nLearn about credential sharing.\nFind information on setting up credentials for your services in the credentials library."
  },
  {
    "file_path": "data\\binary-data.md",
    "content": "Binary data\nBinary data is any file-type data, such as image files or documents.\nThis page collects resources relating to binary data in n8n.\nWorking with binary data in your workflows\nYou can process binary data in n8n workflows. n8n provides nodes to help you work with binary data. You can also use code.\nNodes\nThere are three key nodes dedicated to handling binary data files:\nRead/Write Files from Disk to read and write files from/to the machine where n8n is running.\nConvert to File to take input data and output it as a file.\nExtract From File to get data from a binary format and convert it to JSON.\nThere are separate nodes for working with XML and HTML data:\nHTML\nXML\nAnd nodes for performing common tasks:\nCompression\nEdit Image\nFTP\nYou can trigger a workflow based on changes to a local file using the Local File trigger.\nTo split or concatenate binary data items, use the data transformation nodes.\nCode\nYou can use the Code node to manipulate binary data in your workflows. For example, Get the binary data buffer: get the binary data available in your workflow.\nConfigure binary data mode when self-hosting\nYou can configure how your self-hosted n8n instance handles binary data using the Binary data environment variables. This includes tasks such as setting the storage path and choosing how to store binary data.\nYour configuration affects how well n8n scales: Scaling | Binary data filesystem mode.\nReading and writing binary files can have security implications. If you want to disable reading and writing binary data, use the NODES_EXCLUDE environment variable. Refer to Environment variables | Nodes for more information."
  },
  {
    "file_path": "data\\code.md",
    "content": "Processing data with code\nFunction\nA function is a block of code designed to perform a certain task. In n8n, you can write custom JavaScript or Python code snippets to add, remove, and update the data you receive from a node.\nThe Code node gives you access to the incoming data and you can manipulate it. With this node you can create any function you want using JavaScript code."
  },
  {
    "file_path": "data\\data-editing.md",
    "content": "Data editing\nn8n allows you to edit pinned data. This means you can check different scenarios without setting up each scenario and sending the relevant data from your external system. It makes it easier to test edge cases.\nEdit output data\nTo edit output data:\nRun the node to load data.\nIn the OUTPUT view, select JSON to switch to JSON view.\nSelect Edit !Edit data icon{.off-glb}.\nEdit your data.\nSelect Save. n8n saves your data changes and pins your data.\nUse data from previous executions\nYou can copy data from nodes in previous workflow executions:\nOpen the left menu.\nSelect Executions.\nBrowse the workflow executions list to find the one with the data you want to copy.\nSelect Open Past Execution !Open past execution icon{.off-glb}.\nDouble click the node whose data you want to copy.\nIf it's table layout, select JSON to switch to JSON view.\nThere are two ways to copy the JSON:\nSelect the JSON you want by highlighting it, like selecting text. Then use ctrl + c to copy it.\nSelect the JSON you want to copy by clicking on a parameter. Then:\nHover over the JSON. n8n displays the Copy !Copy data icon{.off-glb} button.\nSelect Copy !Copy data icon{.off-glb}.\nYou can choose what to copy:\nCopy Item Path and Copy Parameter Path gives you expressions that access parts of the JSON.\nCopy Value: copies the entire selected JSON.\nReturn to the workflow you're working on:\nOpen the left menu.\nSelect Workflows.\nSelect Open.\nSelect the workflow you want to open.\nOpen the node where you want to use the copied data.\nIf there is no data, run the node to load data.\nIn the OUTPUT view, select JSON to switch to JSON view.\nSelect Edit !Edit data icon{.off-glb}.\nPaste in the data from the previous execution.\nSelect Save. n8n saves your data changes and pins your data."
  },
  {
    "file_path": "data\\data-filtering.md",
    "content": "Data filtering\nSearch and filter data in the node INPUT and OUTPUT panels. Use this to check your node's data.\nTo search:\nIn a node, select Search !Search icon{.off-glb} in the INPUT or OUTPUT panel.\nEnter your search term.\nn8n filters as you type your search, displaying the objects or rows containing the term.\nFiltering is purely visual: n8n doesn't change or delete data. The filter resets when you close and reopen the node."
  },
  {
    "file_path": "data\\data-mocking.md",
    "content": "Data mocking\nData mocking is simulating or faking data. It's useful when developing a workflow. By mocking data, you can:\nAvoid making repeated calls to your data source. This saves time and costs.\nWork with a small, predictable dataset during initial development.\nAvoid the risk of overwriting live data: in the early stages of building your workflow, you don't need to connect your real data source.\nMocking with real data using data pinning\nUsing data pinning, you load real data into your workflow, then pin it in the output panel of a node. Using this approach you have realistic data, with only one call to your data source. You can edit pinned data.\nUse this approach when you need to configure your workflow to handle the exact data structure and parameters provided by your data source.\nGenerate custom data using the Code or Edit Fields nodes\nYou can create a custom dataset in your workflow using either the Code node or the Edit Fields (Set) node.\nIn the Code node, you can create any data set you want, and return it as the node output. In the Edit Fields node, select Add fields to add your custom data.\nThe Edit Fields node is a good choice for small tests. To create more complex datasets, use the Code node.\nOutput a sample data set from the Customer Datastore node\nThe Customer Datastore node provides a fake dataset to work with. Add and execute the node to explore the data.\nUse this approach if you need some test data when exploring n8n, and you don't have a real use-case to work with."
  },
  {
    "file_path": "data\\data-pinning.md",
    "content": "Data pinning\nYou can 'pin' data during workflow development. Data pinning means saving the output data of a node, and using the saved data instead of fetching fresh data in future workflow executions.\nYou can use this when working with data from external sources to avoid having to repeat requests to the external system. This can save time and resources:\nIf your workflow relies on an external system to trigger it, such as a webhook call, being able to pin data means you don't need to use the external system every time you test the workflow.\nIf the external resource has data or usage limits, pinning data during tests avoids consuming your resource limits.\nYou can fetch and pin the data you want to test, then have confidence that the data is consistent in all your workflow tests.\nYou can only pin data for nodes that have a single main output (\"error\" outputs don't count for this purpose).\nPin data\nUnpin data\nWhen data pinning is active, a banner appears at the top of the node's output panel indicating that n8n has pinned the data. To unpin data and fetch fresh data on the next execution, select the Unpin link in the banner."
  },
  {
    "file_path": "data\\data-structure.md",
    "content": "Data structure\nIn n8n, all data passed between nodes is an array of objects. It has the following structure:\nData item processing"
  },
  {
    "file_path": "data\\index.md",
    "content": "Data\nData is the information that n8n nodes receive and process. For basic usage of n8n you don't need to understand data structures and manipulation. However, it becomes important if you want to:\nCreate your own node\nWrite custom expressions\nUse the Function or Function Item node\nThis section covers:\nData structure\nData flow within nodes\nTransforming data\nProcess data using code\nPinning and editing data during workflow development.\nData mapping and Item linking: how data items link to each other.\nRelated resources\nData transformation nodes\nn8n provides a collection of nodes to transform data:\nAggregate: take separate items, or portions of them, and group them together into individual items.\nLimit: remove items beyond a defined maximum number.\nRemove Duplicates: identify and delete items that are identical across all fields or a subset of fields.\nSort: organize lists of in a desired ordering, or generate a random selection.\nSplit Out: separate a single data item containing a list into multiple items.\nSummarize: aggregate items together, in a manner similar to Excel pivot tables."
  },
  {
    "file_path": "data\\schema-preview.md",
    "content": "Schema Preview\nSchema Preview exposes expected schema data from the previous node in the Node Editor without the user having to provide credentials or execute the node. This makes it possible to construct workflows without having to provide credentials in advance. The preview doesn't include mock data, but it does expose the expected fields, making it possible to select and incorporate them into the input of subsequent nodes.\nUsing the preview\nThere must be a node with Schema Preview available in your workflow.\nWhen clicking on the details of the next node in the sequence, the Schema Preview data will show up in the Node Editor where schema data would typically be exposed.\nUse data from the Schema Preview just as you would other schemas - drag and drop fields as input into your node parameters and settings."
  },
  {
    "file_path": "data\\transforming-data.md",
    "content": "Transforming data\nn8n uses a predefined data structure that allows all nodes to process incoming data correctly.\nYour incoming data may have a different data structure, in which case you will need to transform it to allow each item to be processed individually.\nFor example, the image below shows the output of an HTTP Request node that returns data incompatible with n8n's data structure. The node returns the data and displays that only one item was returned.\n!HTTP Request node output\nTo transform this kind of structure into the n8n data structure you can use the data transformation nodes:\nAggregate: take separate items, or portions of them, and group them together into individual items.\nLimit: remove items beyond a defined maximum number.\nRemove Duplicates: identify and delete items that are identical across all fields or a subset of fields.\nSort: organize lists of in a desired ordering, or generate a random selection.\nSplit Out: separate a single data item containing a list into multiple items.\nSummarize: aggregate items together, in a manner similar to Excel pivot tables."
  },
  {
    "file_path": "data\\data-mapping\\data-mapping-expressions.md",
    "content": "Mapping in the expressions editor\nThese examples show how to access linked items in the expressions editor. Refer to expressions for more information on expressions, including built in variables and methods.\nFor information on errors with mapping and linking items, refer to Item linking errors.\nAccess the linked item in a previous node's output\nWhen you use this, n8n works back up the item linking chain, to find the parent item in the given node.\nAs a longer example, consider a scenario where a node earlier in the workflow has the following output data:\nTo extract the name, use the following expression:\nAccess the linked item in the current node's input\nIn this case, the item linking is within the node: find the input item that the node links to an output item.\nAs a longer example, consider a scenario where the current node has the following input data:\nTo extract the name, you'd normally use drag-and-drop Data mapping, but you could also write the following expression:"
  },
  {
    "file_path": "data\\data-mapping\\data-mapping-ui.md",
    "content": "Mapping in the UI\nData mapping means referencing data from previous nodes. It doesn't include changing (transforming) data, just referencing it.\nYou can map data in the following ways:\nUsing the expressions editor.\nBy dragging and dropping data from the INPUT into parameters. This generates the expression for you.\nFor information on errors with mapping and linking items, refer to Item linking errors.\nHow to drag and drop data\nRun your workflow to load data.\nOpen the node where you need to map data.\nYou can map in table, JSON, and schema view:\nIn table view: click and hold a table heading to map top level data, or a field in the table to map nested data.\nIn JSON view: click and hold a key.\nIn schema view: click and hold a key.\nDrag the item into the field where you want to use the data.\nUnderstand what you're mapping with drag and drop\nData mapping maps the key path, and loads the key's value into the field. For example, given the following data:\nYou can map fruit by dragging and dropping fruit from the INPUT into the field where you want to use its value. This creates an expression, {{ $json.fruit }}. When the node iterates over input items, the value of the field becomes the value of fruit for each item.\nUnderstand nested data\nGiven the following data:\nn8n displays it in table form like this:\n!\"Screenshot of a table in the INPUT panel. It includes a top level field named \"nested.\" This field contains nested data, which is indicated in bold.\""
  },
  {
    "file_path": "data\\data-mapping\\index.md",
    "content": "Data mapping\nData mapping means referencing data from previous nodes.\nThis section contains guidance on:\nMapping data in most scenarios: Data mapping in the UI and Data mapping in expression\nHow to handle item linking when using the Code node or building your own nodes."
  },
  {
    "file_path": "data\\data-mapping\\data-item-linking\\index.md",
    "content": "Data item linking\nAn item is a single piece of data. Nodes receive one or more items, operate on them, and output new items. Each item links back to previous items.\nYou need to understand this behavior if you're:\nBuilding a programmatic-style node that implements complex behaviors with its input and output data.\nUsing the Code node or expressions editor to access data from earlier items in the workflow.\nUsing the Code node for complex behaviors with input and output data.\nThis section provides:\nA conceptual overview of Item linking concepts.\nInformation on Item linking for node creators.\nSupport for end users who need to Work with the data path to retrieve item data from previous nodes, and link items when using the Code node.\nGuidance on troubleshooting Errors."
  },
  {
    "file_path": "data\\data-mapping\\data-item-linking\\item-linking-concepts.md",
    "content": "Item linking concepts\nEach output item created by a node includes metadata that links them to the input item (or items) that the node used to generate them. This creates a chain of items that you can work back along to access previous items. This can be complicated to understand, especially if the node splits or merges data. You need to understand item linking when building your own programmatic nodes, or in some scenarios using the Code node.\nThis document provides a conceptual overview of this feature. For usage details, refer to:\nItem linking for node creators, for details on how to handle item linking when building a node.\nItem linking in the Code node, to learn how to handle item linking in the Code node.\nItem linking errors, to understand the errors you may encounter in the editor UI.\nn8n's automatic item linking\nIf a node doesn't control how to link input items to output items, n8n tries to guess how to link the items automatically:\nSingle input, single output: the output links to the input.\nSingle input, multiple outputs: all outputs link to that input.\nMultiple inputs and outputs:\nIf you keep the input items, but change the order (or remove some but keep others), n8n can automatically add the correct linked item information.\nIf the number of inputs and outputs is equal, n8n links the items in order. This means that output-1 links to input-1, output-2 to input-2, and so on.\nIf the number isn't equal, or you create completely new items, n8n can't automatically link items.\nIf n8n can't link items automatically, and the node doesn't handle the item linking, n8n displays an error. Refer to Item linking errors for more information.\nItem linking example\n!A diagram showing the threads linking multiple items back through a workflow\nIn this example, it's possible for n8n to link an item in one node back several steps, despite the item order changing. This means the node that sorts movies alphabetically can access information about the linked item in the node that gets famous movie actors.\nThe methods for accessing linked items are different depending on whether you're using the UI, expressions, or the code node. Explore the following resources:\nMapping in the UI\nMapping in the expressions editor\nItem linking in the Code node\nItem linking errors"
  },
  {
    "file_path": "data\\data-mapping\\data-item-linking\\item-linking-errors.md",
    "content": "Item linking errors\nIn n8n you can reference data from any previous node. This doesn't have to be the node just before: it can be any previous node in the chain. When referencing nodes further back, you use the expression syntax $(node_name).item.\n!A diagram showing the threads linking multiple items back through a workflow\nDiagram of threads for different items. Due to the item linking, you can get the actor for each movie using $('Get famous movie actors').item.\nSince the previous node can have multiple items in it, n8n needs to know which one to use. When using .item, n8n figures this out for you behind the scenes. Refer to Item linking concepts for detailed information on how this works.\n.item fails if information is missing. To figure out which item to use, n8n maintains a thread back through the workflow's nodes for each item. For a given item, this thread tells n8n which items in previous nodes generated it. To find the matching item in a given previous node, n8n follows this thread back until it reaches the node in question.\nWhen using .item, n8n displays an error when:\nThe thread is broken\nThe thread points to more than one item in the previous node (as it's unclear which one to use)\nTo solve these errors, you can either avoid using .item, or fix the root cause.\nYou can avoid .item by using .first(), .last() or .all()[index] instead. They require you to know the position of the item that you‚Äôre targeting within the target node's output items. Refer to Built in methods and variables TABLE_PLACEHOLDER_0- Reference a different node that contains the same information, but doesn't have multiple matching items."
  },
  {
    "file_path": "embed\\configuration.md",
    "content": "Configuration\nAuthentication\nYou can secure n8n by setting up User management, n8n's built-in authentication feature.\nn8n supports LDAP and SAML.\nCredential overwrites\nTo offer OAuth login to users, it's possible to overwrite credentials on a global basis. This credential data isn't visible to users but the backend uses it automatically.\nIn the Editor UI, n8n hides all overwritten fields by default. This means that users are able to authenticate using  OAuth by pressing the \"connect\" button on the credentials.\nn8n offers two ways to apply credential overwrites: using Environment Variable and using the REST API.\nUsing environment variables\nYou can set credential overwrites using environment variable by setting the CREDENTIALS_OVERWRITE_DATA to { CREDENTIAL_NAME: { PARAMETER: VALUE }}.\nUsing REST APIs\nThe recommended way is to load the data using a custom REST endpoint. Set the CREDENTIALS_OVERWRITE_ENDPOINT to a path under which this endpoint should be made available.\nFor example:\nActivate the endpoint by setting the environment variable in the environment n8n runs under:\nA JSON file with the credentials to overwrite is then needed. For example, a oauth-credentials.json file to overwrite credentials for Asana and GitHub could look like this:\nThen apply it to the instance by sending it using curl:\nEnvironment variables\nn8n has many environment variables you can configure. Here are the most relevant environment variables for your hosted solution:\nTABLE_PLACEHOLDER_0\nRegistering hooks\nYou can set hooks by loading the hooks script on the page. One way to do this is by creating a hooks file in the project and adding a script tag in your editor-ui/public/index.html file:\nFrontend hook files\nFrontend external hook files are regular JavaScript files which have the following format:\nFrontend hook functions\nYou can define multiple hook functions per hook. Each hook function is invoked with the following arguments arguments:\nstore: The Vuex store object. You can use this to change or get data from the store.\nmetadata: The object that contains any data provided by the hook. To see what's passed, search for the hook in the editor-ui package."
  },
  {
    "file_path": "embed\\deployment.md",
    "content": "Deployment\nSee the hosting documentation for detailed setup options.\nUser data\nn8n recommends that you follow the same or similar practices used internally for n8n Cloud: Save user data using Rook and, if an n8n server goes down, a new instance starts on another machine using the same data.\nDue to this, you don't need to use backups except in case of a catastrophic failure, or when a user wants to reactivate their account within your prescribed retention period (two weeks for n8n Cloud).\nBackups\nn8n recommends creating nightly backups by attaching another container, and copying all data to this second container. In this manner, RAM usage is negligible, and so doesn't impact the amount of users you can place on the server.\nRestarting\nIf your instance is down or restarting, missed executions (for example, Cron or Webhook nodes) during this time aren't recoverable. If it's important for you to maintain 100% uptime, you need to build another proxy in front of it which caches the data."
  },
  {
    "file_path": "embed\\index.md",
    "content": "n8n Embed\nn8n Embed is part of n8n's paid offering. Using Embed, you can white label n8n, or incorporate it in your software as part of your commercial product.\nFor more information about when to use Embed, as well as costs and licensing processes, refer to Embed on the n8n website.\nSupport\nThe community forum can help with various issues. If you are a current Embed customer, you can also contact n8n support, using the email provided when you bought the license.\nRussia and Belarus\nn8n Embed isn't available in Russia and Belarus. Refer to n8n's blog post Update on n8n cloud accounts in Russia and Belarus for more information."
  },
  {
    "file_path": "embed\\managing-workflows.md",
    "content": "Workflow management in Embed\nWhen managing an embedded n8n deployment, spanning across teams or organizations, you will likely need to run the same (or similar) workflows for multiple users. There are two available options for doing so:\nTABLE_PLACEHOLDER_0\nWorkflow per user\nThere are three general steps to follow:\nObtain the credentials for each user, and any additional parameters that may be required based on the workflow.\nCreate the n8n credentials for this user.\nCreate the workflow.\n1. Obtain user credentials\nHere you need to capture all credentials for any node/service this user must authenticate with, along with any additional parameters required for the particular workflow. The credentials and any parameters needed will depend on your workflow and what you are trying to do.\n2. Create user credentials\nAfter all relevant credential details have been obtained, you can proceed to create the relevant service credentials in n8n. This can be done using the Editor UI or API call.\nUsing the Editor UI\nFrom the menu select Credentials > New.\nUse the drop-down to select the Credential type to create, for example Airtable.\n!Create New Credentials drop-down\nIn the Create New Credentials modal, enter the corresponding credentials details for the user, and select the nodes that will have access to these credentials.\n!Create New Credentials modal\nClick Create to finish and save.\nUsing the API\nThe frontend API used by the Editor UI can also be called to achieve the same result. The API endpoint is in the format:\nFor example, to create the credentials in the Editor UI example above, the request would be:\nWith the request body:\nThe response will contain the ID of the new credentials, which you will use when creating the workflow for this user:\n3. Create the workflow\nBest practice is to have a ‚Äúbase‚Äù workflow that you then duplicate and customize for each new user with their credentials (and any other details).\nYou can duplicate and customize your template workflow using either the Editor UI or API call.\nUsing the Editor UI\nFrom the menu select Workflows > Open to open the template workflow to be duplicated.\nSelect Workflows > Duplicate, then enter a name for this new workflow and click Save.\n!Duplicate workflow\nUpdate all relevant nodes to use the credentials for this user (created above).\nSave this workflow set it to Active using the toggle in the top-right corner.\nUsing the API\nFetch the JSON of the template workflow using the endpoint:\nThe response will contain the JSON data of the selected workflow:\nSave the returned JSON data and update any relevant credentials and fields for the new user.\nCreate a new workflow using the updated JSON as the request body at endpoint:\nThe response will contain the ID of the new workflow, which you will use in the next step.\nLastly, activate the new workflow:\nPassing the additional value active in your JSON payload:\nSingle workflow\nThere are four steps to follow to implement this method:\nObtain the credentials for each user, and any additional parameters that may be required based on the workflow. See Obtain user credentials above.\nCreate the n8n credentials for this user. See Create user credentials above.\nCreate the workflow.\nCall the workflow as needed.\nCreate the workflow\nThe details and scope of this workflow will vary greatly according to the individual use case, however there are a few design implementations to keep in mind:\nThis workflow must be triggered by a Webhook node.\nThe incoming webhook call must contain the user‚Äôs credentials and any other workflow parameters required.\nEach node where the user‚Äôs credentials are needed should use an expression so that the node‚Äôs credential field reads the credential provided in the webhook call.\nSave and activate the workflow, ensuring the production URL is selected for the Webhook node. Refer to webhook node for more information.\nCall the workflow\nFor each new user, or for any existing user as may be needed, call the webhook defined as the workflow trigger and provide the necessary credentials (and any other workflow parameters)."
  },
  {
    "file_path": "embed\\prerequisites.md",
    "content": "Prerequisites\nThe requirements provided here are an example based on n8n Cloud and are for illustrative purposes only. Your requirements may vary depending on the number of users, workflows, and executions. Contact n8n for more information.\nTABLE_PLACEHOLDER_0\nCPU considerations\nn8n isn't CPU intensive so even small instances (of providers such as AWS and GCP) should be enough for most use cases. Usually, memory requirements supersede CPU requirements, so focus resources there when planning your infrastructure.\nDatabase considerations\nn8n uses its database to store credentials, past executions, and workflows.\nA core feature of n8n is the flexibility to choose a database. All the supported databases have different advantages and disadvantages, which you have to consider individually and pick the one that best suits your needs. By default n8n creates an SQLite database if no database exists at the given location.\nn8n recommends that every n8n instance have a dedicated database. This helps to prevent dependencies and potential performance degradation. If it isn't possible to provide a dedicated database for every n8n instance, n8n recommends making use of Postgres's schema feature.\nFor Postgres, the database must already exist on the DB-instance. The database user for the n8n process needs to have full permissions on all tables that they're using or creating. n8n creates and maintains the database schema.\nBest practices\nSSD storage.\nIn containerized cloud environments, ensure that the volume is persisted and mounted when stopping/starting a container. If not, all data is lost.\nIf using Postgres, don't use the tablePrefix configuration option. It will be deprecated in the near future.\nPay attention to the changelog of new versions and consider reverting migrations before downgrading.\nSet up at least the basic database security and stability mechanisms such as IP allow lists and backups.\nMemory considerations\nAn n8n instance doesn't typically require large amounts of available memory. For example an n8n Cloud instance at idle requires ~100MB. It's the nature of your workflows and the data being processed that determines your memory requirements.\nFor example, while most nodes just pass data to the next node in the workflow, the Code node creates a pre-processing and post-processing copy of the data. When dealing will large binary files, this can consume all available resources."
  },
  {
    "file_path": "embed\\white-labelling.md",
    "content": "White labelling\nWhite labelling n8n means customizing the frontend styling and assets to match your brand identity. The process involves changing two packages in n8n's source code github.com/n8n-io/n8n:\npackages/frontend/@n8n/design-system: n8n's storybook design system with CSS styles and Vue.js components\npackages/frontend/editor-ui: n8n's Vue.js frontend build with Vite.js\nPrerequisites\nYou need the following installed on your development machine:\nCreate a fork of n8n's repository and clone your new repository.\nInstall all dependencies, build and start n8n.\nWhenever you make changes you need to rebuild and restart n8n. While developing you can use npm run dev to automatically rebuild and restart n8n anytime you make code changes.\nTheme colors\nTo customize theme colors open packages/frontend/@n8n/design-system and start with:\npackages/frontend/@n8n/design-system/src/css/_tokens.scss\npackages/frontend/@n8n/design-system/src/css/_tokens.dark.scss\nAt the top of _tokens.scss you will find --color-primary variables as HSL colors:\nIn the following example the primary color changes to #0099ff. To convert to HSL you can use a color converter tool.\n!Example Theme Color Customization\nTheme logos\nTo change the editor‚Äôs logo assets look into packages/frontend/editor-ui/public and replace:\nfavicon-16x16.png\nfavicon-32x32.png\nfavicon.ico\nn8n-logo.svg\nn8n-logo-collapsed.svg\nn8n-logo-expanded.svg\nReplace these logo assets. n8n uses them in Vue.js components, including:\nMainSidebar.vue: top/left logo in the main sidebar.\nLogo.vue: reused in other components.\nIn the following example replace n8n-logo-collapsed.svg and n8n-logo-expanded.svg to update the main sidebar's logo assets.\n!Example Logo Main Sidebar\nIf your logo assets require different sizing or placement you can customize SCSS styles at the bottom of MainSidebar.vue.\nText localization\nTo change all text occurrences like n8n or n8n.io to your brand identity you can customize n8n's English internationalization file: packages/frontend/@n8n/i18n/src/locales/en.json.\nn8n uses the Vue I18n internationalization plugin for Vue.js to translate the majority of UI texts. To search and replace text occurrences inside en.json you can use Linked locale messages.\nIn the following example add the _brand.name translation key to white label n8n's AboutModal.vue.\n!Example About Modal Localization\nWindow title\nTo change n8n's window title to your brand name, edit the following:\npackages/frontend/editor-ui/index.html\npackages/frontend/editor-ui/src/composables/useDocumentTitle.ts\nThe following example replaces all occurrences of n8n and n8n.io with My Brand in index.html and useDocumentTitle.ts.\n!Example Window Title Localization"
  },
  {
    "file_path": "embed\\workflow-templates.md",
    "content": "Workflow templates\nn8n provides a library of workflow templates. When embedding n8n, you can:\nContinue to use n8n's workflow templates library (this is the default behavior)\nDisable workflow templates\nCreate your own workflow templates library\nDisable workflow templates\nUse your own workflow templates library\nAdd your workflows to the n8n library"
  },
  {
    "file_path": "flow-logic\\error-handling.md",
    "content": "Error handling\nWhen designing your flow logic, it's a good practice to consider potential errors, and set up methods to handle them gracefully. With an error workflow, you can control how n8n responds to a workflow execution failure.\nCreate and set an error workflow\nFor each workflow, you can set an error workflow in Workflow Settings. It runs if an execution fails. This means you can, for example, send email or Slack alerts when a workflow execution errors. The error workflow must start with the Error Trigger.\nYou can use the same error workflow for multiple workflows.\nError data\nCause a workflow execution failure using Stop And Error\nWhen you create and set an error workflow, n8n runs it when an execution fails. Usually, this is due to things like errors in node settings, or the workflow running out of memory.\nYou can add the Stop And Error node to your workflow to force executions to fail under your chosen circumstances, and trigger the error workflow."
  },
  {
    "file_path": "flow-logic\\execution-order.md",
    "content": "Execution order in multi-branch workflows\nn8n's node execution order depends on the version of n8n you're using:\nFor workflows created before version 1.0: n8n executes the first node of each branch, then the second node of each branch, and so on.\nFor workflows created in version 1.0 and above: executes each branch in turn, completing one branch before starting another. n8n orders the branches based on their position on the canvas, from topmost to bottommost. If two branches are at the same height, the leftmost branch executes first.\nYou can change the execution order in your workflow settings."
  },
  {
    "file_path": "flow-logic\\index.md",
    "content": "Flow logic\nn8n allows you to represent complex logic in your workflows.\nThis section covers:\nRelated sections\nYou need some understanding of Data in n8n, including Data structure and Data flow within nodes.\nWhen building your logic, you'll use n8n's Core nodes, including:\nSplitting: IF and Switch.\nMerging: Merge, Compare Datasets, and Code.\nLooping: IF and Loop Over Items.\nWaiting: Wait.\nCreating sub-workflows: Execute Workflow and Execute Workflow Trigger.\nError handling: Stop And Error and Error Trigger."
  },
  {
    "file_path": "flow-logic\\looping.md",
    "content": "Looping in n8n\nLooping is useful when you want to process multiple items or perform an action repeatedly, such as sending a message to every contact in your address book. n8n handles this repetitive processing automatically, meaning you don't need to specifically build loops into your workflows. There are some nodes where this isn't true.\nUsing loops in n8n\nn8n nodes take any number of items as input, process these items, and output the results. You can think of each item as a single data point, or a single row in the output table of a node.\n!The Customer Datastore node output\nNodes usually run once for each item. For example, if you wanted to send the name and notes of the customers in the Customer Datastore node as a message on Slack, you would:\nConnect the Slack node to the Customer Datastore node.\nConfigure the parameters.\nExecute the node.\nYou would receive five messages: one for each item.\nThis is how you can process multiple items without having to explicitly connect nodes in a loop.\nExecuting nodes once\nFor situations where you don't want a node to process all received items, for example sending a Slack message only to the first customer, you can do so by toggling the Execute Once parameter in the Settings tab of that node This setting is helpful when the incoming data contains multiple items and you want to only process the first one.\nCreating loops\nn8n typically handles the iteration for all incoming items. However, there are certain scenarios where you will have to create a loop to iterate through all items. Refer to Node exceptions for a list of nodes that don't automatically iterate over all incoming items.\nLoop until a condition is met\nTo create a loop in an n8n workflow, connect the output of one node to the input of a previous node. Add an IF node to check when to stop the loop.\nHere is an example workflow that implements a loop with an IF node:\n!Editor UI view of sample workflow\nLoop until all items are processed\nUse the Loop Over Items node when you want to loop until all items are processed. To process each item individually, set Batch Size to 1.\nYou can batch the data in groups and process these batches. This approach is useful for avoiding API rate limits when processing large incoming data or when you want to process a specific group of returned items.\nThe Loop Over Items node stops executing after all the incoming items get divided into batches and passed on to the next node in the workflow so it's not necessary to add an IF node to stop the loop.\nNode exceptions\nNodes and operations where you need to design a loop into your workflow:\nCrateDB executes once for insert and update.\nCode node in Run Once for All Items mode: processes all the items based on the entered code snippet.\nExecute Workflow node in Run Once for All Items mode.\nHTTP Request: you must handle pagination yourself. If your API call returns paginated results you must create a loop to fetch one page at a time.\nMicrosoft SQL executes once for insert, update, and delete.\nMongoDB executes once for insert and update.\nQuestDB executes once for insert.\nRedis:\nInfo: this operation executes only once, regardless of the number of items in the incoming data.\nRSS Read executes once for the requested URL.\nTimescaleDB executes once for insert and update."
  },
  {
    "file_path": "flow-logic\\merging.md",
    "content": "Merging data\nMerging brings multiple data streams together. You can achieve this using different nodes depending on your workflow requirements.\nMerge data from different data streams or nodes: Use the Merge node to combine data from various sources into one.\nMerge data from multiple node executions: Use the Code node for complex scenarios where you need to merge data from multiple executions of a node or multiple nodes.\nCompare and merge data: Use the Compare Datasets node to compare, merge, and output data streams based on the comparison.\nExplore each method in more detail in the sections below.\nMerge data from different data streams\nIf your workflow splits, you combine the separate streams back into one stream.\nHere's an example workflow showing different types of merging: appending data sets, keeping only new items, and keeping only existing items. The Merge node documentation contains details on each of the merge operations.\nMerge data from different nodes\nYou can use the Merge node to combine data from two previous nodes, even if the workflow hasn't split into separate data streams. This can be useful if you want to generate a single dataset from the data generated by multiple nodes.\n!Merging data from two previous nodes. The diagram shows three nodes lined up sequentially. The first node is labeled Fetch data, the second is labeled Modify data, and the third is labeled Merge: append both data sets. Arrows connect nodes 1 to 2, 2 to 3, and 1 to 3.\nMerging data from two previous nodes\nMerge data from multiple node executions\nUse the Code node to merge data from multiple node executions. This is useful in some Looping scenarios.\nRefer to this example workflow using Loop Over Items and Wait to artificially create multiple executions.\nCompare, merge, and split again\nThe Compare Datasets node compares data streams before merging them. It outputs up to four different data streams.\nRefer to this example workflow for an example."
  },
  {
    "file_path": "flow-logic\\splitting.md",
    "content": "Splitting workflows with conditional nodes\nSplitting uses the IF or Switch nodes. It turns a single-branch workflow into a multi-branch workflow. This is a key piece of representing complex logic in n8n.\nCompare these workflows:\n!\"Diagram representing two workflows. One has three steps and follows a linear process, with a user submitting a bug, and the workflow emailing a support team. The second workflow starts the same way, but then splits depending on whether the user marked the issue as urgent. It then splits again depending on the user's support plan\"\nThis is the power of splitting and conditional nodes in n8n.\nRefer to the IF or Switch documentation for usage details."
  },
  {
    "file_path": "flow-logic\\subworkflows.md",
    "content": "Sub-workflows\nYou can call one workflow from another workflow. This allows you to build modular, microservice-like workflows. It can also help if your workflow grows large enough to encounter memory issues. Creating sub-workflows uses the Execute Workflow and Execute Sub-workflow Trigger nodes.\nSub-wokflow executions don't count towards your plan's monthly execution or active workflow limits.\nSet up and use a sub-workflow\nThis section walks through setting up both the parent workflow and sub-workflow.\nHow data passes between workflows\nSub-workflow conversion\nSee sub-workflow conversion for how to divide your existing workflows into sub-workflows."
  },
  {
    "file_path": "flow-logic\\waiting.md",
    "content": "Waiting\nWaiting allows you to pause a workflow mid-execution, then resume where the workflow left off, with the same data. This is useful if you need to rate limit your calls to a service, or wait for an external event to complete. You can wait for a specified duration, or until a webhook fires.\nMaking a workflow wait uses the Wait node. Refer to the node documentation for usage details.\nn8n provides a workflow template with a basic example of Rate limiting and waiting for external events."
  },
  {
    "file_path": "help-community\\contributing.md",
    "content": "How can you contribute?\nThere are a several ways in which you can contribute to n8n, depending on your skills and interests. Each form of contribution is valuable to us!\nShare some love: Review us\nStar n8n on GitHub and Docker Hub.\nFollow us on Twitter, LinkedIn, and Facebook.\nUpvote n8n on AlternativeTo and Alternative.me.\nAdd n8n to your stack on Stackshare.\nWrite a review about n8n on G2, Slant, and Capterra.\nHelp out the community\nYou can participate in the forum and help the community members out with their questions.\nWhen sharing workflows in the community forum for debugging, use code blocks. Use triple backticks  `  to wrap the workflow JSON in a code block.\nThe following video demonstrates the steps of sharing workflows on the community forum:\nContribute a workflow template\nBuild a node\nCreate an integration for a third party service. Check out the node creation docs for guidance on how to create and publish a community node.\nContribute to the code\nThere are different ways in which you can contribute to the n8n code base:\nFix issues reported on GitHub. The CONTRIBUTING guide will help you get your development environment ready in minutes.\nAdd additional functionality to an existing third party integration.\nAdd a new feature to n8n.\nContribute to the docs\nYou can contribute to the n8n documentation, for example by documenting nodes or fixing issues.\nThe repository for the docs is here and the guidelines for contributing to the docs are here.\nContribute to the blog\nYou can write an article for the n8n blog. Your article can be, for example, a workflow tutorial, an opinion piece on automation, or some domain-specific automation guides.\nHow to submit a post\nn8n appreciates all contributions. Publishing a tutorial on your own site that supports the community is a great contribution. If you want n8n to highlight your post on the blog, follow these steps:\nEmail your idea to marketing@n8n.io with the subject \"Blog contribution: [Your Topic].\"\nSubmit your draft:\nWrite your post in a Google Doc following the style guide.\nIf your blog post includes example workflows, include the workflow JSON in a separate section at the end.\nFor author credit, provide a second Google Doc with your full name, a short byline, and your image. n8n will use this to create your author page and credit you as the author of the post.\nWait for feedback. We will respond if your draft fits with the blog's strategy and requirements. If you don't hear back within 30 days, it means we won't be moving forward with your blog post.\nRefer a candidate\nDo you know someone who would be a great fit for one of our open positions? Refer them to us! In return, we'll pay you ‚Ç¨1,000 when the referral successfully passes their probationary period.\nHere's how this works:\nSearch: Have a look at the description and requirements of each role, and consider if someone you know would be a great fit.\nReferral: Once you've identified a potential candidate, send an email to Jobs at n8n with the subject line Employee referral - [job title] and a short description of the person you're referring (and the reason why). Also, tell your referral to apply for the job through our careers page.\nEvaluation: We'll screen the application and inform you about the next steps of the hiring process.\nReward: As soon as your referral has successfully finished the probationary period, we'll reward you for your efforts by transferring the ‚Ç¨1,000 to your bank account."
  },
  {
    "file_path": "help-community\\help.md",
    "content": "Get help with n8n\nIf you need more help with n8n, you can ask for support in the forum. This is the best source of answers, as both the n8n support team and community members can help.\nIf your Cloud instance is having issues, or if you're an enterprise customer who needs support, you can contact help@n8n.io.\nUse the About n8n debug tool\nWhether you're posting to the forum or emailing customer support, you'll get help faster if you provide details about your n8n instance in your first post or email.\nThe fastest way to do this is to use the About n8n debug tool:\nOpen the left-side panel.\nSelect Help.\nSelect About n8n.\nThe About n8n modal opens to display your current information.\nSelect Copy debug information to copy your information.\nn8n recommends pasting this information into your forum post or support email."
  },
  {
    "file_path": "hosting\\cli-commands.md",
    "content": "CLI commands for n8n\nn8n includes a CLI (command line interface), allowing you to perform actions using the CLI rather than the n8n editor. These include starting workflows, and exporting and importing workflows and credentials.\nRunning CLI commands\nYou can use CLI commands with self-hosted n8n. Depending on how you choose to install n8n, there are differences in how to run the commands:\nnpm: the n8n command is directly available. The documentation uses this in the examples below.\nDocker: the n8n command is available within your Docker container:\nStart a workflow\nYou can start workflows directly using the CLI.\nExecute a saved workflow by its ID:\nChange the active status of a workflow\nYou can change the active status of a workflow using the CLI.\nSet the active status of a workflow by its ID to false:\nSet the active status of a workflow by its ID to true:\nSet the active status to false for all the workflows:\nSet the active status to true for all the workflows:\nExport workflows and credentials\nYou can export your workflows and credentials from n8n using the CLI.\nCommand flags:\nTABLE_PLACEHOLDER_0\nNodes\nUninstall a community node by package name:\nFor example, to uninstall the Evolution API community node, type:\nCredentials\nUninstall a community node credential:\nFor example, to uninstall the Evolution API community node credential, visit the repository and navigate to the credentials.ts file to find the name:\nSecurity audit\nYou can run a security audit on your n8n instance, to detect common security issues."
  },
  {
    "file_path": "hosting\\community-edition-features.md",
    "content": "Community Edition Features\nThe community edition includes almost the complete feature set of n8n, except for the features listed here.\nThe community edition doesn't include these features:\nCustom Variables\nEnvironments\nExternal secrets\nExternal storage for binary data\nLog streaming (Logging is included)\nMulti-main mode (Queue mode is included)\nProjects\nSSO (SAML, LDAP)\nSharing (workflows, credentials) (Only the instance owner and the user who creates them can access workflows and credentials)\nVersion control using Git\nWorkflow history (You can get one day of workflow history with the community edition by registering)\nThese features are available on the Enterprise Cloud plan, including the self-hosted Enterprise edition. Some of these features are available on the Starter and Pro Cloud plan.\nSee pricing for reference.\nRegistered Community Edition\nYou can unlock extra features by registering your n8n community edition. You register with your email and receive a license key.\nRegistering unlocks these features for the community edition:\nFolders: Organize your workflows into tidy folders\nDebug in editor: Copy and pin execution data when working on a workflow\nOne day of workflow history: 24 hours of workflow history so you can revert back to previous workflow versions\nCustom execution data: Save, find, and annotate execution metadata\nTo register a new community edition instance, select the option during your initial account creation.\nTo register an existing community edition instance:\nSelect the three dots icon !three dots icon{.off-glb} in the lower-left corner.\nSelect Settings and then Usage and plan.\nSelect Unlock to enter your email and then select Send me a free license key.\nCheck your email for the account you entered.\nOnce you have a license key, activate it by clicking the button in the license email or by visiting Options > Settings > Usage and plan and selecting Enter activation key.\nOnce activated, your license will not expire. We may change the unlocked features in the future. This will not impact previously unlocked features."
  },
  {
    "file_path": "hosting\\index.md",
    "content": "Self-hosting n8n\nThis section provides guidance on setting up n8n for both the Enterprise and Community self-hosted editions. The Community edition is free, the Enterprise edition isn't.\nSee Community edition features for a list of available features.\n- __Installation and server setups__\nInstall n8n on any platform using npm or Docker. Or follow our guides to popular hosting platforms.\n:octicons-arrow-right-24: Docker installation guide\n- __Configuration__\nLearn how to configure n8n with environment variables.\n:octicons-arrow-right-24: Environment Variables\n- __Users and authentication__\nChoose and set up user authentication for your n8n instance.\n:octicons-arrow-right-24: Authentication\n- __Scaling__\nManage data, modes, and processes to keep n8n running smoothly at scale.\n:octicons-arrow-right-24: Scaling\n- __Securing n8n__\nSecure your n8n instance by setting up SSL, SSO, or 2FA or blocking or opting out of some data collection or features.\n:octicons-arrow-right-24: Securing n8n guide\n- __Starter kits__\nNew to n8n or AI? Try our Self-hosted AI Starter Kit. Curated by n8n, it combines the self-hosted n8n platform with compatible AI products and components to get you started building self-hosted AI workflows.\n:octicons-arrow-right-24: Starter kits"
  },
  {
    "file_path": "hosting\\architecture\\database-structure.md",
    "content": "Database structure\nThis page describes the purpose of each table in the n8n database.\nDatabase and query technology\nBy default, n8n uses SQLite as the database. If you are using another database the structure will be similar, but the data-types may be different depending on the database.\nn8n uses TypeORM for queries and migrations.\nTo inspect the n8n database, you can use DBeaver, which is an open-source universal database tool.\nTables\nThese are the tables n8n creates during setup.\nauth_identity\nStores details of external authentication providers when using SAML.\nauth_provider_sync_history\nStores the history of a SAML connection.\ncredentials_entity\nStores the credentials used to authenticate with integrations.\nevent_destinations\nContains the destination configurations for Log streaming.\nexecution_data\nContains the workflow at time of running, and the execution data.\nexecution_entity\nStores all saved workflow executions. Workflow settings can affect which executions n8n saves.\nexecution_metadata\nStores Custom executions data.\ninstalled_nodes\nLists the community nodes installed in your n8n instance.\ninstalled_packages\nDetails of npm community nodes packages installed in your n8n instance. installed_nodes lists each individual node. installed_packages lists npm packages, which may contain more than one node.\nmigrations\nA log of all database migrations. Read more about Migrations in TypeORM's documentation.\nproject\nLists the projects in your instance.\nproject_relation\nDescribes the relationship between a user and a project, including the user's role type.\nrole\nNot currently used. For use in future work on custom roles.\nsettings\nRecords custom instance settings. These are settings that you can't control using environment variables. They include:\nWhether the instance owner is set up\nWhether the user chose to skip owner and user management setup\nLicense key\nshared_credentials\nMaps credentials to users.\nshared_workflow\nMaps workflows to users.\ntag_entity\nAll workflow tags created in the n8n instance. This table lists the tags. workflows_tags records which workflows have which tags.\nuser\nContains user data.\nvariables\nStore variables.\nwebhook_entity\nRecords the active webhooks in your n8n instance's workflows. This isn't just webhooks uses in the Webhook node. It includes all active webhooks used by any trigger node.\nworkflow_entity\nYour n8n instance's saved workflows.\nworkflow_history\nStore previous versions of workflows.\nworkflow_statistics\nCounts workflow IDs and their status.\nworkflows_tags\nMaps tags to workflows. tag_entity contains tag details.\nEntity Relationship Diagram (ERD)\n!\"n8n ERD\""
  },
  {
    "file_path": "hosting\\architecture\\overview.md",
    "content": "Architecture\nUnderstanding n8n's underlying architecture is helpful if you need to:\nEmbed n8n\nCustomize n8n's default databases\nThis section is a work in progress. If you have questions, please try the forum and let n8n know which architecture documents would be useful for you."
  },
  {
    "file_path": "hosting\\configuration\\configuration-methods.md",
    "content": "Configuration\nYou can change n8n's settings using environment variables. For a full list of available configurations see Environment Variables.\nSet environment variables by command line\nnpm\nFor npm, set your desired environment variables in terminal using the export command as shown below:\nDocker\nIn Docker you can use the -e flag from the command line:\nSet environment variables using a file\nYou can also configure n8n using a configuration file.\nOnly define the values that need to be different from the default in your configuration file. You can use multiple files. For example, you can have a file with generic base settings, and files with specific values for different environments.\nnpm\nSet the path to the JSON configuration file using the environment variable N8N_CONFIG_FILES:\nExample file:\nDocker\nIn Docker, you can set your environment variables in the n8n: environment: element of your docker-compose.yaml file.\nFor example:\nKeeping sensitive data in separate files\nYou can append _FILE to individual environment variables to provide their configuration in a separate file, enabling you to avoid passing sensitive details using environment variables. n8n loads the data from the file with the given name, making it possible to load data from Docker-Secrets and Kubernetes-Secrets.\nRefer to Environment variables for details on each variable.\nWhile most environment variables can use the _FILE suffix, it's more beneficial for sensitive data such as credentials and database configuration. Here are some examples:"
  },
  {
    "file_path": "hosting\\configuration\\supported-databases-settings.md",
    "content": "Supported databases\nBy default, n8n uses SQLite to save credentials, past executions, and workflows. n8n also supports PostgresDB.\nShared settings\nThe following environment variables get used by all databases:\nDB_TABLE_PREFIX (default: -) - Prefix for table names\nPostgresDB\nTo use PostgresDB as the database, you can provide the following environment variables:\nDB_TYPE=postgresdb\nDB_POSTGRESDB_DATABASE (default: 'n8n')\nDB_POSTGRESDB_HOST (default: 'localhost')\nDB_POSTGRESDB_PORT (default: 5432)\nDB_POSTGRESDB_USER (default: 'postgres')\nDB_POSTGRESDB_PASSWORD (default: empty)\nDB_POSTGRESDB_SCHEMA (default: 'public')\nDB_POSTGRESDB_SSL_CA (default: undefined): Path to the server's CA certificate used to validate the connection (opportunistic encryption isn't supported)\nDB_POSTGRESDB_SSL_CERT (default: undefined): Path to the client's TLS certificate\nDB_POSTGRESDB_SSL_KEY (default: undefined): Path to the client's private key corresponding to the certificate\nDB_POSTGRESDB_SSL_REJECT_UNAUTHORIZED (default: true): If TLS connections that fail validation should be rejected\nRequired permissions\nn8n needs to create and modify the schemas of the tables it uses.\nRecommended permissions:\nTLS\nYou can choose between these configurations:\nNot declaring (default): Connect with SSL=off\nDeclaring only the CA and unauthorized flag: Connect with SSL=on and verify the server's signature\nDeclaring _{CERT,KEY} and the above: Use the certificate and key for client TLS authentication\nSQLite\nThis is the default database that gets used if nothing is defined.\nThe database file is located at:\n~/.n8n/database.sqlite"
  },
  {
    "file_path": "hosting\\configuration\\task-runners.md",
    "content": "Task runners\nTask runners are a generic mechanism to execute tasks in a secure and performant way. They're used to execute user-provided JavaScript code in the Code node.\nThis document describes how task runners work and how you can configure them.\nHow it works\nThe task runner feature consists of three components: a task runner, a task broker, and a task requester.\n!Task runner overview\nTask runners connect to the task broker using a websocket connection. A task requester submits a task request to the broker where an available task runner can pick it up for execution.\nThe runner executes the task and submits the results to the task requester. The task broker coordinates communication between the runner and the requester.\nThe n8n instance (main and worker) acts as the broker. The Code node in this case is the task requester.\nTask runner modes\nYou can use task runners in two different modes: internal and external.\nInternal mode\nIn internal mode, the n8n instance launches the task runner as a child process. The n8n process monitors and manages the life cycle of the task runner. The task runner process shares the same uid and gid as n8n.\nExternal mode\nIn external mode, an external orchestrator (for example, Kubernetes) launches the task runner instead of n8n. Typically, this means you would configure the task runner to run as a side-car container next to n8n.\n!Task runner deployed as a side-car container\nIn this mode, the orchestrator monitors and manages the life cycle of the task runner container. The task runner is fully isolated from the n8n instance.\nWhen using the Queue mode, each n8n container (main and workers) needs to have its own task runner.\nSetting up external mode\nUse the following details to configure task runners in external mode\nConfiguring n8n instance in external mode\nYou can configure n8n to use external task runners by setting the following environment variables:\nTABLE_PLACEHOLDER_0\nFor full list of environment variables see task runner environment variables."
  },
  {
    "file_path": "hosting\\configuration\\user-management-self-hosted.md",
    "content": "Configure self-hosted n8n for user management\nUser management in n8n allows you to invite people to work in your n8n instance.\nThis document describes how to configure your n8n instance to support user management, and the steps to start inviting users.\nRefer to the main User management guide for more information about usage, including:\nManaging users\nAccount types\nBest practices\nFor LDAP setup information, refer to LDAP.\nFor SAML setup information, refer to SAML.\nSetup\nThere are three stages to set up user management in n8n:\nConfigure your n8n instance to use your SMTP server.\nStart n8n and follow the setup steps in the app.\nInvite users.\nStep one: SMTP\nn8n recommends setting up an SMTP server, for user invites and password resets.\nGet the following information from your SMTP provider:\nServer name\nSMTP username\nSMTP password\nSMTP sender name\nTo set up SMTP with n8n, configure the SMTP environment variables for your n8n instance. For information on how to set environment variables, refer to Configuration\nTABLE_PLACEHOLDER_0\nIf your n8n instance is already running, you need to restart it to enable the new SMTP settings.\nStep two: In-app setup\nStep three: Invite users"
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\base-url.md",
    "content": "Configure the Base URL for n8n's front end access\nYou can configure the Base URL that the front end uses to connect to the back end's REST API. This is relevant when you want to host n8n's front end and back end separately.\nRefer to Environment variables reference for more information on this variable."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\custom-certificate-authority.md",
    "content": "Configure n8n to use your own certificate authority or self-signed certificate\nYou can add your own certificate authority (CA) or self-signed certificate to n8n. This means you are able to trust a certain SSL certificate instead of trusting all invalid certificates, which is a potential security risk.\nTo use this feature you need to place your certificates in a folder and mount the folder to /opt/custom-certificates in the container.\nDocker\nThe examples below assume you have a folder called pki that contains your certificates in either the directory you run the command from or next to your docker compose file.\nDocker CLI\nWhen using the CLI you can use the -v flag from the command line:\nDocker Compose\nYou should also give the right permissions to the imported certs. You can do this once the container is running (assuming n8n as the container name):"
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\custom-nodes-location.md",
    "content": "Specify location for your custom nodes\nEvery user can add custom nodes that get loaded by n8n on startup. The default\nlocation is in the subfolder .n8n/custom of the user who started n8n.\nYou can define more folders with an environment variable:\nRefer to Environment variables reference for more information on this variable."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\encryption-key.md",
    "content": "Set a custom encryption key\nn8n creates a random encryption key automatically on the first launch and saves\nit in the ~/.n8n folder. n8n uses that key to encrypt the credentials before\nthey get saved to the database. If the key isn't yet in the settings file,\nyou can set it using an environment variable, so that n8n\nuses your custom key instead of generating a new one.\nIn queue mode, you must specify the encryption key environment variable for all workers.\nRefer to Environment variables reference for more information on this variable."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\execution-timeout.md",
    "content": "Configure workflow timeout settings\nA workflow times out and gets canceled after this time (in seconds). If the workflow runs in the main process, a soft timeout happens (takes effect after the current node finishes). If a workflow runs in its own process, n8n attempts a soft timeout first, then kills the process after waiting for a fifth of the given timeout duration.\nEXECUTIONS_TIMEOUT default is -1. For example, if you want to set the timeout to one hour:\nYou can also set maximum execution time (in seconds) for each workflow individually. For example, if you want to set maximum execution time to two hours:\nRefer to Environment variables reference for more information on these variables."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\index.md",
    "content": "Configuration examples\nThis section contains examples for how to configure n8n to solve particular use cases."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\isolation.md",
    "content": "Isolate n8n\nBy default, a self-hosted n8n instance sends data to n8n's servers. It notifies users about available updates, workflow templates, and diagnostics.\nTo prevent your n8n instance from connecting to n8n's servers, set these environment variables to false:\nUnset n8n's diagnostics configuration:\nRefer to Environment variables reference for more information on these variables."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\modules-in-code-node.md",
    "content": "Enable modules in Code node\nFor security reasons, the Code node restricts importing modules. It's possible to lift that restriction for built-in and external modules by setting the following environment variables:\nNODE_FUNCTION_ALLOW_BUILTIN: For built-in modules\nNODE_FUNCTION_ALLOW_EXTERNAL: For external modules sourced from n8n/node_modules directory. External module support is disabled when an environment variable isn't set.\nRefer to Environment variables reference for more information on these variables."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\prometheus.md",
    "content": "Enable Prometheus metrics\nTo collect and expose metrics, n8n uses the prom-client library.\nThe /metrics endpoint is disabled by default, but it's possible to enable it using the N8N_METRICS environment variable.\nRefer to the respective Environment Variables (N8N_METRICS_INCLUDE_*) for configuring which metrics and labels should get exposed.\nBoth main and worker instances are able to expose metrics.\nQueue metrics\nTo enable queue metrics, set the N8N_METRICS_INCLUDE_QUEUE_METRICS env var to true. You can adjust the refresh rate with N8N_METRICS_QUEUE_METRICS_INTERVAL.\nQueue metrics are only available for the main instance in single-main mode."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\time-zone.md",
    "content": "Set the self-hosted instance timezone\nThe default timezone is America/New_York. For instance, the Schedule node uses it to know at what time the workflow should start. To set a different default timezone, set GENERIC_TIMEZONE to the appropriate value. For example, if you want to set the timezone to Berlin (Germany):\nYou can find the name of your timezone here.\nRefer to Environment variables reference for more information on this variable."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\user-folder.md",
    "content": "Specify user folder path\nn8n saves user-specific data like the encryption key, SQLite database file, and\nthe ID of the tunnel (if used) in the subfolder .n8n of the user who started n8n. It's possible to overwrite the user-folder using an environment variable.\nRefer to Environment variables reference for more information on this variable."
  },
  {
    "file_path": "hosting\\configuration\\configuration-examples\\webhook-url.md",
    "content": "Configure n8n webhooks with reverse proxy\nn8n creates the webhook URL by combining N8N_PROTOCOL, N8N_HOST and N8N_PORT. If n8n runs behind a reverse proxy, that won't work. That's because n8n runs internally on port 5678 but the reverse proxy exposes it to the web on port 443. In that case, it's important to set the webhook URL manually so that n8n can display it in the Editor UI and register the correct webhook URLs with external services.\nRefer to Environment variables reference for more information on this variable."
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\binary-data.md",
    "content": "Binary data environment variables\nBy default, n8n uses memory to store binary data. Enterprise users can choose to use an external service instead. Refer to External storage for more information on using external storage for binary data.\nTABLE_PLACEHOLDER_0| N8N_DEFAULT_BINARY_DATA_MODE | String | default | The default binary data mode. default keeps binary data in memory. Set to filesystem to use the filesystem, or s3 to AWS S3. Note that binary data pruning operates on the active binary data mode. For example, if your instance stored data in S3, and you later switched to filesystem mode, n8n only prunes binary data in the filesystem. This may change in future. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\credentials.md",
    "content": "Credentials environment variables\nEnable credential overwrites using the following environment variables. Refer to Credential overwrites for details.\nTABLE_PLACEHOLDER_0| CREDENTIALS_DEFAULT_NAME | String | My credentials | The default name for credentials. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\database.md",
    "content": "Database environment variables\nBy default, n8n uses SQLite. n8n also supports PostgreSQL. n8n deprecated support for MySQL and MariaDB in v1.0.\nThis page outlines environment variables to configure your chosen database for your self-hosted n8n instance.\nTABLE_PLACEHOLDER_0| DB_SQLITE_VACUUM_ON_STARTUP | Boolean | false | Runs VACUUM operation on startup to rebuild the database. Reduces file size and optimizes indexes. This is a long running blocking operation and increases start-up time. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\deployment.md",
    "content": "Deployment environment variables\nThis page lists the deployment configuration options for your self-hosted n8n instance, including setting up access URLs, enabling templates, customizing encryption, and configuring server details.\nTABLE_PLACEHOLDER_0| N8N_PROXY_HOPS | Number | 0 | Number of reverse-proxies n8n is running behind. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\endpoints.md",
    "content": "Endpoints environment variables\nThis page lists environment variables for customizing endpoints in n8n.\nTABLE_PLACEHOLDER_0| N8N_DISABLE_PRODUCTION_MAIN_PROCESS | Boolean | false | Disable production webhooks from main process. This helps ensure no HTTP traffic load to main process when using webhook-specific processes. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\executions.md",
    "content": "Executions environment variables\nThis page lists environment variables to configure workflow execution settings.\nTABLE_PLACEHOLDER_0| N8N_CONCURRENCY_PRODUCTION_LIMIT | Number | -1 | Max production executions allowed to run concurrently, in both regular and scaling modes. -1 to disable in regular mode. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\external-data-storage.md",
    "content": "External data storage environment variables\nRefer to External storage for more information on using external storage for binary data.\nTABLE_PLACEHOLDER_0| N8N_EXTERNAL_STORAGE_S3_AUTH_AUTO_DETECT | Boolean | - | Use automatic credential detection to authenticate S3 calls for external storage. This will ignore the access key and access secret and use the default credential provider chain. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\external-hooks.md",
    "content": "External hooks environment variables\nYou can define external hooks that n8n executes whenever a specific operation runs. Refer to Backend hooks for examples of available hooks and Hook files for information on file formatting.\nTABLE_PLACEHOLDER_0| EXTERNAL_FRONTEND_HOOKS_URLS | String | URLs to files containing frontend external hooks. Provide multiple URLs as a colon-separated list (\":\"). |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\external-secrets.md",
    "content": "External secrets environment variables\nYou can use an external secrets store to manage credentials for n8n. Refer to External secrets for details.\nTABLE_PLACEHOLDER_0| N8N_EXTERNAL_SECRETS_UPDATE_INTERVAL | Number | 300 (5 minutes) | How often (in seconds) to check for secret updates. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\index.md",
    "content": "Environment variables overview\nThis section lists the environment variables that you can use to change n8n's configuration settings when self-hosting n8n."
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\insights.md",
    "content": "Insights environment variables\nInsights gives instance owners and admins visibility into how workflows perform over time. Refer to Insights for details.\nTABLE_PLACEHOLDER_0 | N8N_INSIGHTS_FLUSH_INTERVAL_SECONDS                    | Number | 30      | The interval (in seconds) at which the insights data should be flushed to the database. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\licenses.md",
    "content": "License environment variables\nTo enable certain licensed features, you must first activate your license. You can do this either through the UI or by setting environment variables. For more information, see license key.\nTABLE_PLACEHOLDER_0| https_proxy_license_server | String |  | Proxy server URL for HTTPS requests to retrieve license. This variable name needs to be lowercase. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\logs.md",
    "content": "Logs environment variables\nThis page lists environment variables to set up logging for debugging. Refer to Logging in n8n for details.\nn8n logs\nTABLE_PLACEHOLDER_0| N8N_EVENTBUS_LOGWRITER_LOGBASENAME | String | n8nEventLog | Basename of the event log file. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\nodes.md",
    "content": "Nodes environment variables\nThis page lists the environment variables configuration options for managing nodes in n8n, including specifying which nodes to load or exclude, importing built-in or external modules in the Code node, and enabling community nodes.\nTABLE_PLACEHOLDER_0| NODES_INCLUDE                          | Array of strings | -                             | Specify which nodes to load.                                                                                                                                                                                                          |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\queue-mode.md",
    "content": "Queue mode environment variables\nYou can run n8n in different modes depending on your needs. Queue mode provides the best scalability. Refer to Queue mode for more information.\nTABLE_PLACEHOLDER_0| N8N_MULTI_MAIN_SETUP_CHECK_INTERVAL | Number | 3 | Interval (in seconds) for leader check in multi-main setup. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\security.md",
    "content": "Security environment variables\nTABLE_PLACEHOLDER_0| N8N_SAMESITE_COOKIE | Enum string: strict, lax, none | lax | Controls cross-site cookie behavior (learn more):strict: Sent only for first-party requests.lax (default): Sent with top-level navigation requests.none: Sent in all contexts (requires HTTPS). |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\source-control.md",
    "content": "Source control environment variables\nn8n uses Git-based source control to support environments. Refer to Source control and environments for more information on how to link a Git repository to an n8n instance and configure your source control.\nTABLE_PLACEHOLDER_0| N8N_SOURCECONTROL_DEFAULT_SSH_KEY_TYPE | String | ed25519 | Set to rsa to make RSA the default SSH key type for Source control setup. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\task-runners.md",
    "content": "Task runner environment variables\nTask runners execute code defined by the Code node.\nn8n instance environment variables\nTABLE_PLACEHOLDER_0| GENERIC_TIMEZONE | * | America/New_York | The same default timezone as configured for the n8n instance. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\timezone-localization.md",
    "content": "Timezone and localization environment variables\nTABLE_PLACEHOLDER_0| N8N_DEFAULT_LOCALE | String | en | A locale identifier, compatible with the Accept-Language header. n8n doesn't support regional identifiers, such as de-AT. When running in a locale other than the default, n8n displays UI strings in the selected locale, and falls back to en for any untranslated strings. |"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\user-management-smtp-2fa.md",
    "content": "User management SMTP, and two-factor authentication environment variables\nRefer to User management for more information on setting up user management and emails.\nTABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "hosting\\configuration\\environment-variables\\workflows.md",
    "content": "Workflows environment variables\nTABLE_PLACEHOLDER_0| WORKFLOWS_DEFAULT_NAME | String | My workflow | The default name used for new workflows. |"
  },
  {
    "file_path": "hosting\\installation\\docker.md",
    "content": "Docker Installation\nDocker offers the following advantages:\nInstalls n8n in a clean environment.\nEasier setup for your preferred database.\nCan avoid issues due to different operating systems, as Docker provides a consistent system.\nCan avoid compatibility issues due to differences in operating systems and tools.\nMakes migrating to new hosts or environments more straightforward.\nYou can also use n8n in Docker with Docker Compose. You can find Docker Compose configurations for various architectures in the n8n-hosting repository.\nPrerequisites\nBefore proceeding, install Docker Desktop.\nStarting n8n\nFrom your terminal, run:\nThis command creates a volume to store persistent data, downloads the required n8n image, and starts your container, exposed on port 5678. To save your work between container restarts, it also mounts a docker volume, n8n_data, to persist your data locally.\nOnce running, you can access n8n by opening:\n)\nUsing with PostgreSQL\nBy default, n8n uses SQLite to save [credentials, past executions, and workflows. n8n also supports PostgreSQL, configurable using environment variables as detailed below.\nWhen using PostgreSQL, it's still important to persist the data stored in the /home/node/.n8n folder. This includes n8n user data and, even more importantly, the encryption key for credentials. It's also the name of the webhook when using the n8n tunnel.\nIf n8n can't find the /home/node/.n8n directory on startup, it automatically creates one. In this case, all existing credentials that n8n saved with a different encryption key will no longer work.\nTo use n8n with PostgreSQL, execute the following commands, replacing the placeholders (depicted within angled brackets, for example ) with your actual values:\nYou can find a complete docker-compose file for PostgreSQL in the n8n hosting repository.\nSetting timezone\nTo define the timezone n8n should use, you can set the GENERIC_TIMEZONE environment variable. Schedule-oriented nodes, like the Schedule Trigger node use this to determine the correct timezone.\nYou can set the system timezone, which controls what some scripts and commands like date return, using the TZ environment variable.\nThis example sets the same timezone for both variables:\nUpdating\nTo update n8n, in Docker Desktop, navigate to the Images tab and select Pull from the context menu to download the latest n8n image:\n!Docker Desktop\nYou can also use the command line to pull the latest, or a specific version:\nAfter pulling the updated image, stop your n8n container and start it again. You can also use the command line. Replace  in the commands below with the container ID you find in the first command:\nUpdating Docker Compose\nFurther reading\nYou can find more information about Docker setup in the README file for the Docker image.\nStart n8n with --tunnel by running:\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\npm.md",
    "content": "npm\nnpm is a quick way to get started with n8n on your local machine. You must have Node.js installed. n8n requires a Node.js version between 20.19 and 24.x, inclusive.\nTry n8n with npx\nYou can try n8n without installing it using npx.\nFrom the terminal, run:\nThis command will download everything that's needed to start n8n. You can then access n8n and start building workflows by opening ).\nInstall globally with npm\nTo install n8n globally, use npm:\nTo install or update to a specific version of n8n use the @ syntax to specify the version. For example:\nTo install next:\nAfter the installation, start n8n by running:\nNext steps\nTry out n8n using the [Quickstarts.\nUpdating\nTo update your n8n instance to the latest version, run:\nTo install the next version:\nStart n8n with --tunnel by running:\nReverting an upgrade\nInstall the older version that you want to go back to.\nIf the upgrade involved a database migration:\nCheck the feature documentation and release notes to see if there are any manual changes you need to make.\nRun n8n db:revert on your current version to roll back the database. If you want to revert more than one database migration, you need to repeat this process.\nWindows troubleshooting\nIf you are experiencing issues running n8n on Windows, make sure your Node.js environment is correctly set up. Follow Microsoft's guide to Install NodeJS on Windows."
  },
  {
    "file_path": "hosting\\installation\\updating.md",
    "content": "Update self-hosted n8n\nIt's important to keep your n8n version up to date. This ensures you get the latest features and fixes.\nSome tips when updating:\nFor instructions on how to update, refer to the documentation for your installation method:\nInstalled with npm\nInstalled with Docker"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\aws.md",
    "content": "Hosting n8n on Amazon Web Services\nThis hosting guide shows you how to self-host n8n with Amazon Web Services (AWS). It uses n8n with Postgres as a database backend using Kubernetes to manage the necessary resources and reverse proxy.\nHosting options\nAWS offers several ways suitable for hosting n8n, including EC2 (virtual machines), and EKS (containers running with Kubernetes).\nThis guide uses EKS as the hosting option. Using Kubernetes requires some additional complexity and configuration, but is the best method for scaling n8n as demand changes.\nPrerequisites\nThe steps in this guide use a mix of the AWS UI and the eksctl CLI tool for EKS.\nWhile not mentioned in the documentation for eksctl, you also need to install the AWS CLI tool, and configure authentication of the tool.\nCreate a cluster\nUse the eksctl tool to create a cluster specifying a name and a region with the following command:\nThis can take a while to create the cluster.\nOnce the cluster is created, eksctl automatically sets the kubectl context to the cluster.\nClone configuration repository\nKubernetes and n8n require a series of configuration files. You can clone these from this repository. The following steps tell you what each file does, and what settings you need to change.\nClone the repository with the following command:\nAnd change directory to the root of the repository you cloned:\nConfigure Postgres\nFor larger scale n8n deployments, Postgres provides a more robust database backend than SQLite.\nConfigure volume for persistent storage\nTo maintain data between pod restarts, the Postgres deployment needs a persistent volume. The default AWS storage class, gp2, is suitable for this purpose. This is defined in the postgres-claaim0-persistentvolumeclaim.yaml manifest.\nPostgres environment variables\nPostgres needs some environment variables set to pass to the application running in the containers.\nThe example postgres-secret.yaml file contains placeholders you need to replace with values of your own for user details and the database to use.\nThe postgres-deployment.yaml manifest then uses the values from this manifest file to send to the application pods.\nConfigure n8n\nCreate a volume for file storage\nWhile not essential for running n8n, using persistent volumes helps maintain files uploaded while using n8n and if you want to persist manual n8n encryption keys between restarts, which saves a file containing the key into file storage during startup.\nThe n8n-claim0-persistentvolumeclaim.yaml manifest creates this, and the n8n Deployment mounts that claim in the volumes section of the n8n-deployment.yaml manifest.\nPod resources\nKubernetes lets you specify the minimum resources application containers need and the limits they can run to. The example YAML files cloned above contain the following in the resources section of the n8n-deployment.yaml file:\nThis defines a minimum of 250mb per container, a maximum of 500mb, and lets Kubernetes handle CPU. You can change these values to match your own needs. As a guide, here are the resources values for the n8n cloud offerings:\nOptional: Environment variables\nYou can configure n8n settings and behaviors using environment variables.\nCreate an n8n-secret.yaml file. Refer to Environment variables for n8n environment variables details.\nDeployments\nThe two deployment manifests (n8n-deployment.yaml and postgres-deployment.yaml) define the n8n and Postgres applications to Kubernetes.\nThe manifests define the following:\nSend the environment variables defined to each application pod\nDefine the container image to use\nSet resource consumption limits\nThe volumes defined earlier and volumeMounts to define the path in the container to mount volumes.\nScaling and restart policies. The example manifests define one instance of each pod. You should change this to meet your needs.\nServices\nThe two service manifests (postgres-service.yaml and n8n-service.yaml) expose the services to the outside world using the Kubernetes load balancer using ports 5432 and 5678 respectively by default.\nSend to Kubernetes cluster\nSend all the manifests to the cluster by running the following command in the n8n-kubernetes-hosting directory:\nSet up DNS\nn8n typically operates on a subdomain. Create a DNS record with your provider for the subdomain and point it to a static address of the instance.\nTo find the address of the n8n service running on the instance:\nOpen the Clusters section of the Amazon Elastic Kubernetes Service page in the AWS console.\nSelect the name of the cluster to open its configuration page.\nSelect the Resources tab, then Service and networking > Services.\nSelect the n8n service and copy the Load balancer URLs value. Use this value suffixed with the n8n service port (5678) for DNS.\nDelete resources\nIf you need to delete the setup, you can remove the resources created by the manifests with the following command:\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\azure.md",
    "content": "Hosting n8n on Azure\nThis hosting guide shows you how to self-host n8n on Azure. It uses n8n with Postgres as a database backend using Kubernetes to manage the necessary resources and reverse proxy.\nPrerequisites\nYou need The Azure command line tool\nHosting options\nAzure offers several ways suitable for hosting n8n, including Azure Container Instances (optimized for running containers), Linux Virtual Machines, and Azure Kubernetes Service (containers running with Kubernetes).\nThis guide uses the Azure Kubernetes Service (AKS) as the hosting option. Using Kubernetes requires some additional complexity and configuration, but is the best method for scaling n8n as demand changes.\nThe steps in this guide use a mix of the Azure UI and command line tool, but you can use either to accomplish most tasks.\nOpen the Azure Kubernetes Service\nFrom the Azure portal select Kubernetes services.\nCreate a cluster\nFrom the Kubernetes services page, select Create > Create a Kubernetes cluster.\nYou can select any of the configuration options that suit your needs, then select Create when done.\nSet Kubectl context\nThe remainder of the steps in this guide require you to set the Azure instance as the Kubectl context. You can find the connection details for a cluster instance by opening its details page and then the Connect button. The resulting code snippets shows the steps to paste and run into a terminal to change your local Kubernetes settings to use the new cluster.\nClone configuration repository\nKubernetes and n8n require a series of configuration files. You can clone these from this repository. The following steps tell you which file configures what and what you need to change.\nClone the repository with the following command:\nAnd change directory to the root of the repository you cloned:\nConfigure Postgres\nFor larger scale n8n deployments, Postgres provides a more robust database backend than SQLite.\nConfigure volume for persistent storage\nTo maintain data between pod restarts, the Postgres deployment needs a persistent volume. The default storage class is suitable for this purpose and is defined in the postgres-claim0-persistentvolumeclaim.yaml manifest.\nPostgres environment variables\nPostgres needs some environment variables set to pass to the application running in the containers.\nThe example postgres-secret.yaml file contains placeholders you need to replace with your own values. Postgres will use these details when creating the database..\nThe postgres-deployment.yaml manifest then uses the values from this manifest file to send to the application pods.\nConfigure n8n\nCreate a volume for file storage\nWhile not essential for running n8n, using persistent volumes is required for:\nUsing nodes that interact with files, such as the binary data node.\nIf you want to persist manual n8n encryption keys between restarts. This saves a file containing the key into file storage during startup.\nThe n8n-claim0-persistentvolumeclaim.yaml manifest creates this, and the n8n Deployment mounts that claim in the volumes section of the n8n-deployment.yaml manifest.\nPod resources\nKubernetes lets you optionally specify the minimum resources application containers need and the limits they can run to. The example YAML files cloned above contain the following in the resources section of the n8n-deployment.yaml file:\nThis defines a minimum of 250mb per container, a maximum of 500mb, and lets Kubernetes handle CPU. You can change these values to match your own needs. As a guide, here are the resources values for the n8n cloud offerings:\nOptional: Environment variables\nYou can configure n8n settings and behaviors using environment variables.\nCreate an n8n-secret.yaml file. Refer to Environment variables for n8n environment variables details.\nDeployments\nThe two deployment manifests (n8n-deployment.yaml and postgres-deployment.yaml) define the n8n and Postgres applications to Kubernetes.\nThe manifests define the following:\nSend the environment variables defined to each application pod\nDefine the container image to use\nSet resource consumption limits with the resources object\nThe volumes defined earlier and volumeMounts to define the path in the container to mount volumes.\nScaling and restart policies. The example manifests define one instance of each pod. You should change this to meet your needs.\nServices\nThe two service manifests (postgres-service.yaml and n8n-service.yaml) expose the services to the outside world using the Kubernetes load balancer using ports 5432 and 5678 respectively.\nSend to Kubernetes cluster\nSend all the manifests to the cluster with the following command:\nSet up DNS\nn8n typically operates on a subdomain. Create a DNS record with your provider for the subdomain and point it to the IP address of the n8n service. Find the IP address of the n8n service from the Services & ingresses menu item of the cluster you want to use under the External IP column. You need to add the n8n port, \"5678\" to the URL.\nDelete resources\nRemove the resources created by the manifests with the following command:\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\digital-ocean.md",
    "content": "Hosting n8n on DigitalOcean\nThis hosting guide shows you how to self-host n8n on a DigitalOcean droplet. It uses:\nCaddy (a reverse proxy) to allow access to the Droplet from the internet. Caddy will also automatically create and manage SSL / TLS certificates for your n8n instance.\nDocker Compose to create and define the application components and how they work together.\nCreate a Droplet\nLog in to DigitalOcean.\nSelect the project to host the Droplet, or create a new project.\nIn your project, select Droplets from the Manage menu.\nCreate a new Droplet using the Docker image available on the Marketplace tab.\nLog in to your Droplet and create new user\nThe rest of this guide requires you to log in to the Droplet using a terminal with SSH. Refer to How to Connect to Droplets with SSH for more information.\nYou should create a new user, to avoid working as the root user:\nLog in as root.\nCreate a new user:\nFollow the prompts in the CLI to finish creating the user.\nGrant the new user administrative privileges:\nYou can now run commands with superuser privileges by using sudo before the command.\n5. Follow the steps to set up SSH for the new user: Add Public Key Authentication.\n5. Log out of the droplet.\n6. Log in using SSH as the new user.\nClone configuration repository\nDocker Compose, n8n, and Caddy require a series of folders and configuration files. You can clone these from this repository into the home folder of the logged-in user on your Droplet. The following steps will tell you which file to change and what changes to make.\nClone the repository with the following command:\nAnd change directory to the root of the repository you cloned:\nDefault folders and files\nThe host operating system (the DigitalOcean Droplet) copies the two folders you created to Docker containers to make them available to Docker. The two folders are:\ncaddy_config: Holds the Caddy configuration files.\nlocal_files: A folder for files you upload or add using n8n.\nCreate Docker volumes\nTo persist the Caddy cache between restarts and speed up start times, create a Docker volume that Docker reuses between restarts:\nCreate a Docker volume for the n8n data:\nSet up DNS\nn8n typically operates on a subdomain. Create a DNS record with your provider for the subdomain and point it to the IP address of the Droplet. The exact steps for this depend on your DNS provider, but typically you need to create a new \"A\" record for the n8n subdomain. DigitalOcean provide An Introduction to DNS Terminology, Components, and Concepts.\nOpen ports\nn8n runs as a web application, so the Droplet needs to allow incoming access to traffic on port 80 for non-secure traffic, and port 443 for secure traffic.\nOpen the following ports in the Droplet's firewall by running the following two commands:\nConfigure n8n\nn8n needs some environment variables set to pass to the application running in the Docker container. The example .env file contains placeholders you need to replace with values of your own.\nOpen the file with the following command:\nThe file contains inline comments to help you know what to change.\nRefer to Environment variables for n8n environment variables details.\nThe Docker Compose file\nThe Docker Compose file (docker-compose.yml) defines the services the application needs, in this case Caddy and n8n.\nThe Caddy service definition defines the ports it uses and the local volumes to copy to the containers.\nThe n8n service definition defines the ports it uses, the environment variables n8n needs to run (some defined in the .env file), and the volumes it needs to copy to the containers.\nThe Docker Compose file uses the environment variables set in the .env file, so you shouldn't need to change it's content, but to take a look, run the following command:\nConfigure Caddy\nCaddy needs to know which domains it should serve, and which port to expose to the outside world. Edit the Caddyfile file in the caddy_config folder.\nChange the placeholder domain to yours. If you followed the steps to name the subdomain n8n, your full domain is similar to n8n.example.com. The n8n in the reverse_proxy setting tells Caddy to use the service definition defined in the docker-compose.yml file:\nIf you were to use automate.example.com, your Caddyfile may look something like:\nStart Docker Compose\nStart n8n and Caddy with the following command:\nThis may take a few minutes.\nTest your setup\nIn your browser, open the URL formed of the subdomain and domain name defined earlier. Enter the user name and password defined earlier, and you should be able to access n8n.\nStop n8n and Caddy\nYou can stop n8n and Caddy with the following command:\nUpdating\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\docker-compose.md",
    "content": "Docker-Compose\nIf you have already installed Docker and Docker-Compose, then you can start with step 3.\nYou can find Docker Compose configurations for various architectures in the n8n-hosting repository.\n1. Install Docker and Docker Compose\nHow you install Docker and Docker Compose can vary depending on the Linux distribution you use. You can find detailed instructions in both the Docker and Docker Compose installation documentation. The following example is for Ubuntu:\nVerify that Docker and Docker Compose are available by typing:\n2. Optional: Non-root user access\nYou can optionally grant access to run Docker without the sudo command.\nTo grant access to the user that you're currently logged in with (assuming they have sudo access), run:\nTo grant access to a different user, type the following, substituting  with the appropriate username:\nYou will need to run exec sg docker newgrp from any of that user's existing sessions for it to access the new group permissions.\nYou can verify that your current session recognizes the docker group by typing:\n3. DNS setup\nTo host n8n online or on a network, create a dedicated subdomain pointed at your server.\nAdd an A record to route the subdomain accordingly:\nType: A\nName: n8n (or the desired subdomain)\nIP address: (your server's IP address)\n4. Create an .env file\nCreate a project directory to store your n8n environment configuration and Docker Compose files and navigate inside:\nInside the n8n-compose directory, create an .env file to customize your n8n instance's details. Change it to match your own information:\n5. Create local files directory\nInside your project directory, create a directory called local-files for sharing files between the n8n instance and the host system (for example, using the Read/Write Files from Disk node):\nThe Docker Compose file below can automatically create this directory, but doing it manually ensures that it's created with the right ownership and permissions.\n6. Create Docker Compose file\nCreate a compose.yaml file. Paste the following in the file:\nThe above Docker Compose file configures two containers: one for n8n, and one to run traefik, an application proxy to manage TLS/SSL certificates and handle routing.\nIt also creates and mounts two Docker Volumes and mounts the local-files directory you created earlier:\nTABLE_PLACEHOLDER_0\n7. Start Docker Compose\nYou can now start n8n by typing:\nTo stop the container, type:\n8. Done\nYou can now reach n8n using the subdomain + domain combination you defined in your .env file configuration. The above example would result in\nn8n is only accessible using secure HTTPS, not over plain HTTP.\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\google-cloud.md",
    "content": "Hosting n8n on Google Cloud\nThis hosting guide shows you how to self-host n8n on Google Cloud (GCP). It uses n8n with Postgres as a database backend using Kubernetes to manage the necessary resources and reverse proxy.\nPrerequisites\nThe gcloud command line tool\nThe gke-gcloud-auth-plugin (install the gcloud CLI first)\nHosting options\nGoogle Cloud offers several options suitable for hosting n8n, including Cloud Run (optimized for running containers), Compute Engine (VMs), and Kubernetes Engine (containers running with Kubernetes).\nThis guide uses the Google Kubernetes Engine (GKE) as the hosting option. Using Kubernetes requires some additional complexity and configuration, but is the best method for scaling n8n as demand changes.\nMost of the steps in this guide use the Google Cloud UI, but you can also use the gcloud command line tool instead to undertake all the steps.\nCreate project\nGCP encourages you to create projects to logically organize resources and configuration. Create a new project for your n8n deployment from your Google Cloud Console: select the project dropdown menu and then the NEW PROJECT button. Then select the newly created project. As you follow the other steps in this guide, make sure you have the correct project selected.\nEnable the Kubernetes Engine API\nGKE isn't enabled by default. Search for \"Kubernetes\" in the top search bar and select \"Kubernetes Engine\" from the results.\nSelect ENABLE to enable the Kubernetes Engine API for this project.\nCreate a cluster\nFrom the GKE service page, select Clusters > CREATE. Make sure you select the \"Standard\" cluster option, n8n doesn't work with an \"Autopilot\" cluster. You can leave the cluster configuration on defaults unless there's anything specifically you need to change, such as location.\nSet Kubectl context\nThe rest of the steps in this guide require you to set the GCP instance as the Kubectl context. You can find the connection details for a cluster instance by opening its details page and selecting CONNECT. The displayed code snippet shows a connection string for the gcloud CLI tool. Paste and run the code snippet in the gcloud CLI to change your local Kubernetes settings to use the new gcloud cluster.\nClone configuration repository\nKubernetes and n8n require a series of configuration files. You can clone these from this repository locally. The following steps explain the file configuration and how to add your information.\nClone the repository with the following command:\nAnd change directory to the root of the repository you cloned:\nConfigure Postgres\nFor larger scale n8n deployments, Postgres provides a more robust database backend than SQLite.\nCreate a volume for persistent storage\nTo maintain data between pod restarts, the Postgres deployment needs a persistent volume. Running Postgres on GCP requires a specific Kubernetes Storage Class. You can read this guide for specifics, but the storage.yaml manifest creates it for you. You may want to change the regions to create the storage in under the allowedTopologies > matchedLabelExpressions > values key. By default, they're set to us-central.\nPostgres environment variables\nPostgres needs some environment variables set to pass to the application running in the containers.\nThe example postgres-secret.yaml file contains placeholders you need to replace with your own values. Postgres will use these details when creating the database..\nThe postgres-deployment.yaml manifest then uses the values from this manifest file to send to the application pods.\nConfigure n8n\nCreate a volume for file storage\nWhile not essential for running n8n, using persistent volumes is required for:\nUsing nodes that interact with files, such as the binary data node.\nIf you want to persist manual n8n encryption keys between restarts. This saves a file containing the key into file storage during startup.\nThe n8n-claim0-persistentvolumeclaim.yaml manifest creates this, and the n8n Deployment mounts that claim in the volumes section of the n8n-deployment.yaml manifest.\nPod resources\nKubernetes lets you optionally specify the minimum resources application containers need and the limits they can run to. The example YAML files cloned above contain the following in the resources section of the n8n-deployment.yaml and postgres-deployment.yaml files:\nThis defines a minimum of 250mb per container, a maximum of 500mb, and lets Kubernetes handle CPU. You can change these values to match your own needs. As a guide, here are the resources values for the n8n cloud offerings:\nOptional: Environment variables\nYou can configure n8n settings and behaviors using environment variables.\nCreate an n8n-secret.yaml file. Refer to Environment variables for n8n environment variables details.\nDeployments\nThe two deployment manifests (n8n-deployment.yaml and postgres-deployment.yaml) define the n8n and Postgres applications to Kubernetes.\nThe manifests define the following:\nSend the environment variables defined to each application pod\nDefine the container image to use\nSet resource consumption limits with the resources object\nThe volumes defined earlier and volumeMounts to define the path in the container to mount volumes.\nScaling and restart policies. The example manifests define one instance of each pod. You should change this to meet your needs.\nServices\nThe two service manifests (postgres-service.yaml and n8n-service.yaml) expose the services to the outside world using the Kubernetes load balancer using ports 5432 and 5678 respectively.\nSend to Kubernetes cluster\nSend all the manifests to the cluster with the following command:\nSet up DNS\nn8n typically operates on a subdomain. Create a DNS record with your provider for the subdomain and point it to the IP address of the n8n service. Find the IP address of the n8n service from the Services & Ingress menu item of the cluster you want to use under the Endpoints column.\nDelete resources\nRemove the resources created by the manifests with the following command:\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\heroku.md",
    "content": "Hosting n8n on Heroku\nThis hosting guide shows you how to self-host n8n on Heroku. It uses:\nDocker Compose to create and define the application components and how they work together.\nHeroku's PostgreSQL service to host n8n's data storage.\nA Deploy to Heroku button offering a one click, with minor configuration, deployment.\nUse the deployment template to create a Heroku project\nThe quickest way to get started with deploying n8n to Heroku is using the Deploy to Heroku button:\nThis opens the Create New App page on Heroku. Set a name for the project, and choose the region to deploy the project to.\nConfigure environment variables\nHeroku pre-fills the configuration options defined in the env section of the app.json file, which also sets default values for the environment variables n8n uses.\nYou can change any of these values to suit your needs. You must change the following values:\nN8N_ENCRYPTION_KEY, which n8n uses to encrypt user account details before saving to the database.\nWEBHOOK_URL should match the application name you create to ensure that webhooks have the correct URL.\nDeploy n8n\nSelect Deploy app.\nAfter Heroku builds and deploys the app it provides links to Manage App or View the application.\nChanging the deployment template\nYou can make changes to the deployment template by forking the repository and deploying from you fork.\nThe Dockerfile\nBy default the Dockerfile pulls the latest n8n image, if you want to use a different or fixed version, then update the image tag on the top line of the Dockerfile.\nHeroku and exposing ports\nHeroku doesn't allow Docker-based applications to define an exposed port with the EXPOSE command. Instead, Heroku provides a PORT environment variable that it dynamically populates at application runtime. The entrypoint.sh file overrides the default Docker image command to instead set the port variable that Heroku provides. You can then access n8n on port 80 in a web browser.\nConfiguring Heroku\nThe heroku.yml file defines the application you want to create on Heroku. It consists of two sections:\nsetup > addons defines the Heroku addons to use. In this case, the PostgreSQL database addon.\nThe build section defines how Heroku builds the application. In this case it uses the Docker buildpack to build a web service based on the supplied Dockerfile.\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\hetzner.md",
    "content": "Hosting n8n on Hetzner cloud\nThis hosting guide shows you how to self-host n8n on a Hetzner cloud server. It uses:\nCaddy (a reverse proxy) to allow access to the Server from the internet.\nDocker Compose to create and define the application components and how they work together.\nCreate a server\nLog in to the Hetzner Cloud Console.\nSelect the project to host the server, or create a new project by selecting + NEW PROJECT.\nSelect + CREATE SERVER on the project tile you want to add it to.\nYou can change most of the settings to suit your needs, but as this guide uses Docker to run the application, under the Image section, select \"Docker CE\" from the APPS tab.\nLog in to your server\nThe rest of this guide requires you to log in to the server using a terminal with SSH. Refer to Access with SSH/rsync/BorgBackup for more information. You can find the public IP in the listing of the servers in your project.\nInstall Docker Compose\nThe Hetzner Docker app image doesn't have Docker compose installed. Install it with the following commands:\nClone configuration repository\nDocker Compose, n8n, and Caddy require a series of folders and configuration files. You can clone these from this repository into the root user folder of the server. The following steps will tell you which file to change and what changes to make.\nClone the repository with the following command:\nAnd change directory to the root of the repository you cloned:\nDefault folders and files\nThe host operating system (the server) copies the two folders you created to Docker containers to make them available to Docker. The two folders are:\ncaddy_config: Holds the Caddy configuration files.\nlocal_files: A folder for files you upload or add using n8n.\nCreate Docker volume\nTo persist the Caddy cache between restarts and speed up start times, create a Docker volume that Docker reuses between restarts:\nCreate a Docker volume for the n8n data:\nSet up DNS\nn8n typically operates on a subdomain. Create a DNS record with your provider for the subdomain and point it to the IP address of the server. The exact steps for this depend on your DNS provider, but typically you need to create a new \"A\" record for the n8n subdomain. DigitalOcean provide An Introduction to DNS Terminology, Components, and Concepts.\nOpen ports\nn8n runs as a web application, so the server needs to allow incoming access to traffic on port 80 for non-secure traffic, and port 443 for secure traffic.\nOpen the following ports in the server's firewall by running the following two commands:\nConfigure n8n\nn8n needs some environment variables set to pass to the application running in the Docker container. The example .env file contains placeholders you need to replace with values of your own.\nOpen the file with the following command:\nThe file contains inline comments to help you know what to change.\nRefer to Environment variables for n8n environment variables details.\nThe Docker Compose file\nThe Docker Compose file (docker-compose.yml) defines the services the application needs, in this case Caddy and n8n.\nThe Caddy service definition defines the ports it uses and the local volumes to copy to the containers.\nThe n8n service definition defines the ports it uses, the environment variables n8n needs to run (some defined in the .env file), and the volumes it needs to copy to the containers.\nThe Docker Compose file uses the environment variables set in the .env file, so you shouldn't need to change it's content, but to take a look, run the following command:\nConfigure Caddy\nCaddy needs to know which domains it should serve, and which port to expose to the outside world. Edit the Caddyfile file in the caddy_config folder.\nChange the placeholder subdomain to yours. If you followed the steps to name the subdomain n8n, your full domain is similar to n8n.example.com. The n8n in the reverse_proxy setting tells Caddy to use the service definition defined in the docker-compose.yml file:\nStart Docker Compose\nStart n8n and Caddy with the following command:\nThis may take a few minutes.\nTest your setup\nIn your browser, open the URL formed of the subdomain and domain name defined earlier. Enter the user name and password defined earlier, and you should be able to access n8n.\nStop n8n and Caddy\nYou can stop n8n and Caddy with the following command:\nUpdating\nNext steps"
  },
  {
    "file_path": "hosting\\installation\\server-setups\\index.md",
    "content": "Server setups\nSelf-host with Docker Compose:\nDigital Ocean\nHeroku\nHetzner Cloud\nStarting points for a Kubernetes setup:\nAWS\nAzure\nGoogle Cloud Platform\nConfiguration guides to help you get started on other platforms:\nDocker Compose"
  },
  {
    "file_path": "hosting\\logging-monitoring\\logging.md",
    "content": "Logging in n8n\nLogging is an important feature for debugging. n8n uses the winston logging library.\nSetup\nTo set up logging in n8n, you need to set the following environment variables (you can also set the values in the configuration file)\nTABLE_PLACEHOLDER_0\nLog levels\nn8n uses standard log levels to report:\nsilent: outputs nothing at all\nerror: outputs only errors and nothing else\nwarn: outputs errors and warning messages\ninfo: contains useful information about progress\ndebug: the most verbose output. n8n outputs a lot of information to help you debug issues.\nDevelopment\nDuring development, adding log messages is a good practice. It assists in debugging errors. To configure logging for development, follow the guide below.\nImplementation details\nn8n uses the LoggerProxy class, located in the workflow package. Calling the LoggerProxy.init() by passing in an instance of Logger, initializes the class before the usage.\nThe initialization process happens only once. The start.ts file already does this process for you. If you are creating a new command from scratch, you need to initialize the LoggerProxy class.\nOnce the Logger implementation gets created in the cli package, it can be obtained by calling the getInstance convenience method from the exported module.\nCheck the start.ts file to learn more about how this process works.\nAdding logs\nOnce the LoggerProxy class gets initialized in the project, you can import it to any other file and add logs.\nConvenience methods are provided for all logging levels, so new logs can be added whenever needed using the format Logger.('', ...meta), where meta represents any additional properties desired beyond message.\nIn the example above, we use the standard log levels described above. The message argument is a string, and meta is a data object.\nWhen creating new loggers, some useful standards to keep in mind are:\nCraft log messages to be as human-readable as possible. For example, always wrap names in quotes.\nDuplicating information in the log message and metadata, like workflow name in the above example, can be useful as messages are easier to search and metadata enables easier filtering.\nInclude multiple IDs (for example, executionId, workflowId, and sessionId) throughout all logs.\nUse node types instead of node names (or both) as this is more consistent, and so easier to search.\nFront-end logs\nAs of now, front-end logs aren't available. Using Logger or LoggerProxy would yield errors in the editor-ui package. This functionality will get implemented in the future versions."
  },
  {
    "file_path": "hosting\\logging-monitoring\\monitoring.md",
    "content": "Monitoring\nThere are three API endpoints you can call to check the status of your instance: /healthz, healthz/readiness, and /metrics.\nhealthz and healthz/readiness\nThe /healthz endpoint returns a standard HTTP status code. 200 indicates the instance is reachable. It doesn't indicate DB status. It's available for both self-hosted and Cloud users.\nAccess the endpoint:\nThe /healthz/readiness endpoint is similar to the /healthz endpoint, but it returns a HTTP status code of 200 if the DB is connected and migrated and therefore the instance is ready to accept traffic.\nAccess the endpoint:\nmetrics\nThe /metrics endpoint provides more detailed information about the current status of the instance.\nAccess the endpoint:\nEnable metrics and healthz for self-hosted n8n\nThe /metrics and /healthz endpoints are disabled by default. To enable them, configure your n8n instance:\nRefer to Configuration methods for more information on how to configure your instance using environment variables."
  },
  {
    "file_path": "hosting\\scaling\\binary-data.md",
    "content": "Binary data\nBinary data is any file-type data, such as image files or documents generated or processed during the execution of a workflow.\nEnable filesystem mode\nWhen handling binary data, n8n keeps the data in memory by default. This can cause crashes when working with large files.\nTo avoid this, change the N8N_DEFAULT_BINARY_DATA_MODE environment variable to filesystem. This causes n8n to save data to disk, instead of using memory.\nIf you're using queue mode, keep this to default. n8n doesn't support filesystem mode with queue mode.\nBinary data pruning\nn8n executes binary data pruning as part of execution data pruning. Refer to Execution data | Enable data pruning for details.\nIf you configure multiple binary data modes, binary data pruning operates on the active binary data mode. For example, if your instance stored data in S3, and you later switched to filesystem mode, n8n only prunes binary data in the filesystem. Refer to External storage for details."
  },
  {
    "file_path": "hosting\\scaling\\concurrency-control.md",
    "content": "Self-hosted concurrency control\nIn regular mode, n8n doesn't limit how many production executions may run at the same time. This can lead to a scenario where too many concurrent executions thrash the event loop, causing performance degradation and unresponsiveness.\nTo prevent this, you can set a concurrency limit for production executions in regular mode. Use this to control how many production executions run concurrently, and queue up any concurrent production executions over the limit. These executions remain in the queue until concurrency capacity frees up, and are then processed in FIFO order.\nConcurrency control is disabled by default. To enable it:\nKeep in mind:\nConcurrency control applies only to production executions: those started from a webhook or trigger node. It doesn't apply to any other kinds, such as manual executions, sub-workflow executions, error executions, or started from CLI.\nYou can't retry queued executions. Cancelling or deleting a queued execution also removes it from the queue.\nOn instance startup, n8n resumes queued executions up to the concurrency limit and re-enqueues the rest.\nTo monitor concurrency control, watch logs for executions being added to the queue and released. In a future version, n8n will show concurrency control in the UI.\nWhen you enable concurrency control, you can view the number of active executions and the configured limit at the top of a project's or workflow's executions tab.\nComparison to queue mode\nIn queue mode, you can control how many jobs a worker may run concurrently using the --concurrency flag.\nConcurrency control in queue mode is a separate mechanism from concurrency control in regular mode, but the environment variable N8N_CONCURRENCY_PRODUCTION_LIMIT controls both of them. In queue mode, n8n takes the limit from this variable if set to a value other than -1, falling back to the --concurrency flag or its default."
  },
  {
    "file_path": "hosting\\scaling\\execution-data.md",
    "content": "Execution data\nDepending on your executions settings and volume, your n8n database can grow in size and run out of storage.\nTo avoid this, n8n recommends that you don't save unnecessary data, and enable pruning of old executions data.\nTo do this, configure the corresponding environment variables.\nReduce saved data\nYou can select which executions data n8n saves. For example, you can save only executions that result in an Error.\nEnable data pruning\nYou can enable data pruning to automatically delete finished executions after a given time. If you don't set EXECUTIONS_DATA_MAX_AGE, 336 hours (14 days) is the default.\nYou can choose to prune finished executions data before the time set in EXECUTIONS_DATA_MAX_AGE, using EXECUTIONS_DATA_PRUNE_MAX_COUNT. This sets a maximum number of executions to store in the database. Once you reach the limit, n8n starts to delete the oldest execution records. This can help with database performance issues, especially if you use SQLite. The database size can still exceed the limit you set: old executions that haven't finished running don't get deleted, even if they would otherwise be subject to deletion."
  },
  {
    "file_path": "hosting\\scaling\\external-storage.md",
    "content": "External storage\nn8n can store binary data produced by workflow executions externally. This feature is useful to avoid relying on the filesystem for storing large amounts of binary data.\nn8n will introduce external storage for other data types in the future.\nStoring n8n's binary data in S3\nn8n supports AWS S3 as an external store for binary data produced by workflow executions. You can use other S3-compatible services like Cloudflare R2 and Backblaze B2, but n8n doesn't officially support these.\nSetup\nCreate and configure a bucket following the AWS documentation. You can use the following policy, replacing  with the name of the bucket you created:\nSet a bucket-level lifecycle configuration so that S3 automatically deletes old binary data. n8n delegates pruning of binary data to S3, so setting a lifecycle configuration is required unless you want to preserve binary data indefinitely.\nOnce you finish creating the bucket, you will have a host, bucket name and region, and an access key ID and secret access key. You need to set them in n8n's environment:\nTell n8n to store binary data in S3:\nRestart the server to load the new configuration.\nUsage\nAfter you enable S3, n8n writes and reads any new binary data to and from the S3 bucket. n8n writes binary data to your S3 bucket in this format:\nn8n continues to read older binary data stored in the filesystem from the filesystem, if filesystem remains listed as an option in N8N_AVAILABLE_BINARY_DATA_MODES.\nIf you store binary data in S3 and later switch to filesystem mode, the instance continues to read any data stored in S3, as long as s3 remains listed in N8N_AVAILABLE_BINARY_DATA_MODES and your S3 credentials remain valid."
  },
  {
    "file_path": "hosting\\scaling\\memory-errors.md",
    "content": "Memory-related errors\nn8n doesn't restrict the amount of data each node can fetch and process. While this gives you freedom, it can lead to errors when workflow executions require more memory than available. This page explains how to identify and avoid these errors.\nIdentifying out of memory situations\nn8n provides error messages that warn you in some out of memory situations. For example, messages such as Execution stopped at this node (n8n may have run out of memory while executing it).\nError messages including Problem running workflow, Connection Lost, or 503 Service Temporarily Unavailable suggest that an n8n instance has become unavailable.\nWhen self-hosting n8n, you may also see error messages such as Allocation failed - JavaScript heap out of memory in your server logs.\nOn n8n Cloud, or when using n8n's Docker image, n8n restarts automatically when encountering such an issue. However, when running n8n with npm you might need to restart it manually.\nTypical causes\nSuch problems occur when a workflow execution requires more memory than available to an n8n instance. Factors increasing the memory usage for a workflow execution include:\nAmount of JSON data.\nSize of binary data.\nNumber of nodes in a workflow.\nSome nodes are memory-heavy: the Code node and the older Function node can increase memory consumption significantly.\nManual or automatic workflow executions: manual executions increase memory consumption as n8n makes a copy of the data for the frontend.\nAdditional workflows running at the same time.\nAvoiding out of memory situations\nWhen encountering an out of memory situation, there are two options: either increase the amount of memory available to n8n or reduce the memory consumption.\nIncrease available memory\nWhen self-hosting n8n, increasing the amount of memory available to n8n means provisioning your n8n instance with more memory. This may incur additional costs with your hosting provider.\nOn n8n cloud you need to upgrade to a larger plan.\nReduce memory consumption\nThis approach is more complex and means re-building the workflows causing the issue. This section provides some guidelines on how to reduce memory consumption. Not all suggestions are applicable to all workflows.\nIncrease old memory\nThis applies to self-hosting n8n. When encountering JavaScript heap out of memory errors, it's often useful to allocate additional memory to the old memory section of the V8 JavaScript engine. To do this, set the appropriate V8 option --max-old-space-size=SIZE either through the CLI or through the NODE_OPTIONS environment variable."
  },
  {
    "file_path": "hosting\\scaling\\overview.md",
    "content": "Scaling n8n\nWhen running n8n at scale, with a large number of users, workflows, or executions, you need to change your n8n configuration to ensure good performance.\nn8n can run in different modes depending on your needs. The queue mode provides the best scalability. Refer to Queue mode for configuration details.\nYou can configure data saving and pruning to improve database performance. Refer to Execution data for details."
  },
  {
    "file_path": "hosting\\scaling\\performance-benchmarking.md",
    "content": "Performance and benchmarking\nn8n can handle up to 220 workflow executions per second on a single instance, with the ability to scale up further by adding more instances.\nThis document outlines n8n's performance benchmarking. It describes the factors that affect performance, and includes two example benchmarks.\nPerformance factors\nThe performance of n8n depends on factors including:\nThe workflow type\nThe resources available to n8n\nHow you configure n8n's scaling options\nRun your own benchmarking\nTo get an accurate estimate for your use case, run n8n's benchmarking framework. The repository contains more information about the benchmarking.\nExample: Single instance performance\nThis test measures how response time increases as requests per second increase. It looks at the response time when calling the Webhook Trigger node.\nSetup:\nHardware: ECS c5a.large instance (4GB RAM)\nn8n setup: Single n8n instance (running in main mode, with Postgres database)\nWorkflow: Webhook Trigger node, Edit Fields node\n!Graph showing n8n response times by requests per second\nThis graph shows the percentage of requests to the Webhook Trigger node getting a response within 100 seconds, and how that varies with load. Under higher loads n8n usually still processes the data, but takes over 100s to respond.\nExample: Multi-instance performance\nThis test measures how response time increases as requests per second increase. It looks at the response time when calling the Webhook Trigger node.\nSetup:\nHardware: seven ECS c5a.4xlarge instances (8GB RAM each)\nn8n setup: two webhook instances, four worker instances, one database instance (MySQL), one main instance running n8n and Redis\nWorkflow: Webhook Trigger node, Edit Fields node\nMulti-instance setups use Queue mode\n!Graph showing n8n response times by requests per second\nThis graph shows the percentage of requests to the Webhook Trigger node getting a response within 100 seconds, and how that varies with load. Under higher loads n8n usually still processes the data, but takes over 100s to respond."
  },
  {
    "file_path": "hosting\\scaling\\queue-mode.md",
    "content": "Queue mode\nYou can run n8n in different modes depending on your needs. The queue mode provides the best scalability.\nHow it works\nWhen running in queue mode, you have multiple n8n instances set up, with one main instance receiving workflow information (such as triggers) and the worker instances performing the executions.\nEach worker is its own Node.js instance, running in main mode, but able to handle multiple simultaneous workflow executions due to their high IOPS (input-output operations per second).\nBy using worker instances and running in queue mode, you can scale n8n up (by adding workers) and down (by removing workers) as needed to handle the workload.\nThis is the process flow:\nThe main n8n instance handles timers and webhook calls, generating (but not running) a workflow execution.\nIt passes the execution ID to a message broker, Redis, which maintains the queue of pending executions and allows the next available worker to pick them up.\nA worker in the pool picks up message from Redis.\nThe worker uses the execution ID to get workflow information from the database.\nAfter completing the workflow execution, the worker:\nWrites the results to the database.\nPosts to Redis, saying that the execution has finished.\nRedis notifies the main instance.\n!\"Diagram showing the flow of data between the main n8n instance, Redis, the n8n workers, and the n8n database\"\nConfiguring workers\nWorkers are n8n instances that do the actual work. They receive information from the main n8n process about the workflows that have to get executed, execute the workflows, and update the status after each execution is complete.\nSet encryption key\nn8n automatically generates an encryption key upon first startup. You can also provide your own custom key using environment variable if desired.\nThe encryption key of the main n8n instance must be shared with all worker and webhooks processor nodes to ensure these worker nodes are able to access credentials stored in the database.\nSet the encryption key for each worker node in a configuration file or by setting the corresponding environment variable:\nSet executions mode\nSet the environment variable EXECUTIONS_MODE to queue on the main instance and any workers using the following command.\nAlternatively, you can set executions.mode to queue in the configuration file.\nStart Redis\nTo run Redis in a Docker container, follow the instructions below:\nRun the following command to start a Redis instance:\nBy default, Redis runs on localhost on port 6379 with no password. Based on your Redis configuration, set the following configurations for the main n8n process. These will allow n8n to interact with Redis.\nTABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "hosting\\securing\\blocking-nodes.md",
    "content": "Block access to nodes\nFor security reasons, you may want to block your users from accessing or working with specific n8n nodes. This is helpful if your users might be untrustworthy.\nUse the NODES_EXCLUDE environment variable to prevent your users from accessing specific nodes.\nExclude nodes\nUpdate your NODES_EXCLUDE environment variable to include an array of strings containing any nodes you want to block your users from using.\nFor example, setting the variable this way:\nBlocks the Execute Command and Read/Write Files from Disk nodes.\nYour n8n users won't be able to search for or use these nodes.\nSuggested nodes to block\nThe nodes that can pose security risks vary based on your use case and user profile. Here are some nodes you might want to start with:\nExecute Command\nRead/Write Files from Disk\nRelated resources\nRefer to Nodes environment variables for more information on this environment variable.\nRefer to Configuration for more information on setting environment variables."
  },
  {
    "file_path": "hosting\\securing\\disable-public-api.md",
    "content": "Disable the public REST API\nThe n8n public REST API allows you to programmatically perform many of the same tasks as you can in the n8n GUI.\nIf you don't plan on using this API, n8n recommends disabling it to improve the security of your n8n installation.\nTo disable the public REST API, set the N8N_PUBLIC_API_DISABLED environment variable to true, for example:\nDisable the API playground\nTo disable the API playground, set the N8N_PUBLIC_API_SWAGGERUI_DISABLED environment variable to true, for example:\nRelated resources\nRefer to Deployment environment variables for more information on these environment variables.\nRefer to Configuration for more information on setting environment variables."
  },
  {
    "file_path": "hosting\\securing\\hardening-task-runners.md",
    "content": "Hardening task runners\nTask runners are responsible for executing code from the Code node. While Code node executions are secure, you can follow these recommendations to further harden your task runners.\nRun task runners as sidecars in external mode\nTo increase the isolation between the core n8n process and code in the Code node, run task runners in external mode. External task runners launch as separate containers, providing a fully isolated environment to execute the JavaScript defined in the Code node."
  },
  {
    "file_path": "hosting\\securing\\overview.md",
    "content": "Securing n8n\nSecuring your n8n instance can take several forms.\nAt a high level, you can:\nConduct a security audit to identify security risks.\nSet up SSL to enforce secure connections.\nSet up Single Sign-On for user account management.\nUse two-factor authentication (2FA) for your users.\nMore granularly, consider blocking or opting out of features or data collection you don't want:\nDisable the public API if you aren't using it.\nOpt out of data collection of the anonymous data n8n collects automatically.\nBlock certain nodes from being available to your users."
  },
  {
    "file_path": "hosting\\securing\\security-audit.md",
    "content": "Security audit\nYou can run a security audit on your n8n instance, to detect common security issues.\nRun an audit\nYou can run an audit using the CLI, the public API, or the n8n node.\nCLI\nRun n8n audit.\nAPI\nMake a POST call to the /audit endpoint. You must authenticate as the instance owner.\nn8n node\nAdd the n8n node to your workflow. Select Resource > Audit and Operation > Generate.\nReport contents\nThe audit generates five risk reports:\nCredentials\nThis report shows:\nCredentials not used in a workflow.\nCredentials not used in an active workflow.\nCredentials not use in a recently active workflow.\nDatabase\nThis report shows:\nExpressions used in Execute Query fields in SQL nodes.\nExpressions used in Query Parameters fields in SQL nodes.\nUnused Query Parameters fields in SQL nodes.\nFile system\nThis report lists nodes that interact with the file system.\nNodes\nThis report shows:\nOfficial risky nodes. These are n8n built in nodes. You can use them to fetch and run any code on the host system, which exposes the instance to exploits. You can view the list in n8n code | Audit constants, under OFFICIAL_RISKY_NODE_TYPES.\nCommunity nodes.\nCustom nodes.\nInstance\nThis report shows:\nUnprotected webhooks in the instance.\nMissing security settings\nIf your instance is outdated."
  },
  {
    "file_path": "hosting\\securing\\set-up-ssl.md",
    "content": "Set up SSL\nThere are two methods to support TLS/SSL in n8n.\nUse a reverse proxy (recommended)\nUse a reverse proxy like Traefik or a Network Load Balancer (NLB) in front of the n8n instance. This should also take care of certificate renewals.\nRefer to Security | Data encryption for more information.\nPass certificates into n8n directly\nYou can also choose to pass certificates into n8n directly. To do so, set the N8N_SSL_CERT and N8N_SSL_KEY environment variables to point to your generated certificate and key file.\nYou'll need to make sure the certificate stays renewed and up to date.\nRefer to Deployment environment variables for more information on these variables and Configuration for more information on setting environment variables."
  },
  {
    "file_path": "hosting\\securing\\telemetry-opt-out.md",
    "content": "Data collection\nn8n collects some anonymous data from self-hosted n8n installations. Use the instructions below to opt out of data telemetry collection.\nCollected data\nRefer to Privacy | Data collection in self-hosted n8n for details on the data n8n collects.\nHow collection works\nYour n8n instance sends most data to n8n as the events that generate it occur. Workflow execution counts and an instance pulse are sent periodically (every 6 hours). These data types mostly fall into n8n telemetry collection.\nOpting out of data collection\nn8n enables telemetry collection by default. To disable it, configure the following environment variables.\nOpt out of telemetry events\nTo opt out of telemetry events, set the N8N_DIAGNOSTICS_ENABLED environment variable to false, for example:\nOpt out of checking for new versions of n8n\nTo opt out of checking for new versions of n8n, set the N8N_VERSION_NOTIFICATIONS_ENABLED environment variable to false, for example:\nDisable all connection to n8n servers\nIf you want to fully prevent all communication with n8n's servers, refer to Isolate n8n.\nRelated resources\nRefer to Deployment environment variables for more information on these environment variables.\nRefer to Configuration for more information on setting environment variables."
  },
  {
    "file_path": "hosting\\starter-kits\\ai-starter-kit.md",
    "content": "Self-hosted AI Starter Kit\nThe Self-hosted AI Starter Kit is an open, docker compose template that bootstraps a fully featured Local AI and Low Code development environment.\nCurated by n8n, it combines the self-hosted n8n platform with a list of compatible AI products and components to get you started building self-hosted AI workflows.\nWhat‚Äôs included\n‚úÖ Self-hosted n8n: Low-code platform with over 400 integrations and advanced AI components.\n‚úÖ Ollama: Cross-platform LLM platform to install and run the latest local LLMs.\n‚úÖ Qdrant: Open-source, high performance vector store with a comprehensive API.\n‚úÖ PostgreSQL: The workhorse of the Data Engineering world, handles large amounts of data safely.\nWhat you can build\n‚≠êÔ∏è AI Agents{ data-preview} that can schedule appointments\n‚≠êÔ∏è Summaries of company PDFs without leaking data\n‚≠êÔ∏è Smarter Slackbots for company communications and IT-ops\n‚≠êÔ∏è Private, low-cost analyses of financial documents\nGet the kit\nHead to the GitHub repository to clone the repo and get started!"
  },
  {
    "file_path": "integrations\\custom-operations.md",
    "content": "Custom API operations\nPredefined credential types\nA predefined credential type is a credential that already exists in n8n. You can use predefined credential types instead of generic credentials in the HTTP Request node.\nFor example: you create an Asana credential, for use with the Asana node. Later, you want to perform an operation that isn't supported by the Asana node, using Asana's API. You can use your existing Asana credential in the HTTP Request node to perform the operation, without additional authentication setup.\nUsing predefined credential types\nCredential scopes\nSome existing credential types have specific scopes: endpoints that they work with. n8n warns you about this when you select the credential type.\nFor example, follow the steps in Using predefined credential types, and select Google Calendar OAuth2 API as your Credential Type. n8n displays a box listing the two endpoints you can use this credential type with:\n!The scopes box"
  },
  {
    "file_path": "integrations\\index.md",
    "content": "Integrations\nn8n calls integrations nodes.\nNodes are the building blocks of workflows in n8n. They're an entry point for retrieving data, a function to process data, or an exit for sending data. The data process includes filtering, recomposing, and changing data. There can be one or several nodes for your API, service or app. You can connect multiple nodes, which allows you to create complex workflows.\nBuilt-in nodes\nn8n includes a collection of built-in integrations. Refer to Built-in nodes for documentation on all n8n's built-in nodes.\nCommunity nodes\nAs well as using the built-in nodes, you can also install community-built nodes. Refer to Community nodes for more information.\nCredential-only nodes and custom operations\nRefer to Custom operations for more information.\nGeneric integrations\nIf you need to connect to a service where n8n doesn't have a node, or a credential-only node, you can still use the HTTP Request node. Refer to the node page for details on how to set up authentication and create your API call.\nWhere to go next\nIf you want to create your own node, head over to the Creating Nodes section.\nCheck out Community nodes to learn about installing and managing community-built nodes.\nIf you'd like to learn more about the different nodes in n8n, their functionalities and example usage, check out n8n's node libraries: Core nodes, Actions, and Triggers.\nIf you'd like to learn how to add the credentials for the different nodes, head over to the Credentials section."
  },
  {
    "file_path": "integrations\\builtin\\node-types.md",
    "content": "Built-in integrations\nThis section contains the node library: reference documentation for every built-in node in n8n, and their credentials.\nCore nodes\nCore nodes can be actions or triggers. Whereas most nodes connect to a specific external service, core nodes provide functionality such as logic, scheduling, or generic API calls.\nCluster nodes\nCredentials\nExternal services need a way to identify and authenticate users. This data can range from an API key over an email/password combination to a long multi-line private key. You can save these in n8n as credentials.\nNodes in n8n can then request that credential information. As another layer of security, only node types with specific access rights can access the credentials.\nTo make sure that the data is secure, it gets saved to the database encrypted. n8n uses a random personal encryption key, which it automatically generates on the first run of n8n and then saved under ~/.n8n/config.\nTo learn more about creating, managing, and sharing credentials, refer to Manage credentials.\nCommunity nodes\nn8n supports custom nodes built by the community. Refer to Community nodes for guidance on installing and using these nodes.\nFor help building your own custom nodes, and publish them to npm, refer to Creating nodes for more information."
  },
  {
    "file_path": "integrations\\builtin\\rate-limits.md",
    "content": "Handling API rate limits\nAPI rate limits are restrictions on request frequency. For example, an API may limit the number of requests you can make per minute, or per day.\nAPIs can also limits how much data you can send in one request, or how much data the API sends in a single response.\nIdentify rate limit issues\nWhen an n8n node hits a rate limit, it errors. n8n displays the error message in the node output panel. This includes the error message from the service.\nIf n8n received error 429 (too many requests) from the service, the error message is The service is receiving too many requests from you.\nTo check the rate limits for the service you're using, refer to the API documentation for the service.\nHandle rate limits for integrations\nThere are two ways to handle rate limits in n8n's integrations: using the Retry On Fail setting, or using a combination of the Loop Over Items and Wait nodes:\nRetry On Fail adds a pause between API request attempts.\nWith Loop Over Items and Wait you can break you request data into smaller chunks, as well as pausing between requests.\nEnable Retry On Fail\nWhen you enable Retry On Fail, the node automatically tries the request again if it fails the first time.\nOpen the node.\nSelect Settings.\nEnable the Retry On Fail toggle.\nConfigure the retry settings: if using this to work around rate limits, set Wait Between Tries (ms) to more than the rate limit. For example, if the API you're using allows one request per second, set Wait Between Tries (ms) to 1000 to allow a 1 second wait.\nUse Loop Over Items and Wait\nUse the Loop Over Items node to batch the input items, and the Wait node to introduce a pause between each request.\nAdd the Loop Over Items node before the node that calls the API. Refer to Loop Over Items for information on how to configure the node.\nAdd the Wait node after the node that calls the API, and connect it back to the Loop Over Items node. Refer to Wait for information on how to configure the node.\nFor example, to handle rate limits when using OpenAI:\n!\"Screenshot of a workflow using the Loop Over Items node and Wait node to handle API rate limits for the OpenAI APIs\"\nHandle rate limits in the HTTP Request node\nThe HTTP Request node has built-in settings for handling rate limits and large amounts of data.\nBatch requests\nUse the Batching option to send more than one request, reducing the request size, and introducing a pause between requests. This is the equivalent of using Loop Over Items and Wait.\nIn the HTTP Request node, select Add Option > Batching.\nSet Items per Batch: this is the number of input items to include in each request.\nSet Batch Interval (ms) to introduce a delay between requests. For example, if the API you're using allows one request per second, set Wait Between Tries (ms) to 1000 to allow a 1 second wait.\nPaginate results\nAPIs paginate their results when they need to send more data than they can handle in a single response. For more information on pagination in the HTTP Request node, refer to HTTP Request node | Pagination."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\index.md",
    "content": "Actions library\nThis section provides information about n8n's Actions."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.actionnetwork.md",
    "content": "Action Network node\nUse the Action Network node to automate work in Action Network, and integrate Action Network with other applications. n8n has built-in support for a wide range of Action Network features, including creating, updating, and deleting events, people, tags, and signatures.\nOn this page, you'll find a list of operations the Action Network node supports, and links to more resources.\nOperations\nAttendance\nCreate\nGet\nGet All\nEvent\nCreate\nGet\nGet All\nPerson\nCreate\nGet\nGet All\nUpdate\nPerson Tag\nAdd\nRemove\nPetition\nCreate\nGet\nGet All\nUpdate\nSignature\nCreate\nGet\nGet All\nUpdate\nTag\nCreate\nGet\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.activecampaign.md",
    "content": "ActiveCampaign node\nUse the ActiveCampaign node to automate work in ActiveCampaign, and integrate ActiveCampaign with other applications. n8n has built-in support for a wide range of ActiveCampaign features, including creating, getting, updating, and deleting accounts, contact, orders, e-commerce customers, connections, lists, tags, and deals.\nOn this page, you'll find a list of operations the ActiveCampaign node supports and links to more resources.\nOperations\nAccount\nCreate an account\nDelete an account\nGet data of an account\nGet data of all accounts\nUpdate an account\nAccount Contact\nCreate an association\nDelete an association\nUpdate an association\nContact\nCreate a contact\nDelete a contact\nGet data of a contact\nGet data of all contact\nUpdate a contact\nContact List\nAdd contact to a list\nRemove contact from a list\nContact Tag\nAdd a tag to a contact\nRemove a tag from a contact\nConnection\nCreate a connection\nDelete a connection\nGet data of a connection\nGet data of all connections\nUpdate a connection\nDeal\nCreate a deal\nDelete a deal\nGet data of a deal\nGet data of all deals\nUpdate a deal\nCreate a deal note\nUpdate a deal note\nE-commerce Order\nCreate a order\nDelete a order\nGet data of a order\nGet data of all orders\nUpdate a order\nE-Commerce Customer\nCreate a E-commerce Customer\nDelete a E-commerce Customer\nGet data of a E-commerce Customer\nGet data of all E-commerce Customer\nUpdate a E-commerce Customer\nE-commerce Order Products\nGet data of all order products\nGet data of a ordered product\nGet data of an order's products\nList\nGet all lists\nTag\nCreate a tag\nDelete a tag\nGet data of a tag\nGet data of all tags\nUpdate a tag\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.adalo.md",
    "content": "Adalo node\nUse the Adalo node to automate work in Adalo, and integrate Adalo with other applications. n8n has built-in support for a wide range of Adalo features, including like creating, getting, updating and deleting databases, records, and collections.\nOn this page, you'll find a list of operations the Adalo node supports and links to more resources.\nOperations\nCollection\nCreate\nDelete\nGet\nGet Many\nUpdate\nTemplates and examples\nRelated resources\nRefer to Adalo's documentation for more information on using Adalo. Their External Collections with APIs page gives more detail about what you can do with Adalo collections."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.affinity.md",
    "content": "Affinity node\nUse the Affinity node to automate work in Affinity, and integrate Affinity with other applications. n8n has built-in support for a wide range of Affinity features, including creating, getting, updating and deleting lists, entries, organization, and persons.\nOn this page, you'll find a list of operations the Affinity node supports and links to more resources.\nOperations\nList\nGet a list\nGet all lists\nList Entry\nCreate a list entry\nDelete a list entry\nGet a list entry\nGet all list entries\nOrganization\nCreate an organization\nDelete an organization\nGet an organization\nGet all organizations\nUpdate an organization\nPerson\nCreate a person\nDelete a person\nGet a person\nGet all persons\nUpdate a person\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.agilecrm.md",
    "content": "Agile CRM node\nUse the Agile CRM node to automate work in Agile CRM, and integrate Agile CRM with other applications. n8n has built-in support for a wide range of Agile CRM features, including creating, getting, updating and deleting companies, contracts, and deals.\nOn this page, you'll find a list of operations the Agile CRM node supports and links to more resources.\nOperations\nCompany\nCreate a new company\nDelete a company\nGet a company\nGet all companies\nUpdate company properties\nContact\nCreate a new contact\nDelete a contact\nGet a contact\nGet all contacts\nUpdate contact properties\nDeal\nCreate a new deal\nDelete a deal\nGet a deal\nGet all deals\nUpdate deal properties\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.airtop.md",
    "content": "Airtop node\nUse the Airtop node to automate work in Airtop, and integrate Airtop with other applications. n8n has built-in support for a wide range of Airtop features, enabling you to control a cloud-based web browser for tasks like querying, scraping, and interacting with web pages.\nOn this page, you'll find a list of operations the Airtop node supports, and links to more resources.\nOperations\nSession\nCreate session\nSave profile on termination\nTerminate session\nWindow\nCreate a new browser window\nLoad URL\nTake screenshot\nClose window\nExtraction\nQuery page\nQuery page with pagination\nSmart scrape page\nInteraction\nClick an element\nHover on an element\nType\nTemplates and examples\nRelated resources\nRefer to Airtop's documentation for more information about the service.\nContact Airtop's Support for assistance or to create a feature request.\nNode reference\nCreate a session and window\nCreate an Airtop browser session to get a Session ID, then use it to create a new browser window. After this, you can use any extraction or interaction operation.\nExtract content\nExtract content from a web browser using these operations:\nQuery page: Extract information from the current window.\nQuery page with pagination: Extract information from pages with pagination or infinite scrolling.\nSmart scrape page: Get the window content as markdown.\nGet JSON responses by using the JSON Output Schema parameter in query operations.\nInteracting with pages\nClick, hover, or type on elements by describing the element you want to interact with.\nTerminate a session\nEnd your session to save resources. Sessions are automatically terminated based on the Idle Timeout set in the Create Session operation or can be manually terminated using the Terminate Session operation."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.amqp.md",
    "content": "AMQP Sender node\nUse the AMQP Sender node to automate work in AMQP Sender, and integrate AMQP Sender with other applications. n8n has built-in support for a wide range of AMQP Sender features, including sending messages.\nOn this page, you'll find a list of operations the AMQP Sender node supports and links to more resources.\nOperations\nSend message\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.apitemplateio.md",
    "content": "APITemplate.io node\nUse the APITemplate.io node to automate work in APITemplate.io, and integrate APITemplate.io with other applications. n8n has built-in support for a wide range of APITemplate.io features, including getting and creating accounts and PDF.\nOn this page, you'll find a list of operations the APITemplate.io node supports and links to more resources.\nOperations\nAccount\nGet\nImage\nCreate\nPDF\nCreate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.asana.md",
    "content": "Asana node\nUse the Asana node to automate work in Asana, and integrate Asana with other applications. n8n has built-in support for a wide range of Asana features, including creating, updating, deleting, and getting users, tasks, projects, and subtasks.\nOn this page, you'll find a list of operations the Asana node supports and links to more resources.\nOperations\nProject\nCreate a new project\nDelete a project\nGet a project\nGet all projects\nUpdate a project\nSubtask\nCreate a subtask\nGet all subtasks\nTask\nCreate a task\nDelete a task\nGet a task\nGet all tasks\nMove a task\nSearch for tasks\nUpdate a task\nTask Comment\nAdd a comment to a task\nRemove a comment from a task\nTask Tag\nAdd a tag to a task\nRemove a tag from a task\nTask Project\nAdd a task to a project\nRemove a task from a project\nUser\nGet a user\nGet all users\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.automizy.md",
    "content": "Automizy node\nUse the Automizy node to automate work in Automizy, and integrate Automizy with other applications. n8n has built-in support for a wide range of Automizy features, including creating, reading, listing, updating, deleting contacts, and lists.\nOn this page, you'll find a list of operations the Automizy node supports and links to more resources.\nOperations\nContact\nCreate a contact\nDelete a contact\nGet a contact\nGet all contacts\nUpdate a contact\nList\nCreate a list\nDelete a list\nGet a list\nGet all lists\nUpdate a list\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.autopilot.md",
    "content": "Autopilot node\nUse the Autopilot node to automate work in Autopilot, and integrate Autopilot with other applications. n8n has built-in support for a wide range of Autopilot features, including creating, deleting, and updating contacts, as well as adding contacts to a list.\nOn this page, you'll find a list of operations the Autopilot node supports and links to more resources.\nOperations\nContact\nCreate/Update a contact\nDelete a contact\nGet a contact\nGet all contacts\nContact Journey\nAdd contact to list\nContact List\nAdd contact to list\nCheck if contact is on list\nGet all contacts on list\nRemove a contact from a list\nList\nCreate a list\nGet all lists\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awscertificatemanager.md",
    "content": "AWS Certificate Manager node\nUse the AWS Certificate Manager node to automate work in AWS Certificate Manager, and integrate AWS Certificate Manager with other applications. n8n has built-in support for a wide range of AWS Certificate Manager features, including creating, deleting, getting, and renewing SSL certificates.\nOn this page, you'll find a list of operations the AWS Certificate Manager node supports and links to more resources.\nOperations\nCertificate\nDelete\nGet\nGet Many\nGet Metadata\nRenew\nTemplates and examples\nRelated resources\nRefer to AWS Certificate Manager's documentation for more information on this service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awscognito.md",
    "content": "AWS Cognito node\nUse the AWS Cognito node to automate work in AWS Cognito and integrate AWS Cognito with other applications. n8n has built-in support for a wide range of AWS Cognito features, which includes creating, retrieving, updating, and deleting groups, users, and user pools.\nOn this page, you'll find a list of operations the AWS Cognito node supports, and links to more resources.\nOperations\nGroup:\nCreate: Create a new group.\nDelete: Delete an existing group.\nGet: Retrieve details about an existing group.\nGet Many: Retrieve a list of groups.\nUpdate: Update an existing group.\nUser:\nAdd to Group: Add an existing user to a group.\nCreate: Create a new user.\nDelete: Delete a user.\nGet: Retrieve information about an existing user.\nGet Many: Retrieve a list of users.\nRemove From Group: Remove a user from a group.\nUpdate: Update an existing user.\nUser Pool:\nGet: Retrieve information about an existing user pool.\nTemplates and examples\nRelated resources\nRefer to AWS Cognito's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awscomprehend.md",
    "content": "AWS Comprehend node\nUse the AWS Comprehend node to automate work in AWS Comprehend, and integrate AWS Comprehend with other applications. n8n has built-in support for a wide range of AWS Comprehend features, including identifying and analyzing texts.\nOn this page, you'll find a list of operations the AWS Comprehend node supports and links to more resources.\nOperations\nText\nIdentify the dominant language\nAnalyse the sentiment of the text\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awsdynamodb.md",
    "content": "AWS DynamoDB node\nUse the AWS DynamoDB node to automate work in AWS DynamoDB, and integrate AWS DynamoDB with other applications. n8n has built-in support for a wide range of AWS DynamoDB features, including creating, reading, updating, deleting items, and records on a database.\nOn this page, you'll find a list of operations the AWS DynamoDB node supports and links to more resources.\nOperations\nItem\nCreate a new record, or update the current one if it already exists (upsert/put)\nDelete an item\nGet an item\nGet all items\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awselb.md",
    "content": "AWS Elastic Load Balancing node\nUse the AWS Elastic Load Balancing node to automate work in AWS ELB, and integrate AWS ELB with other applications. n8n has built-in support for a wide range of AWS ELB features, including adding, getting, removing, deleting certificates and load balancers.\nOn this page, you'll find a list of operations the AWS ELB node supports and links to more resources.\nOperations\nListener Certificate\nAdd\nGet Many\nRemove\nLoad Balancer\nCreate\nDelete\nGet\nGet Many\nThis node supports creating and managing application and network load balancers. It doesn't currently support gateway load balancers.\nTemplates and examples\nRelated resources\nRefer to AWS ELB's documentation for more information on this service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awslambda.md",
    "content": "AWS Lambda node\nUse the AWS Lambda node to automate work in AWS Lambda, and integrate AWS Lambda with other applications. n8n has built-in support for a wide range of AWS Lambda features, including invoking functions.\nOn this page, you'll find a list of operations the AWS Lambda node supports and links to more resources.\nOperations\nInvoke a function\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awsrekognition.md",
    "content": "AWS Rekognition node\nUse the AWS Rekognition node to automate work in AWS Rekognition, and integrate AWS Rekognition with other applications. n8n has built-in support for a wide range of AWS Rekognition features, including analyzing images.\nOn this page, you'll find a list of operations the AWS Rekognition node supports and links to more resources.\nOperations\nImage\nAnalyze\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awss3.md",
    "content": "AWS S3 node\nUse the AWS S3 node to automate work in AWS S3, and integrate AWS S3 with other applications. n8n has built-in support for a wide range of AWS S3 features, including creating and deleting buckets, copying and downloading files, as well as getting folders.\nOn this page, you'll find a list of operations the AWS S3 node supports and links to more resources.\nOperations\nBucket\nCreate a bucket\nDelete a bucket\nGet all buckets\nSearch within a bucket\nFile\nCopy a file\nDelete a file\nDownload a file\nGet all files\nUpload a file\nFolder\nCreate a folder\nDelete a folder\nGet all folders\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awsses.md",
    "content": "AWS SES node\nUse the AWS SES node to automate work in AWS SES, and integrate AWS SES with other applications. n8n has built-in support for a wide range of AWS SES features, including creating, getting, deleting, sending, updating, and adding templates and emails.\nOn this page, you'll find a list of operations the AWS SES node supports and links to more resources.\nOperations\nCustom Verification Email\nCreate a new custom verification email template\nDelete an existing custom verification email template\nGet the custom email verification template\nGet all the existing custom verification email templates for your account\nAdd an email address to the list of identities\nUpdate an existing custom verification email template.\nEmail\nSend\nSend Template\nTemplate\nCreate a template\nDelete a template\nGet a template\nGet all templates\nUpdate a template\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awssns.md",
    "content": "AWS SNS node\nUse the AWS SNS node to automate work in AWS SNS, and integrate AWS SNS with other applications. n8n has built-in support for a wide range of AWS SNS features, including publishing messages.\nOn this page, you'll find a list of operations the AWS SNS node supports and links to more resources.\nOperations\nPublish a message to a topic\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awssqs.md",
    "content": "AWS SQS node\nUse the AWS SQS node to automate work in AWS SNS, and integrate AWS SQS with other applications. n8n has built-in support for a wide range of AWS SQS features, including sending messages.\nOn this page, you'll find a list of operations the AWS SQS node supports and links to more resources.\nOperations\nSend a message to a queue.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awstextract.md",
    "content": "AWS Textract node\nUse the AWS Textract node to automate work in AWS Textract, and integrate AWS Textract with other applications. n8n has built-in support for a wide range of AWS Textract features, including analyzing invoices.\nOn this page, you'll find a list of operations the AWS Textract node supports and links to more resources.\nOperations\nAnalyze Receipt or Invoice\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.awstranscribe.md",
    "content": "AWS Transcribe node\nUse the AWS Transcribe node to automate work in AWS Transcribe, and integrate AWS Transcribe with other applications. n8n has built-in support for a wide range of AWS Transcribe features, including creating, deleting, and getting transcription jobs.\nOn this page, you'll find a list of operations the AWS Transcribe node supports and links to more resources.\nOperations\nTranscription Job\nCreate a transcription job\nDelete a transcription job\nGet a transcription job\nGet all transcriptions job\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.azurecosmosdb.md",
    "content": "Azure Cosmos DB node\nUse the Azure Cosmos DB node to automate work in Azure Cosmos DB and integrate Azure Cosmos DB with other applications. n8n has built-in support for a wide range of Azure Cosmos DB features, which includes creating, getting, updating, and deleting containers and items.\nOn this page, you'll find a list of operations the Azure Cosmos DB node supports, and links to more resources.\nOperations\nContainer:\nCreate\nDelete\nGet\nGet Many\nItem:\nCreate\nDelete\nGet\nGet Many\nExecute Query\nUpdate\nTemplates and examples\nRelated resources\nRefer to Azure Cosmos DB's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.azurestorage.md",
    "content": "Azure Storage node\nThe Azure Storage node has built-in support for a wide range of features, which includes creating, getting, and deleting blobs and containers. Use this node to automate work within the Azure Storage service or integrate it with other services in your workflow.\nOn this page, you'll find a list of operations the Azure Storage node supports, and links to more resources.\nOperations\nBlob\nCreate blob: Create a new blob or replace an existing one.\nDelete blob: Delete an existing blob.\nGet blob: Retrieve data for a specific blob.\nGet many blobs: Retrieve a list of blobs.\nContainer\nCreate container: Create a new container.\nDelete container: Delete an existing container.\nGet container: Retrieve data for a specific container.\nGet many containers: Retrieve a list of containers.\nTemplates and examples\nRelated resources\nRefer to Microsoft's Azure Storage documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.bamboohr.md",
    "content": "BambooHR node\nUse the BambooHR node to automate work in BambooHR, and integrate BambooHR with other applications. n8n has built-in support for a wide range of BambooHR features, including creating, deleting, downloading, and getting company reports, employee documents, and files.\nOn this page, you'll find a list of operations the BambooHR node supports and links to more resources.\nOperations\nCompany Report\nGet a company report\nEmployee\nCreate an employee\nGet an employee\nGet all employees\nUpdate an employee\nEmployee Document\nDelete an employee document\nDownload an employee document\nGet all employee document\nUpdate an employee document\nUpload an employee document\nFile\nDelete a company file\nDownload a company file\nGet all company files\nUpdate a company file\nUpload a company file\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.bannerbear.md",
    "content": "Bannerbear node\nUse the Bannerbear node to automate work in Bannerbear, and integrate Bannerbear with other applications. n8n has built-in support for a wide range of Bannerbear features, including creating and getting images and templates.\nOn this page, you'll find a list of operations the Bannerbear node supports and links to more resources.\nOperations\nImage\nCreate an image\nGet an image\nTemplate\nGet a template\nGet all templates\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.baserow.md",
    "content": "Baserow node\nUse the Baserow node to automate work in Baserow, and integrate Baserow with other applications. n8n has built-in support for a wide range of Baserow features, including creating, getting, retrieving, and updating rows.\nOn this page, you'll find a list of operations the Baserow node supports and links to more resources.\nOperations\nRow\nCreate a row\nDelete a row\nRetrieve a row\nRetrieve all rows\nUpdate a row\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.beeminder.md",
    "content": "Beeminder node\nUse the Beeminder node to automate work in Beeminder, and integrate Beeminder with other applications. n8n has built-in support for a wide range of Beeminder features, including creating, deleting, and updating data points.\nOn this page, you'll find a list of operations the Beeminder node supports and links to more resources.\nOperations\ndata point\n- Create data point for a goal\n- Delete a data point\n- Get all data points for a goal\n- Update a data point\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.bitly.md",
    "content": "Bitly node\nUse the Bitly node to automate work in Bitly, and integrate Bitly with other applications. n8n has built-in support for a wide range of Bitly features, including creating, getting, and updating links.\nOn this page, you'll find a list of operations the Bitly node supports and links to more resources.\nOperations\nLink\nCreate a link\nGet a link\nUpdate a link\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.bitwarden.md",
    "content": "Bitwarden node\nUse the Bitwarden node to automate work in Bitwarden, and integrate Bitwarden with other applications. n8n has built-in support for a wide range of Bitwarden features, including creating, getting, deleting, and updating collections, events, groups, and members.\nOn this page, you'll find a list of operations the Bitwarden node supports and links to more resources.\nOperations\nCollection\nDelete\nGet\nGet All\nUpdate\nEvent\nGet All\nGroup\nCreate\nDelete\nGet\nGet All\nGet Members\nUpdate\nUpdate Members\nMember\nCreate\nDelete\nGet\nGet All\nGet Groups\nUpdate\nUpdate Groups\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.box.md",
    "content": "Box node\nUse the Box node to automate work in Box, and integrate Box with other applications. n8n has built-in support for a wide range of Box features, including creating, copying, deleting, searching, uploading, and downloading files and folders.\nOn this page, you'll find a list of operations the Box node supports and links to more resources.\nOperations\nFile\nCopy a file\nDelete a file\nDownload a file\nGet a file\nSearch files\nShare a file\nUpload a file\nFolder\nCreate a folder\nGet a folder\nDelete a folder\nSearch files\nShare a folder\nUpdate folder\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.brandfetch.md",
    "content": "Brandfetch node\nUse the Brandfetch node to automate work in Brandfetch, and integrate Brandfetch with other applications. n8n has built-in support for a wide range of Brandfetch features, including returning a company‚Äôs information.\nOn this page, you'll find a list of operations the Brandfetch node supports and links to more resources.\nOperations\nReturn a company's colors\nReturn a company's data\nReturn a company's fonts\nReturn a company's industry\nReturn a company's logo & icon\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.brevo.md",
    "content": "Brevo node\nUse the Brevo node to automate work in Brevo, and integrate Brevo with other applications. n8n has built-in support for a wide range of Brevo features, including creating, updating, deleting, and getting contacts, attributes, as well as sending emails.\nOn this page, you'll find a list of operations the Brevo node supports and links to more resources.\nOperations\nContact\nCreate\nCreate or Update\nDelete\nGet\nGet All\nUpdate\nContact Attribute\nCreate\nDelete\nGet All\nUpdate\nEmail\nSend\nSend Template\nSender\nCreate\nDelete\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.bubble.md",
    "content": "Bubble node\nUse the Bubble node to automate work in Bubble, and integrate Bubble with other applications. n8n has built-in support for a wide range of Bubble features, including creating, deleting, getting, and updating objects.\nOn this page, you'll find a list of operations the Bubble node supports and links to more resources.\nOperations\nObject\nCreate\nDelete\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.chargebee.md",
    "content": "Chargebee node\nUse the Chargebee node to automate work in Chargebee, and integrate Chargebee with other applications. n8n has built-in support for a wide range of Chargebee features, including creating customers, returning invoices, and canceling subscriptions.\nOn this page, you'll find a list of operations the Chargebee node supports and links to more resources.\nOperations\nCustomer\nCreate a customer\nInvoice\nReturn the invoices\nGet URL for the invoice PDF\nSubscription\nCancel a subscription\nDelete a subscription\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.circleci.md",
    "content": "CircleCI node\nUse the CircleCI node to automate work in CircleCI, and integrate CircleCI with other applications. n8n has built-in support for a wide range of CircleCI features, including getting and triggering pipelines.\nOn this page, you'll find a list of operations the CircleCI node supports and links to more resources.\nOperations\nPipeline\nGet a pipeline\nGet all pipelines\nTrigger a pipeline\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.ciscowebex.md",
    "content": "Webex by Cisco node\nUse the Webex by Cisco node to automate work in Webex, and integrate Webex with other applications. n8n has built-in support for a wide range of Webex features, including creating, getting, updating, and deleting meetings and messages.\nOn this page, you'll find a list of operations the Webex node supports and links to more resources.\nOperations\nMeeting\nCreate\nDelete\nGet\nGet All\nUpdate\nMessage\nCreate\nDelete\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.clearbit.md",
    "content": "Clearbit node\nUse the Clearbit node to automate work in Clearbit, and integrate Clearbit with other applications. n8n has built-in support for a wide range of Clearbit features, including autocompleting and looking up companies and persons.\nOn this page, you'll find a list of operations the Clearbit node supports and links to more resources.\nOperations\nCompany\nAuto-complete company names and retrieve logo and domain\nLook up person and company data based on an email or domain\nPerson\nLook up a person and company data based on an email or domain\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.clickup.md",
    "content": "ClickUp node\nUse the ClickUp node to automate work in ClickUp, and integrate ClickUp with other applications. n8n has built-in support for a wide range of ClickUp features, including creating, getting, deleting, and updating folders, checklists, tags, comments, and goals.\nOn this page, you'll find a list of operations the ClickUp node supports and links to more resources.\nOperations\nChecklist\nCreate a checklist\nDelete a checklist\nUpdate a checklist\nChecklist Item\nCreate a checklist item\nDelete a checklist item\nUpdate a checklist item\nComment\nCreate a comment\nDelete a comment\nGet all comments\nUpdate a comment\nFolder\nCreate a folder\nDelete a folder\nGet a folder\nGet all folders\nUpdate a folder\nGoal\nCreate a goal\nDelete a goal\nGet a goal\nGet all goals\nUpdate a goal\nGoal Key Result\nCreate a key result\nDelete a key result\nUpdate a key result\nList\nCreate a list\nRetrieve list's custom fields\nDelete a list\nGet a list\nGet all lists\nGet list members\nUpdate a list\nSpace Tag\nCreate a space tag\nDelete a space tag\nGet all space tags\nUpdate a space tag\nTask\nCreate a task\nDelete a task\nGet a task\nGet all tasks\nGet task members\nSet a custom field\nUpdate a task\nTask List\nAdd a task to a list\nRemove a task from a list\nTask Tag\nAdd a tag to a task\nRemove a tag from a task\nTask Dependency\nCreate a task dependency\nDelete a task dependency\nTime Entry\nCreate a time entry\nDelete a time entry\nGet a time entry\nGet all time entries\nStart a time entry\nStop the current running timer\nUpdate a time Entry\nTime Entry Tag\nAdd tag to time entry\nGet all time entry tags\nRemove tag from time entry\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.clockify.md",
    "content": "Clockify node\nUse the Clockify node to automate work in Clockify, and integrate Clockify with other applications. n8n has built-in support for a wide range of Clockify features, including creating, updating, getting, and deleting tasks, time entries, projects, and tags.\nOn this page, you'll find a list of operations the Clockify node supports and links to more resources.\nOperations\nProject\nCreate a project\nDelete a project\nGet a project\nGet all projects\nUpdate a project\nTag\nCreate a tag\nDelete a tag\nGet all tags\nUpdate a tag\nTask\nCreate a task\nDelete a task\nGet a task\nGet all tasks\nUpdate a task\nTime Entry\nCreate a time entry\nDelete a time entry\nGet time entry\nUpdate a time entry\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.cloudflare.md",
    "content": "Cloudflare node\nUse the Cloudflare node to automate work in Cloudflare, and integrate Cloudflare with other applications. n8n has built-in support for a wide range of Cloudflare features, including deleting, getting, and uploading zone certificates.\nOn this page, you'll find a list of operations the Cloudflare node supports and links to more resources.\nOperations\nZone Certificate\nDelete\nGet\nGet Many\nUpload\nTemplates and examples\nRelated resources\nRefer to Cloudflare's API documentation on zone-level authentication for more information on this service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.cockpit.md",
    "content": "Cockpit node\nUse the Cockpit node to automate work in Cockpit, and integrate Cockpit with other applications. n8n has built-in support for a wide range of Cockpit features, including creating a collection entry, storing data from a form submission, and getting singletons.\nOn this page, you'll find a list of operations the Cockpit node supports and links to more resources.\nOperations\nCollection\nCreate a collection entry\nGet all collection entries\nUpdate a collection entry\nForm\nStore data from a form submission\nSingleton\nGet a singleton\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.coda.md",
    "content": "Coda node\nUse the Coda node to automate work in Coda, and integrate Coda with other applications. n8n has built-in support for a wide range of Coda features, including creating, getting, and deleting controls, formulas, tables, and views.\nOn this page, you'll find a list of operations the Coda node supports and links to more resources.\nOperations\nControl\nGet a control\nGet all controls\nFormula\nGet a formula\nGet all formulas\nTable\nCreate/Insert a row\nDelete one or multiple rows\nGet all columns\nGet all the rows\nGet a column\nGet a row\nPushes a button\nView\nDelete view row\nGet a view\nGet all views\nGet all views columns\nGet all views rows\nUpdate row\nPush view button\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.coingecko.md",
    "content": "CoinGecko node\nUse the CoinGecko node to automate work in CoinGecko, and integrate CoinGecko with other applications. n8n has built-in support for a wide range of CoinGecko features, including getting coins and events.\nOn this page, you'll find a list of operations the CoinGecko node supports and links to more resources.\nOperations\nCoin\nGet a candlestick open-high-low-close chart for the selected currency\nGet current data for a coin\nGet all coins\nGet historical data (name, price, market, stats) at a given date for a coin\nGet prices and market related data for all trading pairs that match the selected currency\nGet historical market data include price, market cap, and 24h volume (granularity auto)\nGet the current price of any cryptocurrencies in any other supported currencies that you need\nGet coin tickers\nEvent\nGet all events\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.contentful.md",
    "content": "Contentful node\nUse the Contentful node to automate work in Contentful, and integrate Contentful with other applications. n8n has built-in support for a wide range of Contentful features, including getting assets, content types, entries, locales, and space.\nOn this page, you'll find a list of operations the Contentful node supports and links to more resources.\nOperations\nAsset\nGet\nGet All\nContent Type\nGet\nEntry\nGet\nGet All\nLocale\nGet All\nSpace\nGet\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.convertkit.md",
    "content": "ConvertKit node\nUse the ConvertKit node to automate work in ConvertKit, and integrate ConvertKit with other applications. n8n has built-in support for a wide range of ConvertKit features, including creating and deleting custom fields, getting tags, and adding subscribers.\nOn this page, you'll find a list of operations the ConvertKit node supports and links to more resources.\nOperations\nCustom Field\nCreate a field\nDelete a field\nGet all fields\nUpdate a field\nForm\nAdd a subscriber\nGet all forms\nList subscriptions to a form including subscriber data\nSequence\nAdd a subscriber\nGet all sequences\nGet all subscriptions to a sequence including subscriber data\nTag\nCreate a tag\nGet all tags\nTag Subscriber\nAdd a tag to a subscriber\nList subscriptions to a tag including subscriber data\nDelete a tag from a subscriber\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.copper.md",
    "content": "Copper node\nUse the Copper node to automate work in Copper, and integrate Copper with other applications. n8n has built-in support for a wide range of Copper features, including getting, updating, deleting, and creating companies, customer sources, leads, projects and tasks.\nOn this page, you'll find a list of operations the Copper node supports and links to more resources.\nOperations\nCompany\nCreate\nDelete\nGet\nGet All\nUpdate\nCustomer Source\nGet All\nLead\nCreate\nDelete\nGet\nGet All\nUpdate\nOpportunity\nCreate\nDelete\nGet\nGet All\nUpdate\nPerson\nCreate\nDelete\nGet\nGet All\nUpdate\nProject\nCreate\nDelete\nGet\nGet All\nUpdate\nTask\nCreate\nDelete\nGet\nGet All\nUpdate\nUser\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.cortex.md",
    "content": "Cortex node\nUse the Cortex node to automate work in Cortex, and integrate Cortex with other applications. n8n has built-in support for a wide range of Cortex features, including executing analyzers, and responders, as well as getting job details.\nOn this page, you'll find a list of operations the Cortex node supports and links to more resources.\nOperations\nAnalyzer\nExecute Analyzer\nJob\nGet job details\nGet job report\nResponder\nExecute Responder\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.cratedb.md",
    "content": "CrateDB node\nUse the CrateDB node to automate work in CrateDB, and integrate CrateDB with other applications. n8n has built-in support for a wide range of CrateDB features, including executing, inserting, and updating rows in the database.\nOn this page, you'll find a list of operations the CrateDB node supports and links to more resources.\nOperations\nExecute an SQL query\nInsert rows in database\nUpdate rows in database\nTemplates and examples\nNode reference\nSpecify a column's data type\nTo specify a column's data type, append the column name with :type, where type is the data type you want for the column. For example, if you want to specify the type int for the column id and type text for the column name, you can use the following snippet in the Columns field: id:int,name:text."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.crowddev.md",
    "content": "crowd.dev node\nUse the crowd.dev node to automate work in crowd.dev and integrate crowd.dev with other applications. n8n has built-in support for a wide range of crowd.dev features, which includes creating, updating, and deleting members, notes, organizations, and tasks.\nOn this page, you'll find a list of operations the crowd.dev node supports, and links to more resources.\nOperations\nActivity\nCreate or Update with a Member\nCreate\nAutomation\nCreate\nDestroy\nFind\nList\nUpdate\nMember\nCreate or Update\nDelete\nFind\nUpdate\nNote\nCreate\nDelete\nFind\nUpdate\nOrganization\nCreate\nDelete\nFind\nUpdate\nTask\nCreate\nDelete\nFind\nUpdate\nTemplates and examples\nRelated resources\nn8n provides a trigger node for crowd.dev. You can find the trigger node docs here.\nRefer to crowd.dev's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.customerio.md",
    "content": "Customer.io node\nUse the Customer.io node to automate work in Customer.io, and integrate Customer.io with other applications. n8n has built-in support for a wide range of Customer.io features, including creating and updating customers, tracking events, and getting campaigns.\nOn this page, you'll find a list of operations the Customer.io node supports and links to more resources.\nOperations\nCustomer\nCreate/Update a customer.\nDelete a customer.\nEvent\nTrack a customer event.\nTrack an anonymous event.\nCampaign\nGet\nGet All\nGet Metrics\nSegment\nAdd Customer\nRemove Customer\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.deepl.md",
    "content": "DeepL node\nUse the DeepL node to automate work in DeepL, and integrate DeepL with other applications. n8n has built-in support for a wide range of DeepL features, including translating languages.\nOn this page, you'll find a list of operations the DeepL node supports and links to more resources.\nOperations\nLanguage\nTranslate data\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.demio.md",
    "content": "Demio node\nUse the Demio node to automate work in Demio, and integrate Demio with other applications. n8n has built-in support for a wide range of Demio features, including getting, and registering events and reports.\nOn this page, you'll find a list of operations the Demio node supports and links to more resources.\nOperations\nEvent\nGet an event\nGet all events\nRegister someone to an event\nReport\nGet an event report\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.dhl.md",
    "content": "DHL node\nUse the DHL node to automate work in DHL, and integrate DHL with other applications. n8n has built-in support for a wide range of DHL features, including tracking shipment.\nOn this page, you'll find a list of operations the DHL node supports and links to more resources.\nOperations\nShipment\nGet Tracking Details\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.discourse.md",
    "content": "Discourse node\nUse the Discourse node to automate work in Discourse, and integrate Discourse with other applications. n8n has built-in support for a wide range of Discourse features, including creating, getting, updating, and removing categories, groups, posts, and users.\nOn this page, you'll find a list of operations the Discourse node supports and links to more resources.\nOperations\nCategory\nCreate a category\nGet all categories\nUpdate a category\nGroup\nCreate a group\nGet a group\nGet all groups\nUpdate a group\nPost\nCreate a post\nGet a post\nGet all posts\nUpdate a post\nUser\nCreate a user\nGet a user\nGet all users\nUser Group\nCreate a user to group\nRemove user from group\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.disqus.md",
    "content": "Disqus node\nUse the Disqus node to automate work in Disqus, and integrate Disqus with other applications. n8n has built-in support for a wide range of Disqus features, including returning forums.\nOn this page, you'll find a list of operations the Disqus node supports and links to more resources.\nOperations\nForum\nReturn forum details\nReturn a list of categories within a forum\nReturn a list of threads within a forum\nReturn a list of posts within a forum\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.drift.md",
    "content": "Drift node\nUse the Drift node to automate work in Drift, and integrate Drift with other applications. n8n has built-in support for a wide range of Drift features, including creating, updating, deleting, and getting contacts.\nOn this page, you'll find a list of operations the Drift node supports and links to more resources.\nOperations\nContact\nCreate a contact\nGet custom attributes\nDelete a contact\nGet a contact\nUpdate a contact\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.dropbox.md",
    "content": "Dropbox node\nUse the Dropbox node to automate work in Dropbox, and integrate Dropbox with other applications. n8n has built-in support for a wide range of Dropbox features, including creating, downloading, moving, and copying files and folders.\nOn this page, you'll find a list of operations the Dropbox node supports and links to more resources.\nOperations\nFile\nCopy a file\nDelete a file\nDownload a file\nMove a file\nUpload a file\nFolder\nCopy a folder\nCreate a folder\nDelete a folder\nReturn the files and folders in a given folder\nMove a folder\nSearch\nQuery\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.dropcontact.md",
    "content": "Dropcontact node\nUse the Dropcontact node to automate work in Dropcontact, and integrate Dropcontact with other applications. n8n has built-in support for a wide range of Dropcontact features, including  fetching contacts.\nOn this page, you'll find a list of operations the Dropcontact node supports and links to more resources.\nOperations\nContact\n- Enrich\n- Fetch Request\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.egoi.md",
    "content": "E-goi node\nUse the E-goi node to automate work in E-goi, and integrate E-goi with other applications. n8n has built-in support for a wide range of E-goi features, including creating, updating, deleting, and getting contacts.\nOn this page, you'll find a list of operations the E-goi node supports and links to more resources.\nOperations\nContact\nCreate a member\nGet a member\nGet all members\nUpdate a member\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.elasticsearch.md",
    "content": "Elasticsearch node\nUse the Elasticsearch node to automate work in Elasticsearch, and integrate Elasticsearch with other applications. n8n has built-in support for a wide range of Elasticsearch features, including creating, updating, deleting, and getting documents and indexes.\nOn this page, you'll find a list of operations the Elasticsearch node supports and links to more resources.\nOperations\nDocument\nCreate a document\nDelete a document\nGet a document\nGet all documents\nUpdate a document\nIndex\nCreate\nDelete\nGet\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.elasticsecurity.md",
    "content": "Elastic Security node\nUse the Elastic Security node to automate work in Elastic Security, and integrate Elastic Security with other applications. n8n's has built-in support for a wide range of Elastic Security features, including creating, updating, deleting, retrieving, and getting cases.\nOn this page, you'll find a list of operations the Elastic Security node supports and links to more resources.\nOperations\nCase\nCreate a case\nDelete a case\nGet a case\nRetrieve all cases\nRetrieve a summary of all case activity\nUpdate a case\nCase Comment\nAdd a comment to a case\nGet a case comment\nRetrieve all case comments\nRemove a comment from a case\nUpdate a comment in a case\nCase Tag\nAdd a tag to a case\nRemove a tag from a case\nConnector\nCreate a connector\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.emelia.md",
    "content": "Emelia node\nUse the Emelia node to automate work in Emelia, and integrate Emelia with other applications. n8n has built-in support for a wide range of Emelia features, including creating campaigns, and adding contacts to a list.\nOn this page, you'll find a list of operations the Emelia node supports and links to more resources.\nOperations\nCampaign\nAdd Contact\nCreate\nGet\nGet All\nPause\nStart\nContact List\nAdd\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.erpnext.md",
    "content": "ERPNext node\nUse the ERPNext node to automate work in ERPNext, and integrate ERPNext with other applications. n8n has built-in support for a wide range of ERPNext features, including creating, updating, retrieving, and deleting documents.\nOn this page, you'll find a list of operations the ERPNext node supports and links to more resources.\nOperations\nDocument\n- Create a document\n- Delete a document\n- Retrieve a document\n- Retrieve all documents\n- Update a document\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.facebookgraphapi.md",
    "content": "Facebook Graph API node\nUse the Facebook Graph API node to automate work in Facebook Graph API, and integrate Facebook Graph API with other applications. n8n has built-in support for a wide range of Facebook Graph API features, including using queries GET POST DELETE for several parameters like host URL, request methods and much more.\nOn this page, you'll find a list of operations the Facebook Graph API node supports and links to more resources.\nOperations\nDefault\nGET\nPOST\nDELETE\nVideo Uploads\nGET\nPOST\nDELETE\nParameters\nHost URL: The host URL for the request. The following options are available:\nDefault: Requests are passed to the graph.facebook.com host URL. Used for the majority of requests.\nVideo: Requests are passed to the graph-video.facebook.com host URL. Used for video upload requests only.\nHTTP Request Method: The method to be used for this request, from the following options:\nGET\nPOST\nDELETE\nGraph API Version: The version of the Facebook Graph API to be used for this request.\nNode: The node on which to operate, for example //feed. Read more about it in the official Facebook Developer documentation.\nEdge: Edge of the node on which to operate. Edges represent collections of objects which are attached to the node.\nIgnore SSL Issues: Toggle to still download the response even if SSL certificate validation isn't possible.\nSend Binary File: Available for POST operations. If enabled binary data is sent as the body. Requires setting the following:\nInput Binary Field: Name of the binary property which contains the data for the file to be uploaded.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.filemaker.md",
    "content": "FileMaker node\nUse the FileMaker node to automate work in FileMaker, and integrate FileMaker with other applications. n8n has built-in support for a wide range of FileMaker features, including creating, finding, getting, editing, and duplicating files.\nOn this page, you'll find a list of operations the FileMaker node supports and links to more resources.\nOperations\nFind Records\nGet Records\nGet Records by Id\nPerform Script\nCreate Record\nEdit Record\nDuplicate Record\nDelete Record\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.flow.md",
    "content": "Flow node\nUse the Flow node to automate work in Flow, and integrate Flow with other applications. n8n has built-in support for a wide range of Flow features, including creating, updating, and getting tasks.\nOn this page, you'll find a list of operations the Flow node supports and links to more resources.\nOperations\nTask\nCreate a new task\nUpdate a task\nGet a task\nGet all the tasks\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.freshdesk.md",
    "content": "Freshdesk node\nUse the Freshdesk node to automate work in Freshdesk and integrate Freshdesk with other applications. n8n has built-in support for a wide range of Freshdesk features, including creating, updating, deleting, and getting contacts and tickets.\nOn this page, you'll find a list of operations the Freshdesk node supports and links to more resources.\nOperations\nContact\nCreate a new contact\nDelete a contact\nGet a contact\nGet all contacts\nUpdate a contact\nTicket\nCreate a new ticket\nDelete a ticket\nGet a ticket\nGet all tickets\nUpdate a ticket\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.freshservice.md",
    "content": "Freshservice node\nUse the Freshservice node to automate work in Freshservice and integrate Freshservice with other applications. n8n has built-in support for a wide range of Freshdesk features, including creating, updating, deleting, and getting agent information and departments.\nOn this page, you'll find a list of operations the Freshservice node supports and links to more resources.\nOperations\nAgent\nCreate an agent\nDelete an agent\nRetrieve an agent\nRetrieve all agents\nUpdate an agent\nAgent Group\nCreate an agent group\nDelete an agent group\nRetrieve an agent group\nRetrieve all agent groups\nUpdate an agent group\nAgent Role\nRetrieve an agent role\nRetrieve all agent roles\nAnnouncement\nCreate an announcement\nDelete an announcement\nRetrieve an announcement\nRetrieve all announcements\nUpdate an announcement\nAsset Type\nCreate an asset type\nDelete an asset type\nRetrieve an asset type\nRetrieve all asset types\nUpdate an asset type\nChange\nCreate a change\nDelete a change\nRetrieve a change\nRetrieve all changes\nUpdate a change\nDepartment\nCreate a department\nDelete a department\nRetrieve a department\nRetrieve all departments\nUpdate a department\nLocation\nCreate a location\nDelete a location\nRetrieve a location\nRetrieve all locations\nUpdate a location\nProblem\nCreate a problem\nDelete a problem\nRetrieve a problem\nRetrieve all problems\nUpdate a problem\nProduct\nCreate a product\nDelete a product\nRetrieve a product\nRetrieve all products\nUpdate a product\nRelease\nCreate a release\nDelete a release\nRetrieve a release\nRetrieve all releases\nUpdate a release\nRequester\nCreate a requester\nDelete a requester\nRetrieve a requester\nRetrieve all requesters\nUpdate a requester\nRequester Group\nCreate a requester group\nDelete a requester group\nRetrieve a requester group\nRetrieve all requester groups\nUpdate a requester group\nSoftware\nCreate a software application\nDelete a software application\nRetrieve a software application\nRetrieve all software applications\nUpdate a software application\nTicket\nCreate a ticket\nDelete a ticket\nRetrieve a ticket\nRetrieve all tickets\nUpdate a ticket\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.freshworkscrm.md",
    "content": "Freshworks CRM node\nUse the Freshworks CRM node to automate work in Freshworks CRM, and integrate Freshworks CRM with other applications. n8n has built-in support for a wide range of Freshworks CRM features, including creating, updating, deleting, and retrieve, accounts, appointments, contacts, deals, notes, sales activity and more.\nOn this page, you'll find a list of operations the Freshworks CRM node supports and links to more resources.\nOperations\nAccount\nCreate an account\nDelete an account\nRetrieve an account\nRetrieve all accounts\nUpdate an account\nAppointment\nCreate an appointment\nDelete an appointment\nRetrieve an appointment\nRetrieve all appointments\nUpdate an appointment\nContact\nCreate a contact\nDelete a contact\nRetrieve a contact\nRetrieve all contacts\nUpdate a contact\nDeal\nCreate a deal\nDelete a deal\nRetrieve a deal\nRetrieve all deals\nUpdate a deal\nNote\nCreate a note\nDelete a note\nUpdate a note\nSales Activity\nRetrieve a sales activity\nRetrieve all sales activities\nTask\nCreate a task\nDelete a task\nRetrieve a task\nRetrieve all tasks\nUpdate a task\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.getresponse.md",
    "content": "GetResponse node\nUse the GetResponse node to automate work in GetResponse, and integrate GetResponse with other applications. n8n has built-in support for a wide range of GetResponse features, including creating, updating, deleting, and getting contacts.\nOn this page, you'll find a list of operations the GetResponse node supports and links to more resources.\nOperations\nContact\nCreate a new contact\nDelete a contact\nGet a contact\nGet all contacts\nUpdate contact properties\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.ghost.md",
    "content": "Ghost node\nUse the Ghost node to automate work in Ghost, and integrate Ghost with other applications. n8n has built-in support for a wide range of Ghost features, including creating, updating, deleting, and getting posts for the Admin and content API.\nOn this page, you'll find a list of operations the Ghost node supports and links to more resources.\nOperations\nAdmin API\nPost\nCreate a post\nDelete a post\nGet a post\nGet all posts\nUpdate a post\nContent API\nPost\nGet a post\nGet all posts\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.github.md",
    "content": "GitHub node\nUse the GitHub node to automate work in GitHub, and integrate GitHub with other applications. n8n has built-in support for a wide range of GitHub features, including creating, updating, deleting, and editing files, repositories, issues, releases, and users.\nOn this page, you'll find a list of operations the GitHub node supports and links to more resources.\nOperations\nFile\nCreate\nDelete\nEdit\nGet\nList\nIssue\nCreate\nCreate Comment\nEdit\nGet\nLock\nOrganization\nGet Repositories\nRelease\nCreate\nDelete\nGet\nGet Many\nUpdate\nRepository\nGet\nGet Issues\nGet License\nGet Profile\nGet Pull Requests\nList Popular Paths\nList Referrers\nReview\nCreate\nGet\nGet Many\nUpdate\nUser\nGet Repositories\nInvite\nWorkflow\nDisable\nDispatch\nEnable\nGet\nGet Usage\nList\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gitlab.md",
    "content": "GitLab node\nUse the GitLab node to automate work in GitLab, and integrate GitLab with other applications. n8n has built-in support for a wide range of GitLab features, including creating, updating, deleting, and editing issues, repositories, releases and users.\nOn this page, you'll find a list of operations the GitLab node supports and links to more resources.\nOperations\nFile\nCreate\nDelete\nEdit\nGet\nList\nIssue\nCreate a new issue\nCreate a new comment on an issue\nEdit an issue\nGet the data of a single issue\nLock an issue\nRelease\nCreate a new release\nDelete a new release\nGet a new release\nGet all releases\nUpdate a new release\nRepository\nGet the data of a single repository\nReturns issues of a repository\nUser\nReturns the repositories of a user\nTemplates and examples\nRelated resources\nRefer to GitLab's documentation for more information about the service.\nn8n provides a trigger node for GitLab. You can find the trigger node docs here."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gong.md",
    "content": "Gong node\nUse the Gong node to automate work in Gong and integrate Gong with other applications. n8n has built-in support for a wide range of Gong features, which includes getting one or more calls and users.\nOn this page, you'll find a list of operations the Gong node supports, and links to more resources.\nOperations\nCall\nGet\nGet Many\nUser\nGet\nGet Many\nTemplates and examples\nRelated resources\nRefer to Gong's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googleads.md",
    "content": "Google Ads node\nUse the Google Ads node to automate work in Google Ads, and integrate Google Ads with other applications. n8n has built-in support for a wide range of Google Ads features, including getting campaigns.\nOn this page, you'll find a list of operations the Google Ads node supports and links to more resources.\nOperations\nCampaign\nGet all campaigns\nGet a campaign\nTemplates and examples\nRelated resources\nRefer to Google Ads' documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googleanalytics.md",
    "content": "Google Analytics node\nUse the Google Analytics node to automate work in Google Analytics, and integrate Google Analytics with other applications. n8n has built-in support for a wide range of Google Analytics features, including returning reports and user activities.\nOn this page, you'll find a list of operations the Google Analytics node supports and links to more resources.\nOperations\nReport\nGet\nUser Activity\nSearch\nTemplates and examples\nRelated resources\nRefer to Google Analytics' documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlebigquery.md",
    "content": "Google BigQuery node\nUse the Google BigQuery node to automate work in Google BigQuery, and integrate Google BigQuery with other applications. n8n has built-in support for a wide range of Google BigQuery features, including creating, and retrieving records.\nOn this page, you'll find a list of operations the Google BigQuery node supports and links to more resources.\nOperations\nExecute Query\nInsert\nTemplates and examples\nRelated resources\nRefer to Google BigQuery's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlebooks.md",
    "content": "Google Books node\nUse the Google Books node to automate work in Google Books, and integrate Google Books with other applications. n8n has built-in support for a wide range of Google Books features, including retrieving a specific bookshelf resource for the specified user, adding volume to a bookshelf, and getting volume.\nOn this page, you'll find a list of operations the Google Books node supports and links to more resources.\nOperations\nBookshelf\nRetrieve a specific bookshelf resource for the specified user\nGet all public bookshelf resource for the specified user\nBookshelf Volume\nAdd a volume to a bookshelf\nClears all volumes from a bookshelf\nGet all volumes in a specific bookshelf for the specified user\nMoves a volume within a bookshelf\nRemoves a volume from a bookshelf\nVolume\nGet a volume resource based on ID\nGet all volumes filtered by query\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlebusinessprofile.md",
    "content": "Google Business Profile node\nUse the Google Business Profile node to automate work in Google Business Profile and integrate Google Business Profile with other applications. n8n has built-in support for a wide range of Google Business Profile features, which includes creating, updating, and deleting posts and reviews.\nOn this page, you'll find a list of operations the Google Business Profile node supports, and links to more resources.\nOperations\nPost\nCreate\nDelete\nGet\nGet Many\nUpdate\nReview\nDelete Reply\nGet\nGet Many\nReply\nTemplates and examples\nRelated resources\nn8n provides a trigger node for Google Business Profile. You can find the trigger node docs here.\nRefer to Google Business Profile's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlechat.md",
    "content": "Google Chat node\nUse the Google Chat node to automate work in Google Chat, and integrate Google Chat with other applications. n8n has built-in support for a wide range of Google Chat features, including getting membership and spaces, as well as creating and deleting messages.\nOn this page, you'll find a list of operations the Google Chat node supports and links to more resources.\nOperations\nMember\nGet a membership\nGet all memberships in a space\nMessage\nCreate a message\nDelete a message\nGet a message\nSend and Wait for Response\nUpdate a message\nSpace\nGet a space\nGet all spaces the caller is a member of\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecloudfirestore.md",
    "content": "Google Cloud Firestore node\nUse the Google Cloud Firestore node to automate work in Google Cloud Firestore, and integrate Google Cloud Firestore with other applications. n8n has built-in support for a wide range of Google Cloud Firestore features, including creating, deleting, and getting documents.\nOn this page, you'll find a list of operations the Google Cloud Firestore node supports and links to more resources.\nOperations\nDocument\nCreate a document\nCreate/Update a document\nDelete a document\nGet a document\nGet all documents from a collection\nRuns a query against your documents\nCollection\nGet all root collections\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecloudnaturallanguage.md",
    "content": "Google Cloud Natural Language node\nUse the Google Cloud Natural Language node to automate work in Google Cloud Natural Language, and integrate Google Cloud Natural Language with other applications. n8n has built-in support for a wide range of Google Cloud Natural Language features, including analyzing documents.\nOn this page, you'll find a list of operations the Google Cloud Natural Language node supports and links to more resources.\nOperations\nDocument\nAnalyze Sentiment\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecloudrealtimedatabase.md",
    "content": "Google Cloud Realtime Database node\nUse the Google Cloud Realtime Database node to automate work in Google Cloud Realtime Database, and integrate Google Cloud Realtime Database with other applications. n8n has built-in support for a wide range of Google Cloud Realtime Database features, including writing, deleting, getting, and appending databases.\nOn this page, you'll find a list of operations the Google Cloud Realtime Database node supports and links to more resources.\nOperations\nWrite data to a database\nDelete data from a database\nGet a record from a database\nAppend to a list of data\nUpdate item on a database\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecloudstorage.md",
    "content": "Google Cloud Storage node\nUse the Google Cloud Storage node to automate work in Google Cloud Storage, and integrate Google Cloud Storage with other applications. n8n has built-in support for a wide range of Google Cloud Storage features, including creating, updating, deleting, and getting buckets and objects.\nOn this page, you'll find a list of operations the Google Cloud Storage node supports and links to more resources.\nOperations\nBucket\nCreate\nDelete\nGet\nGet Many\nUpdate\nObject\nCreate\nDelete\nGet\nGet Many\nUpdate\nTemplates and examples\nRelated resources\nRefer to Google's Cloud Storage API documentation for detailed information about the API that this node integrates with."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecontacts.md",
    "content": "Google Contacts node\nUse the Google Contacts node to automate work in Google Contacts, and integrate Google Contacts with other applications. n8n has built-in support for a wide range of Google Contacts features, including creating, updating, retrieving, deleting, and getting contacts.\nOn this page, you'll find a list of operations the Google Contacts node supports and links to more resources.\nOperations\nContact\nCreate a contact\nDelete a contact\nGet a contact\nRetrieve all contacts\nUpdate a contact\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googledocs.md",
    "content": "Google Docs node\nUse the Google Docs node to automate work in Google Docs, and integrate Google Docs with other applications. n8n has built-in support for a wide range of Google Docs features, including creating, updating, and getting documents.\nOn this page, you'll find a list of operations the Google Docs node supports and links to more resources.\nOperations\nDocument\nCreate\nGet\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googleperspective.md",
    "content": "Google Perspective node\nUse the Google Perspective node to automate work in Google Perspective, and integrate Google Perspective with other applications. n8n has built-in support for a wide range of Google Perspective features, including analyzing comments.\nOn this page, you'll find a list of operations the Google Perspective node supports and links to more resources.\nOperations\nAnalyze Comment\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googleslides.md",
    "content": "Google Slides node\nUse the Google Slides node to automate work in Google Slides, and integrate Google Slides with other applications. n8n has built-in support for a wide range of Google Slides features, including creating presentations, and getting pages.\nOn this page, you'll find a list of operations the Google Slides node supports and links to more resources.\nOperations\nPage\nGet a page\nGet a thumbnail\nPresentation\nCreate a presentation\nGet a presentation\nGet presentation slides\nReplace text in a presentation\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googletasks.md",
    "content": "Google Tasks node\nUse the Google Tasks node to automate work in Google Tasks, and integrate Google Tasks with other applications. n8n has built-in support for a wide range of Google Tasks features, including adding, updating, and retrieving contacts.\nOn this page, you'll find a list of operations the Google Tasks node supports and links to more resources.\nOperations\nTask\nAdd a task to task list\nDelete a task\nRetrieve a task\nRetrieve all tasks from a task list\nUpdate a task\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googletranslate.md",
    "content": "Google Translate node\nUse the Google Translate node to automate work in Google Translate, and integrate Google Translate with other applications. n8n has built-in support for a wide range of Google Translate features, including translating languages.\nOn this page, you'll find a list of operations the Google Translate node supports and links to more resources.\nOperations\nLanguage\nTranslate data\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gotify.md",
    "content": "Gotify node\nUse the Gotify node to automate work in Gotify, and integrate Gotify with other applications. n8n has built-in support for a wide range of Gotify features, including creating, deleting, and getting messages.\nOn this page, you'll find a list of operations the Gotify node supports and links to more resources.\nOperations\nMessage\nCreate\nDelete\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gotowebinar.md",
    "content": "GoToWebinar node\nUse the GoToWebinar node to automate work in GoToWebinar, and integrate GoToWebinar with other applications. n8n has built-in support for a wide range of GoToWebinar features, including creating, getting, and deleting attendees, organizers, and registrants.\nOn this page, you'll find a list of operations the GoToWebinar node supports and links to more resources.\nOperations\nAttendee\nGet\nGet All\nGet Details\nCo-Organizer\nCreate\nDelete\nGet All\nRe-invite\nPanelist\nCreate\nDelete\nGet All\nRe-invite\nRegistrant\nCreate\nDelete\nGet\nGet All\nSession\nGet\nGet All\nGet Details\nWebinar\nCreate\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.grafana.md",
    "content": "Grafana node\nUse the Grafana node to automate work in Grafana, and integrate Grafana with other applications. n8n has built-in support for a wide range of Grafana features, including creating, updating, deleting, and getting dashboards, teams, and users.\nOn this page, you'll find a list of operations the Grafana node supports and links to more resources.\nOperations\nDashboard\nCreate a dashboard\nDelete a dashboard\nGet a dashboard\nGet all dashboards\nUpdate a dashboard\nTeam\nCreate a team\nDelete a team\nGet a team\nRetrieve all teams\nUpdate a team\nTeam Member\nAdd a member to a team\nRetrieve all team members\nRemove a member from a team\nUser\nDelete a user from the current organization\nRetrieve all users in the current organization\nUpdate a user in the current organization\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.grist.md",
    "content": "Grist node\nUse the Grist node to automate work in Grist, and integrate Grist with other applications. n8n has built-in support for a wide range of Grist features, including creating, updating, deleting, and reading rows in a table.\nOn this page, you'll find a list of operations the Grist node supports and links to more resources.\nOperations\nCreate rows in a table\nDelete rows from a table\nRead rows from a table\nUpdate rows in a table\nTemplates and examples\nGet the Row ID\nTo update or delete a particular record, you need the Row ID. There are two ways to get the Row ID:\nCreate a Row ID column in Grist\nCreate a new column in your Grist table with the formula $id.\nUse the Get All operation\nThe Get All operation returns the Row ID of each record along with the fields.\nYou can get it with the expression {{$node[\"GristNodeName\"].json[\"id\"]}}.\nFilter records when using the Get All operation\nSelect Add Option and select Filter from the dropdown list.\nYou can add filters for any number of columns. The result will only include records which match all the columns.\nFor each column, you can enter any number of values separated by commas. The result will include records which match any of the values for that column."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gsuiteadmin.md",
    "content": "Google Workspace Admin node\nUse the Google Workspace Admin node to automate work in Google Workspace Admin, and integrate Google Workspace Admin with other applications. n8n has built-in support for a wide range of Google Workspace Admin features, including creating, updating, deleting, and getting users, groups, and ChromeOS devices.\nOn this page, you'll find a list of operations the Google Workspace Admin node supports and links to more resources.\nOperations\nChromeOS Device\nGet a ChromeOS device\nGet many ChromeOS devices\nUpdate a ChromeOS device\nChange the status of a ChromeOS device\nGroup\nCreate a group\nDelete a group\nGet a group\nGet many groups\nUpdate a group\nUser\nAdd an existing user to a group\nCreate a user\nDelete a user\nGet a user\nGet many users\nRemove a user from a group\nUpdate a user\nTemplates and examples\nHow to control which custom fields to fetch for a user\nThere are three different ways to control which custom fields to retrieve when getting a user's information. Use the Custom Fields parameter to select one of the following:\nDon't Include: Doesn't include any custom fields.\nCustom: Includes the custom fields from schemas in Custom Schema Names or IDs.\nInclude All: Include all the fields associated with the user.\nTo include custom fields, follow these steps:\nSelect Custom from the Custom Fields dropdown list.\nSelect the schema names you want to include in the Custom Schema Names or IDs dropdown list."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.hackernews.md",
    "content": "Hacker News node\nUse the Hacker News node to automate work in Hacker News, and integrate Hacker News with other applications. n8n has built-in support for a wide range of Hacker News features, including getting articles, and users.\nOn this page, you'll find a list of operations the Hacker News node supports and links to more resources.\nOperations\nAll\nGet all items\nArticle\nGet a Hacker News article\nUser\nGet a Hacker News user\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.halopsa.md",
    "content": "HaloPSA node\nUse the HaloPSA node to automate work in HaloPSA, and integrate HaloPSA with other applications. n8n has built-in support for a wide range of HaloPSA features, including creating, updating, deleting, and getting clients, sites and tickets.\nOn this page, you'll find a list of operations the HaloPSA node supports and links to more resources.\nOperations\nClient\nCreate a client\nDelete a client\nGet a client\nGet all clients\nUpdate a client\nSite\nCreate a site\nDelete a site\nGet a site\nGet all sites\nUpdate a site\nTicket\nCreate a ticket\nDelete a ticket\nGet a ticket\nGet all tickets\nUpdate a ticket\nUser\nCreate a user\nDelete a user\nGet a user\nGet all users\nUpdate a user\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.harvest.md",
    "content": "Harvest node\nUse the Harvest node to automate work in Harvest, and integrate Harvest with other applications. n8n has built-in support for a wide range of Harvest features, including creating, updating, deleting, and getting clients, contacts, invoices, tasks, expenses, users, and projects.\nOn this page, you'll find a list of operations the Harvest node supports and links to more resources.\nOperations\nClient\nCreate a client\nDelete a client\nGet data of a client\nGet data of all clients\nUpdate a client\nCompany\nRetrieves the company for the currently authenticated user\nContact\nCreate a contact\nDelete a contact\nGet data of a contact\nGet data of all contacts\nUpdate a contact\nEstimate\nCreate an estimate\nDelete an estimate\nGet data of an estimate\nGet data of all estimates\nUpdate an estimate\nExpense\nGet data of an expense\nGet data of all expenses\nCreate an expense\nUpdate an expense\nDelete an expense\nInvoice\nGet data of an invoice\nGet data of all invoices\nCreate an invoice\nUpdate an invoice\nDelete an invoice\nProject\nCreate a project\nDelete a project\nGet data of a project\nGet data of all projects\nUpdate a project\nTask\nCreate a task\nDelete a task\nGet data of a task\nGet data of all tasks\nUpdate a task\nTime Entries\nCreate a time entry using duration\nCreate a time entry using start and end time\nDelete a time entry\nDelete a time entry's external reference.\nGet data of a time entry\nGet data of all time entries\nRestart a time entry\nStop a time entry\nUpdate a time entry\nUser\nCreate a user\nDelete a user\nGet data of a user\nGet data of all users\nGet data of authenticated user\nUpdate a user\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.helpscout.md",
    "content": "Help Scout node\nUse the Help Scout node to automate work in Help Scout, and integrate Help Scout with other applications. n8n has built-in support for a wide range of Help Scout features, including creating, updating, deleting, and getting conversations, and customers.\nOn this page, you'll find a list of operations the Help Scout node supports and links to more resources.\nOperations\nConversation\nCreate a new conversation\nDelete a conversation\nGet a conversation\nGet all conversations\nCustomer\nCreate a new customer\nGet a customer\nGet all customers\nGet customer property definitions\nUpdate a customer\nMailbox\nGet data of a mailbox\nGet all mailboxes\nThread\nCreate a new chat thread\nGet all chat threads\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.highlevel.md",
    "content": "HighLevel node\nUse the HighLevel node to automate work in HighLevel, and integrate HighLevel with other applications. n8n has built-in support for a wide range of HighLevel features, including creating, updating, deleting, and getting contacts, opportunities, and tasks, as well as booking appointments and getting free time slots in calendars.\nOn this page, you'll find a list of operations the HighLevel node supports and links to more resources.\nOperations\nContact\nCreate or update\nDelete\nGet\nGet many\nUpdate\nOpportunity\nCreate\nDelete\nGet\nGet many\nUpdate\nTask\nCreate\nDelete\nGet\nGet many\nUpdate\nCalendar\nBook an appointment\nGet free slots\nTemplates and examples\nRelated resources\nRefer to HighLevel's API documentation and support forums for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.homeassistant.md",
    "content": "Home Assistant node\nUse the Home Assistant node to automate work in Home Assistant, and integrate Home Assistant with other applications. n8n has built-in support for a wide range of Home Assistant features, including getting, creating, and checking camera proxies, configurations, logs, services, and templates.\nOn this page, you'll find a list of operations the Home Assistant node supports and links to more resources.\nOperations\nCamera Proxy\nGet the camera screenshot\nConfig\nGet the configuration\nCheck the configuration\nEvent\nCreate an event\nGet all events\nLog\nGet a log for a specific entity\nGet all logs\nService\nCall a service within a specific domain\nGet all services\nState\nCreate a new record, or update the current one if it already exists (upsert)\nGet a state for a specific entity\nGet all states\nTemplate\nCreate a template\nTemplates and examples\nRelated resources\nRefer to Home Assistant's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.hubspot.md",
    "content": "HubSpot node\nUse the HubSpot node to automate work in HubSpot, and integrate HubSpot with other applications. n8n has built-in support for a wide range of HubSpot features, including creating, updating, deleting, and getting contacts, deals, lists, engagements and companies.\nOn this page, you'll find a list of operations the HubSpot node supports and links to more resources.\nOperations\nContact\nCreate/Update a contact\nDelete a contact\nGet a contact\nGet all contacts\nGet recently created/updated contacts\nSearch contacts\nContact List\nAdd contact to a list\nRemove a contact from a list\nCompany\nCreate a company\nDelete a company\nGet a company\nGet all companies\nGet recently created companies\nGet recently modified companies\nSearch companies by domain\nUpdate a company\nDeal\nCreate a deal\nDelete a deal\nGet a deal\nGet all deals\nGet recently created deals\nGet recently modified deals\nSearch deals\nUpdate a deal\nEngagement\nCreate an engagement\nDelete an engagement\nGet an engagement\nGet all engagements\nForm\nGet all fields from a form\nSubmit data to a form\nTicket\nCreate a ticket\nDelete a ticket\nGet a ticket\nGet all tickets\nUpdate a ticket\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.humanticai.md",
    "content": "Humantic AI node\nUse the Humantic AI node to automate work in Humantic AI, and integrate Humantic AI with other applications. n8n has built-in support for a wide range of Humantic AI features, including creating, retrieving, and updating profiles.\nOn this page, you'll find a list of operations the Humantic AI node supports and links to more resources.\nOperations\nProfile\nCreate a profile\nRetrieve a profile\nUpdate a profile\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.hunter.md",
    "content": "Hunter node\nUse the Hunter node to automate work in Hunter, and integrate Hunter with other applications. n8n has built-in support for a wide range of Hunter features, including getting, generating, and verifying email addresses.\nOn this page, you'll find a list of operations the Hunter node supports and links to more resources.\nOperations\nGet every email address found on the internet using a given domain name, with sources\nGenerate or retrieve the most likely email address from a domain name, a first name and a last name\nVerify the deliverability of an email address\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.intercom.md",
    "content": "Intercom node\nUse the Intercom node to automate work in Intercom, and integrate Intercom with other applications. n8n has built-in support for a wide range of Intercom features, including creating, updating, deleting, and getting companies, leads, and users.\nOn this page, you'll find a list of operations the Intercom node supports and links to more resources.\nOperations\nCompany\nCreate a new company\nGet data of a company\nGet data of all companies\nUpdate a company\nList company's users\nLead\nCreate a new lead\nDelete a lead\nGet data of a lead\nGet data of all leads\nUpdate new lead\nUser\nCreate a new user\nDelete a user\nGet data of a user\nGet data of all users\nUpdate a user\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.invoiceninja.md",
    "content": "Invoice Ninja node\nUse the Invoice Ninja node to automate work in Invoice Ninja, and integrate Invoice Ninja with other applications. n8n has built-in support for a wide range of Invoice Ninja features, including creating, updating, deleting, and getting clients, expense, invoice, payments and quotes.\nOn this page, you'll find a list of operations the Invoice Ninja node supports and links to more resources.\nOperations\nClient\nCreate a new client\nDelete a client\nGet data of a client\nGet data of all clients\nExpense\nCreate a new expense\nDelete an expense\nGet data of an expense\nGet data of all expenses\nInvoice\nCreate a new invoice\nDelete a invoice\nEmail an invoice\nGet data of a invoice\nGet data of all invoices\nPayment\nCreate a new payment\nDelete a payment\nGet data of a payment\nGet data of all payments\nQuote\nCreate a new quote\nDelete a quote\nEmail an quote\nGet data of a quote\nGet data of all quotes\nTask\nCreate a new task\nDelete a task\nGet data of a task\nGet data of all tasks\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.iterable.md",
    "content": "Iterable node\nUse the Iterable node to automate work in Iterable, and integrate Iterable with other applications. n8n has built-in support for a wide range of Iterable features, including creating users, recording the actions performed by the users, and adding and removing users from the list.\nOn this page, you'll find a list of operations the Iterable node supports and links to more resources.\nOperations\nEvent\nRecord the actions a user perform\nUser\nCreate/Update a user\nDelete a user\nGet a user\nUser List\nAdd user to list\nRemove a user from a list\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.jenkins.md",
    "content": "Jenkins node\nUse the Jenkins node to automate work in Jenkins, and integrate Jenkins with other applications. n8n has built-in support for a wide range of Jenkins features, including listing builds, managing instances, and creating and copying jobs.\nOn this page, you'll find a list of operations the Jenkins node supports and links to more resources.\nOperations\nBuild\nList Builds\nInstance\nCancel quiet down state\nPut Jenkins in quiet mode, no builds can be started, Jenkins is ready for shutdown\nRestart Jenkins immediately on environments where it's possible\nRestart Jenkins once no jobs are running on environments where it's possible\nShutdown once no jobs are running\nShutdown Jenkins immediately\nJob\nCopy a specific job\nCreate a new job\nTrigger a specific job\nTrigger a specific job\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.jinaai.md",
    "content": "Jina AI node\nUse the Jina AI node to automate work in Jina AI and integrate Jina AI with other applications. n8n has built-in support for a wide range of Jina AI features.\nOn this page, you'll find a list of operations the Jina AI node supports, and links to more resources.\nOperations\nReader:\nRead: Fetches content from a URL and converts it to clean, LLM-friendly formats.\nSearch: Performs a web search using Jina AI and returns the top results as clean, LLM-friendly formats.\nResearch:\nDeep Research: Research a topic and generate a structured research report.\nTemplates and examples\nRelated resources\nRefer to Jina AI's reader API documentation and Jina AI's search API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.jira.md",
    "content": "Jira Software node\nUse the Jira Software node to automate work in Jira, and integrate Jira with other applications. n8n has built-in support for a wide range of Jira features, including creating, updating, deleting, and getting issues, and users.\nOn this page, you'll find a list of operations the Jira Software node supports and links to more resources.\nOperations\nIssue\nGet issue changelog\nCreate a new issue\nDelete an issue\nGet an issue\nGet all issues\nCreate an email notification for an issue and add it to the mail queue\nReturn either all transitions or a transition that can be performed by the user on an issue, based on the issue's status\nUpdate an issue\nIssue Attachment\nAdd attachment to issue\nGet an attachment\nGet all attachments\nRemove an attachment\nIssue Comment\nAdd comment to issue\nGet a comment\nGet all comments\nRemove a comment\nUpdate a comment\nUser\nCreate a new user.\nDelete a user.\nRetrieve a user.\nTemplates and examples\nRelated resources\nRefer to the official JQL documentation about Jira Query Language (JQL) to learn more about it.\nFetch issues for a specific project\nThe Get All operation returns all the issues from Jira. To fetch issues for a particular project, you need to use Jira Query Language (JQL).\nFor example, if you want to receive all the issues of a project named n8n, you'd do something like this:\nSelect Get All from the Operation dropdown list.\nToggle Return All to true.\nSelect Add Option and select JQL.\nEnter project=n8n in the JQL field.\nThis query will fetch all the issues in the project named n8n. Enter the name of your project instead of n8n to fetch all the issues for your project."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.kafka.md",
    "content": "Kafka node\nUse the Kafka node to automate work in Kafka, and integrate Kafka with other applications. n8n has built-in support for a wide range of Kafka features, including sending messages.\nOn this page, you'll find a list of operations the Kafka node supports and links to more resources.\nOperations\nSend message\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.keap.md",
    "content": "Keap node\nUse the Keap node to automate work in Keap, and integrate Keap with other applications. n8n has built-in support for a wide range of Keap features, including creating, updating, deleting, and getting companies, products, ecommerce orders, emails, and files.\nOn this page, you'll find a list of operations the Keap node supports and links to more resources.\nOperations\nCompany\nCreate a company\nRetrieve all companies\nContact\nCreate/update a contact\nDelete an contact\nRetrieve an contact\nRetrieve all contacts\nContact Note\nCreate a note\nDelete a note\nGet a notes\nRetrieve all notes\nUpdate a note\nContact Tag\nAdd a list of tags to a contact\nDelete a contact's tag\nRetrieve all contact's tags\nEcommerce Order\nCreate an ecommerce order\nGet an ecommerce order\nDelete an ecommerce order\nRetrieve all ecommerce orders\nEcommerce Product\nCreate an ecommerce product\nDelete an ecommerce product\nGet an ecommerce product\nRetrieve all ecommerce product\nEmail\nCreate a record of an email sent to a contact\nRetrieve all sent emails\nSend Email\nFile\nDelete a file\nRetrieve all files\nUpload a file\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.kitemaker.md",
    "content": "Kitemaker node\nUse the Kitemaker node to automate work in Kitemaker, and integrate Kitemaker with other applications. n8n has built-in support for a wide range of Kitemaker features, including retrieving data on organizations, spaces and users, as well as creating, getting, and updating work items.\nOn this page, you'll find a list of operations the Kitemaker node supports and links to more resources.\nOperations\nOrganization\nRetrieve data on the logged-in user's organization.\nSpace\nRetrieve data on all the spaces in the logged-in user's organization.\nUser\nRetrieve data on all the users in the logged-in user's organization.\nWork Item\nCreate\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.kobotoolbox.md",
    "content": "KoboToolbox node\nUse the KoboToolbox node to automate work in KoboToolbox, and integrate KoboToolbox with other applications. n8n has built-in support for a wide range of KoboToolbox features, including creating, updating, deleting, and getting files, forms, hooks, and submissions.\nOn this page, you'll find a list of operations the KoboToolbox node supports and links to more resources.\nOperations\nFile\nCreate\nDelete\nGet\nGet Many\nForm\nGet\nGet Many\nRedeploy\nHook\nGet\nGet Many\nLogs\nRetry All\nRetry One\nSubmission\nDelete\nGet\nGet Many\nGet Validation Status\nUpdate Validation Status\nTemplates and examples\nOptions\nQuery Options\nThe Query Submission operation supports query options:\nIn the main section of the Parameters panel:\nStart controls the index offset to start the query from (to use the API pagination logic).\nLimit sets the maximum number of records to return. Note that the API always has a limit of 30,000 returned records, whatever value you provide.\nIn the Query Options section, you can activate the following parameters:\nQuery lets you specify filter predicates in MongoDB's JSON query format. For example: {\"status\": \"success\", \"_submission_time\": {\"$lt\": \"2021-11-01T01:02:03\"}} queries for all submissions with the value success for the field status, and submitted before November 1st, 2021, 01:02:03.\nFields lets you specify the list of fields you want to fetch, to make the response lighter.\nSort lets you provide a list of sorting criteria in MongoDB JSON format. For example, {\"status\": 1, \"_submission_time\": -1} specifies a sort order by ascending status, and then descending submission time.\nMore details about these options can be found in the Formhub API docs#api-parameters)\nSubmission options\nAll operations that return form submission data offer options to tweak the response. These include:\nDownload options lets you download any attachment linked to each particular form submissions, such as pictures and videos. It also lets you select the naming pattern, and the file size to download (if available - typically for images).\nFormatting options perform some reformatting as described in About reformatting.\nAbout reformatting\nThe default JSON format for KoboToolbox submission data is sometimes hard to deal with, because it's not schema-aware, and all fields are therefore returned as strings.\nThis node provides a lightweight opinionated reformatting logic, enabled with the Reformat? parameter, available on all operations that return form submissions: the submission query, get, and the attachment download operations.\nWhen enabled, the reformatting:\nReorganizes the JSON into a multi-level hierarchy following the form's groups. By default, question grouping hierarchy is materialized by a / character in the field names, for example Group1/Question1. With reformatting enabled, n8n reorganizes these into Group1.Question1, as nested JSON objects.\nRenames fields to trim _ (not supported by many downstream systems).\nParses all geospatial fields (Point, Line, and Area question types) into their standard GeoJSON equivalent.\nSplits all fields matching any of the Multiselect Mask wildcard masks into an array. Since the multi-select fields appear as space-separated strings, they can't be guessed algorithmically, so you must provide a field naming mask. Format the masks as a comma-separated list. Lists support the * wildcard.\nConverts all fields matching any of the Number Mask wildcard masks into a JSON float.\nHere's a detailed example in JSON:\nWith reformatting enabled, and the appropriate masks for multi-select and number formatting (for example, Crops_ and _sqm respectively), n8n parses it into:"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.lemlist.md",
    "content": "Lemlist node\nUse the Lemlist node to automate work in Lemlist, and integrate Lemlist with other applications. n8n has built-in support for a wide range of Lemlist features, including getting activities, teams and campaigns, as well as creating, updating, and deleting leads.\nOn this page, you'll find a list of operations the Lemlist node supports and links to more resources.\nOperations\nActivity\nGet Many: Get many activities\nCampaign\nGet Many: Get many campaigns\nGet Stats: Get campaign stats\nEnrichment\nGet: Fetches a previously completed enrichment\nEnrich Lead: Enrich a lead using an email or LinkedIn URL\nEnrich Person: Enrich a person using an email or LinkedIn URL\nLead\nCreate: Create a new lead\nDelete: Delete an existing lead\nGet: Get an existing lead\nUnsubscribe: Unsubscribe an existing lead\nTeam\nGet: Get an existing team\nGet Credits: Get an existing team's credits\nUnsubscribe\nAdd: Add an email to an unsubscribe list\nDelete: Delete an email from an unsubscribe list\nGet Many: Get many unsubscribed emails\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.line.md",
    "content": "Line node\nUse the Line node to automate work in Line, and integrate Line with other applications. n8n has built-in support for a wide range of Line features, including sending notifications.\nOn this page, you'll find a list of operations the Line node supports and links to more resources.\nOperations\nNotification\nSends notifications to users or groups\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.linear.md",
    "content": "Linear node\nUse the Linear node to automate work in Linear, and integrate Linear with other applications. n8n has built-in support for a wide range of Linear features, including creating, updating, deleting, and getting issues.\nOn this page, you'll find a list of operations the Linear node supports and links to more resources.\nOperations\nIssue\nCreate\nDelete\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.lingvanex.md",
    "content": "LingvaNex node\nUse the LingvaNex node to automate work in LingvaNex, and integrate LingvaNex with other applications. n8n has built-in support for translating data with LingvaNex.\nOn this page, you'll find a list of operations the LingvaNex node supports and links to more resources.\nOperations\nTranslate data\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.linkedin.md",
    "content": "LinkedIn node\nUse the LinkedIn node to automate work in LinkedIn, and integrate LinkedIn with other applications. n8n supports creating posts.\nOn this page, you'll find a list of operations the LinkedIn node supports and links to more resources.\nOperations\nPost\nCreate\nParameters\nPost As: choose whether to post as a Person or Organization.\nPerson Name or ID and Organization URN: enter an identifier for the person or organization.\nText: the post contents.\nMedia Category: use this when including images or article URLs in your post.\nTemplates and examples\nRelated resources\nRefer to LinkedIn's API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.lonescale.md",
    "content": "LoneScale node\nUse the LoneScale node to automate work in LoneScale and integrate LoneScale with other applications. n8n has built-in support for managing Lists and Items in LoneScale.\nOn this page, you'll find a list of operations the LoneScale node supports, and links to more resources.\nOperations\nList\nCreate\nItem\nCreate\nTemplates and examples\nRelated resources\nRefer to LoneScales documentation for more information about the service.\nn8n provides a trigger node for LoneScale. You can find the trigger node docs here."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.magento2.md",
    "content": "Magento 2 node\nUse the Magento 2 node to automate work in Magento 2, and integrate Magento 2 with other applications. n8n has built-in support for a wide range of Magento 2 features, including creating, updating, deleting, and getting customers, invoices, orders, and projects.\nOn this page, you'll find a list of operations the Magento 2 node supports and links to more resources.\nOperations\nCustomer\nCreate a new customer\nDelete a customer\nGet a customer\nGet all customers\nUpdate a customer\nInvoice\nCreate an invoice\nOrder\nCancel an order\nGet an order\nGet all orders\nShip an order\nProduct\nCreate a product\nDelete a product\nGet a product\nGet all products\nUpdate a product\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mailcheck.md",
    "content": "Mailcheck node\nUse the Mailcheck node to automate work in Mailcheck, and integrate Mailcheck with other applications. n8n has built-in support for a wide range of Mailcheck features, including checking emails.\nOn this page, you'll find a list of operations the Mailcheck node supports and links to more resources.\nOperations\nEmail\nCheck\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mailchimp.md",
    "content": "Mailchimp node\nUse the Mailchimp node to automate work in Mailchimp, and integrate Mailchimp with other applications. n8n has built-in support for a wide range of Mailchimp features, including creating, updating, and deleting campaigns, as well as getting list groups.\nOn this page, you'll find a list of operations the Mailchimp node supports and links to more resources.\nOperations\nCampaign\nDelete a campaign\nGet a campaign\nGet all the campaigns\nReplicate a campaign\nCreates a Resend to Non-Openers version of this campaign\nSend a campaign\nList Group\nGet all groups\nMember\nCreate a new member on list\nDelete a member on list\nGet a member on list\nGet all members on list\nUpdate a new member on list\nMember Tag\nAdd tags from a list member\nRemove tags from a list member\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mailerlite.md",
    "content": "MailerLite node\nUse the MailerLite node to automate work in MailerLite, and integrate MailerLite with other applications. n8n has built-in support for a wide range of MailerLite features, including creating, updating, deleting, and getting subscribers.\nOn this page, you'll find a list of operations the MailerLite node supports and links to more resources.\nOperations\nSubscriber\nCreate a new subscriber\nGet an subscriber\nGet all subscribers\nUpdate an subscriber\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mailgun.md",
    "content": "Mailgun node\nUse the Mailgun node to automate work in Mailgun, and integrate Mailgun with other applications. n8n has built-in support for sending emails with Mailgun.\nOn this page, you'll find a list of operations the Mailgun node supports and links to more resources.\nOperations\nSend an email\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mailjet.md",
    "content": "Mailjet node\nUse the Mailjet node to automate work in Mailjet, and integrate Mailjet with other applications. n8n has built-in support for a wide range of Mailjet features, including sending emails, and SMS.\nOn this page, you'll find a list of operations the Mailjet node supports and links to more resources.\nOperations\nEmail\nSend an email\nSend an email template\nSMS\nSend an SMS\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mandrill.md",
    "content": "Mandrill node\nUse the Mandrill node to automate work in Mandrill, and integrate Mandrill with other applications. n8n supports sending messages based on templates or HTML with Mandrill.\nOn this page, you'll find a list of operations the Mandrill node supports and links to more resources.\nOperations\nMessage\nSend message based on template.\nSend message based on HTML.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.marketstack.md",
    "content": "marketstack node\nUse the marketstack node to automate work in marketstack, and integrate marketstack with other applications. n8n has built-in support for a wide range of marketstack features, including getting exchanges, end-of-day data, and tickers.\nOn this page, you'll find a list of operations the marketstack node supports and links to more resources.\nOperations\nEnd-of-Day Data\nGet All\nExchange\nGet\nTicker\nGet\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.matrix.md",
    "content": "Matrix node\nUse the Matrix node to automate work in Matrix, and integrate Matrix with other applications. n8n has built-in support for a wide range of Matrix features, including getting current user's account information, sending media and messages to a room, and getting room members and messages.\nOn this page, you'll find a list of operations the Matrix node supports and links to more resources.\nOperations\nAccount\nGet current user's account information\nEvent\nGet single event by ID\nMedia\nSend media to a chat room\nMessage\nSend a message to a room\nGets all messages from a room\nRoom\nNew chat room with defined settings\nInvite a user to a room\nJoin a new room\nKick a user from a room\nLeave a room\nRoom Member\nGet all members\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mattermost.md",
    "content": "Mattermost node\nUse the Mattermost node to automate work in Mattermost, and integrate Mattermost with other applications. n8n has built-in support for a wide range of Mattermost features, including creating, deleting, and getting channels, and users, as well as posting messages, and adding reactions.\nOn this page, you'll find a list of operations the Mattermost node supports and links to more resources.\nOperations\nChannel\nAdd a user to a channel\nCreate a new channel\nSoft delete a channel\nGet a page of members for a channel\nRestores a soft deleted channel\nSearch for a channel\nGet statistics for a channel\nMessage\nSoft delete a post, by marking the post as deleted in the database\nPost a message into a channel\nPost an ephemeral message into a channel\nReaction\nAdd a reaction to a post.\nRemove a reaction from a post\nGet all the reactions to one or more posts\nUser\nCreate a new user\nDeactivates the user and revokes all its sessions by archiving its user object.\nRetrieve all users\nGet a user by email\nGet a user by ID\nInvite user to team\nTemplates and examples\nRelated resources\nRefer to Mattermost's documentation for more information about the service.\nChannel ID field error\nIf you're not the System Administrator, you might get an error: there was a problem loading the parameter options from server: \"Mattermost error response: You do not have the appropriate permissions. next to the Channel ID field.\nAsk your system administrator to grant you the post:channel permission.\nFind the channel ID\nTo find the channel ID in Mattermost:\nSelect the channel from the left sidebar.\nSelect the channel name at the top.\nSelect View Info."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mautic.md",
    "content": "Mautic node\nUse the Mautic node to automate work in Mautic, and integrate Mautic with other applications. n8n has built-in support for a wide range of Mautic features, including creating, updating, deleting, and getting companies, and contacts, as well as adding and removing campaign contacts.\nOn this page, you'll find a list of operations the Mautic node supports and links to more resources.\nOperations\nCampaign Contact\nAdd contact to a campaign\nRemove contact from a campaign\nCompany\nCreate a new company\nDelete a company\nGet data of a company\nGet data of all companies\nUpdate a company\nCompany Contact\nAdd contact to a company\nRemove a contact from a company\nContact\nCreate a new contact\nDelete a contact\nEdit contact's points\nAdd/remove contacts from/to the don't contact list\nGet data of a contact\nGet data of all contacts\nSend email to contact\nUpdate a contact\nContact Segment\nAdd contact to a segment\nRemove contact from a segment\nSegment Email\nSend\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.medium.md",
    "content": "Medium node\nUse the Medium node to automate work in Medium, and integrate Medium with other applications. n8n has built-in support for a wide range of Medium features, including creating posts, and getting publications.\nOn this page, you'll find a list of operations the Medium node supports and links to more resources.\nOperations\nPost\nCreate a post\nPublication\nGet all publications\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.messagebird.md",
    "content": "MessageBird node\nUse the MessageBird node to automate work in MessageBird, and integrate MessageBird with other applications. n8n has built-in support for a wide range of MessageBird features, including sending messages, and getting balances.\nOn this page, you'll find a list of operations the MessageBird node supports and links to more resources.\nOperations\nSMS\nSend text messages (SMS)\nBalance\nGet the balance\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.metabase.md",
    "content": "Metabase node\nUse the Metabase node to automate work in Metabase, and integrate Metabase with other applications. n8n has built-in support for a wide range of Metabase features, including adding, and getting alerts, databases, metrics, and questions.\nOn this page, you'll find a list of operations the Metabase node supports and links to more resources.\nOperations\nAlert\nGet\nGet All\nDatabase\nAdd\nGet All\nGet Fields\nMetric\nGet\nGet All\nQuestion\nGet\nGet All\nResult Data\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftdynamicscrm.md",
    "content": "Microsoft Dynamics CRM node\nUse the Microsoft Dynamics CRM node to automate work in Microsoft Dynamics CRM, and integrate Microsoft Dynamics CRM with other applications. n8n has built-in support for  creating, updating, deleting, and getting Microsoft Dynamics CRM accounts.\nOn this page, you'll find a list of operations the Microsoft Dynamics CRM node supports and links to more resources.\nOperations\nAccount\nCreate\nDelete\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftentra.md",
    "content": "Microsoft Entra ID node\nUse the Microsoft Entra ID node to automate work in Microsoft Entra ID and integrate Microsoft Entra ID with other applications. n8n has built-in support for a wide range of Microsoft Entra ID features, which includes creating, getting, updating, and deleting users and groups, as well as adding users to and removing them from groups.\nOn this page, you'll find a list of operations the Microsoft Entra ID node supports, and links to more resources.\nOperations\nGroup\nCreate: Create a new group\nDelete: Delete an existing group\nGet: Retrieve data for a specific group\nGet Many: Retrieve a list of groups\nUpdate: Update a group\nUser\nCreate: Create a new user\nDelete: Delete an existing user\nGet: Retrieve data for a specific user\nGet Many: Retrieve a list of users\nUpdate: Update a user\nAdd to Group: Add user to a group\nRemove from Group: Remove user from a group\nTemplates and examples\nRelated resources\nRefer to Microsoft Entra ID's documentation for more information about the service.\nCommon issues\nHere are some common errors and issues with the Microsoft Entra ID node and steps to resolve or troubleshoot them.\nUpdating the Allow External Senders and Auto Subscribe New Members options fails\nYou can't update the Allow External Senders and Auto Subscribe New Members options directly after creating a new group. You must wait after creating a group before you can change the values of these options.\nWhen designing workflows that use multiple Microsoft Entra ID nodes to first create groups and then update these options, add a Wait node between the two operations. A Wait node configured to pause for at least two seconds allows time for the group to fully initialize. After the wait, the update operation can complete without erroring."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftexcel.md",
    "content": "Microsoft Excel 365 node\nUse the Microsoft Excel node to automate work in Microsoft Excel, and integrate Microsoft Excel with other applications. n8n has built-in support for a wide range of Microsoft Excel features, including adding and retrieving lists of table data, and workbooks, as well as getting worksheets.\nOn this page, you'll find a list of operations the Microsoft Excel node supports and links to more resources.\nOperations\nTable\nAdds rows to the end of the table\nRetrieve a list of table columns\nRetrieve a list of table rows\nLooks for a specific column value and then returns the matching row\nWorkbook\nAdds a new worksheet to the workbook.\nGet data of all workbooks\nWorksheet\nGet all worksheets\nGet worksheet content\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftgraphsecurity.md",
    "content": "Microsoft Graph Security node\nUse the Microsoft Graph Security node to automate work in Microsoft Graph Security, and integrate Microsoft Graph Security with other applications. n8n has built-in support for a wide range of Microsoft Graph Security features, including getting, and updating scores, and profiles.\nOn this page, you'll find a list of operations the Microsoft Graph Security node supports and links to more resources.\nOperations\nSecure Score\nGet\nGet All\nSecure Score Control Profile\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftonedrive.md",
    "content": "Microsoft OneDrive node\nUse the Microsoft OneDrive node to automate work in Microsoft OneDrive, and integrate Microsoft OneDrive with other applications. n8n has built-in support for a wide range of Microsoft OneDrive features, including creating, updating, deleting, and getting files, and folders.\nOn this page, you'll find a list of operations the Microsoft OneDrive node supports and links to more resources.\nOperations\nFile\nCopy a file\nDelete a file\nDownload a file\nGet a file\nRename a file\nSearch a file\nShare a file\nUpload a file up to 4MB in size\nFolder\nCreate a folder\nDelete a folder\nGet Children (get items inside a folder)\nRename a folder\nSearch a folder\nShare a folder\nTemplates and examples\nRelated resources\nRefer to Microsoft's OneDrive API documentation for more information about the service.\nFind the folder ID\nTo perform operations on folders, you need to supply the ID. You can find this:\nIn the URL of the folder\nBy searching for it using the node. You need to do this if using MS 365 (where OneDrive uses SharePoint behind the scenes):\nSelect Resource > Folder.\nSelect Operation > Search.\nIn Query, enter the folder name.\nSelect Execute step. n8n runs the query and returns data about the folder, including an id field containing the folder ID."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftoutlook.md",
    "content": "Microsoft Outlook node\nUse the Microsoft Outlook node to automate work in Microsoft Outlook, and integrate Microsoft Outlook with other applications. n8n has built-in support for a wide range of Microsoft Outlook features, including creating, updating, deleting, and getting folders, messages, and drafts.\nOn this page, you'll find a list of operations the Microsoft Outlook node supports and links to more resources.\nOperations\nCalendar\nCreate\nDelete\nGet\nGet Many\nUpdate\nContact\nCreate\nDelete\nGet\nGet Many\nUpdate\nDraft\nCreate\nDelete\nGet\nSend\nUpdate\nEvent\nCreate\nDelete\nGet\nGet Many\nUpdate\nFolder\nCreate\nDelete\nGet\nGet Many\nUpdate\nFolder Message\nGet Many\nMessage\nDelete\nGet\nGet Many\nMove\nReply\nSend\nSend and Wait for Response\nUpdate\nMessage Attachment\nAdd\nDownload\nGet\nGet Many\nTemplates and examples\nRelated resources\nRefer to Outlook's API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftsharepoint.md",
    "content": "Microsoft SharePoint node\nUse the Microsoft SharePoint node to automate work in Microsoft SharePoint and integrate Microsoft SharePoint with other applications. n8n has built-in support for a wide range of Microsoft SharePoint features, which includes downloading, uploading, and updating files, managing items in a list, and getting lists and list items.\nOn this page, you'll find a list of operations the Microsoft SharePoint node supports, and links to more resources.\nOperations\nFile:\nDownload: Download a file.\nUpdate: Update a file.\nUpload: Upload an existing file.\nItem:\nCreate: Create an item in an existing list.\nCreate or Update: Create a new item, or update the current one if it already exists (upsert).\nDelete: Delete an item from a list.\nGet: Retrieve an item from a list.\nGet Many: Get specific items in a list or list many items.\nUpdate: Update an item in an existing list.\nList:\nGet: Retrieve details of a single list.\nGet Many: Retrieve a list of lists.\nTemplates and examples\nRelated resources\nRefer to Microsoft's SharePoint documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftsql.md",
    "content": "Microsoft SQL node\nUse the Microsoft SQL node to automate work in Microsoft SQL, and integrate Microsoft SQL with other applications. n8n has built-in support for a wide range of Microsoft SQL features, including executing SQL queries, and inserting rows into the database.\nOn this page, you'll find a list of operations the Microsoft SQL node supports and links to more resources.\nOperations\nExecute an SQL query\nInsert rows in database\nUpdate rows in database\nDelete rows in database\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsoftteams.md",
    "content": "Microsoft Teams node\nUse the Microsoft Teams node to automate work in Microsoft Teams, and integrate Microsoft Teams with other applications. n8n has built-in support for a wide range of Microsoft Teams features, including creating and deleting, channels, messages, and tasks.\nOn this page, you'll find a list of operations the Microsoft Teams node supports and links to more resources.\nOperations\nChannel\nCreate\nDelete\nGet\nGet Many\nUpdate\nChannel Message\nCreate\nGet Many\nChat Message\nCreate\nGet\nGet Many\nSend and Wait for Response\nTask\nCreate\nDelete\nGet\nGet Many\nUpdate\nTemplates and examples\nRelated resources\nRefer to Microsoft Teams' API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.microsofttodo.md",
    "content": "Microsoft To Do node\nUse the Microsoft To Do node to automate work in Microsoft To Do, and integrate Microsoft To Do with other applications. n8n has built-in support for a wide range of Microsoft To Do features, including creating, updating, deleting, and getting linked resources, lists, and tasks.\nOn this page, you'll find a list of operations the Microsoft To Do node supports and links to more resources.\nOperations\nLinked Resource\nCreate\nDelete\nGet\nGet All\nUpdate\nList\nCreate\nDelete\nGet\nGet All\nUpdate\nTask\nCreate\nDelete\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mindee.md",
    "content": "Mindee node\nUse the Mindee node to automate work in Mindee, and integrate Mindee with other applications. n8n has built-in support for a wide range of Mindee features, including predicting invoices.\nOn this page, you'll find a list of operations the Mindee node supports and links to more resources.\nOperations\nInvoice\nPredict\nReceipt\nPredict\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.misp.md",
    "content": "MISP node\nUse the MISP node to automate work in MISP, and integrate MISP with other applications. n8n has built-in support for a wide range of MISP features, including creating, updating, deleting and getting events, feeds, and organizations.\nOn this page, you'll find a list of operations the MISP node supports and links to more resources.\nOperations\nAttribute\nCreate\nDelete\nGet\nGet All\nSearch\nUpdate\nEvent\nCreate\nDelete\nGet\nGet All\nPublish\nSearch\nUnpublish\nUpdate\nEvent Tag\nAdd\nRemove\nFeed\nCreate\nDisable\nEnable\nGet\nGet All\nUpdate\nGalaxy\nDelete\nGet\nGet All\nNoticelist\nGet\nGet All\nObject\nSearch\nOrganisation\nCreate\nDelete\nGet\nGet All\nUpdate\nTag\nCreate\nDelete\nGet All\nUpdate\nUser\nCreate\nDelete\nGet\nGet All\nUpdate\nWarninglist\nGet\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mistralai.md",
    "content": "Mistral AI node\nUse the Mistral AI node to automate work in Mistral AI and integrate Mistral AI with other applications. n8n has built-in support for extracting text with various models, file types, and input methods.\nOn this page, you'll find a list of operations the Mistral AI node supports, and links to more resources.\nNode parameters\nResource: The resource that Mistral AI should operate on. The current implementation supports the \"Document\" resource.\nOperation: The operation to perform:\nExtract Text: Extracts text from a document or image using optical character recognition (OCR).\nModel: The model to use for the given operation. The current version requires the mistral-ocr-latest model.\nDocument Type: The document format to process. Can be \"Document\" or \"Image\".\nInput Type: How to input the document:\nBinary Data: Pass the document to this node as a binary field.\nURL: Fetch the document from a given URL.\nInput Binary Field: When using the \"Binary Data\" input type, defines the name of the input binary field containing the file.\nURL: When using the \"URL\" input type, the URL of the document or image to process.\nNode options\nEnable Batch Processing: Whether to process multiple documents in the same API call. This may reduce your costs by bundling requests.\nBatch Size: When using \"Enable Batch Processing\", sets the maximum number of documents to process per batch.\nDelete Files After Processing: When using \"Enable Batch Processing\", whether to delete the files from Mistral Cloud after processing.\nTemplates and examples\nRelated resources\nRefer to Mistral AI's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mocean.md",
    "content": "Mocean node\nUse the Mocean node to automate work in Mocean, and integrate Mocean with other applications. n8n has built-in support for a wide range of Mocean features, including sending SMS, and voice messages.\nOn this page, you'll find a list of operations the Mocean node supports and links to more resources.\nOperations\nSMS\nSend SMS/Voice message\nVoice\nSend SMS/Voice message\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mondaycom.md",
    "content": "monday.com node\nUse the monday.com node to automate work in monday.com, and integrate monday.com with other applications. n8n has built-in support for a wide range of monday.com features, including creating a new board, and adding, deleting, and getting items on the board.\nOn this page, you'll find a list of operations the monday.com node supports and links to more resources.\nOperations\nBoard\nArchive a board\nCreate a new board\nGet a board\nGet all boards\nBoard Column\nCreate a new column\nGet all columns\nBoard Group\nDelete a group in a board\nCreate a group in a board\nGet list of groups in a board\nBoard Item\nAdd an update to an item.\nChange a column value for a board item\nChange multiple column values for a board item\nCreate an item in a board's group\nDelete an item\nGet an item\nGet all items\nGet items by column value\nMove item to group\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mongodb.md",
    "content": "MongoDB node\nUse the MongoDB node to automate work in MongoDB, and integrate MongoDB with other applications. n8n has built-in support for a wide range of MongoDB features, including aggregating, updating, finding, deleting, and getting documents.\nOn this page, you'll find a list of operations the MongoDB node supports and links to more resources.\nOperations\nAggregate documents\nDelete documents\nFind documents\nFind and replace documents\nFind and update documents\nInsert documents\nUpdate documents\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.monicacrm.md",
    "content": "Monica CRM node\nUse the Monica CRM node to automate work in Monica CRM, and integrate Monica CRM with other applications. n8n has built-in support for a wide range of Monica CRM features, including creating, updating, deleting, and getting activities, calls, contracts, messages, tasks, and notes.\nOn this page, you'll find a list of operations the Monica CRM node supports and links to more resources.\nOperations\nActivity\nCreate an activity\nDelete an activity\nRetrieve an activity\nRetrieve all activities\nUpdate an activity\nCall\nCreate a call\nDelete a call\nRetrieve a call\nRetrieve all calls\nUpdate a call\nContact\nCreate a contact\nDelete a contact\nRetrieve a contact\nRetrieve all contacts\nUpdate a contact\nContact Field\nCreate a contact field\nDelete a contact field\nRetrieve a contact field\nUpdate a contact field\nContact Tag\nAdd\nRemove\nConversation\nCreate a conversation\nDelete a conversation\nRetrieve a conversation\nUpdate a conversation\nConversation Message\nAdd a message to a conversation\nUpdate a message in a conversation\nJournal Entry\nCreate a journal entry\nDelete a journal entry\nRetrieve a journal entry\nRetrieve all journal entries\nUpdate a journal entry\nNote\nCreate a note\nDelete a note\nRetrieve a note\nRetrieve all notes\nUpdate a note\nReminder\nCreate a reminder\nDelete a reminder\nRetrieve a reminder\nRetrieve all reminders\nUpdate a reminder\nTag\nCreate a tag\nDelete a tag\nRetrieve a tag\nRetrieve all tags\nUpdate a tag\nTask\nCreate a task\nDelete a task\nRetrieve a task\nRetrieve all tasks\nUpdate a task\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mqtt.md",
    "content": "MQTT node\nUse the MQTT node to automate work in MQTT, and integrate MQTT with other applications. n8n supports transporting messages with MQTT.\nOn this page, you'll find a list of operations the MQTT node supports and links to more resources.\nOperations\nUse the MQTT node to send a message. You can set the message topic, and choose whether to send the node input data as part of the message.\nTemplates and examples\nRelated resources\nn8n provides a trigger node for MQTT. You can find the trigger node docs here.\nRefer to MQTT's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.msg91.md",
    "content": "MSG91 node\nUse the MSG91 node to automate work in MSG91, and integrate MSG91 with other applications. n8n supports sending SMS with MSG91.\nOn this page, you'll find a list of operations the MSG91 node supports and links to more resources.\nOperations\nSMS\nSend SMS\nTemplates and examples\nFind your Sender ID\nLog in to your MSG91 dashboard.\nSelect Sender Id in the left panel.\nIf you don't already have one, select Add Sender Id +, fill in the details, and select Save Sender Id."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.n8ntrainingcustomerdatastore.md",
    "content": "Customer Datastore (n8n Training) node\nUse this node only for the n8n new user onboarding tutorial. It provides dummy data for testing purposes and has no further functionality."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.n8ntrainingcustomermessenger.md",
    "content": "Customer Messenger (n8n Training) node\nUse this node only for the n8n new user onboarding tutorial. It provides no further functionality."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.nasa.md",
    "content": "NASA node\nUse the NASA node to automate work in NASA, and integrate NASA with other applications. n8n has built-in support for a wide range of NASA features, including retrieving imagery and data.\nOn this page, you'll find a list of operations the NASA node supports and links to more resources.\nOperations\nAstronomy Picture of the Day\nGet the Astronomy Picture of the Day\nAsteroid Neo-Feed\nRetrieve a list of asteroids based on their closest approach date to Earth\nAsteroid Neo-Lookup\nLook up an asteroid based on its NASA SPK-ID\nAsteroid Neo-Browse\nBrowse the overall asteroid dataset\nDONKI Coronal Mass Ejection\nRetrieve DONKI coronal mass ejection data\nDONKI Interplanetary Shock\nRetrieve DONKI interplanetary shock data\nDONKI Solar Flare\nRetrieve DONKI solar flare data\nDONKI Solar Energetic Particle\nRetrieve DONKI solar energetic particle data\nDONKI Magnetopause Crossing\nRetrieve data on DONKI magnetopause crossings\nDONKI Radiation Belt Enhancement\nRetrieve DONKI radiation belt enhancement data\nDONKI High Speed Stream\nRetrieve DONKI high speed stream data\nDONKI WSA+EnlilSimulation\nRetrieve DONKI WSA+EnlilSimulation data\nDONKI Notifications\nRetrieve DONKI notifications data\nEarth Imagery\nRetrieve Earth imagery\nEarth Assets\nRetrieve Earth assets\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.netlify.md",
    "content": "Netlify node\nUse the Netlify node to automate work in Netlify, and integrate Netlify with other applications. n8n has built-in support for a wide range of Netlify features, including getting and cancelling deployments, as well as deleting, and getting sites.\nOn this page, you'll find a list of operations the Netlify node supports and links to more resources.\nOperations\nDeploy\nCancel a deployment\nCreate a new deployment\nGet a deployment\nGet all deployments\nSite\nDelete a site\nGet a site\nReturns all sites\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.netscaleradc.md",
    "content": "Netscaler ADC node\nUse the Netscaler ADC node to automate work in Netscaler ADC, and integrate Netscaler ADC with other applications. n8n has built-in support for a wide range of Netscaler ADC features, including creating and installing certificates and files.\nOn this page, you'll find a list of operations the Netscaler ADC node supports and links to more resources.\nOperations\nCertificate\nCreate\nInstall\nFile\nDelete\nDownload\nUpload\nTemplates and examples\nRelated resources\nRefer to Netscaler ADC's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.nextcloud.md",
    "content": "Nextcloud node\nUse the Nextcloud node to automate work in Nextcloud, and integrate Nextcloud with other applications. n8n has built-in support for a wide range of Nextcloud features, including creating, updating, deleting, and getting files, and folders as well as retrieving, and inviting users.\nOn this page, you'll find a list of operations the Nextcloud node supports and links to more resources.\nOperations\nFile\nCopy a file\nDelete a file\nDownload a file\nMove a file\nShare a file\nUpload a file\nFolder\nCopy a folder\nCreate a folder\nDelete a folder\nReturn the contents of a given folder\nMove a folder\nShare a folder\nUser\nInvite a user to a Nextcloud organization\nDelete a user.\nRetrieve information about a single user.\nRetrieve a list of users.\nEdit attributes related to a user.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.nocodb.md",
    "content": "NocoDB node\nUse the NocoDB node to automate work in NocoDB, and integrate NocoDB with other applications. n8n has built-in support for a wide range of NocoDB features, including creating, updating, deleting, and retrieving rows.\nOn this page, you'll find a list of operations the NocoDB node supports and links to more resources.\nOperations\nRow\nCreate\nDelete\nGet\nGet Many\nUpdate a row\nTemplates and examples\nRelates resources\nRefer to NocoDB's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.npm.md",
    "content": "npm node\nUse the npm node to automate work in npm, and integrate npm with other applications.\nOn this page, you'll find a list of operations the npm node supports and links to more resources.\nOperations\nPackage\nGet Package Metadata\nGet Package Versions\nSearch for Packages\nDistribution Tag\nGet All Tags\nUpdate a Tag\nTemplates and examples\nRelated resources\nRefer to npm's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.odoo.md",
    "content": "Odoo node\nUse the Odoo node to automate work in Odoo, and integrate Odoo with other applications. n8n has built-in support for a wide range of Odoo features, including creating, updating, deleting, and getting contracts, resources, and opportunities.\nOn this page, you'll find a list of operations the Odoo node supports and links to more resources.\nOperations\nContact\nCreate a new contact\nDelete a contact\nGet a contact\nGet all contacts\nUpdate a contact\nCustom Resource\nCreate a new item\nDelete an item\nGet an item\nGet all items\nUpdate an item\nNote\nCreate a new note\nDelete a note\nGet a note\nGet all notes\nUpdate a note\nOpportunity\nCreate a new opportunity\nDelete an opportunity\nGet an opportunity\nGet all opportunities\nUpdate an opportunity\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.okta.md",
    "content": "Okta node\nUse the Okta node to automate work in Okta and integrate Okta with other applications. n8n has built-in support for a wide range of Okta features, which includes creating, updating, and deleting users.\nOn this page, you'll find a list of operations the Okta node supports, and links to more resources.\nOperations\nUser\nCreate a new user\nDelete an existing user\nGet details of a user\nGet many users\nUpdate an existing user\nTemplates and examples\nRelated resources\nRefer to Okta's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.onesimpleapi.md",
    "content": "One Simple API node\nUse the One Simple API node to automate work in One Simple API, and integrate One Simple API with other applications. n8n has built-in support for a wide range of One Simple API features, including getting profiles, retrieving information, and generating utilities.\nOn this page, you'll find a list of operations the One Simple API node supports and links to more resources.\nOperations\nInformation\nConvert a value between currencies\nRetrieve image metadata from a URL\nSocial Profile\nGet details about an Instagram profile\nGet details about a Spotify Artist\nUtility\nExpand a shortened url\nGenerate a QR Code\nValidate an email address\nWebsite\nGenerate a PDF from a webpage\nGet SEO information from website\nCreate a screenshot from a webpage\nTemplates and examples\nRelated resources\nRefer to One Simple API's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.onfleet.md",
    "content": "Onfleet node\nUse the Onfleet node to automate work in Onfleet, and integrate Onfleet with other applications. n8n has built-in support for a wide range of Onfleet features, including creating and deleting tasks in Onfleet as well as retrieving organizations' details.\nOn this page, you'll find a list of operations the Onfleet node supports and links to more resources.\nOperations\nAdmin\nCreate a new Onfleet admin\nDelete an Onfleet admin\nGet all Onfleet admins\nUpdate an Onfleet admin\nContainer\nAdd task at index (or append)\nGet container information\nFully replace a container's tasks\nDestination\nCreate a new destination\nGet a specific destination\nHub\nCreate a new Onfleet hub\nGet all Onfleet hubs\nUpdate an Onfleet hub\nOrganization\nRetrieve your own organization's details\nRetrieve the details of an organization with which you are connected\nRecipient\nCreate a new Onfleet recipient\nGet a specific Onfleet recipient\nUpdate an Onfleet recipient\nTask\nCreate a new Onfleet task\nClone an Onfleet task\nForce-complete a started Onfleet task\nDelete an Onfleet task\nGet all Onfleet tasks\nGet a specific Onfleet task\nUpdate an Onfleet task\nTeam\nAutomatically dispatch tasks assigned to a team to on-duty drivers\nCreate a new Onfleet team\nDelete an Onfleet team\nGet a specific Onfleet team\nGet all Onfleet teams\nGet estimated times for upcoming tasks for a team, returns a selected driver\nUpdate an Onfleet team\nWorker\nCreate a new Onfleet worker\nDelete an Onfleet worker\nGet a specific Onfleet worker\nGet all Onfleet workers\nGet a specific Onfleet worker schedule\nUpdate an Onfleet worker\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.openthesaurus.md",
    "content": "OpenThesaurus node\nUse the OpenThesaurus node to automate work in OpenThesaurus, and integrate OpenThesaurus with other applications. n8n supports synonym look-up for German words.\nOn this page, you'll find a list of operations the OpenThesaurus node supports and links to more resources.\nOperations\nGet synonyms for a German word in German\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.openweathermap.md",
    "content": "OpenWeatherMap node\nUse the OpenWeatherMap node to automate work in OpenWeatherMap, and integrate OpenWeatherMap with other applications. n8n supports retrieving current and upcoming weather data with OpenWeatherMap.\nOn this page, you'll find a list of operations the OpenWeatherMap node supports and links to more resources.\nOperations\nReturns the current weather data\nReturns the weather data for the next 5 days\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.oura.md",
    "content": "Oura node\nUse the Oura node to automate work in Oura, and integrate Oura with other applications. n8n has built-in support for a wide range of Oura features, including getting profiles, and summaries.\nOn this page, you'll find a list of operations the Oura node supports and links to more resources.\nOperations\nProfile\nGet the user's personal information.\nSummary\nGet the user's activity summary.\nGet the user's readiness summary.\nGet the user's sleep summary\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.paddle.md",
    "content": "Paddle node\nUse the Paddle node to automate work in Paddle, and integrate Paddle with other applications. n8n has built-in support for a wide range of Paddle features, including creating, updating, and getting coupons, as well as getting plans, products, and users.\nOn this page, you'll find a list of operations the Paddle node supports and links to more resources.\nOperations\nCoupon\nCreate a coupon.\nGet all coupons.\nUpdate a coupon.\nPayment\nGet all payment.\nReschedule payment.\nPlan\nGet a plan.\nGet all plans.\nProduct\nGet all products.\nUser\nGet all users\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.pagerduty.md",
    "content": "PagerDuty node\nUse the PagerDuty node to automate work in PagerDuty, and integrate PagerDuty with other applications. n8n has built-in support for a wide range of PagerDuty features, including creating incident notes, as well as updating, and getting all log entries and users.\nOn this page, you'll find a list of operations the PagerDuty node supports and links to more resources.\nOperations\nIncident\nCreate an incident\nGet an incident\nGet all incidents\nUpdate an incident\nIncident Note\nCreate an incident note\nGet all incident's notes\nLog Entry\nGet a log entry\nGet all log entries\nUser\nGet a user\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.paypal.md",
    "content": "PayPal node\nUse the PayPal node to automate work in PayPal, and integrate PayPal with other applications. n8n has built-in support for a wide range of PayPal features, including creating a batch payout and canceling unclaimed payout items.\nOn this page, you'll find a list of operations the PayPal node supports and links to more resources.\nOperations\nPayout\nCreate a batch payout\nShow batch payout details\nPayout Item\nCancels an unclaimed payout item\nShow payout item details\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.peekalink.md",
    "content": "Peekalink node\nUse the Peekalink node to automate work in Peekalink, and integrate Peekalink with other applications. n8n supports checking, and reviewing links with Peekalink.\nOn this page, you'll find a list of operations the Peekalink node supports and links to more resources.\nOperations\nCheck whether preview for a given link is available\nReturn the preview for a link\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.phantombuster.md",
    "content": "PhantomBuster node\nUse the PhantomBuster node to automate work in PhantomBuster, and integrate PhantomBuster with other applications. n8n has built-in support for a wide range of PhantomBuster features, including adding, deleting, and getting agents.\nOn this page, you'll find a list of operations the PhantomBuster node supports and links to more resources.\nOperations\nAgent\nDelete an agent by ID.\nGet an agent by ID.\nGet all agents of the current user's organization.\nGet the output of the most recent container of an agent.\nAdd an agent to the launch queue.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.philipshue.md",
    "content": "Philips Hue node\nUse the Philips Hue node to automate work in Philips Hue, and integrate Philips Hue with other applications. n8n has built-in support for a wide range of Philips Hue features, including deleting, retrieving, and updating lights.\nOn this page, you'll find a list of operations the Philips Hue node supports and links to more resources.\nOperations\nLight\nDelete a light\nRetrieve a light\nRetrieve all lights\nUpdate a light\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.pipedrive.md",
    "content": "Pipedrive node\nUse the Pipedrive node to automate work in Pipedrive, and integrate Pipedrive with other applications. n8n has built-in support for a wide range of Pipedrive features, including creating, updating, deleting, and getting activity, files, notes, organizations, and leads.\nOn this page, you'll find a list of operations the Pipedrive node supports and links to more resources.\nOperations\nActivity\nCreate an activity\nDelete an activity\nGet data of an activity\nGet data of all activities\nUpdate an activity\nDeal\nCreate a deal\nDelete a deal\nDuplicate a deal\nGet data of a deal\nGet data of all deals\nSearch a deal\nUpdate a deal\nDeal Activity\nGet all activities of a deal\nDeal Product\nAdd a product to a deal\nGet all products in a deal\nRemove a product from a deal\nUpdate a product in a deal\nFile\nCreate a file\nDelete a file\nDownload a file\nGet data of a file\nLead\nCreate a lead\nDelete a lead\nGet data of a lead\nGet data of all leads\nUpdate a lead\nNote\nCreate a note\nDelete a note\nGet data of a note\nGet data of all notes\nUpdate a note\nOrganization\nCreate an organization\nDelete an organization\nGet data of an organization\nGet data of all organizations\nUpdate an organization\nSearch organizations\nPerson\nCreate a person\nDelete a person\nGet data of a person\nGet data of all persons\nSearch all persons\nUpdate a person\nProduct\nGet data of all products\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.plivo.md",
    "content": "Plivo node\nUse the Plivo node to automate work in Plivo, and integrate Plivo with other applications. n8n has built-in support for a wide range of Plivo features, including making calls, and sending SMS/MMS.\nOn this page, you'll find a list of operations the Plivo node supports and links to more resources.\nOperations\nCall\nMake a voice call\nMMS\nSend an MMS message (US/Canada only)\nSMS\nSend an SMS message.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.postbin.md",
    "content": "PostBin node\nPostBin is a service that helps you test API clients and webhooks. Use the PostBin node to automate work in PostBin, and integrate PostBin with other applications. n8n has built-in support for a wide range of PostBin features, including creating and deleting bins, and getting and sending requests.\nOn this page, you'll find a list of operations the PostBin node supports, and links to more resources.\nOperations\nBin\nCreate\nGet\nDelete\nRequest\nGet\nRemove First\nSend\nTemplates and examples\nSend requests\nTo send requests to a PostBin bin:\nGo to PostBin and follow the steps to generate a new bin. PostBin gives you a unique URL, including a bin ID.\nIn the PostBin node, select the Request resource.\nChoose the type of Operation you want to perform.\nEnter your bin ID in Bin ID.\nCreate and manage bins\nYou can create and manage PostBin bins using the PostBin node.\nIn Resource, select Bin.\nChoose an Operation. You can create, delete, or get a bin."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.posthog.md",
    "content": "PostHog node\nUse the PostHog node to automate work in PostHog, and integrate PostHog with other applications. n8n has built-in support for a wide range of PostHog features, including creating aliases, events, and identity, as well as tracking pages.\nOn this page, you'll find a list of operations the PostHog node supports and links to more resources.\nOperations\nAlias\nCreate an alias\nEvent\nCreate an event\nIdentity\nCreate\nTrack\nTrack a page\nTrack a screen\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.profitwell.md",
    "content": "ProfitWell node\nUse the ProfitWell node to automate work in ProfitWell, and integrate ProfitWell with other applications. n8n supports getting your company's account settings and retrieving financial metrics from ProfitWell.\nOn this page, you'll find a list of operations the ProfitWell node supports and links to more resources.\nOperations\nCompany\nGet your company's ProfitWell account settings\nMetric\nRetrieve financial metric broken down by day for either the current month or the last\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.pushbullet.md",
    "content": "Pushbullet node\nUse the Pushbullet node to automate work in Pushbullet, and integrate Pushbullet with other applications. n8n has built-in support for a wide range of Pushbullet features, including creating, updating, deleting, and getting a push.\nOn this page, you'll find a list of operations the Pushbullet node supports and links to more resources.\nOperations\nPush\nCreate a push\nDelete a push\nGet all pushes\nUpdate a push\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.pushcut.md",
    "content": "Pushcut node\nUse the Pushcut node to automate work in Pushcut, and integrate Pushcut with other applications. n8n supports sending notifications with Pushcut.\nOn this page, you'll find a list of operations the Pushcut node supports and links to more resources.\nOperations\nNotification\nSend a notification\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.pushover.md",
    "content": "Pushover node\nUse the Pushover node to automate work in Pushover, and integrate Pushover with other applications. n8n supports sending push notifications with Pushover.\nOn this page, you'll find a list of operations the Pushover node supports and links to more resources.\nOperations\nMessage\nPush\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.questdb.md",
    "content": "QuestDB node\nUse the QuestDB node to automate work in QuestDB, and integrate QuestDB with other applications. n8n supports executing an SQL query and inserting rows in a database with QuestDB.\nOn this page, you'll find a list of operations the QuestDB node supports and links to more resources.\nOperations\nExecutes a SQL query.\nInsert rows in database.\nTemplates and examples\nNode reference\nSpecify a column's data type\nTo specify a column's data type, append the column name with :type, where type is the data type you want for column. For example, if you want to specify the type int for the column id and type text for the column name, you can use the following snippet in the Columns field: id:int,name:text."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.quickbase.md",
    "content": "Quick Base node\nUse the Quick Base node to automate work in Quick Base, and integrate Quick Base with other applications. n8n has built-in support for a wide range of Quick Base features, including creating, updating, deleting, and getting records, as well as getting fields, and downloading files.\nOn this page, you'll find a list of operations the Quick Base node supports and links to more resources.\nOperations\nField\nGet all fields\nFile\nDelete a file\nDownload a file\nRecord\nCreate a record\nDelete a record\nGet all records\nUpdate a record\nUpsert a record\nReport\nGet a report\nRun a report\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.quickbooks.md",
    "content": "QuickBooks Online node\nUse the QuickBooks node to automate work in QuickBooks, and integrate QuickBooks with other applications. n8n has built-in support for a wide range of QuickBooks features, including creating, updating, deleting, and getting bills, customers, employees, estimates, and invoices.\nOn this page, you'll find a list of operations the QuickBooks node supports and links to more resources.\nOperations\nBill\nCreate\nDelete\nGet\nGet All\nUpdate\nCustomer\nCreate\nGet\nGet All\nUpdate\nEmployee\nCreate\nGet\nGet All\nUpdate\nEstimate\nCreate\nDelete\nGet\nGet All\nSend\nUpdate\nInvoice\nCreate\nDelete\nGet\nGet All\nSend\nUpdate\nVoid\nItem\nGet\nGet All\nPayment\nCreate\nDelete\nGet\nGet All\nSend\nUpdate\nVoid\nPurchase\nGet\nGet All\nTransaction\nGet Report\nVendor\nCreate\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.quickchart.md",
    "content": "QuickChart node\nUse the QuickChart node to automate work in QuickChart, and integrate QuickChart with other applications. n8n has built-in support for a wide range of QuickChart chart types, including bar, doughnut, line, pie, and polar charts.\nOn this page, you'll find a list of operations the QuickChart node supports and links to more resources.\nOperations\nCreate a chart by selecting the chart type:\nChart Type\nBar Chart\nDoughnut Chart\nLine Chart\nPie Chart\nPolar Chart\nTemplates and examples\nRelated resources\nRefer to QuickChart's API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.rabbitmq.md",
    "content": "RabbitMQ node\nUse the RabbitMQ node to automate work in RabbitMQ, and integrate RabbitMQ with other applications. n8n has built-in support for a wide range of RabbitMQ features, including accepting, and forwarding messages.\nOn this page, you'll find a list of operations the RabbitMQ node supports and links to more resources.\nOperations\nDelete From Queue\nSend a Message to RabbitMQ\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.raindrop.md",
    "content": "Raindrop node\nUse the Raindrop node to automate work in Raindrop, and integrate Raindrop with other applications. n8n has built-in support for a wide range of Raindrop features, including getting users, deleting tags, and creating, updating, deleting and getting collections and bookmarks.\nOn this page, you'll find a list of operations the Raindrop node supports and links to more resources.\nOperations\nBookmark\nCreate\nDelete\nGet\nGet All\nUpdate\nCollection\nCreate\nDelete\nGet\nGet All\nUpdate\nTag\nDelete\nGet All\nUser\nGet\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.reddit.md",
    "content": "Reddit node\nUse the Reddit node to automate work in Reddit, and integrate Reddit with other applications. n8n has built-in support for a wide range of Reddit features, including getting profiles, and users, retrieving post comments and subreddit, as well as submitting, getting, and deleting posts.\nOn this page, you'll find a list of operations the Reddit node supports and links to more resources.\nOperations\nPost\nSubmit a post to a subreddit\nDelete a post from a subreddit\nGet a post from a subreddit\nGet all posts from a subreddit\nSearch posts in a subreddit or in all of Reddit.\nPost Comment\nCreate a top-level comment in a post\nRetrieve all comments in a post\nRemove a comment from a post\nWrite a reply to a comment in a post\nProfile\nGet\nSubreddit\nRetrieve background information about a subreddit.\nRetrieve information about subreddits from all of Reddit.\nUser\nGet\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.redis.md",
    "content": "Redis node\nUse the Redis node to automate work in Redis, and integrate Redis with other applications. n8n has built-in support for a wide range of Redis features, including deleting keys, getting key values, setting key value, and publishing messages to the Redis channel.\nOn this page, you'll find a list of operations the Redis node supports and links to more resources.\nOperations\nDelete a key from Redis.\nGet the value of a key from Redis.\nReturns generic information about the Redis instance.\nAtomically increments a key by 1. Creates the key if it doesn't exist.\nReturns all the keys matching a pattern.\nSet the value of a key in Redis.\nPublish message to Redis channel.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.rocketchat.md",
    "content": "Rocket.Chat node\nUse the Rocket.Chat node to automate work in Rocket.Chat, and integrate Rocket.Chat with other applications. n8n supports posting messages to channels, and sending direct messages, with Rocket.Chat.\nOn this page, you'll find a list of operations the Rocket.Chat node supports and links to more resources.\nOperations\nChat\nPost a message to a channel or a direct message\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.rundeck.md",
    "content": "Rundeck node\nUse the Rundeck node to automate work in Rundeck, and integrate Rundeck with other applications. n8n has built-in support for executing jobs and getting metadata.\nOn this page, you'll find a list of operations the Rundeck node supports and links to more resources.\nOperations\nJob\nExecute a job\nGet metadata of a job\nTemplates and examples\nFind the job ID\nAccess your Rundeck dashboard.\nOpen the project that contains the job you want to use with n8n.\nIn the sidebar, select JOBS.\nUnder All Jobs, select the name of the job you want to use with n8n.\nIn the top left corner, under the name of the job, copy the string that's displayed in smaller font below the job name. This is your job ID.\nPaste this job ID in the Job Id field in n8n."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.s3.md",
    "content": "S3 node\nUse the S3 node to automate work in non-AWS S3 storage and integrate S3 with other applications. n8n has built-in support for a wide range of S3 features, including creating, deleting, and getting buckets, files, and folders. For AWS S3, use AWS S3.\nUse the S3 node for non-AWS S3 solutions like:\nMinIO\nWasabi\nDigital Ocean spaces\nOn this page, you'll find a list of operations the S3 node supports and links to more resources.\nOperations\nBucket\nCreate a bucket\nDelete a bucket\nGet all buckets\nSearch within a bucket\nFile\nCopy a file\nDelete a file\nDownload a file\nGet all files\nUpload a file\nFolder\nCreate a folder\nDelete a folder\nGet all folders\nTemplates and examples\nNode reference\nSetting file permissions in Wasabi\nWhen uploading files to Wasabi, you must set permissions for the files using the ACL dropdown and not the toggles.\n!File permissions when using the S3 node with Wasabi"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.salesforce.md",
    "content": "Salesforce node\nUse the Salesforce node to automate work in Salesforce, and integrate Salesforce with other applications. n8n has built-in support for a wide range of Salesforce features, including creating, updating, deleting, and getting accounts, attachments, cases, and leads, as well as uploading documents.\nOn this page, you'll find a list of operations the Salesforce node supports and links to more resources.\nOperations\nAccount\nAdd note to an account\nCreate an account\nCreate a new account, or update the current one if it already exists (upsert)\nGet an account\nGet all accounts\nReturns an overview of account's metadata.\nDelete an account\nUpdate an account\nAttachment\nCreate a attachment\nDelete a attachment\nGet a attachment\nGet all attachments\nReturns an overview of attachment's metadata.\nUpdate a attachment\nCase\nAdd a comment to a case\nCreate a case\nGet a case\nGet all cases\nReturns an overview of case's metadata\nDelete a case\nUpdate a case\nContact\nAdd lead to a campaign\nAdd note to a contact\nCreate a contact\nCreate a new contact, or update the current one if it already exists (upsert)\nDelete a contact\nGet a contact\nReturns an overview of contact's metadata\nGet all contacts\nUpdate a contact\nCustom Object\nCreate a custom object record\nCreate a new record, or update the current one if it already exists (upsert)\nGet a custom object record\nGet all custom object records\nDelete a custom object record\nUpdate a custom object record\nDocument\nUpload a document\nFlow\nGet all flows\nInvoke a flow\nLead\nAdd lead to a campaign\nAdd note to a lead\nCreate a lead\nCreate a new lead, or update the current one if it already exists (upsert)\nDelete a lead\nGet a lead\nGet all leads\nReturns an overview of Lead's metadata\nUpdate a lead\nOpportunity\nAdd note to an opportunity\nCreate an opportunity\nCreate a new opportunity, or update the current one if it already exists (upsert)\nDelete an opportunity\nGet an opportunity\nGet all opportunities\nReturns an overview of opportunity's metadata\nUpdate an opportunity\nSearch\nExecute a SOQL query that returns all the results in a single response\nTask\nCreate a task\nDelete a task\nGet a task\nGet all tasks\nReturns an overview of task's metadata\nUpdate a task\nUser\nGet a user\nGet all users\nTemplates and examples\nWorking with Salesforce custom fields\nTo add custom fields to your request:\nSelect Additional Fields > Add Field.\nIn the dropdown, select Custom Fields.\nYou can then find and add your custom fields."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.salesmate.md",
    "content": "Salesmate node\nUse the Salesmate node to automate work in Salesmate, and integrate Salesmate with other applications. n8n has built-in support for a wide range of Salesmate features, including creating, updating, deleting, and getting activities, companies, and deals.\nOn this page, you'll find a list of operations the Salesmate node supports and links to more resources.\nOperations\nActivity\nCreate an activity\nDelete an activity\nGet an activity\nGet all companies\nUpdate an activity\nCompany\nCreate a company\nDelete a company\nGet a company\nGet all companies\nUpdate a company\nDeal\nCreate a deal\nDelete a deal\nGet a deal\nGet all deals\nUpdate a deal\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.seatable.md",
    "content": "SeaTable node\nUse the SeaTable node to automate work in SeaTable, and integrate SeaTable with other applications. n8n has built-in support for a wide range of SeaTable features, including creating, updating, deleting, updating, and getting rows.\nOn this page, you'll find a list of operations the SeaTable node supports and links to more resources.\nOperations\nRow\nCreate\nDelete\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.securityscorecard.md",
    "content": "SecurityScorecard node\nUse the SecurityScorecard node to automate work in SecurityScorecard, and integrate SecurityScorecard with other applications. n8n has built-in support for a wide range of SecurityScorecard features, including creating, updating, deleting, and getting portfolio, as well as getting a company's data.\nOn this page, you'll find a list of operations the SecurityScorecard node supports and links to more resources.\nOperations\nCompany\nGet company factor scores and issue counts\nGet company's historical factor scores\nGet company's historical scores\nGet company information and summary of their scorecard\nGet company's score improvement plan\nIndustry\nGet Factor Scores\nGet Historical Factor Scores\nGet Score\nInvite\nCreate an invite for a company/user\nPortfolio\nCreate a portfolio\nDelete a portfolio\nGet all portfolios\nUpdate a portfolio\nPortfolio Company\nAdd a company to portfolio\nGet all companies in a portfolio\nRemove a company from portfolio\nReport\nDownload a generated report\nGenerate a report\nGet list of recently generated report\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.segment.md",
    "content": "Segment node\nUse the Segment node to automate work in Segment, and integrate Segment with other applications. n8n has built-in support for a wide range of Segment features, including adding users to groups, creating identities, and tracking activities.\nOn this page, you'll find a list of operations the Segment node supports and links to more resources.\nOperations\nGroup\nAdd a user to a group\nIdentify\nCreate an identity\nTrack\nRecord the actions your users perform. Every action triggers an event, which can also have associated properties.\nRecord page views on your website, along with optional extra information about the page being viewed.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.sendgrid.md",
    "content": "SendGrid node\nUse the SendGrid node to automate work in SendGrid, and integrate SendGrid with other applications. n8n has built-in support for a wide range of SendGrid features, including creating, updating, deleting, and getting contacts, and lists, as well as sending emails.\nOn this page, you'll find a list of operations the SendGrid node supports and links to more resources.\nOperations\nContact\nCreate/update a contact\nDelete a contact\nGet a contact by ID\nGet all contacts\nList\nCreate a list\nDelete a list\nGet a list\nGet all lists\nUpdate a list\nMail\nSend an email.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.sendy.md",
    "content": "Sendy node\nUse the Sendy node to automate work in Sendy, and integrate Sendy with other applications. n8n has built-in support for a wide range of Sendy features, including creating campaigns, and adding, counting, deleting, and getting subscribers.\nOn this page, you'll find a list of operations the Sendy node supports and links to more resources.\nOperations\nCampaign\nCreate a campaign\nSubscriber\nAdd a subscriber to a list\nCount subscribers\nDelete a subscriber from a list\nUnsubscribe user from a list\nGet the status of subscriber\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.sentryio.md",
    "content": "Sentry.io node\nUse the Sentry.io node to automate work in Sentry.io, and integrate Sentry.io with other applications. n8n has built-in support for a wide range of Sentry.io features, including creating, updating, deleting, and getting, issues, projects, and releases, as well as getting all events.\nOn this page, you'll find a list of operations the Sentry.io node supports and links to more resources.\nOperations\nEvent\nGet event by ID\nGet all events\nIssue\nDelete an issue\nGet issue by ID\nGet all issues\nUpdate an issue\nProject\nCreate a new project\nDelete a project\nGet project by ID\nGet all projects\nUpdate a project\nRelease\nCreate a release\nDelete a release\nGet release by version identifier\nGet all releases\nUpdate a release\nOrganization\nCreate an organization\nGet organization by slug\nGet all organizations\nUpdate an organization\nTeam\nCreate a new team\nDelete a team\nGet team by slug\nGet all teams\nUpdate a team\nTemplates and examples\nRelated resources\nRefer to Sentry.io's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.servicenow.md",
    "content": "ServiceNow node\nUse the ServiceNow node to automate work in ServiceNow, and integrate ServiceNow with other applications. n8n has built-in support for a wide range of ServiceNow features, including getting business services, departments, configuration items, and dictionary as well as creating, updating, and deleting incidents, users, and table records.\nOn this page, you'll find a list of operations the ServiceNow node supports and links to more resources.\nOperations\nBusiness Service\nGet All\nConfiguration Items\nGet All\nDepartment\nGet All\nDictionary\nGet All\nIncident\nCreate\nDelete\nGet\nGet All\nUpdate\nTable Record\nCreate\nDelete\nGet\nGet All\nUpdate\nUser\nCreate\nDelete\nGet\nGet All\nUpdate\nUser Group\nGet All\nUser Role\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.shopify.md",
    "content": "Shopify node\nUse the Shopify node to automate work in Shopify, and integrate Shopify with other applications. n8n has built-in support for a wide range of Shopify features, including creating, updating, deleting, and getting orders and products.\nOn this page, you'll find a list of operations the Shopify node supports and links to more resources.\nOperations\nOrder\nCreate an order\nDelete an order\nGet an order\nGet all orders\nUpdate an order\nProduct\nCreate a product\nDelete a product\nGet a product\nGet all products\nUpdate a product\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.signl4.md",
    "content": "SIGNL4 node\nUse the SIGNL4 node to automate work in SIGNL4, and integrate SIGNL4 with other applications. n8n supports sending and resolving alerts with SIGNL4.\nOn this page, you'll find a list of operations the SIGNL4 node supports and links to more resources.\nOperations\nAlert\nSend an alert\nResolve an alert\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.slack.md",
    "content": "Slack node\nUse the Slack node to automate work in Slack, and integrate Slack with other applications. n8n has built-in support for a wide range of Slack features, including creating, archiving, and closing channels, getting users and files, as well as deleting messages.\nOn this page, you'll find a list of operations the Slack node supports and links to more resources.\nOperations\nChannel\nArchive a channel.\nClose a direct message or multi-person direct message.\nCreate a public or private channel-based conversation.\nGet information about a channel.\nGet Many: Get a list of channels in Slack.\nHistory: Get a channel's history of messages and events.\nInvite a user to a channel.\nJoin an existing channel.\nKick: Remove a user from a channel.\nLeave a channel.\nMember: List the members of a channel.\nOpen or resume a direct message or multi-person direct message.\nRename a channel.\nReplies: Get a thread of messages posted to a channel.\nSets purpose of a channel.\nSets topic of a channel.\nUnarchive a channel.\nFile\nGet a file.\nGet Many: Get and filter team files.\nUpload: Create or upload an existing file.\nMessage\nDelete a message\nGet permalink: Get a message's permalink.\nSearch for messages\nSend a message\nSend and Wait for Approval: Send a message and wait for approval from the recipient before continuing.\nUpdate a message\nReaction\nAdd a reaction to a message.\nGet a message's reactions.\nRemove a reaction from a message.\nStar\nAdd a star to an item.\nDelete a star from an item.\nGet Many: Get a list of an authenticated user's stars.\nUser\nGet information about a user.\nGet Many: Get a list of users.\nGet User's Profile.\nGet User's Status.\nUpdate User's Profile.\nUser Group\nCreate a user group.\nDisable a user group.\nEnable a user group.\nGet Many: Get a list of user groups.\nUpdate a user group.\nTemplates and examples\nRelated resources\nRefer to Slack's documentation for more information about the service.\nRequired scopes\nOnce you create a Slack app for your Slack credentials, you must add the appropriate scopes to your Slack app for this node to work. Start with the scopes listed in the Scopes TABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.sms77.md",
    "content": "seven node\nUse the seven node to automate work in seven, and integrate seven with other applications. n8n has built-in support for a wide range of seven features, including sending SMS, and converting text to voice.\nOn this page, you'll find a list of operations the seven node supports and links to more resources.\nOperations\nSMS\nSend SMS\nVoice Call\nConverts text to voice and calls a given number\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.snowflake.md",
    "content": "Snowflake node\nUse the Snowflake node to automate work in Snowflake, and integrate Snowflake with other applications. n8n has built-in support for a wide range of Snowflake features, including executing SQL queries, and inserting rows in a database.\nOn this page, you'll find a list of operations the Snowflake node supports and links to more resources.\nOperations\nExecute an SQL query.\nInsert rows in database.\nUpdate rows in database.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.splunk.md",
    "content": "Splunk node\nUse the Splunk node to automate work in Splunk, and integrate Splunk with other applications. n8n has built-in support for a wide range of Splunk features, including getting fired alerts reports, as well as deleting and getting search configuration.\nOn this page, you'll find a list of operations the Splunk node supports and links to more resources.\nOperations\nFired Alert\nGet a fired alerts report\nSearch Configuration\nDelete a search configuration\nGet a search configuration\nGet many search configurations\nSearch Job\nCreate a search job\nDelete a search job\nGet a search job\nGet many search jobs\nSearch Result\nGet many search results\nUser\nCreate a user\nDelete a user\nGet a user\nGet many users\nUpdate a user\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.spontit.md",
    "content": "Spontit node\nUse the Spontit node to automate work in Spontit, and integrate Spontit with other applications. n8n supports creating push notifications with Spontit.\nOn this page, you'll find a list of operations the Spontit node supports and links to more resources.\nOperations\nPush\nCreate a push notification\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.spotify.md",
    "content": "Spotify node\nUse the Spotify node to automate work in Spotify, and integrate Spotify with other applications. n8n has built-in support for a wide range of Spotify features, including getting album and artist information.\nOn this page, you'll find a list of operations the Spotify node supports and links to more resources.\nOperations\nAlbum\nGet an album by URI or ID.\nGet a list of new album releases.\nGet an album's tracks by URI or ID.\nSearch albums by keyword.\nArtist\nGet an artist by URI or ID.\nGet an artist's albums by URI or ID.\nGet an artist's related artists by URI or ID.\nGet an artist's top tracks by URI or ID.\nSearch artists by keyword.\nLibrary\nGet the user's liked tracks.\nMy Data\nGet your followed artists.\nPlayer\nAdd a song to your queue.\nGet your currently playing track.\nSkip to your next track.\nPause your music.\nSkip to your previous song.\nGet your recently played tracks.\nResume playback on the current active device.\nSet volume on the current active device.\nStart playing a playlist, artist, or album.\nPlaylist\nAdd tracks from a playlist by track and playlist URI or ID.\nCreate a new playlist.\nGet a playlist by URI or ID.\nGet a playlist's tracks by URI or ID.\nGet a user's playlists.\nRemove tracks from a playlist by track and playlist URI or ID.\nSearch playlists by keyword.\nTrack\nGet a track by its URI or ID.\nGet audio features for a track by URI or ID.\nSearch tracks by keyword\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.stackby.md",
    "content": "Stackby node\nUse the Stackby node to automate work in Stackby, and integrate Stackby with other applications. n8n has built-in support for a wide range of Stackby features, including appending, deleting, listing and reading.\nOn this page, you'll find a list of operations the Stackby node supports and links to more resources.\nOperations\nAppend\nDelete\nList\nRead\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.storyblok.md",
    "content": "Storyblok node\nUse the Storyblok node to automate work in Storyblok, and integrate Storyblok with other applications. n8n has built-in support for a wide range of Storyblok features, including getting, deleting, and publishing stories.\nOn this page, you'll find a list of operations the Storyblok node supports and links to more resources.\nOperations\nContent API\nStory\nGet a story\nGet all stories\nManagement API\nStory\nDelete a story\nGet a story\nGet all stories\nPublish a story\nUnpublish a story\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.strapi.md",
    "content": "Strapi node\nUse the Strapi node to automate work in Strapi, and integrate Strapi with other applications. n8n has built-in support for a wide range of Strapi features, including creating and deleting entries.\nOn this page, you'll find a list of operations the Strapi node supports and links to more resources.\nOperations\nEntry\nCreate\nDelete\nGet\nGet Many\nUpdate\nTemplates and examples\nRelated resources\nRefer to Strapi's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.strava.md",
    "content": "Strava node\nUse the Strava node to automate work in Strava, and integrate Strava with other applications. n8n has built-in support for a wide range of Strava features, including creating new activities, and getting activity information.\nOn this page, you'll find a list of operations the Strava node supports and links to more resources.\nOperations\nActivity\nCreate a new activity\nGet an activity\nGet all activities\nGet all activity comments\nGet all activity kudos\nGet all activity laps\nGet all activity zones\nUpdate an activity\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.stripe.md",
    "content": "Stripe node\nUse the Stripe node to automate work in Stripe, and integrate Stripe with other applications. n8n has built-in support for a wide range of Stripe features, including getting balance, creating charge, and deleting customers.\nOn this page, you'll find a list of operations the Stripe node supports and links to more resources.\nOperations\nBalance\nGet a balance\nCharge\nCreate a charge\nGet a charge\nGet all charges\nUpdate a charge\nCoupon\nCreate a coupon\nGet all coupons\nCustomer\nCreate a customer\nDelete a customer\nGet a customer\nGet all customers\nUpdate a customer\nCustomer Card\nAdd a customer card\nGet a customer card\nRemove a customer card\nSource\nCreate a source\nDelete a source\nGet a source\nToken\nCreate a token\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.syncromsp.md",
    "content": "SyncroMSP node\nUse the SyncroMSP node to automate work in SyncroMSP, and integrate SyncroMSP with other applications. n8n has built-in support for a wide range of SyncroMSP features, including creating and deleting new customers, tickets, and contacts.\nOn this page, you'll find a list of operations the SyncroMSP node supports and links to more resources.\nOperations\nContact\nCreate new contact\nDelete contact\nRetrieve contact\nRetrieve all contacts\nUpdate contact\nCustomer\nCreate new customer\nDelete customer\nRetrieve customer\nRetrieve all customers\nUpdate customer\nRMM\nCreate new RMM Alert\nDelete RMM Alert\nRetrieve RMM Alert\nRetrieve all RMM Alerts\nMute RMM Alert\nTicket\nCreate new ticket\nDelete ticket\nRetrieve ticket\nRetrieve all tickets\nUpdate ticket\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.taiga.md",
    "content": "Taiga node\nUse the Taiga node to automate work in Taiga, and integrate Taiga with other applications. n8n has built-in support for a wide range of Taiga features, including creating, updating, deleting, and getting issues.\nOn this page, you'll find a list of operations the Taiga node supports and links to more resources.\nOperations\nIssue\nCreate an issue\nDelete an issue\nGet an issue\nGet all issues\nUpdate an issue\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.tapfiliate.md",
    "content": "Tapfiliate node\nUse the Tapfiliate node to automate work in Tapfiliate, and integrate Tapfiliate with other applications. n8n has built-in support for a wide range of Tapfiliate features, including creating and deleting affiliates, and adding affiliate metadata.\nOn this page, you'll find a list of operations the Tapfiliate node supports and links to more resources.\nOperations\nAffiliate\nCreate an affiliate\nDelete an affiliate\nGet an affiliate by ID\nGet all affiliates\nAffiliate Metadata\nAdd metadata to affiliate\nRemove metadata from affiliate\nUpdate affiliate's metadata\nProgram Affiliate\nAdd affiliate to program\nApprove an affiliate for a program\nDisapprove an affiliate\nGet an affiliate in a program\nGet all affiliates in program\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.thehive.md",
    "content": "TheHive node\nUse the TheHive node to automate work in TheHive, and integrate TheHive with other applications. n8n has built-in support for a wide range of TheHive features, including creating alerts, counting tasks logs, cases, and observables.\nOn this page, you'll find a list of operations the TheHive node supports and links to more resources.\nOperations\nThe available operations depend on your API version. To see the operations list, create your credentials, including selecting your API version. Then return to the node, select the resource you want to use, and n8n displays the available operations for your API version.\nAlert\nCase\nLog\nObservable\nTask\nTemplates and examples\nRelated resources\nn8n provides a trigger node for TheHive. You can find the trigger node docs here.\nRefer to TheHive's documentation for more information about the service:\nVersion 3\nVersion 4"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.thehive5.md",
    "content": "TheHive 5 node\nUse the TheHive 5 node to automate work in TheHive, and integrate TheHive with other applications. n8n has built-in support for a wide range of TheHive features, including creating alerts, counting tasks logs, cases, and observables.\nOn this page, you'll find a list of operations the TheHive node supports and links to more resources.\nOperations\nAlert\nCreate\nDelete\nExecute Responder\nGet\nMerge Into Case\nPromote to Case\nSearch\nUpdate\nUpdate Status\nCase\nAdd Attachment\nCreate\nDelete Attachment\nDelete Case\nExecute Responder\nGet\nGet Attachment\nGet Timeline\nSearch\nUpdate\nComment\nCreate\nDelete\nSearch\nUpdate\nObservable\nCreate\nDelete\nExecute Analyzer\nExecute Responder\nGet\nSearch\nUpdate\nPage\nCreate\nDelete\nSearch\nUpdate\nQuery\nExecute Query\nTask\nCreate\nDelete\nExecute Responder\nGet\nSearch\nUpdate\nTask Log\nAdd Attachment\nCreate\nDelete\nDelete Attachment\nExecute Responder\nGet\nSearch\nTemplates and examples\nRelated resources\nn8n provides a trigger node for TheHive. You can find the trigger node docs here.\nRefer to TheHive's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.timescaledb.md",
    "content": "TimescaleDB node\nUse the TimescaleDB node to automate work in TimescaleDB, and integrate TimescaleDB with other applications. n8n has built-in support for a wide range of TimescaleDB features, including executing an SQL query, as well as inserting and updating rows in a database.\nOn this page, you'll find a list of operations the TimescaleDB node supports and links to more resources.\nOperations\nExecute an SQL query\nInsert rows in database\nUpdate rows in database\nTemplates and examples\nSpecify a column's data type\nTo specify a column's data type, append the column name with :type, where type is the data type you want for the column. For example, if you want to specify the type int for the column id and type text for the column name, you can use the following snippet in the Columns field: id:int,name:text."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.todoist.md",
    "content": "Todoist node\nUse the Todoist node to automate work in Todoist, and integrate Todoist with other applications. n8n has built-in support for a wide range of Todoist features, including creating, updating, deleting, and getting tasks.\nOn this page, you'll find a list of operations the Todoist node supports and links to more resources.\nOperations\nTask\nCreate a new task\nClose a task\nDelete a task\nGet a task\nGet all tasks\nReopen a task\nUpdate a task\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.travisci.md",
    "content": "Travis CI node\nUse the Travis CI node to automate work in Travis CI, and integrate Travis CI with other applications. n8n has built-in support for a wide range of Travis CI features, including cancelling and getting builds.\nOn this page, you'll find a list of operations the Travis CI node supports and links to more resources.\nOperations\nBuild\nCancel a build\nGet a build\nGet all builds\nRestart a build\nTrigger a build\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.trello.md",
    "content": "Trello node\nUse the Trello node to automate work in Trello, and integrate Trello with other applications. n8n has built-in support for a wide range of Trello features, including creating and updating cards, and adding and removing members.\nOn this page, you'll find a list of operations the Trello node supports and links to more resources.\nOperations\nAttachment\nCreate a new attachment for a card\nDelete an attachment\nGet the data of an attachment\nReturns all attachments for the card\nBoard\nCreate a new board\nDelete a board\nGet the data of a board\nUpdate a board\nBoard Member\nAdd\nGet All\nInvite\nRemove\nCard\nCreate a new card\nDelete a card\nGet the data of a card\nUpdate a card\nCard Comment\nCreate a comment on a card\nDelete a comment from a card\nUpdate a comment on a card\nChecklist\nCreate a checklist item\nCreate a new checklist\nDelete a checklist\nDelete a checklist item\nGet the data of a checklist\nReturns all checklists for the card\nGet a specific checklist on a card\nGet the completed checklist items on a card\nUpdate an item in a checklist on a card\nLabel\nAdd a label to a card.\nCreate a new label\nDelete a label\nGet the data of a label\nReturns all labels for the board\nRemove a label from a card.\nUpdate a label.\nList\nArchive/Unarchive a list\nCreate a new list\nGet the data of a list\nGet all the lists\nGet all the cards in a list\nUpdate a list\nTemplates and examples\nFind the List ID\nOpen the Trello board that contains the list.\nIf the list doesn't have any cards, add a card to the list.\nOpen the card, add .json at the end of the URL, and press enter.\nIn the JSON file, you will see a field called idList.\nCopy the contents of the idListfield and paste it in the *List ID field in n8n."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.twake.md",
    "content": "Twake node\nUse the Twake node to automate work in Twake, and integrate Twake with other applications. n8n supports sending messages with Twake.\nOn this page, you'll find a list of operations the Twake node supports and links to more resources.\nOperations\nMessage\nSend a message\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.twilio.md",
    "content": "Twilio node\nUse the Twilio node to automate work in Twilio, and integrate Twilio with other applications. n8n supports sending MMS/SMS and WhatsApp messages with Twilio.\nOn this page, you'll find a list of operations the Twilio node supports and links to more resources.\nOperations\nSMS\nSend SMS/MMS/WhatsApp message\nCall\nMake a phone call using text-to-speech to say a message\nTemplates and examples\nRelated resources\nRefer to Twilio's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.twist.md",
    "content": "Twist node\nUse the Twist node to automate work in Twist, and integrate Twist with other applications. n8n has built-in support for a wide range of Twist features, including creating conversations in a channel, as well as creating and deleting comments on a thread.\nOn this page, you'll find a list of operations the Twist node supports and links to more resources.\nOperations\nChannel\nArchive a channel\nInitiates a public or private channel-based conversation\nDelete a channel\nGet information about a channel\nGet all channels\nUnarchive a channel\nUpdate a channel\nComment\nCreate a new comment to a thread\nDelete a comment\nGet information about a comment\nGet all comments\nUpdate a comment\nMessage Conversation\nCreate a message in a conversation\nDelete a message in a conversation\nGet a message in a conversation\nGet all messages in a conversation\nUpdate a message in a conversation\nThread\nCreate a new thread in a channel\nDelete a thread\nGet information about a thread\nGet all threads\nUpdate a thread\nTemplates and examples\nGet the User ID\nTo get the User ID for a user:\nOpen the Team tab.\nSelect a user's avatar.\nCopy the string of characters located after /u/ in your Twist URL. This string is the User ID. For example, if the URL is  the User ID is 475370."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.twitter.md",
    "content": "X (Formerly Twitter) node\nUse the X node to automate work in X and integrate X with other applications. n8n has built-in support for a wide range of X features, including creating direct messages and deleting, searching, liking, and retweeting a tweet.\nOn this page, you'll find a list of operations the X node supports and links to more resources.\nOperations\nDirect Message\nCreate a direct message\nTweet\nCreate or reply a tweet\nDelete a tweet\nSearch tweets\nLike a tweet\nRetweet a tweet\nUser\nGet a user\nList\nAdd a member to a list\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.unleashedsoftware.md",
    "content": "Unleashed Software node\nUse the Unleashed Software node to automate work in Unleashed Software, and integrate Unleashed Software with other applications. n8n has built-in support for a wide range of Unleashed Software features, including getting sales orders and stock on hand.\nOn this page, you'll find a list of operations the Unleashed Software node supports and links to more resources.\nOperations\nSales Order\nGet all sales orders\nStock On Hand\nGet a stock on hand\nGet all stocks on hand\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.uplead.md",
    "content": "UpLead node\nUse the UpLead node to automate work in UpLead, and integrate UpLead with other applications. n8n supports several UpLead operations, including getting company information.\nOn this page, you'll find a list of operations the UpLead node supports and links to more resources.\nOperations\nCompany\nEnrich\nPerson\nEnrich\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.uproc.md",
    "content": "uProc node\nUse the uProc node to automate work in uProc, and integrate uProc with other applications. n8n has built-in support for a wide range of uProc features, including getting advanced human audio file, communication data, company, finance and product information.\nOn this page, you'll find a list of operations the uProc node supports and links to more resources.\nOperations\nAudio\nGet advanced human audio file by provided text and language\nGet an audio file by provided text and language\nCommunication\nDiscover if a domain has a social network presence\nDiscover if an email is valid, hard bounce, soft bounce, spam-trap, free, temporary, and recipient exists\nDiscover if the email recipient exists, returning email status\nCheck if an email domain has an SMTP server to receive emails\nDiscover if the email has a social network presence\nCheck if an email has a valid format\nCheck if an email domain belongs to a disposable email service\nCheck if email belongs to free service provider like Gmail\nCheck if email is catchall\nDiscover if an email exists in the Robinson list (only Spain)\nCheck if email belongs to a system or role-based account\nCheck if an email is a spam trap\nDiscover if an IMEI number has a valid format\nCheck if a LinkedIn profile is a first-degree contact\nDiscover if mobile phone number exists in network operator, with worldwide coverage\nDiscover if a mobile phone number has a valid format with worldwide coverage\nDiscover if a mobile phone number has a valid format (only Spain)\nDiscover if a mobile phone number has a valid prefix, with worldwide coverage\nDiscover if a Spanish mobile phone number has a valid prefix\nDiscover if a mobile number is switched on to call it later, with worldwide coverage\nDiscover if a mobile number can receive SMS with worldwide coverage\nDiscover if a phone (landline or mobile) exists in a Robinson list (only Spain)\nDiscover if a landline or mobile number has a valid prefix\nDiscover if a landline phone number is valid, with Spain coverage\nAllows discovering if landline number has a good international format, depending on the country\nDiscover if a landline phone number prefix exists, with worldwide coverage\nClean a phone removing non allowed characters\nAllows getting country code of a mobile phone number with international format\nAllows getting a domain from an email\nDiscover an email by company website or domain and prospect's first-name and last-name\nCheck if an email is personal or generic\nGet emails list found on the internet by domain or URI\nGet an emails list found on the internet by non-free email\nGet emails list found inside the website by domain or URI\nGet three first web references of an email published on the internet\nAllows you to fix the email domain of those misspelled emails\nFix the international prefix of a phone based on the ISO code of a country\nGet GDPR compliant emails list by domain for your Email Marketing campaigns in Europe\nDiscover if mobile exist using real-time HLR query\nGet personal email by social network profile\nGet portability data about a landline or mobile number, only for Spain\nExtract results from a LinkedIn search (employees in a company)\nGet members in a LinkedIn group\nGet 'Search LinkedIn Contacts' URL\nExtract the last 80 connections from your LinkedIn profile\nExtract the last 80 invitations sent from your LinkedIn\nGet users who comment on a post on LinkedIn\nGet users who like a post on LinkedIn\nExtract a LinkedIn profile\nExtract results from a LinkedIn search (profiles)\nExtract last profiles that have published content on LinkedIn by specific keywords\nDiscover if mobile exist using real-time HLR query, as well as portability and roaming data\nGet existence, portability, and roaming of a mobile phone using MNP query\nDiscover if mobile or landline prefix exists in Spain\nAllows normalizing email address, removing non allowed characters\nAllows normalizing a mobile phone, removing non-allowed characters\nParse phone number in multiple fields and verify format and prefix validity\nAllows getting country prefix number by country code\nDiscover an email by company website or domain and prospect's first-name and last-name\nThis tool parses a social URI address and extracts any available indicators\nSearch all social networks by domain, parses all found URLs, and returns social networks data\nDiscover if a domain or a website has social activity and returns all social network profiles found\nDiscover if an email has social activity, and get all social network profiles found\nDiscover if a mobile phone has social activity, and get all social network profiles found\nGet web references for an email published on the internet\nSend a custom message invitation to a non connected LinkedIn profile\nSend a custom email to a recipient\nSend a custom SMS to a recipient with worldwide coverage\nSend a custom invitation message if a profile is connected or a custom message otherwise\nVisits a profile to show interest and get profile views in return from contact, increasing your LinkedIn network\nSend a custom private message to a connected LinkedIn profile\nGet an email by contact's LinkedIn profile URI\nDiscover an email by company's name and prospect's full name\nDiscover an email by company's website or domain and prospect's full name\nGet email by first name, last name, and company\nGet parsed and validated phone\nCompany\nDiscover if a CIF card number is valid\nCheck if a company is a debtor by TaxID\nCheck if the ISIN number is valid\nCheck if the SS number is valid, only for Spain\nIdentify and classify a prospecting role in detecting the right area and seniority to filter later\nGet a company's contact, social, and technology data by domain\nGet a company's contact, social, and technology data by email\nGet a company's data by CIF\nGet a company's data by DUNS\nGet a company's data by domain\nGet a company's data by email\nGet a company's data by IP address\nGet a company's data by name\nGet a company's data by phone number\nGet a company's data by social networks URI (LinkedIn, Twitter)\nGet a company's name by company domain\nGet professional data of a decision-maker by company name/domain and area\nDiscover more suitable decision-maker using search engines (Bing) by company name and area\nGet professional emails of decision-makers by company domain and area\nDiscover up to ten decision-makers using search engines (Bing) by company name and area\nGet a company's domain by company name\nGet employees by company name or domain, area, seniority, and country\nGet a company's Facebook profile by name without manually searching on Google or Facebook\nGet geocoded company data by IP address\nGet a company's LinkedIn profile by name without manually searching on Google or LinkedIn\nAllows normalizing a CIF number, removing non-allowed characters\nGet a company's phone by company domain\nGet a company's sales data by a company's DUNS number\nGet a company's sales data by a company's domain name\nGet a company's sales data by a company's name\nGet a company's sales data by a company's tax ID (CIF)\nGet a company's Twitter profile by name without manually searching on Google or Twitter\nGet decision maker by search engine\nGet decision makers by search engine\nGet Facebook URI by company's domain\nGet GitHub URI by company's domain\nGet Instagram URI by company's domain\nGet LinkedIn URI by company's domain\nGet Pinterest URI by company's domain\nGet Twitter URI by company's domain\nGet YouTube URI by company's domain\nFinance\nCheck if crypto wallet is valid\nDiscover if a BIC number has a valid format\nDiscover if an account number has a valid format\nCheck if credit card number checksum is valid\nDiscover if an IBAN account number has a valid format\nDiscover if an ISO currency code is valid\nCheck if a TIN exists in Europe\nConvert amount between supported currencies and an exchange date\nGet credit card type\nGet multiple ISO currency codes by a country name\nGet all ISO currency by an IP address\nGet multiple ISO currency codes by a country ISO code\nGet ISO currency code by IP address\nGet ISO currency code by a currency ISO code\nGet ISO currency code by an ISO country code\nGet ISO currency code by a country name\nGet related European TIN in Europe\nGet IBAN by account number of the country\nGet to search data bank information by IBAN account number\nGet country VAT by address\nGet country VAT by coordinates\nGet Swift code lookup\nGet VAT by IP address\nGet VAT value by country ISO code\nGet VAT by phone number, with worldwide coverage\nGet VAT by zip code\nGeographical\nCheck if a country's ISO code exists\nDiscover if the distance between two coordinates is equal to another\nDiscover if the distance (kilometers) between two coordinates is greater than the given input\nDiscover if the distance (kilometers) between two coordinates is greater or equal to the given input\nDiscover if the distance(kilometers) between two coordinates is lower than the given input\nCheck if an address exists by a partial address search\nCheck if a house number exists by a partial address search\nCheck if coordinates have a valid format\nDiscover if a zip code number prefix exists (only for Spain)\nDiscover if a zip code number has a valid format (only for Spain)\nGet cartesian coordinates(X, Y, Z/WGS84) by Latitude and Longitude\nGet location by parameters\nGet multiple cities by phone prefix (only for Spain)\nGet multiple cities by partial initial text (only for Spain)\nGet multiple cities by zip code prefix (only for Spain)\nGet a city from IP\nCity search by partial name (only for Spain)\nDiscover the city name by a local phone number (only for Spain)\nDiscover the city name by the zip code (only for Spain)\nDiscover the community name from a zip code (only for Spain)\nDiscover latitude and longitude coordinates of an IP address\nDiscover latitude and longitude coordinates of a postal address\nGet multiple country names by currency ISO code\nGet multiple countries by ISO code\nGet multiple country names by initial name\nGet country name by currency ISO code\nGet country name by IP address\nGet country name by its ISO code\nGet country by a prefix\nGet country name by phone number, with worldwide coverage\nGet Aplha2 code by a country prefix or a name\nGet decimal coordinates (degrees, minutes, and seconds) by latitude and longitude\nReturns straight-line distance (kilometers) between two addresses\nReturns straight-line distance (kilometers) between two GPS coordinates (latitude and longitude)\nReturns straight-line distance (kilometers) between two IP addresses\nReturns straight-line distance (kilometers) between two landline phones, using city and province of every phone\nReturns straight-line distance (kilometers) between two zip codes, using city and province of every zip code\nGet an exact address by a partial address search\nDiscover geographical, company, timezone, and reputation data by IPv4 address\nDiscover the city name, zip code, province, country, latitude, and longitude from an IPv4 or IPv6 address and geocodes it\nParse postal address into separated fields, getting an improved resolution\nDiscover locale data (currency, language) by IPv4 or IPv6 address\nDiscover the city name, zip code, province, or country by latitude and longitude\nDiscover the city name, zip code, province, country, latitude, and longitude from an IPv4 or IPv6 address\nDiscover the city and the province from a landline phone number (only Spain)\nDiscover location data by name\nDiscover the city and the province from a zip code number (only Spain)\nGet the most relevant locations by name\nGet the most relevant locations by name, category, location, and radius\nGet multiple personal names by a prefix\nDiscover network data by IPv4 or IPv6 address\nAllow normalizing an address by removing non allowed characters\nAllow normalizing a city by removing non allowed characters\nAllow normalizing a country by removing non allowed characters\nAllow normalizing a province by removing non allowed characters\nAllow normalizing a zip code by removing non allowed characters\nGet normalized country\nParse postal address into separated fields, getting a basic resolution\nDiscover the province name from an IP address\nGet the first province by a name prefix (only for Spain)\nDiscover the province name from a landline phone number (only for Spain)\nDiscover the province name from a zip code number (only for Spain)\nGet a province list by a name prefix (only for Spain)\nGet a province list by a phone prefix (only for Spain)\nGet a province list by a zip code prefix (only for Spain)\nDiscover reputation by IPv4 or IPv6 address\nReturns driving routing time, distance, fuel consumption, and cost between two addresses\nReturns driving routing time, distance, fuel consumption, and cost between two GPS coordinates\nReturns driving routing time, distance, fuel consumption, and cost between two  IP addresses\nReturns driving routing time, distance, fuel consumption, and cost between two landline phones, using city and province of every phone (only for Spain)\nReturns driving routing time, distance, fuel consumption, and cost between two zip codes, using city and province of every zip code\nDiscover date-time data by IPv4 or IPv6 address\nGet USNG coordinates by latitude and longitude\nGet UTM coordinates by latitude and longitude\nDiscover the zip code if you have an IP address\nGet the first zip code by prefix, only for Spain\nGet multiple zip codes by prefix, with worldwide coverage\nGet time data by coordinates\nGet time data by postal address\nImage\nGet QR code decoded content by an image URL\nIt allows discovering all geographical and technical EXIF metadata present in a photographic JPEG image\nGet an encoded barcode by number and a required standard\nGet QR code encoded by a text\nGenerate a new image by URL and text\nDiscover logo (favicon) used in a domain\nGenerate a screenshot by URL provided using Chrome browser\nGet OCR text from image\nInternet\nCheck if a domain exists\nCheck if a domain has a DNS record\nCheck if a  domain has the given IP address assigned\nCheck if a domain has an MX record\nCheck if a domain has a valid SSL certificate\nCheck if a domain has a valid format\nCheck if a domain accepts all emails, existing or not\nCheck if a domain is a free service domain provider\nCheck if a domain is temporary or not\nDiscover if a computer is switched on\nDiscover if service in a port is available\nCheck if an URL contains a string or regular expression\nCheck if an URL exists\nCheck that an URL has a valid format\nGet full SSL certificate data by a domain (or website) and monitor your certificate status\nGet feed entries by domain\nGet last feed entry by domain\nGet text data from web, PDF or image allowing to filter some elements by regular expressions or field names\nDecode URL to recover original\nGet valid, existing, and default URL when accessing a domain using a web browser\nGet long version of shortened URL\nDiscover device features by a user agent\nGet the network name of and IP address\nGet the domain record by its type\nEncode URL to avoid problems\nCopy file from one URL to another URL\nFix an IP address to the right format\nGet the IPv4 address linked with a domain\nConvert a number to an IP address\nGet ISP known name of email domain name\nConvert an IP address to numeric notation\nScan a host and returns the most commonly open ports\nObtains a list with multiple results from a website\nObtains the content of a website\nDecode URL into multiple fields\nGenerate a PDF file by URL (provided using Chrome browser)\nGet the root domain of any web address, removing non needed characters\nGenerates shareable URIs to use on social networks and email using a content URI and a text\nGet data from the existing table in an HTML page or a PDF file\nDiscover client and server technologies used in a domain\nDiscover client and server technologies used in web pages\nAnalyze URL's health status about SSL, broken links, conflictive HTTP links with SSL, and more\nGet website visits and rank of any domain\nGet a domain's WHOIS data by fields\nGet WHOIS data fields by IP address provided\nPersonal\nCheck if age is between two numbers\nCheck if date returns an age between 20 and 29\nCheck if date returns an age between 40 and 49\nCheck if age is greater than another\nCheck if birth date returns an age greater than 64\nCheck if birth date belongs to an adult (18 years for Spain)\nCheck if age is lower than another\nCheck if age is lower or equal than another\nCheck if ages are equal\nDiscover if a date is between two dates\nDiscover if a date is greater\nDiscover if a date is greater or equal\nDiscover if a date belongs to a leap year\nDiscover if a date is lower\nDiscover if a date is lower or equal\nDiscover if a date has a valid format\nDiscover if a gender value is valid\nDiscover if an NIE card number is valid\nDiscover if a NIF card number is valid\nCheck if a personal name exists in the INE data source (only for Spain)\nCheck if a name contains accepted characters\nDiscover if a NIF exists in the Robinson list (only for Spain)\nCheck if surname contains accepted characters\nCheck if a personal surname appears in INE data source (only for Spain)\nDiscover if a DNI card number is valid\nDiscover the age of a birth date\nDiscover the age range of a person by birth date\nGet the difference between two dates\nDiscover the gender of a person by the email\nDiscover the gender of a person or company by the name\nGet LinkedIn employee profile URI by business email\nGet LinkedIn employee profile URI by first name, last name, and company\nDiscover the letter of a DNI card number\nGet first personal name matching by prefix and gender from INE data source (only for Spain)\nGet LinkedIn URI by email\nGet LinkedIn URI by phone\nAllow normalizing a DNI number by removing non allowed characters\nAllow normalizing an NIE number by removing non allowed characters\nNormalize name by removing non allowed characters\nNormalize surname\nGet parsed date-time\nNormalize full name, fixing abbreviations, sorting if necessary, and returning first name, last name, and gender\nGet prospect's contact data and the company's location and social data by email\nGet contact, location, and social data by email and company name and location\nGet personal and social data by social profile\nGet personal data by email\nGet personal data by first name, last name, company, and location\nGet personal data by mobile\nGet personal data by social network profile\nGenerate random fake data\nGet first personal surname matching by prefix from INE data source (only for Spain)\nGet personal surname matching by prefix from INE data source (only for Spain)\nGet Twitter profile by first name, last name, and company\nGet XING profile by first name, last name, and company\nAdd a contact email to a person list\nProduct\nCheck if an ASIN code exists on the Amazon Marketplace\nCheck if an ASIN code has a valid format\nCheck if an EAN code exists on Amazon Marketplace\nCheck if an EAN barcode has a valid format\nCheck if an EAN barcode of 13 digits has a valid format\nCheck if an EAN barcode of 14 digits has a valid format\nCheck if an EAN barcode of 18 digits has a valid format\nCheck if an EAN barcode of 8 digits has a valid format\nCheck if a GTIN barcode has a valid format\nCheck if a GTIN barcode of 13 digits has a valid format\nCheck if a GTIN barcode of 14 digits has a valid format\nCheck if a GTIN barcode of 8 digits has a valid format\nCheck if VIN Number is valid\nAllows checking if an ISBN book exists\nAllows checking if an ISBN10/13 code has a valid format\nAllows checking if an ISBN10 code has a valid format\nAllows checking if an ISBN13 code has a valid format\nCheck if a UPC exists\nCheck if a UPC has a valid format\nGet ASIN by EAN\nGet a book by author's surname\nGet all publications by category\nGet book data by an editor's name\nGet book or publication data by 10 or 13 digits ISBN code\nGet book data by title\nGet books by author's surname\nGet all books by category\nGet all books by editor\nGet all books by title\nGet EAN code by ASIN code\nGet product data on a UPC on Amazon Marketplace\nGet ISBN10 code by ISBN13 code\nGet ISBN13 code by ISBN10 code\nGet data By VIN number\nSecurity\nCheck if a Luhn number is valid\nCheck if a password is strong\nCheck if a UUID number is valid\nGet blacklists for a domain\nGet blacklists for an IP address\nText\nCheck if a string only contains alphabets\nCheck if a string is alphanumeric\nCheck if a string is boolean\nCheck if the largest item in a list matches the provided item\nCheck if IPv4 or IPv6 address has a valid format\nCheck if IPv4 address has a valid format\nCheck if IPv6 address has a valid format\nCheck if the length of a list is between two quantities\nChecks if the length of a list equals a specified quantity\nChecks if the length of a list is greater than or equal to a certain amount\nCheck if the length of a list is lower than a certain amount\nCheck if the list contains a specific item\nCheck if the list ends with a specific element\nCheck if a list is sorted in ascending order\nCheck if the list starts with a specific element\nChecks if the smallest element in a list matches the provided element\nCheck if a string contains only numbers\nCheck if a string contains a character\nCheck if a string ends with a character\nCheck if a string has no content\nCheck if a string contains random characters\nCheck if a string contains a value that matches with a regular expression\nCheck if the length of a string is between two numbers\nCheck if the length of a string is equal to a number\nCheck if the length of a string is greater than a number\nCheck if the length of a string is greater or equal to a number\nCheck if the length of a string is lower than a number\nCheck if the length of a string is lower or equal to a number\nCheck if a string starts with a character\nCheck if a string contains only lowercase characters\nCheck if a string contains only uppercase characters\nCheck if a list consists of unique elements\nCheck if the supplied values form a valid list of elements\nCheck if the number of words in a sentence is between two determined quantities\nCheck if the number of words in a sentence equals a certain amount\nCheck if the number of words in a sentence is greater than a certain amount\nCheck if the number of words in a sentence is greater than\nCheck if the word count is lower\nCheck if the number of words present in a sentence is less than or equal to a quantity\nConvert a string to Base64 encoded value\nDiscover banned English words in an email body or subject\nGet field names by analyzing the field value provided\nGet HTML code from Markdown\nGet Markdown text from HTML\nGet text without HTML\nGet spin string\nFormat a string using a format pattern\nGenerate random string using a regular expression as a pattern\nReturn the largest item in a list\nReturn the smallest item in a list\nConvert to lowercase\nConvert a string to MD5 encoded value\nMerge two strings\nNormalize a string depending on the field name\nAnalyze string and return all emails, phones, zip codes, and links\nConvert a string to an SHA encoded value\nAnalyze an English text with emojis and detect sentiment\nReturns an ascending sorted list\nSplit a value into two parts and join them using a separator from the original string\nSplit a value into two parts using a separator from the original string\nGet the length of a string\nLookup string between multiple values by fuzzy logic and regex patterns\nClean abuse words from a string\nReplace the first value found in a string with another\nReplace all values found in a string with another\nTranslate a text into any language\nReturn a single list with no repeating elements\nConvert all letters to uppercase\nCount total words in a text\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.uptimerobot.md",
    "content": "UptimeRobot node\nUse the UptimeRobot node to automate work in UptimeRobot, and integrate UptimeRobot with other applications. n8n has built-in support for a wide range of UptimeRobot features, including creating and deleting alerts, as well as getting account details.\nOn this page, you'll find a list of operations the UptimeRobot node supports and links to more resources.\nOperations\nAccount\nGet account details\nAlert Contact\nCreate an alert contact\nDelete an alert contact\nGet an alert contact\nGet all alert contacts\nUpdate an alert contact\nMaintenance Window\nCreate a maintenance window\nDelete a maintenance window\nGet a maintenance window\nGet all a maintenance windows\nUpdate a maintenance window\nMonitor\nCreate a monitor\nDelete a monitor\nGet a monitor\nGet all monitors\nReset a monitor\nUpdate a monitor\nPublic Status Page\nCreate a public status page\nDelete a public status page\nGet a public status page\nGet all a public status pages\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.urlscanio.md",
    "content": "urlscan.io node\nUse the urlscan.io node to automate work in urlscan.io, and integrate urlscan.io with other applications. n8n has built-in support for a wide range of urlscan.io features, including getting and performing scans.\nOn this page, you'll find a list of operations the urlscan.io node supports and links to more resources.\nOperations\nScan\nGet\nGet All\nPerform\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.venafitlsprotectcloud.md",
    "content": "Venafi TLS Protect Cloud node\nUse the Venafi TLS Protect Cloud node to automate work in Venafi TLS Protect Cloud, and integrate Venafi TLS Protect Cloud with other applications. n8n has built-in support for a wide range of Venafi TLS Protect Cloud features, including deleting and downloading certificates, as well as creating certificates requests.\nOn this page, you'll find a list of operations the Venafi TLS Protect Cloud node supports and links to more resources.\nOperations\nCertificate\nDelete\nDownload\nGet\nGet Many\nRenew\nCertificate Request\nCreate\nGet\nGet Many\nTemplates and examples\nRelated resources\nRefer to Venafi's REST API documentation for more information on this service.\nn8n also provides:\nA trigger node for Venafi TLS Protect Cloud.\nA node for Venafi TLS Protect Datacenter."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.venafitlsprotectdatacenter.md",
    "content": "Venafi TLS Protect Datacenter node\nUse the Venafi TLS Protect Datacenter node to automate work in Venafi TLS Protect Datacenter, and integrate Venafi TLS Protect Datacenter with other applications. n8n has built-in support for a wide range of Venafi TLS Protect Datacenter features, including creating, deleting, and getting certificates.\nOn this page, you'll find a list of operations the Venafi TLS Protect Datacenter node supports and links to more resources.\nOperations\nCertificate\nCreate\nDelete\nDownload\nGet\nGet Many\nRenew\nPolicy\nGet\nTemplates and examples\nRelated resources\nn8n also provides:\nA node and trigger node for Venafi TLS Protect Cloud."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.vero.md",
    "content": "Vero node\nUse the Vero node to automate work in Vero and integrate Vero with other applications. n8n has built-in support for a wide range of Vero features, including creating and deleting users.\nOn this page, you'll find a list of operations the Vero node supports and links to more resources.\nOperations\nUser\nCreate or update a user profile\nChange a users identifier\nUnsubscribe a user.\nResubscribe a user.\nDelete a user.\nAdds a tag to a users profile.\nRemoves a tag from a users profile.\nEvent\nTrack an event for a specific customer\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.vonage.md",
    "content": "Vonage node\nUse the Vonage node to automate work in Vonage, and integrate Vonage with other applications. n8n supports sending SMS with Vonage.\nOn this page, you'll find a list of operations the Vonage node supports and links to more resources.\nOperations\nSMS\nSend\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.webflow.md",
    "content": "Webflow node\nUse the Webflow node to automate work in Webflow, and integrate Webflow with other applications. n8n has built-in support for a wide range of Webflow features, including creating, updating, deleting, and getting items.\nOn this page, you'll find a list of operations the Webflow node supports and links to more resources.\nOperations\nItem\nCreate\nDelete\nGet\nGet All\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.wekan.md",
    "content": "Wekan node\nUse the Wekan node to automate work in Wekan, and integrate Wekan with other applications. n8n has built-in support for a wide range of Wekan features, including creating, updating, deleting, and getting boards and cards.\nOn this page, you'll find a list of operations the Wekan node supports and links to more resources.\nOperations\nBoard\nCreate a new board\nDelete a board\nGet the data of a board\nGet all user boards\nCard\nCreate a new card\nDelete a card\nGet a card\nGet all cards\nUpdate a card\nCard Comment\nCreate a comment on a card\nDelete a comment from a card\nGet a card comment\nGet all card comments\nChecklist\nCreate a new checklist\nDelete a checklist\nGet the data of a checklist\nReturns all checklists for the card\nChecklist Item\nDelete a checklist item\nGet a checklist item\nUpdate a checklist item\nList\nCreate a new list\nDelete a list\nGet the data of a list\nGet all board lists\nTemplates and examples\nLoad all the parameters for the node\nTo load all the parameters, for example, Author ID, you need to give admin permissions to the user. Refer to the Wekan documentation to learn how to change permissions."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.wise.md",
    "content": "Wise node\nUse the Wise node to automate work in Wise, and integrate Wise with other applications. n8n has built-in support for a wide range of Wise features, including getting profiles, exchange rates, and recipients.\nOn this page, you'll find a list of operations the Wise node supports and links to more resources.\nOperations\nAccount\nRetrieve balances for all account currencies of this user.\nRetrieve currencies in the borderless account of this user.\nRetrieve the statement for the borderless account of this user.\nExchange Rate\nGet\nProfile\nGet\nGet All\nRecipient\nGet All\nQuote\nCreate\nGet\nTransfer\nCreate\nDelete\nExecute\nGet\nGet All\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.woocommerce.md",
    "content": "WooCommerce node\nUse the WooCommerce node to automate work in WooCommerce, and integrate WooCommerce with other applications. n8n has built-in support for a wide range of WooCommerce features, including creating and deleting customers, orders, and products.\nOn this page, you'll find a list of operations the WooCommerce node supports and links to more resources.\nOperations\nCustomer\nCreate a customer\nDelete a customer\nRetrieve a customer\nRetrieve all customers\nUpdate a customer\nOrder\nCreate a order\nDelete a order\nGet a order\nGet all orders\nUpdate an order\nProduct\nCreate a product\nDelete a product\nGet a product\nGet all products\nUpdate a product\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.wordpress.md",
    "content": "WordPress node\nUse the WordPress node to automate work in WordPress, and integrate WordPress with other applications. n8n has built-in support for a wide range of WordPress features, including creating, updating, and getting posts and users.\nOn this page, you'll find a list of operations the WordPress node supports and links to more resources.\nOperations\nPost\nCreate a post\nGet a post\nGet all posts\nUpdate a post\nPages\nCreate a page\nGet a page\nGet all pages\nUpdate a page\nUser\nCreate a user\nGet a user\nGet all users\nUpdate a user\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.xero.md",
    "content": "Xero node\nUse the Xero node to automate work in Xero, and integrate Xero with other applications. n8n has built-in support for a wide range of Xero features, including creating, updating, and getting contacts and invoices.\nOn this page, you'll find a list of operations the Xero node supports and links to more resources.\nOperations\nContact\nCreate a contact\nGet a contact\nGet all contacts\nUpdate a contact\nInvoice\nCreate a invoice\nGet a invoice\nGet all invoices\nUpdate a invoice\nTemplates and examples\nRelated resources\nRefer to Xero's API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.yourls.md",
    "content": "Yourls node\nUse the Yourls node to automate work in Yourls, and integrate Yourls with other applications. n8n has built-in support for a wide range of Yourls features, including expanding and shortening URLs.\nOn this page, you'll find a list of operations the Yourls node supports and links to more resources.\nOperations\nURL\nExpand a URL\nShorten a URL\nGet stats about one short URL\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.youtube.md",
    "content": "YouTube node\nUse the YouTube node to automate work in YouTube, and integrate YouTube with other applications. n8n has built-in support for a wide range of YouTube features, including retrieving and updating channels, as well as creating and deleting playlists.\nOn this page, you'll find a list of operations the YouTube node supports and links to more resources.\nOperations\nChannel\nRetrieve a channel\nRetrieve all channels\nUpdate a channel\nUpload a channel banner\nPlaylist\nCreate a playlist\nDelete a playlist\nGet a playlist\nRetrieve all playlists\nUpdate a playlist\nPlaylist Item\nAdd an item to a playlist\nDelete a item from a playlist\nGet a playlist's item\nRetrieve all playlist items\nVideo\nDelete a video\nGet a video\nRetrieve all videos\nRate a video\nUpdate a video\nUpload a video\nVideo Category\nRetrieve all video categories\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.zammad.md",
    "content": "Zammad node\nUse the Zammad node to automate work in Zammad, and integrate Zammad with other applications. n8n has built-in support for a wide range of Zammad features, including creating, retrieving, and deleting groups and organizations.\nOn this page, you'll find a list of operations the Zammad node supports and links to more resources.\nOperations\nGroup\nCreate\nDelete\nGet\nGet many\nUpdate\nOrganization\nCreate\nDelete\nGet\nGet many\nUpdate\nTicket\nCreate\nDelete\nGet\nGet many\nUser\nCreate\nDelete\nGet\nGet many\nGet self\nUpdate\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.zendesk.md",
    "content": "Zendesk node\nUse the Zendesk node to automate work in Zendesk, and integrate Zendesk with other applications. n8n has built-in support for a wide range of Zendesk features, including creating, and deleting tickets, users, and organizations.\nOn this page, you'll find a list of operations the Zendesk node supports and links to more resources.\nOperations\nTicket\nCreate a ticket\nDelete a ticket\nGet a ticket\nGet all tickets\nRecover a suspended ticket\nUpdate a ticket\nTicket Field\nGet a ticket field\nGet all system and custom ticket fields\nUser\nCreate a user\nDelete a user\nGet a user\nGet all users\nGet a user's organizations\nGet data related to the user\nSearch users\nUpdate a user\nOrganization\nCreate an organization\nDelete an organization\nCount organizations\nGet an organization\nGet all organizations\nGet data related to the organization\nUpdate a organization\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.zohocrm.md",
    "content": "Zoho CRM node\nUse the Zoho CRM node to automate work in Zoho CRM, and integrate Zoho CRM with other applications. n8n has built-in support for a wide range of Zoho CRM features, including creating and deleting accounts, contacts, and deals.\nOn this page, you'll find a list of operations the Zoho CRM node supports and links to more resources.\nOperations\nAccount\nCreate an account\nCreate a new record, or update the current one if it already exists (upsert)\nDelete an account\nGet an account\nGet all accounts\nUpdate an account\nContact\nCreate a contact\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a contact\nGet a contact\nGet all contacts\nUpdate a contact\nDeal\nCreate a deal\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a contact\nGet a contact\nGet all contacts\nUpdate a contact\nInvoice\nCreate an invoice\nCreate a new record, or update the current one if it already exists (upsert)\nDelete an invoice\nGet an invoice\nGet all invoices\nUpdate an invoice\nLead\nCreate a lead\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a lead\nGet a lead\nGet all leads\nGet lead fields\nUpdate a lead\nProduct\nCreate a product\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a product\nGet a product\nGet all products\nUpdate a product\nPurchase Order\nCreate a purchase order\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a purchase order\nGet a purchase order\nGet all purchase orders\nUpdate a purchase order\nQuote\nCreate a quote\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a quote\nGet a quote\nGet all quotes\nUpdate a quote\nSales Order\nCreate a sales order\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a sales order\nGet a sales order\nGet all sales orders\nUpdate a sales order\nVendor\nCreate a vendor\nCreate a new record, or update the current one if it already exists (upsert)\nDelete a vendor\nGet a vendor\nGet all vendors\nUpdate a vendor\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.zoom.md",
    "content": "Zoom node\nUse the Zoom node to automate work in Zoom, and integrate Zoom with other applications. n8n has built-in support for a wide range of Zoom features, including creating, retrieving, deleting, and updating meetings.\nOn this page, you'll find a list of operations the Zoom node supports and links to more resources.\nOperations\nMeeting\nCreate a meeting\nDelete a meeting\nRetrieve a meeting\nRetrieve all meetings\nUpdate a meeting\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.zulip.md",
    "content": "Zulip node\nUse the Zulip node to automate work in Zulip, and integrate Zulip with other applications. n8n has built-in support for a wide range of Zulip features, including creating, deleting, and getting users and streams, as well as sending messages.\nOn this page, you'll find a list of operations the Zulip node supports and links to more resources.\nOperations\nMessage\nDelete a message\nGet a message\nSend a private message\nSend a message to stream\nUpdate a message\nUpload a file\nStream\nCreate a stream.\nDelete a stream.\nGet all streams.\nGet subscribed streams.\nUpdate a stream.\nUser\nCreate a user.\nDeactivate a user.\nGet a user.\nGet all users.\nUpdate a user.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.anthropic.md",
    "content": "Anthropic node\nUse the Anthropic node to automate work in Anthropic and integrate Anthropic with other applications. n8n has built-in support for a wide range of Anthropic features, including analyzing, uploading, getting, and deleting documents, files, and images,  and generating, improving, or templatizing prompts.\nOn this page, you'll find a list of operations the Anthropic node supports, and links to more resources.\nOperations\nDocument:\nAnalyze Document: Take in documents and answer questions about them.\nFile:\nUpload File: Upload a file to the Anthropic API for later user.\nGet File Metadata: Get metadata for a file from the Anthropic API.\nList Files: List files from the Anthropic API.\nDelete File: Delete a file from the Anthropic API.\nImage:\nAnalyze Image: Take in images and answer questions about them.\nPrompt:\nGenerate Prompt: Generate a prompt for a model.\nImprove Prompt: Improve a prompt for a model.\nTemplatize Prompt: Templatize a prompt for a model.\nText:\nMessage a Model: Create a completion with an Anthropic model.\nTemplates and examples\nRelated resources\nRefer to Anthropic's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.googlegemini.md",
    "content": "Google Gemini node\nUse the Google Gemini node to automate work in Google Gemini and integrate Google Gemini with other applications. n8n has built-in support for a wide range of Google Gemini features, including working with audio, videos, images, documents, and files to analyze, generate, and transcribe.\nOn this page, you'll find a list of operations the Google Gemini node supports, and links to more resources.\nOperations\nAudio:\nAnalyze Audio: Take in audio and answer questions about it.\nTranscribe a Recording: Transcribes audio into text.\nDocument:\nAnalyze Document: Take in documents and answer questions about them.\nFile:\nUpload File: Upload a file to the Google Gemini API for later user.\nImage:\nAnalyze Image: Take in images and answer questions about them.\nGenerate an Image: Creates an image from a text prompt.\nText:\nMessage a Model: Create a completion with a Google Gemini model.\nVideo:\nAnalyze Video: Take in videos and answer questions about them.\nGenerate a Video: Creates a video from a text prompt.\nDownload Video: Download a generated video from the Google Gemini API using a URL.\nTemplates and examples\nRelated resources\nRefer to Google Gemini's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.perplexity.md",
    "content": "Perplexity node\nUse the Perplexity node to automate work in Perplexity and integrate Perplexity with other applications. n8n has built-in support for messaging a model.\nOn this page, you'll find a list of operations the Perplexity node supports, and links to more resources.\nOperations\nMessage a Model: Create one or more completions for a given text.\nTemplates and examples\nRelated resources\nRefer to Perplexity's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.airtable\\common-issues.md",
    "content": "Airtable node common issues\nHere are some common errors and issues with the Airtable node and steps to resolve or troubleshoot them.\nForbidden - perhaps check your credentials\nThis error displays when trying to perform actions not permitted by your current level of access. The full text looks something like this:\nThe error most often displays when the credential you're using doesn't have the scopes it requires on the resources you're attempting to manage.\nRefer to the Airtable credentials and Airtables scopes documentation for more information.\nService is receiving too many requests from you\nAirtable has a hard API limit on the number of requests generated using personal access tokens.\nIf you send more than five requests per second per base, you will receive a 429 error, indicating that you have sent too many requests. You will have to wait 30 seconds before resuming requests. This same limit applies for sending more than 50 requests across all bases per access token.\nYou can find out more in the Airtable's rate limits documentation. If you find yourself running into rate limits with the Airtable node, consider implementing one of the suggestions on the handling rate limits page."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.airtable\\index.md",
    "content": "Airtable node\nUse the Airtable node to automate work in Airtable, and integrate Airtable with other applications. n8n has built-in support for a wide range of Airtable features, including creating, reading, listing, updating and deleting tables.\nOn this page, you'll find a list of operations the Airtable node supports and links to more resources.\nOperations\nAppend the data to a table\nDelete data from a table\nList data from a table\nRead data from a table\nUpdate data in a table\nTemplates and examples\nRelated resources\nn8n provides a trigger node for Airtable. You can find the trigger node docs here.\nRefer to Airtable's documentation for more information about the service.\nNode reference\nGet the Record ID\nTo fetch data for a particular record, you need the Record ID. There are two ways to get the Record ID.\nCreate a Record ID column in Airtable\nTo create a Record ID column in your table, refer to this article. You can then use this Record ID in your Airtable node.\nUse the List operation\nTo get the Record ID of your record, you can use the List operation of the Airtable node. This operation will return the Record ID along with the fields. You can then use this Record ID in your Airtable node.\nFilter records when using the List operation\nTo filter records from your Airtable base, use the Filter By Formula option. For example, if you want to return all the users that belong to the organization n8n, follow the steps mentioned below:\nSelect 'List' from the Operation dropdown list.\nEnter the base ID and the table name in the Base ID and Table field, respectively.\nClick on Add Option and select 'Filter By Formula' from the dropdown list.\nEnter the following formula in the Filter By Formula field: {Organization}='n8n'.\nSimilarly, if you want to return all the users that don't belong to the organization n8n, use the following formula: NOT({Organization}='n8n').\nRefer to the Airtable documentation to learn more about the formulas.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.discord\\common-issues.md",
    "content": "Discord node common issues\nHere are some common errors and issues with the Discord node and steps to resolve or troubleshoot them.\nAdd extra fields to embeds\nDiscord messages can optionally include embeds, a rich preview component that can include a title, description, image, link, and more.\nThe Discord node supports embeds when using the Send operation on the Message resource. Select Add Embeds to set extra fields including Description, Author, Title, URL, and URL Image.\nTo add fields that aren't included by default, set Input Method to Raw JSON. From here, add a JSON object to the Value parameter defining the field names and values you want to include.\nFor example, to include footer and fields, neither of which are available using the Enter Fields Input Method, you could use a JSON object like this:\nYou can learn more about embeds in Using Webhooks and Embeds | Discord.\nIf you experience issues when working with embeds with the Discord node, you can use the HTTP Request with your existing Discord credentials to POST to the following URL:\nIn the body, include your embed information in the message content like this:\nMention users and channels\nTo mention users and channels in Discord messages, you need to format your message according to Discord's message formatting guidelines.\nTo mention a user, you need to know the Discord user's user ID. Keep in mind that the user ID is different from the user's display name. Similarly, you need a channel ID to link to a specific channel.\nYou can learn how to enable developer mode and copy the user or channel IDs in Discord's documentation on finding User/Server/Message IDs.\nOnce you have the user or channel ID, you can format your message with the following syntax:\nUser: <@USER_ID>\nChannel: <#CHANNEL_ID>\nRole: <@&ROLE_ID>"
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.discord\\index.md",
    "content": "Discord node\nUse the Discord node to automate work in Discord, and integrate Discord with other applications. n8n has built-in support for a wide range of Discord features, including sending messages in a Discord channel and managing channels.\nOn this page, you'll find a list of operations the Discord node supports and links to more resources.\nOperations\nChannel\nCreate\nDelete\nGet\nGet Many\nUpdate\nMessage\nDelete\nGet\nGet Many\nReact with Emoji\nSend\nSend and Wait for Response\nMember\nGet Many\nRole Add\nRole Remove\nTemplates and examples\nRelated resources\nRefer to Discord's documentation for more information about the service.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gmail\\common-issues.md",
    "content": "Gmail node common issues\nHere are some common errors and issues with the Gmail node and steps to resolve or troubleshoot them.\nRemove the n8n attribution from sent messages\nIf you're using the node to send a message or reply to a message, the node appends this statement to the end of the email:\nThis email was sent automatically with n8n\nTo remove this attribution:\nIn the node's Options section, select Add option.\nSelect Append n8n attribution.\nTurn the toggle off.\nRefer to Send options and Reply options for more information.\nForbidden - perhaps check your credentials\nThis error displays next to certain dropdowns in the node, like the Label Names or IDs dropdown. The full text looks something like this:\nThe error most often displays when you're using a Google Service Account as the credential and the credential doesn't have Impersonate a User turned on.\nRefer to Google Service Account: Finish your n8n credential for more information.\n401 unauthorized error\nThe full text of the error looks like this:\nThis error occurs when there's an issue with the credential you're using and its scopes or permissions.\nTo resolve:\nFor OAuth2 credentials, make sure you've enabled the Gmail API in APIs & Services > Library. Refer to Google OAuth2 Single Service - Enable APIs for more information.\nFor Service Account credentials:\nEnable domain-wide delegation.\nMake sure you add the Gmail API as part of the domain-wide delegation configuration.\nBad request - please check your parameters\nThis error most often occurs if you enter a Message ID, Thread ID, or Label ID that doesn't exist.\nTry a Get operation with the ID to confirm it exists."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gmail\\draft-operations.md",
    "content": "Gmail node Draft Operations\nUse the Draft operations to create, delete, or get a draft or list drafts in Gmail. Refer to the Gmail node for more information on the Gmail node itself.\nCreate a draft\nUse this operation to create a new draft.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Draft.\nOperation: Select Create.\nSubject: Enter the subject line.\nSelect the Email Type. Choose from Text or HTML.\nMessage: Enter the email message body.\nCreate draft options\nUse these options to further refine the node's behavior:\nAttachments: Select Add Attachment to add an attachment. Enter the Attachment Field Name (in Input) to identify which field from the input node contains the attachment.\nFor multiple properties, enter a comma-separated list.\nBCC: Enter one or more email addresses for blind copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nCC: Enter one or more email addresses for carbon copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nFrom Alias Name or ID: Select an alias to send the draft from. This field populates based on the credential you selected in the parameters.\nSend Replies To: Enter an email address to set as the reply to address.\nThread ID: If you want this draft attached to a thread, enter the ID for that thread.\nTo Email: Enter one or more email addresses for recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nRefer to the Gmail API Method: users.drafts.create documentation for more information.\nDelete a draft\nUse this operation to delete a draft.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Draft.\nOperation: Select Delete.\nDraft ID: Enter the ID of the draft you wish to delete.\nRefer to the Gmail API Method: users.drafts.delete documentation for more information.\nGet a draft\nUse this operation to get a single draft.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Draft.\nOperation: Select Get.\nDraft ID: Enter the ID of the draft you wish to get information about.\nGet draft options\nUse these options to further refine the node's behavior:\nAttachment Prefix: Enter a prefix for the name of the binary property the node should write any attachments to. n8n adds an index starting with 0 to the prefix. For example, if you enter attachment_' as the prefix, the first attachment saves to 'attachment_0'.\nDownload Attachments: Select whether the node should download the draft's attachments (turned on) or not (turned off).\nRefer to the Gmail API Method: users.drafts.get documentation for more information.\nGet Many drafts\nUse this operation to get two or more drafts.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Draft.\nOperation: Select Get Many.\nReturn All: Choose whether the node returns all drafts (turned on) or only up to a set limit (turned off).\nLimit: Enter the maximum number of drafts to return. Only used if you've turned off Return All.\nGet Many drafts options\nUse these options to further refine the node's behavior:\nAttachment Prefix: Enter a prefix for the name of the binary property the node should write any attachments to. n8n adds an index starting with 0 to the prefix. For example, if you enter attachment_' as the prefix, the first attachment saves to 'attachment_0'.\nDownload Attachments: Select whether the node should download the draft's attachments (turned on) or not (turned off).\nInclude Spam and Trash: Select whether the node should get drafts in the Spam and Trash folders (turned on) or not (turned off).\nRefer to the Gmail API Method: users.drafts.list documentation for more information.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gmail\\index.md",
    "content": "Gmail node\nUse the Gmail node to automate work in Gmail, and integrate Gmail with other applications. n8n has built-in support for a wide range of Gmail features, including creating, updating, deleting, and getting drafts, messages, labels, thread.\nOn this page, you'll find a list of operations the Gmail node supports and links to more resources.\nOperations\nDraft\nCreate a draft\nDelete a draft\nGet a draft\nGet Many drafts\nLabel\nCreate a label\nDelete a label\nGet a label\nGet Many labels\nMessage\nAdd Label to a message\nDelete a message\nGet a message\nGet Many messages\nMark as Read\nMark as Unread\nRemove Label from a message\nReply to a message\nSend a message\nThread\nAdd Label to a thread\nDelete a thread\nGet a thread\nGet Many threads\nRemove Label from thread\nReply to a message\nTrash a thread\nUntrash a thread\nTemplates and examples\nRelated resources\nRefer to Google's Gmail API documentation for detailed information about the API that this node integrates with.\nn8n provides a trigger node for Gmail. You can find the trigger node docs here.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gmail\\label-operations.md",
    "content": "Gmail node Label Operations\nUse the Label operations to create, delete, or get a label or list labels in Gmail. Refer to the Gmail node for more information on the Gmail node itself.\nCreate a label\nUse this operation to create a new label.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Label.\nOperation: Select Create.\nName: Enter a display name for the label.\nCreate label options\nUse these options to further refine the node's behavior:\nLabel List Visibility: Sets the visibility of the label in the label list in the Gmail web interface. Choose from:\nHide: Don't show the label in the label list.\nShow (default): Show the label in the label list.\nShow if Unread: Show the label if there are any unread messages with that label.\nMessage List Visibility: Sets the visibility of messages with this label in the message list in the Gmail web interface. Choose whether to Show or Hide messages with this label.\nRefer to the Gmail API Method: users.labels.create documentation for more information.\nDelete a label\nUse this operation to delete an existing label.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Label.\nOperation: Select Delete.\nLabel ID: Enter the ID of the label you want to delete.\nRefer to the Gmail API Method: users.labels.delete documentation for more information.\nGet a label\nUse this operation to get an existing label.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Label.\nOperation: Select Get.\nLabel ID: Enter the ID of the label you want to get.\nRefer to the Gmail API Method: users.labels.get documentation for more information.\nGet Many labels\nUse this operation to get two or more labels.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Label.\nOperation: Select Get Many.\nReturn All: Choose whether the node returns all labels (turned on) or only up to a set limit (turned off).\nLimit: Enter the maximum number of labels to return. Only used if you've turned off Return All.\nRefer to the Gmail API Method: users.labels.list documentation for more information.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gmail\\message-operations.md",
    "content": "Gmail node Message Operations\nUse the Message operations to send, reply to, delete, mark read or unread, add a label to, remove a label from, or get a message or get a list of messages in Gmail. Refer to the Gmail node for more information on the Gmail node itself.\nAdd Label to a message\nUse this operation to add one or more labels to a message.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Add Label.\nMessage ID: Enter the ID of the message you want to add the label to.\nLabel Names or IDs: Select the Label names you want to add or enter an expression to specify IDs. The dropdown populates based on the Credential you selected.\nRefer to the Gmail API Method: users.messages.modify documentation for more information.\nDelete a message\nUse this operation to immediately and permanently delete a message.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Delete.\nMessage ID: Enter the ID of the message you want to delete.\nRefer to the Gmail API Method: users.messages.delete documentation for more information.\nGet a message\nUse this operation to get a single message.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Get.\nMessage ID: Enter the ID of the message you wish to retrieve.\nSimplify: Choose whether to return a simplified version of the response (turned on) or the raw data (turned off). Default is on.\nThis is the same as setting the format for the API call to metadata, which returns email message IDs, labels, and email headers, including: From, To, CC, BCC, and Subject.\nRefer to the Gmail API Method: users.messages.get documentation for more information.\nGet Many messages\nUse this operation to get two or more messages.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Get Many.\nReturn All: Choose whether the node returns all messages (turned on) or only up to a set limit (turned off).\nLimit: Enter the maximum number of messages to return. Only used if you've turned off Return All.\nSimplify: Choose whether to return a simplified version of the response (turned on) or the raw data (turned off). Default is on.\nThis is the same as setting the format for the API call to metadata, which returns email message IDs, labels, and email headers, including: From, To, CC, BCC, and Subject.\nGet Many messages filters\nUse these filters to further refine the node's behavior:\nInclude Spam and Trash: Select whether the node should get messages in the Spam and Trash folders (turned on) or not (turned off).\nLabel Names or IDs: Only return messages with the selected labels added to them. Select the Label names you want to apply or enter an expression to specify IDs. The dropdown populates based on the Credential you selected.\nSearch: Enter Gmail search refine filters, like from:, to filter the messages returned. Refer to Refine searches in Gmail for more information.\nRead Status: Choose whether to receive Unread and read emails, Unread emails only (default), or Read emails only.\nReceived After: Return only those emails received after the specified date and time. Use the date picker to select the day and time or enter an expression to set a date as a string in ISO format or a timestamp in milliseconds. Refer to ISO 8601 for more information on formatting the string.\nReceived Before: Return only those emails received before the specified date and time. Use the date picker to select the day and time or enter an expression to set a date as a string in ISO format or a timestamp in milliseconds. Refer to ISO 8601 for more information on formatting the string.\nSender: Enter an email or a part of a sender name to return messages from only that sender.\nRefer to the Gmail API Method: users.messages.list documentation for more information.\nMark as Read\nUse this operation to mark a message as read.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Mark as Read.\nMessage ID: Enter the ID of the message you wish to mark as read.\nRefer to the Gmail API Method: users.messages.modify documentation for more information.\nMark as Unread\nUse this operation to mark a message as unread.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Mark as Unread.\nMessage ID: Enter the ID of the message you wish to mark as unread.\nRefer to the Gmail API Method: users.messages.modify documentation for more information.\nRemove Label from a message\nUse this operation to remove one or more labels from a message.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Remove Label.\nMessage ID: Enter the ID of the message you want to remove the label from.\nLabel Names or IDs: Select the Label names you want to remove or enter an expression to specify IDs. The dropdown populates based on the Credential you selected.\nRefer to the Gmail API Method: users.messages.modify documentation for more information.\nReply to a message\nUse this operation to send a message as a reply to an existing message.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Reply.\nMessage ID: Enter the ID of the message you want to reply to.\nSelect the Email Type. Choose from Text or HTML.\nMessage: Enter the email message body.\nReply options\nUse these options to further refine the node's behavior:\nAppend n8n attribution: By default, the node appends the statement This email was sent automatically with n8n to the end of the email. To remove this statement, turn this option off.\nAttachments: Select Add Attachment to add an attachment. Enter the Attachment Field Name (in Input) to identify which field from the input node contains the attachment.\nFor multiple properties, enter a comma-separated list.\nBCC: Enter one or more email addresses for blind copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nCC: Enter one or more email addresses for carbon copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nSender Name: Enter the name you want displayed in your recipients' email as the sender.\nReply to Sender Only: Choose whether to reply all (turned off) or reply to the sender only (turned on).\nRefer to the Gmail API Method: users.messages.send documentation for more information.\nSend a message\nUse this operation to send a message.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Send.\nTo: Enter the email address you want the email sent to.\nSubject: Enter the subject line.\nSelect the Email Type. Choose from Text or HTML.\nMessage: Enter the email message body.\nSend options\nUse these options to further refine the node's behavior:\nAppend n8n attribution: By default, the node appends the statement This email was sent automatically with n8n to the end of the email. To remove this statement, turn this option off.\nAttachments: Select Add Attachment to add an attachment. Enter the Attachment Field Name (in Input) to identify which field from the input node contains the attachment.\nFor multiple properties, enter a comma-separated list.\nBCC: Enter one or more email addresses for blind copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nCC: Enter one or more email addresses for carbon copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nSender Name: Enter the name you want displayed in your recipients' email as the sender.\nSend Replies To: Enter an email address to set as the reply to address.\nReply to Sender Only: Choose whether to reply all (turned off) or reply to the sender only (turned on).\nRefer to the Gmail API Method: users.messages.send documentation for more information.\nSend a message and wait for approval\nUse this operation to send a message and wait for approval from the recipient before continuing the workflow execution.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Message.\nOperation: Select Send and Wait for Approval.\nTo: Enter the email address you want the email sent to.\nSubject: Enter the subject line.\nMessage: Enter the email message body.\nSend and wait for approval options\nUse these options to further refine the node's behavior:\nType of Approval: Choose Approve Only (default) to include only an approval button or Approve and Disapprove to also include a disapproval option.\nApprove Button Label: The label to use for the approval button (Approve by default).\nApprove Button Style: Whether to style the approval button as a Primary (default) or Secondary button.\nDisapprove Button Label: The label to use for the disapproval button (Decline by default). Only visible when you set Type of Approval to Approve and Disapprove.\nDisapprove Button Style: Whether to style the disapproval button as a Primary or Secondary (default) button. Only visible when you set Type of Approval to Approve and Disapprove.\nRefer to the Gmail API Method: users.messages.send documentation for more information.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.gmail\\thread-operations.md",
    "content": "Gmail node Thread Operations\nUse the Thread operations to delete, reply to, trash, untrash, add/remove labels, get one, or list threads. Refer to the Gmail node for more information on the Gmail node itself.\nAdd Label to a thread\nUse this operation to create a new draft.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Add Label.\nThread ID: Enter the ID of the thread you want to add the label to.\nLabel Names or IDs: Select the Label names you want to apply or enter an expression to specify IDs. The dropdown populates based on the Credential you selected.\nRefer to the Gmail API Method: users.threads.modify documentation for more information.\nDelete a thread\nUse this operation to immediately and permanently delete a thread and all its messages.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Delete.\nThread ID: Enter the ID of the thread you want to delete.\nRefer to the Gmail API Method: users.threads.delete documentation for more information.\nGet a thread\nUse this operation to get a single thread.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Get.\nThread ID: Enter the ID of the thread you wish to retrieve.\nSimplify: Choose whether to return a simplified version of the response (turned on) or the raw data (turned off). Default is on.\nThis is the same as setting the format for the API call to metadata, which returns email message IDs, labels, and email headers, including: From, To, CC, BCC, and Subject.\nGet thread options\nUse these options to further refine the node's behavior:\nReturn Only Messages: Choose whether to return only thread messages (turned on).\nRefer to the Gmail API Method: users.threads.get documentation for more information.\nGet Many threads\nUse this operation to get two or more threads.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Get Many.\nReturn All: Choose whether the node returns all threads (turned on) or only up to a set limit (turned off).\nLimit: Enter the maximum number of threads to return. Only used if you've turned off Return All.\nGet Many threads filters\nUse these filters to further refine the node's behavior:\nInclude Spam and Trash: Select whether the node should get threads in the Spam and Trash folders (turned on) or not (turned off).\nLabel Names or IDs: Only return threads with the selected labels added to them. Select the Label names you want to apply or enter an expression to specify IDs. The dropdown populates based on the Credential you selected.\nSearch: Enter Gmail search refine filters, like from:, to filter the threads returned. Refer to Refine searches in Gmail for more information.\nRead Status: Choose whether to receive Unread and read emails, Unread emails only (default), or Read emails only.\nReceived After: Return only those emails received after the specified date and time. Use the date picker to select the day and time or enter an expression to set a date as a string in ISO format or a timestamp in milliseconds. Refer to ISO 8601 for more information on formatting the string.\nReceived Before: Return only those emails received before the specified date and time. Use the date picker to select the day and time or enter an expression to set a date as a string in ISO format or a timestamp in milliseconds. Refer to ISO 8601 for more information on formatting the string.\nRefer to the Gmail API Method: users.threads.list documentation for more information.\nRemove label from a thread\nUse this operation to remove a label from a thread.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Remove Label.\nThread ID: Enter the ID of the thread you want to remove the label from.\nLabel Names or IDs: Select the Label names you want to remove or enter an expression to specify their IDs. The dropdown populates based on the Credential you selected.\nRefer to the Gmail API Method: users.threads.modify documentation for more information.\nReply to a message\nUse this operation to reply to a message.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Reply.\nThread ID: Enter the ID of the thread you want to reply to.\nMessage Snippet or ID: Select the Message you want to reply to or enter an expression to specify its ID. The dropdown populates based on the Credential you selected.\nSelect the Email Type. Choose from Text or HTML.\nMessage: Enter the email message body.\nReply options\nUse these options to further refine the node's behavior:\nAttachments: Select Add Attachment to add an attachment. Enter the Attachment Field Name (in Input) to identify which field from the input node contains the attachment.\nFor multiple properties, enter a comma-separated list.\nBCC: Enter one or more email addresses for blind copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nCC: Enter one or more email addresses for carbon copy recipients. Separate multiple email addresses with a comma, for example jay@gatsby.com, jon@smith.com.\nSender Name: Enter the name you want displayed in your recipients' email as the sender.\nReply to Sender Only: Choose whether to reply all (turned off) or reply to the sender only (turned on).\nRefer to the Gmail API Method: users.messages.send documentation for more information.\nTrash a thread\nUse this operation to move a thread and all its messages to the trash.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Trash.\nThread ID: Enter the ID of the thread you want to move to the trash.\nRefer to the Gmail API Method: users.threads.trash documentation for more information.\nUntrash a thread\nUse this operation to recover a thread and all its messages from the trash.\nEnter these parameters:\nSelect the Credential to connect with or create a new one.\nResource: Select Thread.\nOperation: Select Untrash.\nThread ID: Enter the ID of the thread you want to move to the trash.\nRefer to the Gmail API Method: users.threads.untrash documentation for more information.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecalendar\\calendar-operations.md",
    "content": "Google Calendar Calendar operations\nUse this operation to check availability in a calendar in Google Calendar. Refer to Google Calendar for more information on the Google Calendar node itself.\nAvailability\nUse this operation to check if a time-slot is available in a calendar.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Calendar credentials.\nResource: Select Calendar.\nOperation: Select Availability.\nCalendar: Choose a calendar you want to check against. Select From list to choose the title from the dropdown list or By ID to enter a calendar ID.\nStart Time: The start time for the time-slot you want to check. By default, uses an expression evaluating to the current time ({{ $now }}).\nEnd Time: The end time for the time-slot you want to check. By default, uses an expression evaluating to an hour from now ({{ $now.plus(1, 'hour') }}).\nOptions\nOutput Format: Select the format for the availability information:\nAvailability: Returns if there are already events overlapping with the given time slot or not.\nBooked Slots: Returns the booked slots.\nRAW: Returns the RAW data from the API.\nTimezone: The timezone used in the response. By default, uses the n8n timezone.\nRefer to the Freebusy: query | Google Calendar API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecalendar\\event-operations.md",
    "content": "Google Calendar Event operations\nUse these operations to create, delete, get, and update events in Google Calendar. Refer to Google Calendar for more information on the Google Calendar node itself.\nCreate\nUse this operation to add an event to a Google Calendar.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Calendar credentials.\nResource: Select Event.\nOperation: Select Create.\nCalendar: Choose a calendar you want to add an event to. Select From list to choose the title from the dropdown list or By ID to enter a calendar ID.\nStart Time: The start time for the event. By default, uses an expression evaluating to the current time ({{ $now }}).\nEnd Time: The end time for the event. By default, this uses an expression evaluating to an hour from now ({{ $now.plus(1, 'hour') }}).\nUse Default Reminders: Whether to enable default reminders for the event according to the calendar configuration.\nOptions\nAll Day: Whether the event is all day or not.\nAttendees: Attendees to invite to the event.\nColor Name or ID: The color of the event. Choose from the list or specify the ID using an expression.\nConference Data: Creates a conference link (Hangouts, Meet, etc.) and attaches it to the event.\nDescription: A description for the event.\nGuests Can Invite Others: Whether attendees other than the organizer can invite others to the event.\nGuests Can Modify: Whether attendees other than the organizer can modify the event.\nGuests Can See Other Guests: Whether attendees other than the organizer can see who the event's attendees are.\nID: Opaque identifier of the event.\nLocation: Geographic location of the event as free-form text.\nMax Attendees: The maximum number of attendees to include in the response. If there are more than the specified number of attendees, only returns the participant.\nRepeat Frequency: The repetition interval for recurring events.\nRepeat How Many Times?: The number of instances to create for recurring events.\nRepeat Until: The date at which recurring events should stop.\nRRULE: Recurrence rule. When set, ignores the Repeat Frequency, Repeat How Many Times, and Repeat Until parameters.\nSend Updates: Whether to send notifications about the creation of the new event.\nShow Me As: Whether the event blocks time on the calendar.\nSummary: The title of the event.\nRefer to the Events: insert TABLE_PLACEHOLDER_0\nUpdate\nUse this operation to update an event in a Google Calendar.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Calendar credentials.\nResource: Select Event.\nOperation: Select Update.\nCalendar: Choose a calendar you want to add an event to. Select From list to choose the title from the dropdown list or By ID to enter a calendar ID.\nEvent ID: The ID of the event to update.\nModify: For recurring events, choose whether to update the recurring event or a specific instance of the recurring event.\nUse Default Reminders: Whether to enable default reminders for the event according to the calendar configuration.\nUpdate Fields: The fields of the event to update:\nAll Day: Whether the event is all day or not.\nAttendees: Attendees to invite to the event. You can choose to either add attendees or replace the existing attendee list.\nColor Name or ID: The color of the event. Choose from the list or specify the ID using an expression.\nDescription: A description for the event.\nEnd: The end time of the event.\nGuests Can Invite Others: Whether attendees other than the organizer can invite others to the event.\nGuests Can Modify: Whether attendees other than the organizer can make changes to the event.\nGuests Can See Other Guests: Whether attendees other than the organizer can see who the event's attendees are.\nID: Opaque identifier of the event.\nLocation: Geographic location of the event as free-form text.\nMax Attendees: The maximum number of attendees to include in the response. If there are more than the specified number of attendees, only returns the participant.\nRepeat Frequency: The repetition interval for recurring events.\nRepeat How Many Times?: The number of instances to create for recurring events.\nRepeat Until: The date at which recurring events should stop.\nRRULE: Recurrence rule. When set, ignores the Repeat Frequency, Repeat How Many Times, and Repeat Until parameters.\nSend Updates: Whether to send notifications about the creation of the new event.\nShow Me As: Whether the event blocks time on the calendar.\nStart: The start time of the event.\nSummary: The title of the event.\nVisibility: The visibility of the event:\nConfidential: The event is private. This value is provided for compatibility.\nDefault: Uses the default visibility for events on the calendar.\nPublic: The event is public and the event details are visible to all readers of the calendar.\nPrivate: The event is private and only event attendees may view event details.\nRefer to the Events: update | Google Calendar API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlecalendar\\index.md",
    "content": "Google Calendar node\nUse the Google Calendar node to automate work in Google Calendar, and integrate Google Calendar with other applications. n8n has built-in support for a wide range of Google Calendar features, including adding, retrieving, deleting and updating calendar events.\nOn this page, you'll find a list of operations the Google Calendar node supports and links to more resources.\nOperations\nCalendar\nAvailability: If a time-slot is available in a calendar\nEvent\nCreate: Add an event to calendar\nDelete: Delete an event\nGet: Retrieve an event\nGet Many: Retrieve all events from a calendar\nUpdate: Update an event\nTemplates and examples\nRelated resources\nn8n provides a trigger node for Google Calendar. You can find the trigger node docs here.\nRefer to Google Calendar's documentation for more information about the service.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googledrive\\common-issues.md",
    "content": "Google Drive node common issues\nHere are some common errors and issues with the Google Drive node and steps to resolve or troubleshoot them.\nGoogle hasn't verified this app\nGoogle Cloud app becoming unauthorized\nGoogle Drive OAuth error\nIf using the OAuth authentication method, you may see an error indicating that you can't sign in because the app doesn't meet Google's expectations for keeping apps secure.\nMost often, the actual cause of this issue is that the URLs don't match between Google's OAuth configuration and n8n. To avoid this, start by reviewing any links included in Google's error message. This will contain details about the exact error that occurred.\nIf you are self-hostin n8n, check the n8n configuration items used to construct external URLs. Verify that the N8N_EDITOR_BASE_URL and WEBHOOK_URL environment variables use fully qualified domains.\nGet recent files from Google Drive\nTo retrieve recent files from Google Drive, you need to sort files by modification time. To do this, you need to search for existing files and retrieve their modification times. Next you can sort the files to find the most recent file and use another Google Drive node target the file by ID.\nThe process looks like this:\nAdd a Google Drive node to your canvas.\nSelect the File/Folder resource and the Search operation.\nEnable Return All to sort through all files.\nSet the What to Search filter to Files.\nIn the Options, set the Fields to All.\nConnect a Sort node to the output of the Google Drive node.\nChoose Simple sort type.\nEnter modifiedTime as the Field Name in the Fields To Sort By section.\nChoose Descending sort order.\nAdd a Limit node to the output of the Sort node.\nSet Max Items to 1 to keep the most recent file.\nConnect another Google Drive node to the output of the Limit node.\nSelect File as the Resource and the operation of your choice.\nIn the File selection, choose By ID.\nSelect Expression and enter {{ $json.id }} as the expression."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googledrive\\file-folder-operations.md",
    "content": "Google Drive File and Folder operations\nUse this operation to search for files and folders in Google Drive. Refer to Google Drive for more information on the Google Drive node itself.\nSearch files and folders\nUse this operation to search for files and folders in a drive.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Drive credentials.\nResource: Select File/Folder.\nOperation: Select Search.\nSearch Method: Choose how you want to search:\nSearch File/Folder Name: Fill out the Search Query with the name of the file or folder you want to search for. Returns files and folders that are partial matches for the query as well.\nAdvanced Search: Fill out the Query String to search for files and folders using Google query string syntax.\nReturn All: Choose whether to return all results or only up to a given limit.\nLimit: The maximum number of items to return when Return All is disabled.\nFilter: Choose whether to limit the scope of your search:\nDrive: The drive you want to search in. By default, uses your personal \"My Drive\". Select From list to choose the drive from the dropdown list, By URL to enter the URL of the drive, or By ID to enter the driveId.\nYou can find the driveId by visiting the shared drive in your browser and copying the last URL component:\nFolder: The folder to search in. Select From list to choose the folder from the dropdown list, By URL to enter the URL of the folder, or By ID to enter the folderId.\nYou can find the folderId by visiting the shared folder in your browser and copying the last URL component:\nWhat to Search: Whether to search for Files and Folders, Files, or Folders.\nInclude Trashed Items: Whether to also return items in the Drive's trash.\nOptions\nFields: Select the fields to return. Can be one or more of the following: [All], explicitlyTrashed, exportLinks, hasThumbnail, iconLink, ID, Kind, mimeType, Name, Permissions, Shared, Spaces, Starred, thumbnailLink, Trashed, Version, or webViewLink.\nRefer to the Method: files.list | Google Drive API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googledrive\\file-operations.md",
    "content": "Google Drive File operations\nUse this operation to create, delete, change, and manage files in Google Drive. Refer to Google Drive for more information on the Google Drive node itself.\nCopy a file\nUse this operation to copy a file to a drive.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Drive credentials.\nResource: Select File.\nOperation: Select Copy.\nFile: Choose a file you want to copy.\nSelect From list to choose the title from the dropdown list, By URL to enter the URL of the file, or By ID to enter the fileId.\nYou can find the fileId in a shareable Google Drive file URL:  In your Google Drive, select Share > Copy link to get the shareable file URL.\nFile Name: The name to use for the new copy of the file.\nCopy In The Same Folder: Choose whether to copy the file to the same folder. If disabled, set the following:\nParent Drive: Select From list to choose the drive from the dropdown list, By URL to enter the URL of the drive, or By ID to enter the driveId.\nParent Folder: Select From list to choose the folder from the dropdown list, By URL to enter the URL of the folder, or By ID to enter the folderId.\nYou can find the driveId and folderID by visiting the shared drive or folder in your browser and copying the last URL component:\nOptions\nCopy Requires Writer Permissions: Select whether to enable readers and commenters to copy, print, or download the new file.\nDescription: A short description of the file.\nRefer to the Method: files.copy TABLE_PLACEHOLDER_0\nUpload a file\nUse this operation to upload a file.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Drive credentials.\nResource: Select File.\nOperation: Select Upload.\nInput Data Field Name: The name of the input field that contains the binary file data you wish to use.\nFile Name: The name to use for the new file.\nParent Drive: Select From list to choose the drive from the dropdown list, By URL to enter the URL of the drive, or By ID to enter the driveId.\nParent Folder: Select From list to choose the folder from the dropdown list, By URL to enter the URL of the folder, or By ID to enter the folderId.\nYou can find the driveId and folderID by visiting the shared drive or folder in your browser and copying the last URL component:\nOptions\nAPP Properties: A bundle of arbitrary key-value pairs which are private to the requesting app.\nProperties: A bundle of arbitrary key-value pairs which are visible to all apps.\nKeep Revision Forever: Choose whether to set the keepForever field in the new head revision. This only applies to files with binary content. You can keep a maximum of 200 revisions, after which you must delete the pinned revisions.\nOCR Language: An ISO 639-1 language code to help the OCR interpret the content during import.\nUse Content As Indexable Text: Choose whether to mark the uploaded content as indexable text.\nSimplify Output: Choose whether to return a simplified version of the response instead of including all fields.\nRefer to the Method: files.insert | Google Drive API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googledrive\\folder-operations.md",
    "content": "Google Drive Folder operations\nUse this operation to create, delete, and share folders in Google Drive. Refer to Google Drive for more information on the Google Drive node itself.\nCreate a folder\nUse this operation to create a new folder in a drive.\nEnter these parameters:\n- Credential to connect with: Create or select an existing Google Drive credentials.\n- Resource: Select Folder.\n- Operation: Select Create.\n- Folder Name: The name to use for the new folder.\n- Parent Drive: Select From list to choose the drive from the dropdown list, By URL to enter the URL of the drive, or By ID to enter the driveId.\n- Parent Folder: Select From list to choose the folder from the dropdown list, By URL to enter the URL of the folder, or By ID to enter the folderId.\nYou can find the driveId and folderID by visiting the shared drive or folder in your browser and copying the last URL component:\nOptions\nSimplify Output: Choose whether to return a simplified version of the response instead of including all fields.\nFolder Color: The color of the folder as an RGB hex string.\nRefer to the Method: files.insert TABLE_PLACEHOLDER_0\nShare a folder\nUse this operation to add sharing permissions to a folder.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Drive credentials.\nResource: Select Folder.\nOperation: Select Share.\nFolder: Choose a file you want to move.\nSelect From list to choose the folder from the dropdown list, By URL to enter the URL of the folder, or By ID to enter the folderId.\nYou can find the folderId in a Google Drive folder URL:\nPermissions: The permissions to add to the folder:\nRole: Select what users can do with the folder. Can be one of Commenter, File Organizer, Organizer, Owner, Reader, Writer.\nType: Select the scope of the new permission:\nUser: Grant permission to a specific user, defined by entering their Email Address.\nGroup: Grant permission to a specific group, defined by entering its Email Address.\nDomain: Grant permission to a complete domain, defined by the Domain.\nAnyone: Grant permission to anyone. Can optionally Allow File Discovery to make the file discoverable through search.\nOptions\nEmail Message: A plain text custom message to include in the notification email.\nMove to New Owners Root: Available when trying to transfer ownership while sharing an item not in a shared drive. When enabled, moves the folder to the new owner's My Drive root folder.\nSend Notification Email: Whether to send a notification email when sharing to users or groups.\nTransfer Ownership: Whether to transfer ownership to the specified user and downgrade the current owner to writer permissions.\nUse Domain Admin Access: Whether to perform the action as a domain administrator.\nRefer to the REST Resources: files | Google Drive API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googledrive\\index.md",
    "content": "Google Drive node\nUse the Google Drive node to automate work in Google Drive, and integrate Google Drive with other applications. n8n has built-in support for a wide range of Google Drive features, including creating, updating, listing, deleting, and getting drives, files, and folders.\nOn this page, you'll find a list of operations the Google Drive node supports and links to more resources.\nOperations\nFile\nCopy a file\nCreate from text\nDelete a file\nDownload a file\nMove a file\nShare a file\nUpdate a file\nUpload a file\nFile/Folder\nSearch files and folders\nFolder\nCreate a folder\nDelete a folder\nShare a folder\nShared Drive\nCreate a shared drive\nDelete a shared drive\nGet a shared drive\nGet Many shared drives\nUpdate a shared drive\nTemplates and examples\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googledrive\\shared-drive-operations.md",
    "content": "Google Drive Shared Drive operations\nUse this operation to create, delete, get, and update shared drives in Google Drive. Refer to Google Drive for more information on the Google Drive node itself.\nCreate a shared drive\nUse this operation to create a new shared drive.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Drive credentials.\nResource: Select Shared Drive.\nOperation: Select Create.\nName: The name to use for the new shared drive.\nOptions\nCapabilities: The capabilities to set for the new shared drive (see REST Resources: drives TABLE_PLACEHOLDER_0    - Admin Managed Restrictions: When enabled, restrictions here will override the similarly named fields to true for any file inside of this shared drive.\nCopy Requires Writer Permission: Whether the options to copy, print, or download files inside this shared drive should be disabled for readers and commenters.\nDomain Users Only: Whether to restrict access to this shared drive and items inside this shared drive to users of the domain to which this shared drive belongs.\nDrive Members Only: Whether to restrict access to items inside this shared drive to its members.\nRefer to the Method: drives.update | Google Drive API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlesheets\\common-issues.md",
    "content": "Google Sheets node common issues\nHere are some common errors and issues with the Google Sheets node and steps to resolve or troubleshoot them.\nAppend an array\nTo insert an array of data into Google Sheets, you must convert the array into a valid JSON (key, value) format.\nTo do so, consider using:\nThe Split Out node.\nThe AI Transform node. For example, try entering something like:\nThe Code node.\nColumn names were updated after the node's setup\nYou'll receive this error if the Google Sheet's column names have changed since you set up the node.\nTo refresh the column names, re-select Mapping Column Mode. This should prompt the node to fetch the column names again.\nOnce the column names refresh, update the node parameters."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlesheets\\document-operations.md",
    "content": "Google Sheets Document operations\nUse this operation to create or delete a Google spreadsheet from Google Sheets. Refer to Google Sheets for more information on the Google Sheets node itself.\nCreate a spreadsheet\nUse this operation to create a new spreadsheet.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Sheets credentials.\nResource: Select Document.\nOperation: Select Create.\nTitle: Enter the title of the new spreadsheet you want to create.\nSheets: Add the Title(s) of the sheet(s) you want to create within the spreadsheet.\nOptions\nLocale: Enter the locale of the spreadsheet. This affects formatting details such as functions, dates, and currency. Use one of the following formats:\nen (639-1)\nfil (639-2 if no 639-1 format exists)\nen_US (combination of ISO language and country).\nRefer to List of ISO 639 language codes and List of ISO 3166 country codes for language and country codes. Note that Google doesn't support all locales/languages.\nRecalculation Interval: Enter the desired recalculation interval for the spreadsheet functions. This affects how often NOW, TODAY, RAND, and RANDBETWEEN are updated. Select On Change for recalculating whenever there is a change in the spreadsheet, Minute for recalculating every minute, or Hour for recalculating every hour. Refer to Set a spreadsheet‚Äôs location & calculation settings for more information about these options.\nRefer to the Method: spreadsheets.create | Google Sheets API documentation for more information.\nDelete a spreadsheet\nUse this operation to delete an existing spreadsheet.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Sheets credentials.\nResource: Select Document.\nOperation: Select Delete.\nDocument: Choose a spreadsheet you want to delete.\nSelect From list to choose the title from the dropdown list, By URL to enter the url of the spreadsheet, or By ID to enter the spreadsheetId.\nYou can find the spreadsheetId in a Google Sheets URL:\nRefer to the Method: files.delete | Google Drive API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlesheets\\index.md",
    "content": "Google Sheets\nUse the Google Sheets node to automate work in Google Sheets, and integrate Google Sheets with other applications. n8n has built-in support for a wide range of Google Sheets features, including creating, updating, deleting, appending, removing and getting documents.\nOn this page, you'll find a list of operations the Google Sheets node supports and links to more resources.\nOperations\nDocument\nCreate a spreadsheet.\nDelete a spreadsheet.\nSheet Within Document\nAppend or Update Row: Append a new row, or update the current one if it already exists.\nAppend Row: Create a new row.\nClear all data from a sheet.\nCreate a new sheet.\nDelete a sheet.\nDelete Rows or Columns: Delete columns and rows from a sheet.\nGet Row(s): Read all rows in a sheet.\nUpdate Row: Update a row in a sheet.\nTemplates and examples\nRelated resources\nRefer to Google Sheet's API documentation for more information about the service.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.googlesheets\\sheet-operations.md",
    "content": "Google Sheets Sheet Within Document operations\nUse this operation to create, update, clear or delete a sheet in a Google spreadsheet from Google Sheets. Refer to Google Sheets for more information on the Google Sheets node itself.\nAppend or Update Row\nUse this operation to update an existing row or add a new row at the end of the data if a matching entry isn't found in a sheet.\nEnter these parameters:\nCredential to connect with: Create or select an existing Google Sheets credentials.\nResource: Select Sheet Within Document.\nOperation: Select Append or Update Row.\nDocument: Choose a spreadsheet that contains the sheet you want to append or update row(s) to.\nSelect From list to choose the spreadsheet title from the dropdown list, By URL to enter the url of the spreadsheet, or By ID to enter the spreadsheetId.\nYou can find the spreadsheetId in a Google Sheets URL:\nSheet: Choose a sheet you want to append or update row(s) to.\nSelect From list to choose the sheet title from the dropdown list, By URL to enter the url of the sheet, By ID to enter the sheetId, or By Name to enter the sheet title.\nYou can find the sheetId in a Google Sheets URL:\nMapping Column Mode:\nMap Each Column Manually: Enter Values to Send for each column.\nMap Automatically: n8n looks for incoming data that matches the columns in Google Sheets automatically. In this mode, make sure the incoming data fields are the same as the columns in Google Sheets. (Use an Edit Fields node before this node to change them if required.)\nNothing: Don't map any data.\nOptions\nRefer to the Method: spreadsheets.values.update TABLE_PLACEHOLDER_0    - Let Google Sheets format (default): n8n formats text and numbers in the cells according to Google Sheets' default settings.\n- Let n8n format: New cells in your sheet will have the same data types as the input data provided by n8n.\n- Data Location on Sheet: Use this option when you need to specify where the data range on your sheet.\n- Header Row: Specify the row index that contains the column headers.\n- First Data Row: Specify the row index where the actual data starts.\nRefer to the Method: spreadsheets.batchUpdate | Google Sheets API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mysql\\common-issues.md",
    "content": "MySQL node common issues\nHere are some common errors and issues with the MySQL node and steps to resolve or troubleshoot them.\nUpdate rows by composite key\nThe MySQL node's Update operation lets you to update rows in a table by providing a Column to Match On and a value. This works for tables where single column values can uniquely identify individual rows.\nYou can't use this pattern for tables that use composite keys, where you need multiple columns to uniquely identify a row. A example of this is MySQL's user table in the mysql database, where you need both the user and host columns to uniquely identify rows.\nTo update tables with composite keys, write the query manually with the Execute SQL operation instead. There, you can match on multiple values, like in this example which matches on both customer_id and product_id:\nCan't connect to a local MySQL server when using Docker\nWhen you run either n8n or MySQL in Docker, you need to configure the network so that n8n can connect to MySQL.\nThe solution depends on how you're hosting the two components.\nIf only MySQL is in Docker\nIf only MySQL is running in Docker, configure MySQL to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way).\nWhen running the container, publish the port with the -p flag. By default, MySQL runs on port 3306, so your Docker command should look like this:\nWhen configuring MySQL credentials, the localhost address should work without a problem (set the Host to localhost).\nIf only n8n is in Docker\nIf only n8n is running in Docker, configure MySQL to listen on all interfaces by binding to 0.0.0.0 on the host.\nIf you are running n8n in Docker on Linux, use the --add-host flag to map host.docker.internal to host-gateway when you start the container. For example:\nIf you are using Docker Desktop, this is automatically configured for you.\nWhen configuring MySQL credentials, use host.docker.internal as the Host address instead of localhost.\nIf MySQL and n8n are running in separate Docker containers\nIf both n8n and MySQL are running in Docker in separate containers, you can use Docker networking to connect them.\nConfigure MySQL to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way). Add both the MySQL and n8n containers to the same user-defined bridge network.\nWhen configuring MySQL credentials, use the MySQL container's name as the host address instead of localhost. For example, if you call the MySQL container my-mysql, you would set the Host to my-mysql.\nIf MySQL and n8n are running in the same Docker container\nIf MySQL and n8n are running in the same Docker container, the localhost address doesn't need any special configuration. You can configure MySQL to listen on localhost and configure the Host in the MySQL credentials in n8n to use localhost.\nDecimal numbers returned as strings\nBy default, the MySQL node returns DECIMAL values as strings. This is done intentionally to avoid loss of precision that can occur due to limitation with the way JavaScript represents numbers. You can learn more about the decision in the documentation for the MySQL library that n8n uses.\nTo output decimal values as numbers instead of strings and ignore the risks in loss of precision, enable the Output Decimals as Numbers option. This will output the values as numbers instead of strings.\nAs an alternative, you can manually convert from the string to a decimal using the toFloat() function with toFixed() or with the Edit Fields (Set) node after the MySQL node. Be aware that you may still need to account for a potential loss of precision."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.mysql\\index.md",
    "content": "MySQL node\nUse the MySQL node to automate work in MySQL, and integrate MySQL with other applications. n8n has built-in support for a wide range of MySQL features, including executing an SQL query, as well as inserting, and updating rows in a database.\nOn this page, you'll find a list of operations the MySQL node supports and links to more resources.\nOperations\nDelete\nExecute SQL\nInsert\nInsert or Update\nSelect\nUpdate\nTemplates and examples\nRelated resources\nRefer to MySQL's Connectors and APIs documentation for more information about the service.\nRefer to MySQL's SELECT statement documentation for more information on writing SQL queries.\nUse query parameters\nWhen creating a query to run on a MySQL database, you can use the Query Parameters field in the Options section to load data into the query. n8n sanitizes data in query parameters, which prevents SQL injection.\nFor example, you want to find a person by their email address. Given the following input data:\nYou can write a query like:\nThen in Query Parameters, provide the field values to use. You can provide fixed values or expressions. For this example, use expressions so the node can pull the email address from each input item in turn:\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.notion\\common-issues.md",
    "content": "Notion node common issues\nHere are some common errors and issues with the Notion node and steps to resolve or troubleshoot them.\nRelation property not displaying\nThe Notion node only supports displaying the data relation property for two-way relations. When you connect two Notion databases with a two-way relationship, you can select or filter by the relation property when working with the Notion node's Database Page resource.\nTo enable two-way relations, edit the relation property in Notion and enable the Show on [name of related database] option to create a reverse relation. Select a name to use for the relation in the new context. The relation is now accessible in n8n when filtering or selecting.\nIf you need to work with Notion databases with one-way relationship, you can use the HTTP Request with your existing Notion credentials. For example, to update a one-way relationship, you can send a PATCH request to the following URL:\nEnable Send Body, set the Body Content Type to JSON, and set Specify Body to Using JSON.  Afterward, you can enter a JSON object like the following into the JSON field:\nCreate toggle heading\nThe Notion node allows you to create headings and toggles when adding blocks to Page, Database Page, or Block resources. Creating toggleable headings isn't yet supported by the Notion node itself.\nYou can work around this be creating a regular heading and then modifying it to enable the is_toggleable property:\nAdd a heading with Notion node.\nSelect the resource you want to add a heading to:\nTo add a new page with a heading, select the Page or Database Page resources with the Create operation.\nTo add a heading to an existing page, select the Block resource with the Append After operation.\nSelect Add Block and set the Type Name or ID to either Heading 1, Heading 2, or Heading 3.\nAdd an HTTP Request node connected to the Notion node and select the GET method.\nSet the URL to  For example, if your added the heading to an existing page, you could use the following URL:  $json.results[0].id }}. If you created a new page instead of appending a block, you may need to discover the block ID by querying the page contents first.\nSelect Predefined Credential Type and connect your existing Notion credentials.\nAdd an Edit Fields (Set) node after the HTTP Request node.\nAdd heading_1.is_toggleable as a new Boolean field set to true. Swap heading_1 for a different heading number as necessary.\nAdd a second HTTP Request node after the Edit Fields (Set) node.\nSet the Method to PATCH and use  $json.id }} as the URL value.\nSelect Predefined Credential Type and connect your existing Notion credentials.\nEnable Send Body and set a parameter.\nSet the parameter Name to heading_1 (substitute heading_1 for the heading level you are using).\nSet the parameter Value to {{ $json.heading_1 }} (substitute heading_1 for the heading level you are using).\nThe above sequence will create a regular heading block. It will query the newly created header, add the is_toggleable property, and update the heading block.\nHandle null and empty values\nYou may receive a validation error when working with the Notion node if you submit fields with empty or null values. This can occur any time you populate fields from previous nodes when that data is missing.\nTo work around this, check for the existence of the field data before sending it to Notion or use a default value.\nTo check for the data before executing the Notion node, use an If node to check whether the field is unset. This allows you to use the Edit Fields (Set) node to conditionally remove the field when it doesn't have a valid value.\nAs an alternative, you can set a default value if the incoming data doesn't provide one."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.notion\\index.md",
    "content": "Notion node\nUse the Notion node to automate work in Notion, and integrate Notion with other applications. n8n has built-in support for a wide range of Notion features, including getting and searching databases, creating pages, and getting users.\nOn this page, you'll find a list of operations the Notion node supports and links to more resources.\nOperations\nBlock\nAppend After\nGet Child Blocks\nDatabase\nGet\nGet Many\nSearch\nDatabase Page\nCreate\nGet\nGet Many\nUpdate\nPage\nArchive\nCreate\nSearch\nUser\nGet\nGet Many\nTemplates and examples\nRelated resources\nn8n provides an app node for Notion. You can find the trigger node docs here.\nRefer to Notion's documentation for details about their API.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.postgres\\common-issues.md",
    "content": "Postgres node common issues\nHere are some common errors and issues with the Postgres node and steps to resolve or troubleshoot them.\nDynamically populate SQL IN groups with parameters\nIn Postgres, you can use the SQL IN comparison construct to make comparisons between groups of values:\nWhile you can use n8n expressions in your query to dynamically populate the values in an IN group, combining this with query parameters provides extra protection by automatically sanitizing input.\nTo construct an IN group query with query parameters:\nSet the Operation to Execute Query.\nIn Options, select Query Parameters.\nUse an expression to select an array from the input data. For example, {{ $json.input_shirt_sizes }}.\nIn the Query parameter, write your query with the IN construct with an empty set of parentheses. For example:\nInside of the IN parentheses, use an expression to dynamically create index-based placeholders (like $1, $2, and $3) for the number of items in your query parameter array. You can do this by increasing each array index by one since the placeholder variables are 1 indexed:\nWith this technique, n8n automatically creates the correct number of prepared statement placeholders for the IN values according to the number of items in your array.\nWorking with timestamps and time zones\nTo avoid complications with how n8n and Postgres interpret timestamp and time zone data, follow these general tips:\nUse UTC when storing and passing dates: Using UTC helps avoid confusion over timezone conversions when converting dates between different representations and systems.\nSet the execution timezone: Set the global timezone in n8n using either environment variables (for self-hosted) or in the settings (for n8n Cloud). You can set a workflow-specific timezone in the workflow settings.\nUse ISO 8601 format: The ISO 8601 format encodes the day of the month, month, year, hour, minutes, and seconds in a standardized string. n8n passes dates between nodes as strings and uses Luxon to parse dates. If you need to cast to ISO 8601 explicitly, you can use the Date & Time node and a custom format set to the string yyyy-MM-dd'T'HH:mm:ss.\nOutputting Date columns as date strings instead of ISO datetime strings\nn8n's uses the pg package to integrate with Postgres, which affects how n8n processes date, timestamp, and related types from Postgres.\nThe pg package parses DATE values into new Date(row_value) by default, which produces a date that follows the ISO 8601 datetime string format. For example, a date of 2025-12-25 might produce a datetime sting of 2025-12-25T23:00:00.000Z depending on the instance's timezone settings.\nTo work around this, use the Postgres TO_CHAR function to format the date into the expected format at query time:\nThis will produce the date as a string without the time or timezone components. To continue the earlier example, with this casting, a date of 2025-12-25 would produce the string 2025-12-25. You can find out more in the pg package documentation on dates."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.postgres\\index.md",
    "content": "Postgres node\nUse the Postgres node to automate work in Postgres, and integrate Postgres with other applications. n8n has built-in support for a wide range of Postgres features, including executing queries, as well as inserting and updating rows in a database.\nOn this page, you'll find a list of operations the Postgres node supports and links to more resources.\nOperations\nDelete: Delete an entire table or rows in a table\nExecute Query: Execute an SQL query\nInsert: Insert rows in a table\nInsert or Update: Insert or update rows in a table\nSelect: Select rows from a table\nUpdate: Update rows in a table\nDelete\nUse this operation to delete an entire table or rows in a table.\nEnter these parameters:\nCredential to connect with: Create or select an existing Postgres credential.\nOperation: Select Delete.\nSchema: Choose the schema that contains the table you want to work on. Select From list to choose the schema from the dropdown list or By Name to enter the schema name.\nTable: Choose the table that you want to work on. Select From list to choose the table from the dropdown list or By Name to enter the table name.\nCommand: The deletion action to take:\nTruncate: Removes the table's data but preserves the table's structure.\nRestart Sequences: Whether to reset auto increment columns to their initial values as part of the Truncate process.\nDelete: Delete the rows that match the \"Select Rows\" condition. If you don't select anything, Postgres deletes all rows.\nSelect Rows: Define a Column, Operator, and Value to match rows on.\nCombine Conditions: How to combine the conditions in \"Select Rows\". AND requires all conditions to be true, while OR requires at least one condition to be true.\nDrop: Deletes the table's data and structure permanently.\nDelete options\nCascade: Whether to also drop all objects that depend on the table, like views and sequences. Available if using Truncate or Drop commands.\nConnection Timeout: The number of seconds to try to connect to the database.\nDelay Closing Idle Connection: The number of seconds to wait before considering idle connections eligible for closing.\nQuery Batching: The way to send queries to the database:\nSingle Query: A single query for all incoming items.\nIndependently: Execute one query per incoming item of the execution.\nTransaction: Execute all queries in a transaction. If a failure occurs, Postgres rolls back all changes.\nOutput Large-Format Numbers As: The format to output NUMERIC and BIGINT columns as:\nNumbers: Use this for standard numbers.\nText: Use this if you expect numbers longer than 16 digits. Without this, numbers may be incorrect.\nExecute Query\nUse this operation to execute an SQL query.\nEnter these parameters:\nCredential to connect with: Create or select an existing Postgres credential.\nOperation: Select Execute Query.\nQuery: The SQL query to execute. You can use n8n expressions and tokens like $1, $2, and $3 to build prepared statements to use with query parameters.\nExecute Query options\nConnection Timeout: The number of seconds to try to connect to the database.\nDelay Closing Idle Connection: The number of seconds to wait before considering idle connections eligible for closing.\nQuery Batching: The way to send queries to the database:\nSingle Query: A single query for all incoming items.\nIndependently: Execute one query per incoming item of the execution.\nTransaction: Execute all queries in a transaction. If a failure occurs, Postgres rolls back all changes.\nQuery Parameters: A comma-separated list of values that you want to use as query parameters.\nOutput Large-Format Numbers As: The format to output NUMERIC and BIGINT columns as:\nNumbers: Use this for standard numbers.\nText: Use this if you expect numbers longer than 16 digits. Without this, numbers may be incorrect.\nReplace Empty Strings with NULL: Whether to replace empty strings with NULL in input. This may be useful when working with data exported from spreadsheet software.\nInsert\nUse this operation to insert rows in a table.\nEnter these parameters:\nCredential to connect with: Create or select an existing Postgres credential.\nOperation: Select Insert.\nSchema: Choose the schema that contains the table you want to work on. Select From list to choose the schema from the dropdown list or By Name to enter the schema name.\nTable: Choose the table that you want to work on. Select From list to choose the table from the dropdown list or By Name to enter the table name.\nMapping Column Mode: How to map column names to incoming data:\nMap Each Column Manually: Select the values to use for each column.\nMap Automatically: Automatically map incoming data to matching column names in Postgres. The incoming data field names must match the column names in Postgres for this to work. If necessary, consider using the edit fields (set) node before this node to adjust the format as needed.\nInsert options\nConnection Timeout: The number of seconds to try to connect to the database.\nDelay Closing Idle Connection: The number of seconds to wait before considering idle connections eligible for closing.\nQuery Batching: The way to send queries to the database:\nSingle Query: A single query for all incoming items.\nIndependently: Execute one query per incoming item of the execution.\nTransaction: Execute all queries in a transaction. If a failure occurs, Postgres rolls back all changes.\nOutput Columns: Choose which columns to output. You can select from a list of available columns or specify IDs using expressions.\nOutput Large-Format Numbers As: The format to output NUMERIC and BIGINT columns as:\nNumbers: Use this for standard numbers.\nText: Use this if you expect numbers longer than 16 digits. Without this, numbers may be incorrect.\nSkip on Conflict: Whether to skip the row if the insert violates a unique or exclusion constraint instead of throwing an error.\nReplace Empty Strings with NULL: Whether to replace empty strings with NULL in input. This may be useful when working with data exported from spreadsheet software.\nInsert or Update\nUse this operation to insert or update rows in a table.\nEnter these parameters:\nCredential to connect with: Create or select an existing Postgres credential.\nOperation: Select Insert or Update.\nSchema: Choose the schema that contains the table you want to work on. Select From list to choose the schema from the dropdown list or By Name to enter the schema name.\nTable: Choose the table that you want to work on. Select From list to choose the table from the dropdown list or By Name to enter the table name.\nMapping Column Mode: How to map column names to incoming data:\nMap Each Column Manually: Select the values to use for each column.\nMap Automatically: Automatically map incoming data to matching column names in Postgres. The incoming data field names must match the column names in Postgres for this to work. If necessary, consider using the edit fields (set) node before this node to adjust the format as needed.\nInsert or Update options\nConnection Timeout: The number of seconds to try to connect to the database.\nDelay Closing Idle Connection: The number of seconds to wait before considering idle connections eligible for closing.\nQuery Batching: The way to send queries to the database:\nSingle Query: A single query for all incoming items.\nIndependently: Execute one query per incoming item of the execution.\nTransaction: Execute all queries in a transaction. If a failure occurs, Postgres rolls back all changes.\nOutput Columns: Choose which columns to output. You can select from a list of available columns or specify IDs using expressions.\nOutput Large-Format Numbers As: The format to output NUMERIC and BIGINT columns as:\nNumbers: Use this for standard numbers.\nText: Use this if you expect numbers longer than 16 digits. Without this, numbers may be incorrect.\nReplace Empty Strings with NULL: Whether to replace empty strings with NULL in input. This may be useful when working with data exported from spreadsheet software.\nSelect\nUse this operation to select rows in a table.\nEnter these parameters:\nCredential to connect with: Create or select an existing Postgres credential.\nOperation: Select Select.\nSchema: Choose the schema that contains the table you want to work on. Select From list to choose the schema from the dropdown list or By Name to enter the schema name.\nTable: Choose the table that you want to work on. Select From list to choose the table from the dropdown list or By Name to enter the table name.\nReturn All: Whether to return all results or only up to a given limit.\nLimit: The maximum number of items to return when Return All is disabled.\nSelect Rows: Set the conditions to select rows. Define a Column, Operator, and Value to match rows on. If you don't select anything, Postgres selects all rows.\nCombine Conditions: How to combine the conditions in Select Rows. AND requires all conditions to be true, while OR requires at least one condition to be true.\nSort: Choose how to sort the selected rows. Choose a Column from a list or by ID and a sort Direction.\nSelect options\nConnection Timeout: The number of seconds to try to connect to the database.\nDelay Closing Idle Connection: The number of seconds to wait before considering idle connections eligible for closing.\nQuery Batching: The way to send queries to the database:\nSingle Query: A single query for all incoming items.\nIndependently: Execute one query per incoming item of the execution.\nTransaction: Execute all queries in a transaction. If a failure occurs, Postgres rolls back all changes.\nOutput Columns: Choose which columns to output. You can select from a list of available columns or specify IDs using expressions.\nOutput Large-Format Numbers As: The format to output NUMERIC and BIGINT columns as:\nNumbers: Use this for standard numbers.\nText: Use this if you expect numbers longer than 16 digits. Without this, numbers may be incorrect.\nUpdate\nUse this operation to update rows in a table.\nEnter these parameters:\nCredential to connect with: Create or select an existing Postgres credential.\nOperation: Select Update.\nSchema: Choose the schema that contains the table you want to work on. Select From list to choose the schema from the dropdown list or By Name to enter the schema name.\nTable: Choose the table that you want to work on. Select From list to choose the table from the dropdown list or By Name to enter the table name.\nMapping Column Mode: How to map column names to incoming data:\nMap Each Column Manually: Select the values to use for each column.\nMap Automatically: Automatically map incoming data to matching column names in Postgres. The incoming data field names must match the column names in Postgres for this to work. If necessary, consider using the edit fields (set) node before this node to adjust the format as needed.\nUpdate options\nConnection Timeout: The number of seconds to try to connect to the database.\nDelay Closing Idle Connection: The number of seconds to wait before considering idle connections eligible for closing.\nQuery Batching: The way to send queries to the database:\nSingle Query: A single query for all incoming items.\nIndependently: Execute one query per incoming item of the execution.\nTransaction: Execute all queries in a transaction. If a failure occurs, Postgres rolls back all changes.\nOutput Columns: Choose which columns to output. You can select from a list of available columns or specify IDs using expressions.\nOutput Large-Format Numbers As: The format to output NUMERIC and BIGINT columns as:\nNumbers: Use this for standard numbers.\nText: Use this if you expect numbers longer than 16 digits. Without this, numbers may be incorrect.\nReplace Empty Strings with NULL: Whether to replace empty strings with NULL in input. This may be useful when working with data exported from spreadsheet software.\nTemplates and examples\nRelated resources\nn8n provides a trigger node for Postgres. You can find the trigger node docs here.\nUse query parameters\nWhen creating a query to run on a Postgres database, you can use the Query Parameters field in the Options section to load data into the query. n8n sanitizes data in query parameters, which prevents SQL injection.\nFor example, you want to find a person by their email address. Given the following input data:\nYou can write a query like:\nThen in Query Parameters, provide the field values to use. You can provide fixed values or expressions. For this example, use expressions so the node can pull the email address from each input item in turn:\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.supabase\\common-issues.md",
    "content": "Supabase node common issues\nHere are some common errors and issues with the Supabase node and steps to resolve or troubleshoot them.\nFiltering rows by metadata\nTo filter rows by Supabase metadata, set the Select Type to String.\nFrom there, you can construct a query in the Filters (String) parameter to filter the metadata using the Supabase metadata query language, inspired by the MongoDB selectors format. Access the metadata properties using the Postgres ->> arrow JSON operator like this (curly brackets denote components to fill in):\nFor example to access an age property in the metadata and return results greater than or equal to 21, you could enter the following in the Filters (String) field:\nYou can combine these operators to construct more complex queries.\nCan't connect to a local Supabase database when using Docker\nWhen you run Supabase in Docker, you need to configure the network so that n8n can connect to Supabase.\nThe solution depends on how you're hosting the two components.\nIf only Supabase is in Docker\nIf only Supabase is running in Docker, the Docker Compose file used by the self-hosting guide already runs Supabase bound to the correct interfaces.\nWhen configuring Supabase credentials, the localhost address should work without a problem (set the Host to localhost).\nIf Supabase and n8n are running in separate Docker containers\nIf both n8n and Supabase are running in Docker in separate containers, you can use Docker networking to connect them.\nConfigure Supabase to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official Docker compose configuration already does this this). Add both the Supabase and n8n components to the same user-defined bridge network if you aren't already managing them together in the same Docker Compose file.\nWhen configuring Supabase credentials, use the Supabase API gateway container's name (supabase-kong by default) as the host address instead of localhost. For example, if you use the default configuration, you would set the Host to\nRecords are accessible through Postgres but not Supabase\nIf queries for records return empty using the Supabase node, but are available through the Postgres node or with a Postgres client, there may be a conflict with Supabase's Row Level Security (RLS) policy.\nSupabase always enables RLS when you create a table in a public schema with the Table Editor. When RLS is active, the API doesn't return any data with the public anon key until you create policies. This is a security measure to ensure that you only expose data you intend to.\nTo access data from a table with RLS enabled as the anon role, create a policy to enable the access patterns you intend to use."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.supabase\\index.md",
    "content": "Supabase node\nUse the Supabase node to automate work in Supabase, and integrate Supabase with other applications. n8n has built-in support for a wide range of Supabase features, including creating, deleting, and getting rows.\nOn this page, you'll find a list of operations the Supabase node supports and links to more resources.\nOperations\nRow\nCreate a new row\nDelete a row\nGet a row\nGet all rows\nUpdate a row\nTemplates and examples\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.telegram\\callback-operations.md",
    "content": "Telegram node Callback operations\nUse these operations to respond to callback queries sent from the in-line keyboard or in-line queries. Refer to Telegram for more information on the Telegram node itself.\nAnswer Query\nUse this operation to send answers to callback queries sent from inline keyboards using the Bot API answerCallbackQuery method.\nEnter these parameters:\nCredential to connect with: Create or select an existing Telegram credential.\nResource: Select Callback.\nOperation: Select Answer Query.\nQuery ID: Enter the unique identifier of the query you want to answer.\nTo feed a Query ID directly into this node, use the Telegram Trigger node triggered on the Callback Query.\nResults: Enter a JSON-serialized array of results you want to use as answers to the query. Refer to the Telegram InlineQueryResults documentation for more information on formatting your array.\nRefer to the Telegram Bot API answerCallbackQuery documentation for more information.\nAnswer Query additional fields\nUse the Additional Fields to further refine the behavior of the node. Select Add Field to add any of the following:\nCache Time: Enter the maximum amount of time in seconds that the client may cache the result of the callback query. Telegram defaults to 0 seconds for this method.\nShow Alert: Telegram can display the answer as a notification at the top of the chat screen or as an alert. Choose whether you want to keep the default notification display (turned off) or display the answer as an alert (turned on).\nText: If you want the answer to show text, enter up to 200 characters of text here.\nURL: Enter a URL that will be opened by the user's client. Refer to the url parameter instructions at the Telegram Bot API answerCallbackQuery documentation for more information.\nAnswer Inline Query\nUse this operation to send answers to callback queries sent from inline queries using the Bot API answerInlineQuery method.\nEnter these parameters:\nCredential to connect with: Create or select an existing Telegram credential.\nResource: Select Callback.\nOperation: Select Answer Inline Query.\nQuery ID: Enter the unique identifier of the query you want to answer.\nTo feed a Query ID directly into this node, use the Telegram Trigger node triggered on the Inline Query.\nResults: Enter a JSON-serialized array of results you want to use as answers to the query. Refer to the Telegram InlineQueryResults documentation for more information on formatting your array.\nTelegram allows a maximum of 50 results per query.\nRefer to the Telegram Bot API answerInlineQuery documentation for more information.\nAnswer Inline Query additional fields\nUse the Additional Fields to further refine the behavior of the node. Select Add Field to add any of the following:\nCache Time: The maximum amount of time in seconds that the client may cache the result of the callback query. Telegram defaults to 300 seconds for this method.\nShow Alert: Telegram can display the answer as a notification at the top of the chat screen or as an alert. Choose whether you want to keep the default notification display (turned off) or display the answer as an alert (turned on).\nText: If you want the answer to show text, enter up to 200 characters of text here.\nURL: Enter a URL that the user's client will open."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.telegram\\chat-operations.md",
    "content": "Telegram node Chat operations\nUse these operations to get information about chats, members, administrators, leave chat, and set chat titles and descriptions. Refer to Telegram for more information on the Telegram node itself.\nGet Chat\nUse this operation to get up to date information about a chat using the Bot API getChat method.\nEnter these parameters:\nCredential to connect with: Create or select an existing Telegram credential.\nResource: Select Chat.\nOperation: Select Get.\nChat ID: Enter the Chat ID or username of the target channel in the format @channelusername.\nTo feed a Chat ID directly into this node, use the Telegram Trigger node. Refer to Common Issues TABLE_PLACEHOLDER_0 Title*: Enter the new title you'd like to set the chat to use, maximum of 255 characters.\nRefer to the Telegram Bot API setChatTitle documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.telegram\\common-issues.md",
    "content": "Telegram node common issues\nHere are some common errors and issues with the Telegram node and steps to resolve or troubleshoot them.\nAdd a bot to a Telegram channel\nFor a bot to send a message to a channel, you must add the bot to the channel. If you haven't added the bot to the channel, you'll see an error with a description like:\nError: Forbidden: bot is not a participant of the channel.\nTo add a bot to a channel:\nIn the Telegram app, access the target channel and select the channel name.\nLabel the channel name as public channel.\nSelect Administrators > Add Admin.\nSearch for the bot's username and select it.\nSelect the checkmark on the top-right corner to add the bot to the channel.\nGet the Chat ID\nYou can only use @channelusername on public channels. To interact with a Telegram group, you need that group's Chat ID.\nThere are three ways to get that ID:\nFrom the Telegram Trigger: Use the Telegram Trigger node in your workflow to get a Chat ID. This node can trigger on different events and returns a Chat ID on successful execution.\nFrom your web browser: Open Telegram in a web browser and open the group chat. The group's Chat ID is the series of digits behind the letter \"g.\" Prefix your group Chat ID with a - when you enter it in n8n.\nInvite Telegram's @RawDataBot to the group: Once you add it, the bot outputs a JSON file that includes a chat object. The id for that object is the group Chat ID. Then remove the RawDataBot from your group.\nSend more than 30 messages per second\nThe Telegram API has a limitation of sending only 30 messages per second. Follow these steps to send more than 30 messages:\nLoop Over Items node: Use the Loop Over Items node to get at most 30 chat IDs from your database.\nTelegram node: Connect the Telegram node with the Loop Over Items node. Use the Expression Editor to select the Chat IDs from the Loop Over Items node.\nCode node: Connect the Code node with the Telegram node. Use the Code node to wait for a few seconds before fetching the next batch of chat IDs. Connect this node with the Loop Over Items node.\nYou can also use this workflow.\nRemove the n8n attribution from sent messages\nIf you're using the node to send Telegram messages, the message automatically gets an n8n attribution appended to the end:\nThis message was sent automatically with n8n\nTo remove this attribution:\nIn the node's Additional Fields section, select Add Field.\nSelect Append n8n attribution.\nTurn the toggle off.\nRefer to Send Message additional fields for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.telegram\\file-operations.md",
    "content": "Telegram node File operations\nUse this operation to get a file from Telegram. Refer to Telegram for more information on the Telegram node itself.\nGet File\nUse this operation to get a file from Telegram using the Bot API getFile method.\nEnter these parameters:\nCredential to connect with: Create or select an existing Telegram credential.\nResource: Select File.\nOperation: Select Get.\nFile ID: Enter the ID of the file you want to get.\nDownload: Choose whether you want the node to download the file (turned on) or not (turned off).\nRefer to the Telegram Bot API getFile documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.telegram\\index.md",
    "content": "Telegram node\nUse the Telegram node to automate work in Telegram and integrate Telegram with other applications. n8n has built-in support for a wide range of Telegram features, including getting files as well as deleting and editing messages.\nOn this page, you'll find a list of operations the Telegram node supports and links to more resources.\nOperations\nChat operations\nGet up-to-date information about a chat.\nGet Administrators: Get a list of all administrators in a chat.\nGet Member: Get the details of a chat member.\nLeave a chat.\nSet Description of a chat.\nSet Title of a chat.\nCallback operations\nAnswer Query: Send answers to callback queries sent from inline keyboards.\nAnswer Inline Query: Send answers to callback queries sent from inline queries.\nFile operations\nGet File from Telegram.\nMessage operations\nDelete Chat Message.\nEdit Message Text: Edit the text of an existing message.\nPin Chat Message for the chat.\nSend Animation to the chat.\nFor use with GIFs or H.264/MPEG-4 AVC videos without sound up to 50 MB in size.\nSend Audio file to the chat and display it in the music player.\nSend Chat Action: Tell the user that something is happening on the bot's side. The status is set for 5 seconds or less.\nSend Document to the chat.\nSend Location: Send a geolocation to the chat.\nSend Media Group: Send a group of photos and/or videos.\nSend Message to the chat.\nSend Photo to the chat.\nSend Sticker to the chat.\nFor use with static .WEBP, animated .TGS, or video .WEBM stickers.\nSend Video to the chat.\nUnpin Chat Message from the chat.\nTemplates and examples\nRelated resources\nRefer to Telegram's API documentation for more information about the service.\nn8n provides a trigger node for Telegram. Refer to the trigger node docs here for more information.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.telegram\\message-operations.md",
    "content": "Telegram node Message operations\nUse these operations to send, edit, and delete messages in a chat; send files to a chat; and pin/unpin message from a chat. Refer to Telegram for more information on the Telegram node itself.\ns\nDelete Chat Message\nUse this operation to delete a message from chat using the Bot API deleteMessage method.\nEnter these parameters:\nCredential to connect with: Create or select an existing Telegram credential.\nResource: Select Message.\nOperation: Select Delete Chat Message.\nChat ID: Enter the Chat ID or username of the channel you wish to delete in the format @channelusername.\nTo feed a Chat ID directly into this node, use the Telegram Trigger node. Refer to Common Issues TABLE_PLACEHOLDER_0 Message ID*: Enter the unique identifier of the message you want to unpin.\nRefer to the Telegram Bot API unpinChatMessage documentation for more information.\nReply Markup parameters\nFor most of the Message Send actions (such as Send Animation, Send Audio), use the Reply Markup parameter to set more interface options:\nForce Reply: The Telegram client will act as if the user has selected the bot's message and tapped Reply, automatically displaying a reply interface to the user. Refer to Force Reply parameters for further guidance on this option.\nInline Keyboard: Display an inline keyboard right next to the message. Refer to Inline Keyboard parameters for further guidance on this option.\nReply Keyboard: Display a custom keyboard with reply options. Refer to Reply Keyboard parameters for further guidance on this option.\nReply Keyboard Remove: The Telegram client will remove the current custom keyboard and display the default letter-keyboard. Refer to Reply Keyboard parameters for further guidance on this option.\nForce Reply parameters\nForce Reply is useful if you want to create user-friendly step-by-step interfaces without having to sacrifice privacy mode.\nIf you select Reply Markup > Force Reply, choose from these Force Reply parameters:\nForce Reply: Turn on to show the reply interface to the user, as described above.\nSelective: Turn this on if you want to force reply from these users only:\nUsers that are @mentioned in the text of the message.\nThe sender of the original message, if this Send Animation message is a reply to a message.\nRefer to ForceReply for more information.\nInline Keyboard parameters\nIf you select Reply Markup > Inline Keyboard, define the inline keyboard buttons you want to display using the Add Button option. To add more rows to your keyboard, use Add Keyboard Row.\nRefer to InlineKeyboardMarkup and InlineKeyboardButtons for more information.\nReply Keyboard parameters\nIf you select Reply Markup > Reply Keyboard, use the Reply Keyboard section to define the buttons and rows in your Reply Keyboard.\nUse the Reply Keyboard Options to further refine the keyboard's behavior:\nResize Keyboard: Choose whether to request the Telegram client to resize the keyboard vertically for optimal fit (turned on) or whether to use the same height as the app's standard keyboard (turned off).\nOne Time Keyboard: Choose whether the Telegram client should hide the keyboard as soon as a user uses it (turned on) or to keep displaying it (turned off).\nSelective: Turn this on if you want to show the keyboard to these users only:\nUsers that are @mentioned in the text of the message.\nThe sender of the original message, if this Send Animation message is a reply to a message.\nRefer to ReplyKeyboardMarkup for more information.\nReply Keyboard Remove parameters\nIf you select Reply Markup > Reply Keyboard Remove, choose from these Reply Keyboard Remove parameters:\nRemove Keyboard: Choose whether to request the Telegram client to remove the custom keyboard (turned on) or to keep it (turned off).\nSelective: Turn this on if you want to remove the keyboard for these users only:\nUsers that are @mentioned in the text of the message.\nThe sender of the original message, if this Send Animation message is a reply to a message.\nRefer to ReplyKeyboardRemove for more information."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.whatsapp\\common-issues.md",
    "content": "WhatsApp Business Cloud node common issues\nHere are some common errors and issues with the WhatsApp Business Cloud node and steps to resolve or troubleshoot them.\nBad request - please check your parameters\nThis error occurs when WhatsApp Business Cloud rejects your request because of a problem with its parameters. It's common to see this when using the Send Template operation if the data you send doesn't match the format of your template.\nTo resolve this issue, review the parameters in your message template. Pay attention to each parameter's data type and the order they're defined in the template.\nCheck the data that n8n is mapping to the template parameters. If you're using expressions to set parameter values, check the input data to make sure each item resolves to a valid value. You may want to use the Edit Fields (Set) node or set a fallback value to ensure you send a value with the correct format.\nWorking with non-text media\nThe WhatsApp Business Cloud node can work with non-text messages and media like images, audio, documents, and more.\nIf your operation includes a Input Data Field Name or Property Name parameter, set this to the field name itself rather than referencing the data in an expression.\nFor example, if you are trying to send a message with an \"Image\" MessageType and Take Image From set to \"n8n\", set Input Data Field Name to a field name like data instead of an expression like {{ $json.input.data }}."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-base.whatsapp\\index.md",
    "content": "WhatsApp Business Cloud node\nUse the WhatsApp Business Cloud node to automate work in WhatsApp Business, and integrate WhatsApp Business with other applications. n8n has built-in support for a wide range of WhatsApp Business features, including sending messages, and uploading, downloading, and deleting media.\nOn this page, you'll find a list of operations the WhatsApp Business Cloud node supports and links to more resources.\nOperations\nMessage\nSend\nSend and Wait for Response\nSend Template\nMedia\nUpload\nDownload\nDelete\nTemplates and examples\nRelated resources\nRefer to WhatsApp Business Platform's Cloud API documentation for details about the operations.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.openai\\assistant-operations.md",
    "content": "OpenAI Assistant operations\nUse this operation to create, delete, list, message, or update an assistant in OpenAI. Refer to OpenAI for more information on the OpenAI node itself.\nCreate an Assistant\nUse this operation to create a new assistant.\nEnter these parameters:\nCredential to connect with: Create or select an existing OpenAI credential.\nResource: Select Assistant.\nOperation: Select Create an Assistant.\nModel: Select the model that the assistant will use. If you‚Äôre not sure which model to use, try gpt-4o if you need high intelligence or gpt-4o-mini if you need the fastest speed and lowest cost. Refer to Models overview TABLE_PLACEHOLDER_0\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.openai\\audio-operations.md",
    "content": "OpenAI Audio operations\nUse this operation to generate an audio, or transcribe or translate a recording in OpenAI. Refer to OpenAI for more information on the OpenAI node itself.\nGenerate Audio\nUse this operation to create audio from a text prompt.\nEnter these parameters:\nCredential to connect with: Create or select an existing OpenAI credential.\nResource: Select Audio.\nOperation: Select Generate Audio.\nModel: Select the model you want to use to generate the audio. Refer to TTS TABLE_PLACEHOLDER_0\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.openai\\common-issues.md",
    "content": "OpenAI node common issues\nHere are some common errors and issues with the OpenAI node and steps to resolve or troubleshoot them."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.openai\\file-operations.md",
    "content": "OpenAI File operations\nUse this operation to create, delete, list, message, or update a file in OpenAI. Refer to OpenAI for more information on the OpenAI node itself.\nDelete a File\nUse this operation to delete a file from the server.\nEnter these parameters:\nCredential to connect with: Create or select an existing OpenAI credential.\nResource: Select File.\nOperation: Select Delete a File.\nFile: Enter the ID of the file to use for this operation or select the file name from the dropdown.\nRefer to Delete file TABLE_PLACEHOLDER_0\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.openai\\image-operations.md",
    "content": "OpenAI Image operations\nUse this operation to analyze or generate an image in OpenAI. Refer to OpenAI for more information on the OpenAI node itself.\nAnalyze Image\nUse this operation to take in images and answer questions about them.\nEnter these parameters:\nCredential to connect with: Create or select an existing OpenAI credential.\nResource: Select Image.\nOperation: Select Analayze Image.\nModel: Select the model you want to use to generate an image.\nText Input: Ask a question about the image.\nInput Type: Select how you'd like to input the image. Options include:\nImage URL(s): Enter the URL(s) of the image(s) to analyze. Add multiple URLs in a comma-separated list.\nBinary File(s): Enter the name of the binary property which contains the image(s) in the Input Data Field Name.\nOptions\nDetail: Specify the balance between response time versus token usage.\nLength of Description (Max Tokens): Defaults to 300. Fewer tokens will result in shorter, less detailed image description.\nRefer to Images TABLE_PLACEHOLDER_0\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.openai\\index.md",
    "content": "OpenAI node\nUse the OpenAI node to automate work in OpenAI and integrate OpenAI with other applications. n8n has built-in support for a wide range of OpenAI features, including creating images and assistants, as well as chatting with models.\nOn this page, you'll find a list of operations the OpenAI node supports and links to more resources.\nOperations\nAssistant\nCreate an Assistant\nDelete an Assistant\nList Assistants\nMessage an Assistant\nUpdate an Assistant\nText\nMessage a Model\nClassify Text for Violations\nImage\nAnalyze Image\nGenerate an Image\nAudio\nGenerate Audio\nTranscribe a Recording\nTranslate a Recording\nFile\nDelete a File\nList Files\nUpload a File\nTemplates and examples\nRelated resources\nRefer to OpenAI's documentation for more information about the service.\nRefer to OpenAI's assistants documentation for more information about how assistants work.\nFor help dealing with rate limits, refer to Handling rate limits.\nUsing tools with OpenAI assistants\nSome operations allow you to connect tools. Tools act like addons that your AI can use to access extra context or resources.\nSelect the Tools connector to browse the available tools and add them.\nOnce you add a tool connection, the OpenAI node becomes a root node, allowing it to form a cluster node with the tools sub-nodes. See Node types for more information on cluster nodes and root nodes.\nOperations that support tool connectors\nAssistant\nMessage an Assistant\nText\nMessage a Model\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\app-nodes\\n8n-nodes-langchain.openai\\text-operations.md",
    "content": "OpenAI Text operations\nUse this operation to message a model or classify text for violations in OpenAI. Refer to OpenAI for more information on the OpenAI node itself.\nMessage a Model\nUse this operation to send a message or prompt to an OpenAI model and receive a response.\nEnter these parameters:\nCredential to connect with: Create or select an existing OpenAI credential.\nResource: Select Text.\nOperation: Select Message a Model.\nModel: Select the model you want to use. If you‚Äôre not sure which model to use, try gpt-4o if you need high intelligence or gpt-4o-mini if you need the fastest speed and lowest cost. Refer to Models overview TABLE_PLACEHOLDER_0\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\index.md",
    "content": "Cluster nodes\nRoot nodes\nEach cluster starts with one root node.\nSub-nodes\nEach root node can have one or more sub-nodes attached to it."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\index.md",
    "content": "Root nodes\nRoot nodes are the foundational nodes within a group of cluster nodes."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.chainllm.md",
    "content": "Basic LLM Chain node\nUse the Basic LLM Chain node to set the prompt that the model will use along with setting an optional parser for the response.\nOn this page, you'll find the node parameters for the Basic LLM Chain node and links to more resources.\nNode parameters\nPrompt\nRequire Specific Output Format\nChat Messages\nUse Chat Messages when you're using a chat model to set a message.\nn8n ignores these options if you don't connect a chat model. Select the Type Name or ID you want the node to use:\nAI\nEnter a sample expected response in the Message field. The model will try to respond in the same way in its messages.\nSystem\nEnter a system Message to include with the user input to help guide the model in what it should do.\nUse this option for things like defining tone, for example: Always respond talking like a pirate.\nUser\nEnter a sample user input. Using this with the AI option can help improve the output of the agent. Using both together provides a sample of an input and expected response (the AI Message) for the model to follow.\nSelect one of these input types:\nText: Enter a sample user input as a text Message.\nImage (Binary): Select a binary input from a previous node. Enter the Image Data Field Name to identify which binary field from the previous node contains the image data.\nImage (URL): Use this option to feed an image in from a URL. Enter the Image URL.\nFor both the Image types, select the Image Details to control how the model processes the image and generates its textual understanding. Choose from:\nAuto: The model uses the auto setting, which looks at the image input size and decide if it should use the Low or High setting.\nLow: The model receives a low-resolution 512px x 512px version of the image and represents the image with a budget of 65 tokens. This allows the API to return faster responses and consume fewer input tokens. Use this option for use cases that don't require high detail.\nHigh: The model can access the low-resolution image and then creates detailed crops of input images as 512px squares based on the input image size. Each of the detailed crops uses twice the token budget (65 tokens) for a total of 129 tokens. Use this option for use cases that require high detail.\nTemplates and examples\nRelated resources\nRefer to LangChain's documentation on Basic LLM Chains for more information about the service.\nCommon issues\nHere are some common errors and issues with the Basic LLM Chain node and steps to resolve or troubleshoot them.\nNo prompt specified error\nThis error displays when the Prompt is empty or invalid.\nYou might see this error in one of two scenarios:\nWhen you've set the Prompt to Define below and haven't entered anything in the Text field.\nTo resolve, enter a valid prompt in the Text field.\nWhen you've set the Prompt to Connected Chat Trigger Node and the incoming data has no field called chatInput.\nThe node expects the chatInput field. If your previous node doesn't have this field, add an Edit Fields (Set) node to edit an incoming field name to chatInput."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.chainsummarization.md",
    "content": "Summarization Chain node\nUse the Summarization Chain node to summarize multiple documents.\nOn this page, you'll find the node parameters for the Summarization Chain node, and links to more resources.\nNode parameters\nChoose the type of data you need to summarize in Data to Summarize. The data type you choose determines the other node parameters.\nUse Node Input (JSON) and Use Node Input (Binary): summarize the data coming into the node from the workflow.\nYou can configure the Chunking Strategy: choose what strategy to use to define the data chunk sizes.\nIf you choose Simple (Define Below) you can then set Characters Per Chunk and Chunk Overlap (Characters).\nChoose Advanced if you want to connect a splitter sub-node that provides more configuration options.\nUse Document Loader: summarize data provided by a document loader sub-node.\nNode Options\nYou can configure the summarization method and prompts. Select Add Option > Summarization Method and Prompts.\nOptions in Summarization Method:\nMap Reduce: this is the recommended option. Learn more about Map Reduce in the LangChain documentation.\nRefine: learn more about Refine in the LangChain documentation.\nStuff: learn more about Stuff in the LangChain documentation.\nYou can customize the Individual Summary Prompts and the Final Prompt to Combine. There are examples in the node. You must include the \"{text}\" placeholder.\nTemplates and examples\nRelated resources\nRefer to LangChain's documentation on summarization for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.code.md",
    "content": "LangChain Code node\nUse the LangChain Code node to import LangChain. This means if there is functionality you need that n8n hasn't created a node for, you can still use it. By configuring the LangChain Code node connectors you can use it as a normal node, root node or sub-node.\nOn this page, you'll find the node parameters, guidance on configuring the node, and links to more resources.\nNode parameters\nAdd Code\nAdd your custom code. Choose either Execute or Supply Data mode. You can only use one mode.\nUnlike the Code node, the LangChain Code node doesn't support Python.\nExecute: use the LangChain Code node like n8n's own Code node. This takes input data from the workflow, processes it, and returns it as the node output. This mode requires a main input and output. You must create these connections in Inputs and Outputs.\nSupply Data: use the LangChain Code node as a sub-node, sending data to a root node. This uses an output other than main.\nBy default, you can't load built-in or external modules in this node. Self-hosted users can enable built-in and external modules.\nInputs\nChoose the input types.\nThe main input is the normal connector found in all n8n workflows. If you have a main input and output set in the node, Execute code is required.\nOutputs\nChoose the output types.\nThe main output is the normal connector found in all n8n workflows. If you have a main input and output set in the node, Execute code is required.\nNode inputs and outputs configuration\nBy configuring the LangChain Code node connectors (inputs and outputs) you can use it as an app node, root node or sub-node.\n!Screenshot of a workflow with four LangChain nodes, configured as different node types\nTABLE_PLACEHOLDER_0\nBuilt-in methods\nn8n provides these methods to make it easier to perform common tasks in the LangChain Code node.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.information-extractor.md",
    "content": "Information Extractor node\nUse the Information Extractor node to extract structured information from incoming data.\nOn this page, you'll find the node parameters for the Information Extractor node,\nand links to more resources.\nNode parameters\nText defines the input text to extract information from. This is usually an expression that references a field from the input items. For example, this could be {{ $json.chatInput }} if the input is a chat trigger, or {{ $json.text }} if a previous node is Extract from PDF.\nUse Schema Type to choose how you want to describe the desired output data format. You can choose between:\nFrom Attribute Descriptions: This option allows you to define the schema by specifying the list of attributes and their descriptions.\nGenerate From JSON Example: Input an example JSON object to automatically generate the schema. The node uses the object property types and names. It ignores the actual values. n8n treats every field as mandatory when generating schemas from JSON examples.\nDefine using JSON Schema: Manually input the JSON schema. Read the JSON Schema guides and examples for help creating a valid JSON schema.\nNode options\nSystem Prompt Template: Use this option to change the system prompt that's used for the information extraction. n8n automatically appends format specification instructions to the prompt.\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.sentimentanalysis.md",
    "content": "Sentiment Analysis node\nUse the Sentiment Analysis node to analyze the sentiment of incoming text data.\nThe language model uses the Sentiment Categories in the node options to determine each item's sentiment.\nNode parameters\nText to Analyze defines the input text for sentiment analysis. This is an expression that references a field from the input items. For example, this could be\n{{ $json.chatInput }} if the input is from a chat or message source. By default, it expects a text field.\nNode options\nSentiment Categories: Define the categories that you want to classify your input as.\nBy default, these are Positive, Neutral, Negative. You can customize these categories to fit your specific use case, such as Very Positive, Positive, Neutral, Negative, Very Negative for more granular analysis.\nInclude Detailed Results: When turned on, this option includes sentiment strength and confidence scores in the output. Note that these scores are estimates generated by the language model and are rough indicators rather than precise measurements.\nSystem Prompt Template: Use this option to change the system prompt that's used for the sentiment analysis. It uses the {categories} placeholder for the categories.\nEnable Auto-Fixing: When enabled, the node automatically fixes model outputs to ensure they match the expected format. Do this by sending the schema parsing error to the LLM and asking it to fix it.\nUsage Notes\nModel Temperature Setting\nIt's strongly advised to set the temperature of the connected language model to 0 or a value close to 0. This helps ensure that the results are as deterministic as possible, providing more consistent and reliable sentiment analysis across multiple runs.\nLanguage Considerations\nThe node's performance may vary depending on the language of the input text.\nFor best results, ensure your chosen language model supports the input language.\nProcessing Large Volumes\nWhen analyzing large amounts of text, consider splitting the input into smaller chunks to optimize processing time and resource usage.\nIterative Refinement\nFor complex sentiment analysis tasks, you may need to iteratively refine the system prompt and categories to achieve the desired results.\nExample Usage\nBasic Sentiment Analysis\nConnect a data source (for example, RSS Feed, HTTP Request) to the Sentiment Analysis node.\nSet the \"Text to Analyze\" field to the relevant item property (for example, {{ $json.content }} for blog post content).\nKeep the default sentiment categories.\nConnect the node's outputs to separate paths for processing positive, neutral, and negative sentiments differently.\nCustom Category Analysis\nChange the Sentiment Categories to Excited, Happy, Neutral, Disappointed, Angry.\nAdjust your workflow to handle these five output categories.\nUse this setup to analyze customer feedback with more nuanced emotional categories.\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.text-classifier.md",
    "content": "Text Classifier node\nUse the Text Classifier node to classify (categorize) incoming data. Using the categories provided in the parameters (see below), each item is passed to the model to determine its category.\nOn this page, you'll find the node parameters for the Text Classifier node, and links to more resources.\nNode parameters\nInput Prompt defines the input to classify. This is usually an expression that references a field from the input items. For example, this could be {{ $json.chatInput }} if the input is a chat trigger. By default it references the text field.\nCategories: Add the categories that you want to classify your input as. Categories have a name and a description. Use the description to tell the model what the category means. This is important if the meaning isn't obvious. You can add as many categories as you like.\nNode options\nAllow Multiple Classes To Be True: You can configure the classifier to always output a single class per item (turned off), or allow the model to select multiple classes (turned on).\nWhen No Clear Match: Define what happens if the model can't find a good match for an item. There are two options:\nDiscard Item (the default): If the node doesn't detect any of the categories, it drops the item.\nOutput on Extra, 'Other' Branch: Creates a separate output branch called Other. When the node doesn't detect any of the categories, it outputs items in this branch.\nSystem Prompt Template: Use this option to change the system prompt that's used for the classification. It uses the {categories} placeholder for the categories.\nEnable Auto-Fixing: When enabled, the node automatically fixes model outputs to ensure they match the expected format. Do this by sending the schema parsing error to the LLM and asking it to fix it.\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstoreinmemory.md",
    "content": "Simple Vector Store node\nUse the Simple Vector Store node to store and retrieve embeddings in n8n's in-app memory.\nOn this page, you'll find the node parameters for the Simple Vector Store node, and links to more resources.\nData safety limitations\nBefore using the Simple Vector Store node, it's important to understand its limitations and how it works.\nVector store data isn't persistent\nThis node stores data in memory only. All data is lost when n8n restarts and may also be purged in low-memory conditions.\nAll instance users can access vector store data\nMemory keys for the Simple Vector Store node are global, not scoped to individual workflows.\nThis means that all users of the instance can access vector store data by adding a Simple Vector Store node and selecting the memory key, regardless of the access controls set for the original workflow. Take care not to expose sensitive information when ingesting data with the Simple Vector Store node.\nNode usage patterns\nYou can use the Simple Vector Store node in the following patterns.\nUse as a regular node to insert and retrieve documents\nYou can use the Simple Vector Store as a regular node to insert or get documents. This pattern places the Simple Vector Store in the regular connection flow without using an agent.\nYou can see an example of in step 2 of this template.\nConnect directly to an AI agent as a tool\nYou can connect the Simple Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> Simple Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the Simple Vector Store node to fetch documents from the Simple Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nAn example of the connection flow (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Simple Vector Store.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Simple Vector Store node. Rather than connecting the Simple Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nThe connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Simple Vector store.\nMemory Management\nThe Simple Vector Store implements memory management to prevent excessive memory usage:\nAutomatically cleans up old vector stores when memory pressure increases\nRemoves inactive stores that haven't been accessed for a configurable amount of time\nConfiguration Options\nYou can control memory usage with these environment variables:\nTABLE_PLACEHOLDER_0\nOn n8n Cloud, these values are preset to 100MB (about 8,000 documents, depending on document size and metadata) and 7 days respectively. For self-hosted instances, both values default to -1(no memory limits or time-based cleanup).\nNode parameters\nRerank Results\nGet Many parameters\nMemory Key: Select or create the key containing the vector memory you want to query.\nPrompt: Enter the search query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nInsert Documents parameters\nMemory Key: Select or create the key you want to store the vector memory as.\nClear Store: Use this parameter to control whether to wipe the vector store for the given memory key for this workflow before inserting data (turned on).\nRetrieve Documents (As Vector Store for Chain/Tool) parameters\nMemory Key: Select or create the key containing the vector memory you want to query.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nMemory Key: Select or create the key containing the vector memory you want to query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nTemplates and examples\nRelated resources\nRefer to LangChains's Memory Vector Store documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstoremilvus.md",
    "content": "Milvus Vector Store node\nUse the Milvus node to interact with your Milvus database as vector store. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain, or connect directly to an agent as a tool.\nOn this page, you'll find the node parameters for the Milvus node, and links to more resources.\nNode usage patterns\nYou can use the Milvus Vector Store node in the following patterns.\nUse as a regular node to insert and retrieve documents\nYou can use the Milvus Vector Store as a regular node to insert, or get documents. This pattern places the Milvus Vector Store in the regular connection flow without using an agent.\nSee this example template for how to build a system that stores documents in Milvus and retrieves them to support cited, chat-based answers.\nConnect directly to an AI agent as a tool\nYou can connect the Milvus Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> Milvus Vector Store node. See this example template where data is embedded and indexed in Milvus, and the AI Agent uses the vector store as a knowledge tool for question-answering.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the Milvus Vector Store node to fetch documents from the Milvus Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nA typical node connection flow looks like this: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Milvus Vector Store.\nCheck out this workflow example to see how to ingest external data into Milvus and build a chat-based semantic Q&A system.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Milvus Vector Store node. Rather than connecting the Milvus Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nThe connections flow would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Milvus Vector store.\nNode parameters\nRerank Results\nGet Many parameters\nMilvus Collection: Select or enter the Milvus Collection to use.\nPrompt: Enter your search query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nInsert Documents parameters\nMilvus Collection: Select or enter the Milvus Collection to use.\nClear Collection: Specify whether to clear the collection before inserting new documents.\nRetrieve Documents (As Vector Store for Chain/Tool) parameters\nMilvus collection: Select or enter the Milvus Collection to use.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nMilvus Collection: Select or enter the Milvus Collection to use.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nNode options\nMetadata Filter\nClear Collection\nAvailable in Insert Documents mode. Deletes all data from the collection before inserting the new data.\nRelated resources\nRefer to LangChain's Milvus documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstoremongodbatlas.md",
    "content": "MongoDB Atlas Vector Store node\nMongoDB Atlas Vector Search is a feature of MongoDB Atlas that enables users to store and query vector embeddings. Use this node to interact with Vector Search indexes in your MongoDB Atlas collections. You can insert documents, retrieve documents, and use the vector store in chains or as a tool for agents.\nOn this page, you'll find the node parameters for the MongoDB Atlas Vector Store node, and links to more resources.\nPrerequisites\nBefore using this node, create a Vector Search index in your MongoDB Atlas collection. Follow these steps to create one:\nLog in to the MongoDB Atlas dashboard.\nSelect your organization and project.\nFind \"Search & Vector Search\" section.\nSelect your cluster and click \"Go to search\".\nClick \"Create Search Index\".\nChoose \"Vector Search\" mode and use the visual or JSON editors. For example:\nAdjust the \"dimensions\" value according to your embedding model (For example, 1536 for OpenAI's text-embedding-small-3).\nName your index and create.\nMake sure to note the following values which are required when configuring the node:\nCollection name\nVector index name\nField names for embeddings and metadata\nNode usage patterns\nYou can use the MongoDB Atlas Vector Store node in the following patterns:\nUse as a regular node to insert and retrieve documents\nYou can use the MongoDB Atlas Vector Store as a regular node to insert or get documents. This pattern places the MongoDB Atlas Vector Store in the regular connection flow without using an agent.\nYou can see an example of this in scenario 1 of this template (the template uses the Supabase Vector Store, but the pattern is the same).\nConnect directly to an AI agent as a tool\nYou can connect the MongoDB Atlas Vector Store node directly to the tool connector of an AI agent to use the vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> MongoDB Atlas Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the MongoDB Atlas Vector Store node to fetch documents from the MongoDB Atlas Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nAn example of the connection flow (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> MongoDB Atlas Vector Store.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the MongoDB Atlas Vector Store node. Rather than connecting the MongoDB Atlas Vector Store directly as a tool, this pattern uses a tool specifically designed to summarize data in the vector store.\nThe connections flow (the linked example uses the In-Memory Vector Store, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> In-Memory Vector store.\nNode parameters\nRerank Results\nGet Many parameters\nMongo Collection: Enter the name of the MongoDB collection to use.\nVector Index Name: Enter the name of the Vector Search index in your MongoDB Atlas collection.\nEmbedding Field: Enter the field name in your documents that contains the vector embeddings.\nMetadata Field: Enter the field name in your documents that contains the text metadata.\nInsert Documents parameters\nMongo Collection: Enter the name of the MongoDB collection to use.\nVector Index Name: Enter the name of the Vector Search index in your MongoDB Atlas collection.\nEmbedding Field: Enter the field name in your documents that contains the vector embeddings.\nMetadata Field: Enter the field name in your documents that contains the text metadata.\nRetrieve Documents parameters (As Vector Store for Chain/Tool)\nMongo Collection: Enter the name of the MongoDB collection to use.\nVector Index Name: Enter the name of the Vector Search index in your MongoDB Atlas collection.\nEmbedding Field: Enter the field name in your documents that contains the vector embeddings.\nMetadata Field: Enter the field name in your documents that contains the text metadata.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nMongo Collection: Enter the name of the MongoDB collection to use.\nVector Index Name: Enter the name of the Vector Search index in your MongoDB Atlas collection.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nNode options\nOptions\nMetadata Filter: Filters results based on metadata.\nTemplates and examples\nRelated resources\nRefer to:\nLangChain's MongoDB Atlas Vector Search documentation for more information about the service.\nMongoDB Atlas Vector Search documentation for more information about MongoDB Atlas Vector Search."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstorepgvector.md",
    "content": "PGVector Vector Store node\nPGVector is an extension of Postgresql. Use this node to interact with the PGVector tables in your Postgresql database. You can insert documents into a vector table, get documents from a vector table, retrieve documents to provide them to a retriever connected to a chain, or connect directly to an agent as a tool.\nOn this page, you'll find the node parameters for the PGVector node, and links to more resources.\nNode usage patterns\nYou can use the PGVector Vector Store node in the following patterns.\nUse as a regular node to insert and retrieve documents\nYou can use the PGVector Vector Store as a regular node to insert or get documents. This pattern places the PGVector Vector Store in the regular connection flow without using an agent.\nYou can see an example of this in scenario 1 of this template (the template uses the Supabase Vector Store, but the pattern is the same).\nConnect directly to an AI agent as a tool\nYou can connect the PGVector Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> PGVector Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the PGVector Vector Store node to fetch documents from the PGVector Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nAn example of the connection flow (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> PGVector Vector Store.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the PGVector Vector Store node. Rather than connecting the PGVector Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nThe connections flow (the linked example uses the Simple Vector Store, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Simple Vector store.\nNode parameters\nRerank Results\nGet Many parameters\nTable name: Enter the name of the table you want to query.\nPrompt: Enter your search query.\nLimit: Enter a number to set how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nInsert Documents parameters\nTable name: Enter the name of the table you want to query.\nRetrieve Documents parameters (As Vector Store for Chain/Tool)\nTable name: Enter the name of the table you want to query.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nTable Name: Enter the PGVector table to use.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nNode options\nCollection\nA way to separate datasets in PGVector. This creates a separate table and column to keep track of which collection a vector belongs to.\nUse Collection: Select whether to use a collection (turned on) or not (turned off).\nCollection Name: Enter the name of the collection you want to use.\nCollection Table Name: Enter the name of the table to store collection information in.\nColumn Names\nThe following options specify the names of the columns to store the vectors and corresponding information in:\nID Column Name\nVector Column Name\nContent Column Name\nMetadata Column Name\nMetadata Filter\nTemplates and examples\nRelated resources\nRefer to LangChain's PGVector documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstorepinecone.md",
    "content": "Pinecone Vector Store node\nUse the Pinecone node to interact with your Pinecone database as vector store. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain, or connect directly to an agent as a tool. You can also update an item in a vector database by its ID.\nOn this page, you'll find the node parameters for the Pinecone node, and links to more resources.\nNode usage patterns\nYou can use the Pinecone Vector Store node in the following patterns.\nUse as a regular node to insert, update, and retrieve documents\nYou can use the Pinecone Vector Store as a regular node to insert, update, or get documents. This pattern places the Pinecone Vector Store in the regular connection flow without using an agent.\nYou can see an example of this in scenario 1 of this template.\nConnect directly to an AI agent as a tool\nYou can connect the Pinecone Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> Pinecone Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the Pinecone Vector Store node to fetch documents from the Pinecone Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nAn example of the connection flow would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Pinecone Vector Store.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Pinecone Vector Store node. Rather than connecting the Pinecone Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nThe connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Pinecone Vector store.\nNode parameters\nOperation Mode\nRerank Results\nGet Many parameters\nPinecone Index: Select or enter the Pinecone Index to use.\nPrompt: Enter your search query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nInsert Documents parameters\nPinecone Index: Select or enter the Pinecone Index to use.\nRetrieve Documents (As Vector Store for Chain/Tool) parameters\nPinecone Index: Select or enter the Pinecone Index to use.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nPinecone Index: Select or enter the Pinecone Index to use.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nParameters for Update Documents\nID\nNode options\nPinecone Namespace\nAnother segregation option for how to store your data within the index.\nMetadata Filter\nClear Namespace\nAvailable in Insert Documents mode. Deletes all data from the namespace before inserting the new data.\nTemplates and examples\nRelated resources\nRefer to LangChain's Pinecone documentation for more information about the service.\nFind your Pinecone index and namespace\nYour Pinecone index and namespace are available in your Pinecone account.\n!Screenshot of a Pinecone account, with the Pinecone index labelled"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstoreqdrant.md",
    "content": "Qdrant Vector Store node\nUse the Qdrant node to interact with your Qdrant collection as a vector store. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain or connect it directly to an agent to use as a tool.\nOn this page, you'll find the node parameters for the Qdrant node, and links to more resources.\nNode usage patterns\nYou can use the Qdrant Vector Store node in the following patterns.\nUse as a regular node to insert and retrieve documents\nYou can use the Qdrant Vector Store as a regular node to insert or get documents. This pattern places the Qdrant Vector Store in the regular connection flow without using an agent.\nYou can see an example of this in the first part of this template.\nConnect directly to an AI agent as a tool\nYou can connect the Qdrant Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> Qdrant Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the Qdrant Vector Store node to fetch documents from the Qdrant Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nAn example of the connection flow would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Qdrant Vector Store.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Qdrant Vector Store node. Rather than connecting the Qdrant Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nThe connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Qdrant Vector store.\nNode parameters\nRerank Results\nGet Many parameters\nQdrant collection name: Enter the name of the Qdrant collection to use.\nPrompt: Enter the search query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nThis Operation Mode includes one Node option, the Metadata Filter.\nInsert Documents parameters\nQdrant collection name: Enter the name of the Qdrant collection to use.\nThis Operation Mode includes one Node option:\nCollection Config: Enter JSON options for creating a Qdrant collection creation configuration. Refer to the Qdrant Collections documentation for more information.\nRetrieve Documents (As Vector Store for Chain/Tool) parameters\nQdrant Collection: Enter the name of the Qdrant collection to use.\nThis Operation Mode includes one Node option, the Metadata Filter.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nQdrant Collection: Enter the name of the Qdrant collection to use.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nNode options\nMetadata Filter\nTemplates and examples\nRelated resources\nRefer to LangChain's Qdrant documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstoresupabase.md",
    "content": "Supabase Vector Store node\nUse the Supabase Vector Store to interact with your Supabase database as vector store. You can insert documents into a vector database, get many documents from a vector database, and retrieve documents to provide them to a retriever connected to a chain.\nUse the Supabase Vector Store to interact with your Supabase database as vector store. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain, or connect it directly to an agent to use as a tool. You can also update an item in a vector store by its ID.\nOn this page, you'll find the node parameters for the Supabase node, and links to more resources.\nSupabase provides a quickstart for setting up your vector store. If you use settings other than the defaults in the quickstart, this may affect parameter settings in n8n. Make sure you understand what you're doing.\nNode usage patterns\nYou can use the Supabase Vector Store node in the following patterns.\nUse as a regular node to insert, update, and retrieve documents\nYou can use the Supabase Vector Store as a regular node to insert, update, or get documents. This pattern places the Supabase Vector Store in the regular connection flow without using an agent.\nYou can see an example of this in scenario 1 of this template.\nConnect directly to an AI agent as a tool\nYou can connect the Supabase Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> Supabase Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the Supabase Vector Store node to fetch documents from the Supabase Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nAn example of the connection flow (the example uses Pinecone, but the pattern in the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Supabase Vector Store.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Supabase Vector Store node. Rather than connecting the Supabase Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nThe connections flow in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Supabase Vector store.\nNode parameters\nOperation Mode\nRerank Results\nGet Many parameters\nTable Name: Enter the Supabase table to use.\nPrompt: Enter the search query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nInsert Documents parameters\nTable Name: Enter the Supabase table to use.\nRetrieve Documents (As Vector Store for Chain/Tool) parameters\nTable Name: Enter the Supabase table to use.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nTable Name: Enter the Supabase table to use.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nUpdate Documents\nTable Name: Enter the Supabase table to use.\nID: The ID of an embedding entry.\nParameters for Update Documents\nID\nNode options\nQuery Name\nThe name of the matching function you set up in Supabase. If you follow the Supabase quickstart, this will be match_documents.\nMetadata Filter\nTemplates and examples\nRelated resources\nRefer to LangChain's Supabase documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstoreweaviate.md",
    "content": "Weaviate Vector Store node\nUse the Weaviate node to interact with your Weaviate collection as a vector store. You can insert documents into or retrieve documents from a vector database. You can also retrieve documents to provide them to a retriever connected to a chain or connect this node directly to an agent to use as a tool.\nOn this page, you'll find the node parameters for the Weaviate node, and links to more resources.\nNode usage patterns\nYou can use the Weaviate Vector Store node in the following patterns.\nUse as a regular node to insert and retrieve documents\nYou can use the Weaviate Vector Store as a regular node to insert or get documents. This pattern places the Weaviate Vector Store in the regular connection flow without using an agent.\nConnect directly to an AI agent as a tool\nYou can connect the Weaviate Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> Weaviate Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the Weaviate Vector Store node to fetch documents from the Weaviate Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Weaviate Vector Store node. Rather than connecting the Weaviate Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nNode parameters\nGet Many parameters\nWeaviate Collection: Enter the name of the Weaviate collection to use.\nPrompt: Enter the search query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nInsert Documents parameters\nWeaviate Collection: Enter the name of the Weaviate collection to use.\nEmbedding Batch Size: The number of documents to embed in a single batch. The default is 200 documents.\nRetrieve Documents (As Vector Store for Chain/Tool) parameters\nWeaviate Collection: Enter the name of the Weaviate collection to use.\nRetrieve Documents (As Tool for AI Agent) parameters\nWeaviate Collection: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nWeaviate Collection: Enter the name of the Weaviate collection to use.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nInclude Metadata\nWhether to include document metadata.\nYou can use this with the Get Many and Retrieve Documents (As Tool for AI Agent) modes.\nRerank Results\nNode options\nSearch Filters\nAvailable for the Get Many, Retrieve Documents (As Vector Store for Chain/Tool), and Retrieve Documents (As Tool for AI Agent) operation modes.\nWhen searching for data, use this to match metadata associated with documents. You can learn more about the operators and query structure in Weaviate's conditional filters documentation.\nYou can use both AND and OR with different operators. Operators are case insensitive:\nSupported operators:\nTABLE_PLACEHOLDER_0\nWhen inserting data, the document loader sets the metadata. Refer to Default Data Loader for more information on loading documents.\nMetadata Keys\nYou can define which metadata keys you want Weaviate to return on your queries. This can reduce network load, as you will only get properties you have defined. Returns all properties from the server by default.\nAvailable for the Get Many, Retrieve Documents (As Vector Store for Chain/Tool), and Retrieve Documents (As Tool for AI Agent) operation modes.\nTenant Name\nThe specific tenant to store or retrieve documents for.\nText Key\nThe key in the document that contains the embedded text.\nSkip Init Checks\nWhether to skip initialization checks when instantiating the client.\nInit Timeout\nNumber of seconds to wait before timing out during initial checks.\nInsert Timeout\nNumber of seconds to wait before timing out during inserts.\nQuery Timeout\nNumber of seconds to wait before timing out during queries.\nGRPC Proxy\nA proxy to use for gRPC requests.\nClear Data\nAvailable for the Insert Documents operation mode.\nWhether to clear the collection or tenant before inserting new data.\nTemplates and examples\nRelated resources\nRefer to LangChain's Weaviate documentation for more information about the service.\nRefer to Weaviate Installation for a self hosted Weaviate Cluster."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.vectorstorezep.md",
    "content": "Zep Vector Store node\nUse the Zep Vector Store to interact with Zep vector databases. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a chain, or connect it directly to an agent to use as a tool.\nOn this page, you'll find the node parameters for the Zep Vector Store node, and links to more resources.\nNode usage patterns\nYou can use the Zep Vector Store node in the following patterns.\nUse as a regular node to insert, update, and retrieve documents\nYou can use the Zep Vector Store as a regular node to insert or get documents. This pattern places the Zep Vector Store in the regular connection flow without using an agent.\nYou can see an example of this in scenario 1 of this template (the example uses Supabase, but the pattern is the same).\nConnect directly to an AI agent as a tool\nYou can connect the Zep Vector Store node directly to the tool connector of an AI agent to use a vector store as a resource when answering queries.\nHere, the connection would be: AI agent (tools connector) -> Zep Vector Store node.\nUse a retriever to fetch documents\nYou can use the Vector Store Retriever node with the Zep Vector Store node to fetch documents from the Zep Vector Store node. This is often used with the Question and Answer Chain node to fetch documents from the vector store that match the given chat input.\nAn example of the connection flow (the example uses Pinecone, but the pattern in the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Zep Vector Store.\nUse the Vector Store Question Answer Tool to answer questions\nAnother pattern uses the Vector Store Question Answer Tool to summarize results and answer questions from the Zep Vector Store node. Rather than connecting the Zep Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\nThe connections flow (this example uses Supabase, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Zep Vector store.\nNode parameters\nRerank Results\nInsert Documents parameters\nCollection Name: Enter the collection name to store the data in.\nGet Many parameters\nCollection Name: Enter the collection name to retrieve the data from.\nPrompt: Enter the search query.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nRetrieve Documents (As Vector Store for Chain/Tool) parameters\nCollection Name: Enter the collection name to retrieve the data from.\nRetrieve Documents (As Tool for AI Agent) parameters\nName: The name of the vector store.\nDescription: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\nCollection Name: Enter the collection name to retrieve the data from.\nLimit: Enter how many results to retrieve from the vector store. For example, set this to 10 to get the ten best results.\nNode options\nEmbedding Dimensions\nMust be the same when embedding the data and when querying it.\nThis sets the size of the array of floats used to represent the semantic meaning of a text document.\nIs Auto Embedded\nAvailable in the Insert Documents Operation Mode, enabled by default.\nDisable this to configure your embeddings in Zep instead of in n8n.\nMetadata Filter\nTemplates and examples\nRelated resources\nRefer to LangChain's Zep documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\common-issues.md",
    "content": "AI Agent node common issues\nHere are some common errors and issues with the AI Agent node and steps to resolve or troubleshoot them.\nInternal error: 400 Invalid value for 'content'\nA full error message might look like this:\nThis error can occur if the Prompt input contains a null value.\nYou might see this in one of two scenarios:\nWhen you've set the Prompt to Define below and have an expression in your Text that isn't generating a value.\nTo resolve, make sure your expressions reference valid fields and that they resolve to valid input rather than null.\nWhen you've set the Prompt to Connected Chat Trigger Node and the incoming data has null values.\nTo resolve, remove any null values from the chatInput field of the input node.\nError in sub-node Simple Memory\nThis error displays when n8n runs into an issue with the Simple Memory sub-node.\nIt most often occurs when your workflow or the workflow template you copied uses an older version of the Simple memory node (previously known as \"Window Buffer Memory\").\nTry removing the Simple Memory node from your workflow and re-adding it, which will guarantee you're using the latest version of the node.\nA Chat Model sub-node must be connected error\nThis error displays when n8n tries to execute the node without having a Chat Model connected.\nTo resolve this, click the + Chat Model button at the bottom of your screen when the node is open, or click the Chat Model + connector when the node is closed. n8n will then open a selection of possible Chat Models to pick from.\nNo prompt specified error\nThis error occurs when the agent expects to get the prompt from the previous node automatically. Typically, this happens when you're using the Chat Trigger Node.\nTo resolve this issue, find the Prompt parameter of the AI Agent node and change it from Connected Chat Trigger Node to Define below. This allows you to manually build your prompt by referencing output data from other nodes or by adding static text."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\conversational-agent.md",
    "content": "Conversational AI Agent node\nThe Conversational Agent has human-like conversations. It can maintain context, understand user intent, and provide relevant answers. This agent is typically used for building chatbots, virtual assistants, and customer support systems.\nThe Conversational Agent describes tools in the system prompt and parses JSON responses for tool calls. If your preferred AI model doesn't support tool calling or you're handling simpler interactions, this agent is a good general option. It's more flexible but may be less accurate than the Tools Agent.\nRefer to AI Agent for more information on the AI Agent node itself.\nNode parameters\nConfigure the Conversational Agent using the following parameters.\nPrompt\nRequire Specific Output Format\nNode options\nRefine the Conversational Agent node's behavior using these options:\nHuman Message\nTell the agent about the tools it can use and add context to the user's input.\nYou must include these expressions and variable:\n{tools}: A LangChain expression that provides a string of the tools you've connected to the Agent. Provide some context or explanation about who should use the tools and how they should use them.\n{format_instructions}: A LangChain expression that provides the schema or format from the output parser node you've connected. Since the instructions themselves are context, you don't need to provide context for this expression.\n{{input}}: A LangChain variable containing the user's prompt. This variable populates with the value of the Prompt parameter. Provide some context that this is the user's input.\nHere's an example of how you might use these strings:\nExample:\nSystem Message\nMax Iterations\nReturn Intermediate Steps\nTemplates and examples\nRefer to the main AI Agent node's Templates and examples section.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\index.md",
    "content": "AI Agent node\nAn AI agent is an autonomous system that receives data, makes rational decisions, and acts within its environment to achieve specific goals. The AI agent's environment is everything the agent can access that isn't the agent itself. This agent uses external tools and APIs to perform actions and retrieve information. It can understand the capabilities of different tools and determine which tool to use depending on the task.\nTemplates and examples\nRelated resources\nRefer to LangChain's documentation on agents for more information about the service.\nNew to AI Agents? Read the n8n blog introduction to AI agents.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\openai-functions-agent.md",
    "content": "OpenAI Functions Agent node\nUse the OpenAI Functions Agent node to use an OpenAI functions model. These are models that detect when a function should be called and respond with the inputs that should be passed to the function.\nRefer to AI Agent for more information on the AI Agent node itself.\nNode parameters\nConfigure the OpenAI Functions Agent using the following parameters.\nPrompt\nRequire Specific Output Format\nNode options\nRefine the OpenAI Functions Agent node's behavior using these options:\nSystem Message\nMax Iterations\nReturn Intermediate Steps\nTemplates and examples\nRefer to the main AI Agent node's Templates and examples section.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\plan-execute-agent.md",
    "content": "Plan and Execute Agent node\nThe Plan and Execute Agent is like the ReAct agent but with a focus on planning. It first creates a high-level plan to solve the given task and then executes the plan step by step. This agent is most useful for tasks that require a structured approach and careful planning.\nRefer to AI Agent for more information on the AI Agent node itself.\nNode parameters\nConfigure the Plan and Execute Agent using the following parameters.\nPrompt\nRequire Specific Output Format\nNode options\nRefine the Plan and Execute Agent node's behavior using these options:\nHuman Message Template\nEnter a message that n8n will send to the agent during each step execution.\nAvailable LangChain expressions:\n{previous_steps}: Contains information about the previous steps the agent's already completed.\n{current_step}: Contains information about the current step.\n{agent_scratchpad}: Information to remember for the next iteration.\nTemplates and examples\nRefer to the main AI Agent node's Templates and examples section.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\react-agent.md",
    "content": "ReAct AI Agent node\nThe ReAct Agent node implements ReAct logic. ReAct (reasoning and acting) brings together the reasoning powers of chain-of-thought prompting and action plan generation.\nThe ReAct Agent reasons about a given task, determines the necessary actions, and then executes them. It follows the cycle of reasoning and acting until it completes the task. The ReAct agent can break down complex tasks into smaller sub-tasks, prioritise them, and execute them one after the other.\nRefer to AI Agent for more information on the AI Agent node itself.\nNode parameters\nConfigure the ReAct Agent using the following parameters.\nPrompt\nRequire Specific Output Format\nNode options\nUse the options to create a message to send to the agent at the start of the conversation. The message type depends on the model you're using:\nChat models: These models have the concept of three components interacting (AI, system, and human). They can receive system messages and human messages (prompts).\nInstruct models: These models don't have the concept of separate AI, system, and human components. They receive one body of text, the instruct message.\nHuman Message Template\nUse this option to extend the user prompt. This is a way for the agent to pass information from one iteration to the next.\nAvailable LangChain expressions:\n{input}: Contains the user prompt.\n{agent_scratchpad}: Information to remember for the next iteration.\nPrefix Message\nEnter text to prefix the tools list at the start of the conversation. You don't need to add the list of tools. LangChain automatically adds the tools list.\nSuffix Message for Chat Model\nAdd text to append after the tools list at the start of the conversation when the agent uses a chat model. You don't need to add the list of tools. LangChain automatically adds the tools list.\nSuffix Message for Regular Model\nAdd text to append after the tools list at the start of the conversation when the agent uses a regular/instruct model. You don't need to add the list of tools. LangChain automatically adds the tools list.\nReturn Intermediate Steps\nRelated resources\nRefer to LangChain's ReAct Agents documentation for more information.\nTemplates and examples\nRefer to the main AI Agent node's Templates and examples section.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\sql-agent.md",
    "content": "SQL AI Agent node\nThe SQL Agent uses a SQL database as a data source. It can understand natural language questions, convert them into SQL queries, execute the queries, and present the results in a user-friendly format. This agent is valuable for building natural language interfaces to databases.\nRefer to AI Agent for more information on the AI Agent node itself.\nNode parameters\nConfigure the SQL Agent using the following parameters.\nData Source\nChoose the database to use as a data source for the node. Options include:\nMySQL: Select this option to use a MySQL database.\nAlso select the Credential for MySQL.\nSQLite: Select this option to use a SQLite database.\nYou must add a Read/Write File From Disk node before the Agent to read your SQLite file.\nAlso enter the Input Binary Field name of your SQLite file coming from the Read/Write File From Disk node.\nPostgres: Select this option to use a Postgres database.\nAlso select the Credential for Postgres.\nPrompt\nNode options\nRefine the SQL Agent node's behavior using these options:\nIgnored Tables\nIf you'd like the node to ignore any tables from the database, enter a comma-separated list of tables you'd like it to ignore.\nIf left empty, the agent doesn't ignore any tables.\nInclude Sample Rows\nEnter the number of sample rows to include in the prompt to the agent. Default is 3.\nSample rows help the agent understand the schema of the database, but they also increase the number of tokens used.\nIncluded Tables\nIf you'd only like to include specific tables from the database, enter a comma-separated list of tables to include.\nIf left empty, the agent includes all tables.\nPrefix Prompt\nEnter a message you'd like to send to the agent before the Prompt text. This initial message can provide more context and guidance to the agent about what it can and can't do, and how to format the response.\nn8n fills this field with an example.\nSuffix Prompt\nEnter a message you'd like to send to the agent after the Prompt text.\nAvailable LangChain expressions:\n{chatHistory}: A history of messages in this conversation, useful for maintaining context.\n{input}: Contains the user prompt.\n{agent_scratchpad}: Information to remember for the next iteration.\nn8n fills this field with an example.\nLimit\nEnter the maximum number of results to return.\nDefault is 10.\nTemplates and examples\nRefer to the main AI Agent node's Templates and examples section.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.agent\\tools-agent.md",
    "content": "Tools AI Agent node\nThe Tools Agent uses external tools and APIs to perform actions and retrieve information. It can understand the capabilities of different tools and determine which tool to use depending on the task. This agent helps integrate LLMs with various external services and databases.\nThis agent has an enhanced ability to work with tools and can ensure a standard output format.\nThe Tools Agent implements Langchain's tool calling interface. This interface describes available tools and their schemas. The agent also has improved output parsing capabilities, as it passes the parser to the model as a formatting tool.\nRefer to AI Agent for more information on the AI Agent node itself.\nThis agent supports the following chat models:\nOpenAI Chat Model\nGroq Chat Model\nMistral Cloud Chat Model\nAnthropic Chat Model\nAzure OpenAI Chat Model\n??? Details \"The Tools Agent can use the following tools...\"\n* Call n8n Workflow\n* Code\n* HTTP Request\n* Action Network\n* ActiveCampaign\n* Affinity\n* Agile CRM\n* Airtable\n* APITemplate.io\n* Asana\n* AWS Lambda\n* AWS S3\n* AWS SES\n* AWS Textract\n* AWS Transcribe\n* Baserow\n* Bubble\n* Calculator\n* ClickUp\n* CoinGecko\n* Compression\n* Crypto\n* DeepL\n* DHL\n* Discord\n* Dropbox\n* Elasticsearch\n* ERPNext\n* Facebook Graph API\n* FileMaker\n* Ghost\n* Git\n* GitHub\n* GitLab\n* Gmail\n* Google Analytics\n* Google BigQuery\n* Google Calendar\n* Google Chat\n* Google Cloud Firestore\n* Google Cloud Realtime Database\n* Google Contacts\n* Google Docs\n* Google Drive\n* Google Sheets\n* Google Slides\n* Google Tasks\n* Google Translate\n* Google Workspace Admin\n* Gotify\n* Grafana\n* GraphQL\n* Hacker News\n* Home Assistant\n* HubSpot\n* Jenkins\n* Jira Software\n* JWT\n* Kafka\n* LDAP\n* Line\n* LinkedIn\n* Mailcheck\n* Mailgun\n* Mattermost\n* Mautic\n* Medium\n* Microsoft Excel 365\n* Microsoft OneDrive\n* Microsoft Outlook\n* Microsoft SQL\n* Microsoft Teams\n* Microsoft To Do\n* Monday.com\n* MongoDB\n* MQTT\n* MySQL\n* NASA\n* Nextcloud\n* NocoDB\n* Notion\n* Odoo\n* OpenWeatherMap\n* Pipedrive\n* Postgres\n* Pushover\n* QuickBooks Online\n* QuickChart\n* RabbitMQ\n* Reddit\n* Redis\n* RocketChat\n* S3\n* Salesforce\n* Send Email\n* SendGrid\n* SerpApi (Google Search)\n* Shopify\n* Slack\n* Spotify\n* Stripe\n* Supabase\n* Telegram\n* Todoist\n* TOTP\n* Trello\n* Twilio\n* urlscan.io\n* Vector Store\n* Webflow\n* Wikipedia\n* Wolfram|Alpha\n* WooCommerce\n* Wordpress\n* X (Formerly Twitter)\n* YouTube\n* Zendesk\n* Zoho CRM\n* Zoom\nNode parameters\nConfigure the Tools Agent using the following parameters.\nPrompt\nRequire Specific Output Format\nNode options\nRefine the Tools Agent node's behavior using these options:\nSystem Message\nMax Iterations\nReturn Intermediate Steps\nAutomatically Passthrough Binary Images\nTemplates and examples\nRefer to the main AI Agent node's Templates and examples section.\nDynamic parameters for tools with $fromAI()\nTo learn how to dynamically populate parameters for app node tools, refer to Let AI specify tool parameters with $fromAI().\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.chainretrievalqa\\common-issues.md",
    "content": "Question and Answer Chain node common issues\nHere are some common errors and issues with the Question and Answer Chain node and steps to resolve or troubleshoot them.\nNo prompt specified error\nThis error displays when the Prompt is empty or invalid.\nYou might see this in one of two scenarios:\nWhen you've set the Prompt to Define below and have an expression in your Text that isn't generating a value.\nTo resolve, enter a valid prompt in the Text field.\nMake sure any expressions reference valid fields and that they resolve to valid input rather than null.\nWhen you've set the Prompt to Connected Chat Trigger Node and the incoming data has null values.\nTo resolve, make sure your input contains a chatInput field. Add an Edit Fields (Set) node to edit an incoming field name to chatInput.\nRemove any null values from the chatInput field of the input node.\nA Retriever sub-node must be connected error\nThis error displays when n8n tries to execute the node without having a Retriever connected.\nTo resolve this, click the + Retriever button at the bottom of your screen when the node is open, or click the Retriever + connector when the node isn't open. n8n will then open a selection of possible Retrievers to pick from.\nCan't produce longer responses\nIf you need to generate longer responses than the Question and Answer Chain node produces by default, you can try one or more of the following techniques:\nConnect a more verbose model: Some AI models produce more terse results than others. Swapping your model for one with a larger context window and more verbose output can increase the word length of your responses.\nIncrease the maximum number of tokens: Many model nodes (for example the OpenAI Chat Model) include a Maximum Number of Tokens option. You can set this to increase the maximum number of tokens the model can use to produce a response.\nBuild larger responses in stages: For more detailed answers, you may want to construct replies in stages using a variety of AI nodes. You can use AI split up a single question into multiple prompts and create responses for each. You can then compose a final reply by combining the responses again. Though the details are different, you can find a good example of the general idea in this template for writing a WordPress post with AI."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\root-nodes\\n8n-nodes-langchain.chainretrievalqa\\index.md",
    "content": "Question and Answer Chain node\nUse the Question and Answer Chain node to use a vector store as a retriever.\nOn this page, you'll find the node parameters for the Question and Answer Chain node, and links to more resources.\nNode parameters\nQuery\nThe question you want to ask.\nTemplates and examples\nRelated resources\nRefer to LangChain's documentation on retrieval chains for examples of how LangChain can use a vector store as a retriever.\nCommon issues\nFor common errors or issues and suggested resolution steps, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\index.md",
    "content": "Sub nodes\nSub nodes attach to root nodes within a group of cluster nodes. They configure the overall functionality of the cluster."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.documentdefaultdataloader.md",
    "content": "Default Data Loader node\nUse the Default Data Loader node to load binary data files or JSON data for vector stores or summarization.\nOn this page, you'll find a list of parameters the Default Data Loader node supports, and links to more resources.\nNode parameters\nText Splitting: Choose from:\nSimple: Uses the Recursive Character Text Splitter with a chunk size of 1000 and an overlap of 200.\nCustom: Allows you to connect a text splitter of your choice.\nType of Data: Select Binary or JSON.\nMode: Choose from:\nLoad All Input Data: Use all the node's input data.\nLoad Specific Data: Use expressions to define the data you want to load. You can add text as well as expressions. This means you can create a custom document from a mix of text and expressions.\nData Format: Displays when you set Type of Data to Binary. Select the file MIME type for your binary data. Set to Automatically Detect by MIME Type if you want n8n to set the data format for you. If you set a specific data format and the incoming file MIME type doesn't match it, the node errors. If you use Automatically Detect by MIME Type, the node falls back to text format if it can't match the file MIME type to a supported data format.\nNode options\nMetadata: Set the metadata that should accompany the document in the vector store. This is what you match to using the Metadata Filter option when retrieving data using the vector store nodes.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.documentgithubloader.md",
    "content": "GitHub Document Loader node\nUse the GitHub Document Loader node to load data from a GitHub repository for vector stores or summarization.\nOn this page, you'll find the node parameters for the GitHub Document Loader node, and links to more resources.\nNode parameters\nText Splitting: Choose from:\nSimple: Uses the Recursive Character Text Splitter with a chunk size of 1000 and an overlap of 200.\nCustom: Allows you to connect a text splitter of your choice.\nRepository Link: Enter the URL of your GitHub repository.\nBranch: Enter the branch name to use.\nNode options\nRecursive: Select whether to include sub-folders and files (turned on) or not (turned off).\nIgnore Paths: Enter directories to ignore.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsawsbedrock.md",
    "content": "Embeddings AWS Bedrock node\nUse the Embeddings AWS Bedrock node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings AWS Bedrock node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding.\nLearn more about available models in the Amazon Bedrock documentation.\nTemplates and examples\nRelated resources\nRefer to LangChains's AWS Bedrock embeddings documentation and the AWS Bedrock documentation for more information about AWS Bedrock."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsazureopenai.md",
    "content": "Embeddings Azure OpenAI node\nUse the Embeddings Azure OpenAI node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings Azure OpenAI node, and links to more resources.\nNode options\nModel (Deployment) Name: Select the model (deployment) to use for generating embeddings.\nBatch Size: Enter the maximum number of documents to send in each request.\nStrip New Lines: Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default.\nTimeout: Enter the maximum amount of time a request can take in seconds. Set to -1 for no timeout.\nTemplates and examples\nRelated resources\nRefer to LangChains's OpenAI embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingscohere.md",
    "content": "Embeddings Cohere node\nUse the Embeddings Cohere node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings Cohere node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding. Choose from:\nEmbed-English-v2.0(4096 Dimensions)\nEmbed-English-Light-v2.0(1024 Dimensions)\nEmbed-Multilingual-v2.0(768 Dimensions)\nLearn more about available models in Cohere's models documentation.\nTemplates and examples\nRelated resources\nRefer to Langchain's Cohere embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsgooglegemini.md",
    "content": "Embeddings Google Gemini node\nUse the Embeddings Google Gemini node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings Google Gemini node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding.\nLearn more about available models in Google Gemini's models documentation.\nTemplates and examples\nRelated resources\nRefer to Langchain's Google Generative AI embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsgooglepalm.md",
    "content": "Embeddings Google PaLM node\nUse the Embeddings Google PaLM node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings Google PaLM node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding.\nn8n dynamically loads models from the Google PaLM API and you'll only see the models available to your account.\nTemplates and examples\nRelated resources\nRefer to Langchain's Google PaLM embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsgooglevertex.md",
    "content": "Embeddings Google Vertex node\nUse the Embeddings Google Vertex node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings Google Vertex node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding.\nLearn more about available embedding models in Google VertexAI embeddings API documentation.\nTemplates and examples\nRelated resources\nRefer to LangChain's Google Generative AI embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingshuggingfaceinference.md",
    "content": "Embeddings HuggingFace Inference node\nUse the Embeddings HuggingFace Inference node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings HuggingFace Inference, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding.\nRefer to the Hugging Face models documentation for available models.\nNode options\nCustom Inference Endpoint: Enter the URL of your deployed model, hosted by HuggingFace. If you set this, n8n ignores the Model Name.\nRefer to HuggingFace's guide to inference for more information.\nTemplates and examples\nRelated resources\nRefer to Langchain's HuggingFace Inference embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsmistralcloud.md",
    "content": "Embeddings Mistral Cloud node\nUse the Embeddings Mistral Cloud node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings Mistral Cloud node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding.\nLearn more about available models in Mistral's models documentation.\nNode options\nBatch Size: Enter the maximum number of documents to send in each request.\nStrip New Lines: Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default.\nTemplates and examples\nRelated resources\nRefer to Langchain's Mistral embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsollama.md",
    "content": "Embeddings Ollama node\nUse the Embeddings Ollama node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings Ollama node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the embedding. Choose from:\nall-minilm (384 Dimensions)\nnomic-embed-text (768 Dimensions)\nLearn more about available models in Ollama's models documentation.\nTemplates and examples\nRelated resources\nRefer to Langchain's Ollama embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.embeddingsopenai.md",
    "content": "Embeddings OpenAI node\nUse the Embeddings OpenAI node to generate embeddings for a given text.\nOn this page, you'll find the node parameters for the Embeddings OpenAI node, and links to more resources.\nNode options\nModel: Select the model to use for generating embeddings.\nBase URL: Enter the URL to send the request to. Use this if you are using a self-hosted OpenAI-like model.\nBatch Size: Enter the maximum number of documents to send in each request.\nStrip New Lines: Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default.\nTimeout: Enter the maximum amount of time a request can take in seconds. Set to -1 for no timeout.\nTemplates and examples\nRelated resources\nRefer to LangChains's OpenAI embeddings documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatanthropic.md",
    "content": "Anthropic Chat Model node\nUse the Anthropic Chat Model node to use Anthropic's Claude family of chat models with conversational agents.\nOn this page, you'll find the node parameters for the Anthropic Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model that generates the completion. Choose from:\nClaude\nClaude Instant\nLearn more in the Anthropic model documentation.\nNode options\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTop K: Enter the number of token choices the model uses to generate the next token.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nRefer to LangChains's Anthropic documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatawsbedrock.md",
    "content": "AWS Bedrock Chat Model node\nThe AWS Bedrock Chat Model node allows you use LLM models utilising AWS Bedrock platform.\nOn this page, you'll find the node parameters for the AWS Bedrock Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model that generates the completion.\nLearn more about available models in the Amazon Bedrock model documentation.\nNode options\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nProxy limitations\nThis node doesn't support the NO_PROXY environment variable.\nTemplates and examples\nRelated resources\nRefer to LangChains's AWS Bedrock Chat Model documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatazureopenai.md",
    "content": "Azure OpenAI Chat Model node\nUse the Azure OpenAI Chat Model node to use OpenAI's chat models with conversational agents.\nOn this page, you'll find the node parameters for the Azure OpenAI Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the completion.\nNode options\nFrequency Penalty: Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nResponse Format: Choose Text or JSON. JSON ensures the model returns valid JSON.\nPresence Penalty: Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTimeout: Enter the maximum request time in milliseconds.\nMax Retries: Enter the maximum number of times to retry a request.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nProxy limitations\nThis node doesn't support the NO_PROXY environment variable.\nTemplates and examples\nRelated resources\nRefer to LangChains's Azure OpenAI documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatcohere.md",
    "content": "Cohere Chat Model node\nUse the Cohere Chat Model node to access Cohere's large language models for conversational AI and text generation tasks.\nOn this page, you'll find the node parameters for the Cohere Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model which will generate the completion. n8n dynamically loads available models from the Cohere API. Learn more in the Cohere model documentation.\nNode options\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nMax Retries: Enter the maximum number of times to retry a request.\nTemplates and examples\nRelated resources\nRefer to Cohere's API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatdeepseek.md",
    "content": "DeepSeek Chat Model node\nUse the DeepSeek Chat Model node to use DeepSeek's chat models with conversational agents.\nOn this page, you'll find the node parameters for the DeepSeek Chat Model node and links to more resources.\nNode parameters\nModel\nSelect the model to use to generate the completion.\nn8n dynamically loads models from DeepSeek and you'll only see the models available to your account.\nNode options\nUse these options to further refine the node's behavior.\nBase URL\nEnter a URL here to override the default URL for the API.\nFrequency Penalty\nUse this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\nMaximum Number of Tokens\nEnter the maximum number of tokens used, which sets the completion length.\nResponse Format\nChoose Text or JSON. JSON ensures the model returns valid JSON.\nPresence Penalty\nUse this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\nSampling Temperature\nUse this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTimeout\nEnter the maximum request time in milliseconds.\nMax Retries\nEnter the maximum number of times to retry a request.\nTop P\nUse this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nAs DeepSeek is API-compatible with OpenAI, you can refer to LangChains's OpenAI documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatgooglegemini.md",
    "content": "Google Gemini Chat Model node\nUse the Google Gemini Chat Model node to use Google's Gemini chat models with conversational agents.\nOn this page, you'll find the node parameters for the Google Gemini Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the completion.\nn8n dynamically loads models from the Google Gemini API and you'll only see the models available to your account.\nNode options\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTop K: Enter the number of token choices the model uses to generate the next token.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nSafety Settings: Gemini supports adjustable safety settings. Refer to Google's Gemini API safety settings for information on the available filters and levels.\nTemplates and examples\nRelated resources\nRefer to LangChain's Google Gemini documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatgooglevertex.md",
    "content": "Google Vertex Chat Model node\nUse the Google Vertex AI Chat Model node to use Google's Vertex AI chat models with conversational agents.\nOn this page, you'll find the node parameters for the Google Vertex AI Chat Model node, and links to more resources.\nNode parameters\nProject ID: Select the project ID from your Google Cloud account to use. n8n dynamically loads projects from the Google Cloud account, but you can also enter it manually.\nModel Name: Select the name of the model to use to generate the completion, for example gemini-1.5-flash-001, gemini-1.5-pro-001, etc. Refer to Google models for a list of available models.\nNode options\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTop K: Enter the number of token choices the model uses to generate the next token.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nSafety Settings: Gemini supports adjustable safety settings. Refer to Google's Gemini API safety settings for information on the available filters and levels.\nTemplates and examples\nRelated resources\nRefer to LangChain's Google Vertex AI documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatgroq.md",
    "content": "Groq Chat Model node\nUse the Groq Chat Model node to access Groq's large language models for conversational AI and text generation tasks.\nOn this page, you'll find the node parameters for the Groq Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model which will generate the completion. n8n dynamically loads available models from the Groq API. Learn more in the Groq model documentation.\nNode options\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTemplates and examples\nRelated resources\nRefer to Groq's API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatmistralcloud.md",
    "content": "Mistral Cloud Chat Model node\nUse the Mistral Cloud Chat Model node to combine Mistral Cloud's chat models with conversational agents.\nOn this page, you'll find the node parameters for the Mistral Cloud Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model to use to generate the completion. n8n dynamically loads models from Mistral Cloud and you'll only see the models available to your account.\nNode options\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTimeout: Enter the maximum request time in milliseconds.\nMax Retries: Enter the maximum number of times to retry a request.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nEnable Safe Mode: Enable safe mode by injecting a safety prompt at the beginning of the completion. This helps prevent the model from generating offensive content.\nRandom Seed: Enter a seed to use for random sampling. If set, different calls will generate deterministic results.\nTemplates and examples\nRelated resources\nRefer to LangChains's Mistral documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatopenrouter.md",
    "content": "OpenRouter Chat Model node\nUse the OpenRouter Chat Model node to use OpenRouter's chat models with conversational agents.\nOn this page, you'll find the node parameters for the OpenRouter Chat Model node and links to more resources.\nNode parameters\nModel\nSelect the model to use to generate the completion.\nn8n dynamically loads models from OpenRouter and you'll only see the models available to your account.\nNode options\nUse these options to further refine the node's behavior.\nFrequency Penalty\nUse this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\nMaximum Number of Tokens\nEnter the maximum number of tokens used, which sets the completion length.\nResponse Format\nChoose Text or JSON. JSON ensures the model returns valid JSON.\nPresence Penalty\nUse this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\nSampling Temperature\nUse this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTimeout\nEnter the maximum request time in milliseconds.\nMax Retries\nEnter the maximum number of times to retry a request.\nTop P\nUse this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nAs OpenRouter is API-compatible with OpenAI, you can refer to LangChains's OpenAI documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatxaigrok.md",
    "content": "xAI Grok Chat Model node\nUse the xAI Grok Chat Model node to access xAI Grok's large language models for conversational AI and text generation tasks.\nOn this page, you'll find the node parameters for the xAI Grok Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model which will generate the completion. n8n dynamically loads available models from the xAI Grok API. Learn more in the xAI Grok model documentation.\nNode options\nFrequency Penalty: Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length. Most models have a context length of 2048 tokens with the newest models supporting up to 32,768 tokens.\nResponse Format: Choose Text or JSON. JSON ensures the model returns valid JSON.\nPresence Penalty: Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTimeout: Enter the maximum request time in milliseconds.\nMax Retries: Enter the maximum number of times to retry a request.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nRefer to xAI Grok's API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmcohere.md",
    "content": "Cohere Model node\nUse the Cohere Model node to use Cohere's models.\nOn this page, you'll find the node parameters for the Cohere Model node, and links to more resources.\nThis node lacks tools support, so it won't work with the AI Agent node. Instead, connect it with the Basic LLM Chain node.\nNode Options\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTemplates and examples\nRelated resources\nRefer to LangChains's Cohere documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmopenhuggingfaceinference.md",
    "content": "Hugging Face Inference Model node\nUse the Hugging Face Inference Model node to use Hugging Face's models.\nOn this page, you'll find the node parameters for the Hugging Face Inference Model node, and links to more resources.\nThis node lacks tools support, so it won't work with the AI Agent node. Instead, connect it with the Basic LLM Chain node.\nNode parameters\nModel: Select the model to use to generate the completion.\nNode options\nCustom Inference Endpoint: Enter a custom inference endpoint URL.\nFrequency Penalty: Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\nMaximum Number of Tokens: Enter the maximum number of tokens used, which sets the completion length.\nPresence Penalty: Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTop K: Enter the number of token choices the model uses to generate the next token.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nRefer to LangChains's Hugging Face Inference Model documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memorymanager.md",
    "content": "Chat Memory Manager node\nThe Chat Memory Manager node manages chat message memories within your workflows. Use this node to load, insert, and delete chat messages in an in-memory vector store.\nOn this page, you'll find a list of operations that the Chat Memory Manager node supports, along with links to more resources.\nNode parameters\nOperation Mode: Choose between Get Many Messages, Insert Messages, and Delete Messages operations.\nInsert Mode: Available in Insert Messages mode. Choose from:\nInsert Messages: Insert messages alongside existing messages.\nOverride All Messages: Replace current memory.\nDelete Mode: available in Delete Messages mode. Choose from:\nLast N: Delete the last N messages.\nAll Messages: Delete messages from memory.\nChat Messages: available in Insert Messages mode. Define the chat messages to insert into the memory, including:\nType Name or ID: Set the message type. Select one of:\nAI: Use this for messages from the AI.\nSystem: Add a message containing instructions for the AI.\nUser: Use this for messages from the user. This message type is sometimes called the 'human' message in other AI tools and guides.\nMessage: Enter the message contents.\nHide Message in Chat: Select whether n8n should display the message to the user in the chat UI (turned off) or not (turned on).\nMessages Count: Available in Delete Messages mode when you select Last N. Enter the number of latest messages to delete.\nSimplify Output: Available in Get Many Messages mode. Turn on to simplify the output to include only the sender (AI, user, or system) and the text.\nTemplates and examples\nRelated resources\nRefer to LangChain's Memory documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memorymongochat.md",
    "content": "MongoDB Chat Memory node\nUse the MongoDB Chat Memory node to use MongoDB as a memory server for storing chat history.\nOn this page, you'll find a list of operations the MongoDB Chat Memory node supports, and links to more resources.\nNode parameters\nSession Key: Enter the key to use to store the memory in the workflow data.\nCollection Name: Enter the name of the collection to store the chat history in. The system will create the collection if it doesn't exist.\nDatabase Name: Enter the name of the database to store the chat history in. If not provided, the database from credentials will be used.\nContext Window Length: Enter the number of previous interactions to consider for context.\nRelated resources\nRefer to LangChain's MongoDB Chat Message History documentation for more information about the service.\nSingle memory instance"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memorymotorhead.md",
    "content": "Motorhead node\nUse the Motorhead node to use Motorhead as a memory server.\nOn this page, you'll find a list of operations the Motorhead node supports, and links to more resources.\nNode parameters\nSession ID: Enter the ID to use to store the memory in the workflow data.\nNode reference\nTemplates and examples\nRelated resources\nRefer to LangChain's Motorhead documentation for more information about the service.\nSingle memory instance"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memorypostgreschat.md",
    "content": "Postgres Chat Memory node\nUse the Postgres Chat Memory node to use Postgres as a memory server for storing chat history.\nOn this page, you'll find a list of operations the Postgres Chat Memory node supports, and links to more resources.\nNode parameters\nSession Key: Enter the key to use to store the memory in the workflow data.\nTable Name: Enter the name of the table to store the chat history in. The system will create the table if doesn't exist.\nContext Window Length: Enter the number of previous interactions to consider for context.\nRelated resources\nRefer to LangChain's Postgres Chat Message History documentation for more information about the service.\nSingle memory instance"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memoryredischat.md",
    "content": "Redis Chat Memory node\nUse the Redis Chat Memory node to use Redis as a memory server.\nOn this page, you'll find a list of operations the Redis Chat Memory node supports, and links to more resources.\nNode parameters\nSession Key: Enter the key to use to store the memory in the workflow data.\nSession Time To Live: Use this parameter to make the session expire after a given number of seconds.\nContext Window Length: Enter the number of previous interactions to consider for context.\nTemplates and examples\nRelated resources\nRefer to LangChain's Redis Chat Memory documentation for more information about the service.\nSingle memory instance"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memoryxata.md",
    "content": "Xata node\nUse the Xata node to use Xata as a memory server.\nOn this page, you'll find a list of operations the Xata node supports, and links to more resources.\nNode parameters\nSession ID: Enter the ID to use to store the memory in the workflow data.\nContext Window Length: Enter the number of previous interactions to consider for context.\nTemplates and examples\nRelated resources\nRefer to LangChain's Xata documentation for more information about the service.\nSingle memory instance"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memoryzep.md",
    "content": "Zep node\nUse the Zep node to use Zep as a memory server.\nOn this page, you'll find a list of operations the Zep node supports, and links to more resources.\nNode parameters\nSession ID: Enter the ID to use to store the memory in the workflow data.\nTemplates and examples\nRelated resources\nRefer to LangChain's Zep documentation for more information about the service.\nSingle memory instance"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.modelselector.md",
    "content": "Model Selector\nThe Model Selector node dynamically selects one of the connected language models during workflow execution based on a set of defined conditions. This enables implementing fallback mechanisms for error handling or choosing the optimal model for specific tasks.\nThis page covers node parameters for the Model Selector node and includes links to related resources.\nNode parameters\nNumber of Inputs\nSpecifies the number of input connections available for attaching language models.\nRules\nEach rule defines the model to use when specific conditions match.\nThe Model Selector node evaluates rules sequentially, starting from the first input, and stops evaluation as soon as it finds a match. This means that if multiple rules would match, n8n will only use the model defined by the first matching rule.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.outputparserautofixing.md",
    "content": "Auto-fixing Output Parser node\nThe Auto-fixing Output Parser node wraps another output parser. If the first one fails, it calls out to another LLM to fix any errors.\nTemplates and examples\nRelated resources\nRefer to LangChain's output parser documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.outputparseritemlist.md",
    "content": "Item List Output Parser node\nUse the Item List Output Parser node to return a list of items with a specific length and separator.\nNode options\nNumber of Items: Enter the maximum items to return. Set to -1 for unlimited items.\nSeparator: Select the separator used to split the results into separate items. Defaults to a new line.\nTemplates and examples\nRelated resources\nRefer to LangChain's output parser documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.rerankercohere.md",
    "content": "Reranker Cohere\nThe Reranker Cohere node allows you to rerank the resulting chunks from a vector store. You can connect this node to a vector store.\nThe reranker reorders the list of documents retrieved from a vector store for a given query in order of descending relevance.\nOn this page, you'll find the node parameters for the Reranker Cohere node, and links to more resources.\nNode parameters\nModel\nChoose the reranking model to use. You can find out more about the available models in Cohere's model documentation.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.retrievercontextualcompression.md",
    "content": "Contextual Compression Retriever node\nThe Contextual Compression Retriever node improves the answers returned from vector store document similarity searches by taking into account the context from the query.\nTemplates and examples\nRelated resources\nRefer to LangChain's contextual compression retriever documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.retrievermultiquery.md",
    "content": "MultiQuery Retriever node\nThe MultiQuery Retriever node automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query.\nOn this page, you'll find the node parameters for the MultiQuery Retriever node, and links to more resources.\nNode options\nQuery Count: Enter how many different versions of the query to generate.\nTemplates and examples\nRelated resources\nRefer to LangChain's retriever conceptual documentation and LangChain's multiquery retriever API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.retrievervectorstore.md",
    "content": "Vector Store Retriever node\nUse the Vector Store Retriever node to retrieve documents from a vector store.\nOn this page, you'll find the node parameters for the Vector Store Retriever node, and links to more resources.\nNode parameters\nLimit: Enter the maximum number of results to return.\nTemplates and examples\nRelated resources\nRefer to LangChain's vector store retriever documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.retrieverworkflow.md",
    "content": "Workflow Retriever node\nUse the Workflow Retriever node to retrieve data from an n8n workflow for use in a Retrieval QA Chain or another Retriever node.\nOn this page, you'll find the node parameters for the Workflow Retriever node, and links to more resources.\nNode parameters\nSource\nTell n8n which workflow to call. You can choose either:\nDatabase and enter a workflow ID.\nParameter and copy in a complete workflow JSON.\nWorkflow values\nTemplates and examples\nRelated resources\nRefer to LangChain's general retriever documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.textsplittercharactertextsplitter.md",
    "content": "Character Text Splitter node\nUse the Character Text Splitter node to split document data based on characters.\nOn this page, you'll find the node parameters for the Character Text Splitter node, and links to more resources.\nNode parameters\nSeparator: Select the separator used to split the document into separate items.\nChunk Size: Enter the number of characters in each chunk.\nChunk Overlap: Enter how much overlap to have between chunks.\nTemplates and examples\nRelated resources\nRefer to LangChain's text splitter documentation and LangChain's API documentation for character text splitting for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.textsplitterrecursivecharactertextsplitter.md",
    "content": "Recursive Character Text Splitter node\nThe Recursive Character Text Splitter node splits document data recursively to keep all paragraphs, sentences then words together as long as possible.\nOn this page, you'll find the node parameters for the Recursive Character Text Splitter node, and links to more resources.\nNode parameters\nChunk Size: Enter the number of characters in each chunk.\nChunk Overlap: Enter how much overlap to have between chunks.\nTemplates and examples\nRelated resources\nRefer to LangChain's text splitter documentation and LangChain's recursively split by character documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.textsplittertokensplitter.md",
    "content": "Token Splitter node\nThe Token Splitter node splits a raw text string by first converting the text into BPE tokens, then splits these tokens into chunks and converts the tokens within a single chunk back into text.\nOn this page, you'll find the node parameters for the Token Splitter node, and links to more resources.\nNode parameters\nChunk Size: Enter the number of characters in each chunk.\nChunk Overlap: Enter how much overlap to have between chunks.\nTemplates and examples\nRelated resources\nRefer to LangChain's token documentation and LangChain's text splitter documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolcalculator.md",
    "content": "Calculator node\nThe Calculator node is a tool that allows an agent to run mathematical calculations.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolcode.md",
    "content": "Custom Code Tool node\nUse the Custom Code Tool node to write code that an agent can run.\nOn this page, you'll find the node parameters for the Custom Code Tool node and links to more resources.\nNode parameters\nDescription\nGive your custom code a description. This tells the agent when to use this tool. For example:\nCall this tool to get a random color. The input should be a string with comma separated names of colors to exclude.\nLanguage\nYou can use JavaScript or Python.\nJavaScript / Python box\nWrite the code here.\nYou can access the tool input using query. For example, to take the input string and lowercase it:\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolhttprequest.md",
    "content": "HTTP Request Tool node\nThe HTTP Request tool works just like the HTTP Request node, but it's designed to be used with an AI agent as a tool to collect information from a website or API.\nOn this page, you'll find a list of operations the HTTP Request node supports and links to more resources.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolmcp.md",
    "content": "MCP Client Tool node\nThe MCP Client Tool node is a Model Context Protocol (MCP) client, allowing you to use the tools exposed by an external MCP server. You can connect the MCP Client Tool node to your models to call external tools with n8n agents.\nNode parameters\nConfigure the node with the following parameters.\nSSE Endpoint: The SSE endpoint for the MCP server you want to connect to.\nAuthentication: The authentication method for authentication to your MCP server. The MCP tool supports bearer and generic header authentication. Select None to attempt to connect without authentication.\nTools to Include: Choose which tools you want to expose to the AI Agent:\nAll: Expose all the tools given by the MCP server.\nSelected: Activates a Tools to Include parameter where you can select the tools you want to expose to the AI Agent.\nAll Except: Activates a Tools to Exclude parameter where you can select the tools you want to avoid sharing with the AI Agent. The AI Agent will have access to all MCP server's tools that aren't selected.\nTemplates and examples\nRelated resources\nn8n also has an MCP Server Trigger node that allows you to expose n8n tools to external AI Agents.\nRefer to the MCP documentation and MCP specification for more details about the protocol, servers, and clients."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolsearxng.md",
    "content": "SearXNG Tool node\nThe SearXNG Tool node allows you to integrate search capabilities into your workflows using SearXNG. SearXNG aggregates results from multiple search engines without tracking you.\nOn this page, you'll find the node options for the SearXNG Tool node, and links to more resources.\nNode Options\nNumber of Results: The number of results to retrieve. The default is 10.\nPage Number: The page number of the search results to retrieve. The default is 1.\nLanguage: A two-letter language code to filter search results by language. For example: en for English, fr for French. The default is en.\nSafe Search: Enables or disables filtering explicit content in the search results. Can be None, Moderate, or Strict. The default is None.\nRunning a SearXNG instance\nThis node requires running the SearXNG service on the same network as your n8n instance. Ensure your n8n instance has network access to the SearXNG service.\nThis node requires results in JSON format, which isn't enabled in the default SearXNG configuration. To enable JSON output, add json to the search.formats section of your SearXNG instance's settings.yml file:\nIf the formats section isn't there, add it. The exact location of the settings.yml file depends on how you installed SearXNG. You can find more by visiting the SearXNG configuration documentation.\nThe quality and availability of search results depend on the configuration and health of the SearXNG instance you use.\nTemplates and examples\nRelated resources\nRefer to SearXNG's documentation for more information about the service. You can also view LangChain's documentation on their SearXNG integration."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolserpapi.md",
    "content": "SerpApi (Google Search) node\nThe SerpAPI node allows an agent in your workflow to call Google's Search API.\nOn this page, you'll find the node parameters for the SerpAPI node, and links to more resources.\nNode options\nCountry: Enter the country code you'd like to use. Refer to Google GL Parameter: Supported Google Countries for supported countries and country codes.\nDevice: Select the device to use to get the search results.\nExplicit Array: Choose whether to force SerpApi to fetch the Google results even if a cached version is already present (turned on) or not (turned off).\nGoogle Domain: Enter the Google Domain to use. Refer to Supported Google Domains for supported domains.\nLanguage: Enter the language code you'd like to use. Refer to Google HL Parameter: Supported Google Languages for supported languages and language codes.\nTemplates and examples\nRelated resources\nRefer to Serp's documentation for more information about the service. You can also view LangChain's documentation on their Serp integration."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolthink.md",
    "content": "Think Tool node\nThe Think Tool node is a tool that invites an agent to do some thinking before answering. Essentially, it allows the agent to reflect on the question by talking to itself before providing an answer. In particular, this is useful for complex queries that require a thoughtful response.\nTemplates and examples\nPrompting example\nAs an example, say you ask the agent:\nJack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don't know if Anne is married. Is a married person looking at an unmarried person?\nThe agent will do its thinking with the tool:\nWe have the following information: 1. Jack is looking at Anne. 2. Anne is looking at George. 3. Jack is married. 4. George is not married. 5. We don't know if Anne is married or not.\nWe need to determine if a married person is looking at an unmarried person. Let's consider both cases for Anne:\nCase 1: Anne is married.\nIf Anne is married, then she (married) is looking at George (unmarried). In this case, a married person (Anne) is looking at an unmarried person (George).\nCase 2: Anne is unmarried.\nIf Anne is unmarried, then Jack (married) is looking at Anne (unmarried). In this case, a married person (Jack) is looking at an unmarried person (Anne).\nIn both scenarios, we find that a married person is looking at an unmarried person. Therefore, the answer to the question is yes.\nand then afterward provide the answer:\nYes, a married person is looking at an unmarried person. In both possible scenarios regarding Anne's marital status, we find that a married individual (either Jack or Anne) is looking at an unmarried individual (either George or Anne).\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolvectorstore.md",
    "content": "Vector Store Question Answer Tool node\nThe Vector Store Question Answer node is a tool that allows an agent to summarize results and answer questions based on chunks from a vector store.\nOn this page, you'll find the node parameters for the Vector Store Question Answer node, and links to more resources.\nNode parameters\nDescription of Data\nEnter a description of the data in the vector store.\nLimit\nThe maximum number of results to return.\nHow n8n populates the tool description\nn8n uses the node name (select the name to edit) and Description of Data parameter to populate the tool description for AI agents using the following format:\nUseful for when you need to answer questions about [node name]. Whenever you need information about [Description of Data], you should ALWAYS use this. Input should be a fully formed question.\nSpaces in the node name are converted to underscores in the tool description.\nRelated resources\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolwikipedia.md",
    "content": "Wikipedia node\nThe Wikipedia node is a tool that allows an agent to search and return information from Wikipedia.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolwolframalpha.md",
    "content": "WolframAlpha tool node\nUse the Wolfram Alpha tool node to connect your agents and chains to Wolfram Alpha's computational intelligence engine.\n## Templates and examples\n## Related resources\nRefer to Wolfram Alpha's documentation for more information about the service. You can also view LangChain's documentation on their WolframAlpha Tool."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.toolworkflow.md",
    "content": "Call n8n Workflow Tool node\nThe Call n8n Workflow Tool node is a tool that allows an agent to run another n8n workflow and fetch its output data.\nOn this page, you'll find the node parameters for the Call n8n Workflow Tool node, and links to more resources.\nNode parameters\nDescription\nEnter a custom code a description. This tells the agent when to use this tool. For example:\nCall this tool to get a random color. The input should be a string with comma separated names of colors to exclude.\nSource\nTell n8n which workflow to call. You can choose either:\nDatabase to select the workflow from a list or enter a workflow ID.\nDefine Below and copy in a complete workflow JSON.\nWorkflow Inputs\nWhen using Database as workflow source, once you choose a sub-workflow (and define the Workflow Input Schema in the sub-workflow), you can define the Workflow Inputs.\nSelect the Refresh button to pull in the input fields from the sub-workflow.\nYou can define the workflow input values using any combination of the following options:\nproviding fixed values\nusing expressions to reference data from the current workflow\nletting the AI model specify the parameter by selecting the button AI button on the right side of the field\nusing the $fromAI() function in expressions to control the way the model fills in data and to mix AI generated input with other custom input\nTo reference data from the current workflow, drag fields from the input panel to the field with the Expressions mode selected.\nTo get started with the $fromAI() function, select the \"Let the model define this parameter\" button on the right side of the field and then use the X on the box to revert to user-defined values. The field will change to an expression field pre-populated with the $fromAI() expression. From here, you can customize the expression to add other static or dynamic content, or tweak the $fromAI() function parameters.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatollama\\common-issues.md",
    "content": "Ollama Chat Model node common issues\nHere are some common errors and issues with the Ollama Chat Model node and steps to resolve or troubleshoot them.\nProcessing parameters\nThe Ollama Chat Model node is a sub-node. Sub-nodes behave differently than other nodes when processing multiple items using expressions.\nMost nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn.\nIn sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name.\nCan't connect to a remote Ollama instance\nThe Ollama Chat Model node is only designed to connect to a locally hosted Ollama instance. It doesn't include the authentication features you'd need to connect to a remotely hosted Ollama instance.\nTo use the Ollama Chat Model, follow the Ollama credentials instructions to set up Ollama locally and configure the instance URL in n8n.\nCan't connect to a local Ollama instance when using Docker\nThe Ollama Chat Model node connects to a locally hosted Ollama instance using the base URL defined by Ollama credentials. When you run either n8n or Ollama in Docker, you need to configure the network so that n8n can connect to Ollama.\nOllama typically listens for connections on localhost, the local network address. In Docker, by default, each container has its own localhost which is only accessible from within the container. If either n8n or Ollama are running in containers, they won't be able to connect over localhost.\nThe solution depends on how you're hosting the two components.\nIf only Ollama is in Docker\nIf only Ollama is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way).\nWhen running the container, publish the ports with the -p flag. By default, Ollama runs on port 11434, so your Docker command should look like this:\nWhen configuring Ollama credentials, the localhost address should work without a problem (set the base URL to ).\nIf only n8n is in Docker\nIf only n8n is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 on the host.\nIf you are running n8n in Docker on Linux, use the --add-host flag to map host.docker.internal to host-gateway when you start the container. For example:\nIf you are using Docker Desktop, this is automatically configured for you.\nWhen configuring Ollama credentials, use host.docker.internal as the host address instead of localhost. For example, to bind to the default port 11434, you could set the base URL to\nIf Ollama and n8n are running in separate Docker containers\nIf both n8n and Ollama are running in Docker in separate containers, you can use Docker networking to connect them.\nConfigure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way).\nWhen configuring Ollama credentials, use the Ollama container's name as the host address instead of localhost. For example, if you call the Ollama container my-ollama and it listens on the default port 11434, you would set the base URL to\nIf Ollama and n8n are running in the same Docker container\nIf Ollama and n8n are running in the same Docker container, the localhost address doesn't need any special configuration. You can configure Ollama to listen on localhost and configure the base URL in the Ollama credentials in n8n to use localhost:\nError: connect ECONNREFUSED ::1:11434\nThis error occurs when your computer has IPv6 enabled, but Ollama is listening to an IPv4 address.\nTo fix this, change the base URL in your Ollama credentials to connect to 127.0.0.1, the IPv4-specific local address, instead of the localhost alias that can resolve to either IPv4 or IPv6:\nOllama and HTTP/HTTPS proxies\nOllama doesn't support custom HTTP agents in its configuration. This makes it difficult to use Ollama behind custom HTTP/HTTPS proxies. Depending on your proxy configuration, it might not work at all, despite setting the HTTP_PROXY or HTTPS_PROXY environment variables.\nRefer to Ollama's FAQ for more information."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatollama\\index.md",
    "content": "Ollama Chat Model node\nThe Ollama Chat Model node allows you use local Llama 2 models with conversational agents.\nOn this page, you'll find the node parameters for the Ollama Chat Model node, and links to more resources.\nNode parameters\nModel: Select the model that generates the completion. Choose from:\nLlama2\nLlama2 13B\nLlama2 70B\nLlama2 Uncensored\nRefer to the Ollama Models Library documentation for more information about available models.\nNode options\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTop K: Enter the number of token choices the model uses to generate the next token.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nRefer to LangChains's Ollama Chat Model documentation for more information about the service.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatopenai\\common-issues.md",
    "content": "OpenAI Chat Model node common issues\nHere are some common errors and issues with the OpenAI Chat Model node and steps to resolve or troubleshoot them.\nProcessing parameters\nThe OpenAI Chat Model node is a sub-node. Sub-nodes behave differently than other nodes when processing multiple items using expressions.\nMost nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn.\nIn sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmchatopenai\\index.md",
    "content": "OpenAI Chat Model node\nUse the OpenAI Chat Model node to use OpenAI's chat models with conversational agents.\nOn this page, you'll find the node parameters for the OpenAI Chat Model node and links to more resources.\nNode parameters\nModel\nSelect the model to use to generate the completion.\nn8n dynamically loads models from OpenAI and you'll only see the models available to your account.\nNode options\nUse these options to further refine the node's behavior.\nBase URL\nEnter a URL here to override the default URL for the API.\nFrequency Penalty\nUse this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\nMaximum Number of Tokens\nEnter the maximum number of tokens used, which sets the completion length.\nResponse Format\nChoose Text or JSON. JSON ensures the model returns valid JSON.\nPresence Penalty\nUse this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\nSampling Temperature\nUse this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTimeout\nEnter the maximum request time in milliseconds.\nMax Retries\nEnter the maximum number of times to retry a request.\nTop P\nUse this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nRefer to LangChains's OpenAI documentation for more information about the service.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmollama\\common-issues.md",
    "content": "Ollama Model node common issues\nHere are some common errors and issues with the Ollama Model node and steps to resolve or troubleshoot them.\nProcessing parameters\nThe Ollama Model node is a sub-node. Sub-nodes behave differently than other nodes when processing multiple items using expressions.\nMost nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn.\nIn sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name.\nCan't connect to a remote Ollama instance\nThe Ollama Model node is only designed to connect to a locally hosted Ollama instance. It doesn't include the authentication features you'd need to connect to a remotely hosted Ollama instance.\nTo use the Ollama Model, follow the Ollama credentials instructions to set up Ollama locally and configure the instance URL in n8n.\nCan't connect to a local Ollama instance when using Docker\nThe Ollama Model node connects to a locally hosted Ollama instance using the base URL defined by Ollama credentials. When you run either n8n or Ollama in Docker, you need to configure the network so that n8n can connect to Ollama.\nOllama typically listens for connections on localhost, the local network address. In Docker, by default, each container has its own localhost which is only accessible from within the container. If either n8n or Ollama are running in containers, they won't be able to connect over localhost.\nThe solution depends on how you're hosting the two components.\nIf only Ollama is in Docker\nIf only Ollama is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way).\nWhen running the container, publish the ports with the -p flag. By default, Ollama runs on port 11434, so your Docker command should look like this:\nWhen configuring Ollama credentials, the localhost address should work without a problem (set the base URL to ).\nIf only n8n is in Docker\nIf only n8n is running in Docker, configure Ollama to listen on all interfaces by binding to 0.0.0.0 on the host.\nIf you are running n8n in Docker on Linux, use the --add-host flag to map host.docker.internal to host-gateway when you start the container. For example:\nIf you are using Docker Desktop, this is automatically configured for you.\nWhen configuring Ollama credentials, use host.docker.internal as the host address instead of localhost. For example, to bind to the default port 11434, you could set the base URL to\nIf Ollama and n8n are running in separate Docker containers\nIf both n8n and Ollama are running in Docker in separate containers, you can use Docker networking to connect them.\nConfigure Ollama to listen on all interfaces by binding to 0.0.0.0 inside of the container (the official images are already configured this way).\nWhen configuring Ollama credentials, use the Ollama container's name as the host address instead of localhost. For example, if you call the Ollama container my-ollama and it listens on the default port 11434, you would set the base URL to\nIf Ollama and n8n are running in the same Docker container\nIf Ollama and n8n are running in the same Docker container, the localhost address doesn't need any special configuration. You can configure Ollama to listen on localhost and configure the base URL in the Ollama credentials in n8n to use localhost:\nError: connect ECONNREFUSED ::1:11434\nThis error occurs when your computer has IPv6 enabled, but Ollama is listening to an IPv4 address.\nTo fix this, change the base URL in your Ollama credentials to connect to 127.0.0.1, the IPv4-specific local address, instead of the localhost alias that can resolve to either IPv4 or IPv6:\nOllama and HTTP/HTTPS proxies\nOllama doesn't support custom HTTP agents in its configuration. This makes it difficult to use Ollama behind custom HTTP/HTTPS proxies. Depending on your proxy configuration, it might not work at all, despite setting the HTTP_PROXY or HTTPS_PROXY environment variables.\nRefer to Ollama's FAQ for more information."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.lmollama\\index.md",
    "content": "Ollama Model node\nThe Ollama Model node allows you use local Llama 2 models.\nOn this page, you'll find the node parameters for the Ollama Model node, and links to more resources.\nThis node lacks tools support, so it won't work with the AI Agent node. Instead, connect it with the Basic LLM Chain node.\nNode parameters\nModel: Select the model that generates the completion. Choose from:\nLlama2\nLlama2 13B\nLlama2 70B\nLlama2 Uncensored\nRefer to the Ollama Models Library documentation for more information about available models.\nNode options\nSampling Temperature: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\nTop K: Enter the number of token choices the model uses to generate the next token.\nTop P: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\nTemplates and examples\nRelated resources\nRefer to LangChains's Ollama documentation for more information about the service.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memorybufferwindow\\common-issues.md",
    "content": "Simple Memory node common issues\nHere are some common errors and issues with the Simple Memory node and steps to resolve or troubleshoot them.\nSingle memory instance\nIf you add more than one Simple Memory node to your workflow, all nodes access the same memory instance by default. Be careful when doing destructive actions that override existing memory contents, such as the override all messages operation in the Chat Memory Manager node. If you want more than one memory instance in your workflow, set different session IDs in different memory nodes.\nManaging the Session ID\nIn most cases, the sessionId is automatically retrieved from the On Chat Message trigger. But you may run into an error with the phrase No sessionId.\nIf you have this error, first check the output of your Chat trigger to ensure it includes a sessionId.\nIf you're not using the On Chat Message trigger, you'll need to manage sessions manually.\nFor testing purposes, you can use a static key like my_test_session. If you use this approach, be sure to set up proper session management before activating the workflow to avoid potential issues in a live environment."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.memorybufferwindow\\index.md",
    "content": "Simple Memory node\nUse the Simple Memory node to persist chat history in your workflow.\nOn this page, you'll find a list of operations the Simple Memory node supports, and links to more resources.\nNode parameters\nConfigure these parameters to configure the node:\nSession Key: Enter the key to use to store the memory in the workflow data.\nContext Window Length: Enter the number of previous interactions to consider for context.\nTemplates and examples\nRelated resources\nRefer to LangChain's Buffer Window Memory documentation for more information about the service.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.outputparserstructured\\common-issues.md",
    "content": "Structured Output Parser node common issues\nHere are some common errors and issues with the Structured Output Parser node and steps to resolve or troubleshoot them.\nProcessing parameters\nThe Structured Output Parser node is a sub-node. Sub-nodes behave differently than other nodes when processing multiple items using expressions.\nMost nodes, including root nodes, take any number of items as input, process these items, and output the results. You can use expressions to refer to input items, and the node resolves the expression for each item in turn. For example, given an input of five name values, the expression {{ $json.name }} resolves to each name in turn.\nIn sub-nodes, the expression always resolves to the first item. For example, given an input of five name values, the expression {{ $json.name }} always resolves to the first name.\nAdding the structured output parser node to AI nodes\nYou can attach output parser nodes to select AI root nodes.\nTo add the Structured Output Parser to a node, enable the Require Specific Output Format option in the AI root node you wish to format. Once the option is enabled, a new output parser attachment point is displayed. Click the output parser attachment point to add the Structured Output Parser node to the node.\nUsing the structured output parser to format intermediary steps\nThe Structured Output Parser node structures the final output from AI agents. It's not intended to structure intermediary output to pass to other AI tools or stages.\nTo request a specific format for intermediary output, include the response structure in the System Message for the AI Agent. The message can include either a schema or example response for the agent to use as a template for its results.\nStructuring output from agents\nStructured output parsing is often not reliable when working with agents.\nIf your workflow uses agents, n8n recommends using a separate LLM-chain to receive the data from the agent and parse it. This leads to better, more consistent results than parsing directly in the agent workflow."
  },
  {
    "file_path": "integrations\\builtin\\cluster-nodes\\sub-nodes\\n8n-nodes-langchain.outputparserstructured\\index.md",
    "content": "Structured Output Parser node\nUse the Structured Output Parser node to return fields based on a JSON Schema.\nOn this page, you'll find the node parameters for the Structured Output Parser node, and links to more resources.\nNode parameters\nSchema Type: Define the output structure and validation. You have two options to provide the schema:\nTemplates and examples\nRelated resources\nRefer to LangChain's output parser documentation for more information about the service.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\index.md",
    "content": "Core nodes library\nThis section provides information about n8n's core nodes."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.activationtrigger.md",
    "content": "Activation Trigger node\nThe Activation Trigger node gets triggered when an event gets fired by n8n or a workflow.\nThe Activation Trigger node gets triggered for the workflow that it gets added to. You can use the Activation Trigger node to trigger a workflow to notify the state of the workflow.\nNode parameters\nEvents\nActivation: Run when the workflow gets activated\nStart: Run when n8n starts or restarts\nUpdate: Run when the workflow gets saved while it's active\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.aggregate.md",
    "content": "Aggregate\nUse the Aggregate node to take separate items, or portions of them, and group them together into individual items.\nNode parameters\nTo begin using the node, select the Aggregate you'd like to use:\nIndividual Fields: Aggregate individual fields separately.\nAll Item Data: Aggregate all item data into a single list.\nIndividual Fields\nInput Field Name: Enter the name of the field in the input data to aggregate together.\nRename Field: This toggle controls whether to give the field a different name in the aggregated output data. Turn this on to add a different field name. If you're aggregating multiple fields, you must provide new output field names. You can't leave multiple fields undefined.\nOutput Field Name: This field is displayed when you turn on Rename Field. Enter the field name for the aggregated output data.\nRefer to Node options for more configuration options.\nAll Item Data\nPut Output in Field: Enter the name of the field to output the data in.\nInclude: Select which fields to include in the output. Choose from:\nAll fields: The output includes data from all fields with no further parameters.\nSpecified Fields: If you select this option, enter a comma-separated list of fields the output should include data from in the Fields To Include parameter. The output will include only the fields in this list.\nAll Fields Except: If you select this option, enter a comma-separated list of fields the output should exclude data from in the Fields To Exclude parameter. The output will include all fields not in this list.\nRefer to Node options for more configuration options.\nNode options\nYou can further configure this node using these Options:\nDisable Dot Notation: The node displays this toggle when you select the Individual Fields Aggregate. It controls whether to disallow referencing child fields using parent.child in the field name (turned on), or allow it (turned off, default).\nMerge Lists: The node displays this toggle when you select the Individual Fields Aggregate. Turn it on if the field to aggregate is a list and you want to output a single flat list rather than a list of lists.\nInclude Binaries: The node displays this toggle for both Aggregate types. Turn it on if you want to include binary data from the input in the new output.\nKeep Missing And Null Values: The node displays this toggle when you select the Individual Fields Aggregate. Turn it on to add a null (empty) entry in the output list when there is a null or missing value in the input. If turned off, the output ignores null or empty values.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.aitransform.md",
    "content": "AI Transform\nUse the AI Transform node to generate code snippets based on your prompt. The AI is context-aware, understanding the workflow‚Äôs nodes and their data types.\nNode parameters\nInstructions\nEnter your prompt for the AI and click the Generate code button to automatically populate the Transformation Code. For example, you can specify how you want to process or categorize your data. Refer to Writing good prompts for more information.\nThe prompt should be in plain English and under 500 characters.\nTransformation Code\nThe code snippet generated by the node is read-only. To edit this code, adjust your prompt in Instructions or copy and paste it into a Code node.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.comparedatasets.md",
    "content": "Compare Datasets\nThe Compare Datasets node helps you compare data from two input streams.\nNode parameters\nDecide which fields to compare. In Input A Field, enter the name of the field you want to use from input stream A. In Input B Field, enter the name of the field you want to use from input stream B.\nOptional: You can compare by multiple fields. Select Add Fields to Match to set up more comparisons.\nChoose how to handle differences between the datasets. In When There Are Differences, select one of the following:\nUse Input A Version to treat input stream A as the source of truth.\nUse Input B Version to treat input stream B as the source of truth.\nUse a Mix of Versions to use different inputs for different fields.\nUse Prefer to select either Input A Version or Input B Version as the main source of truth.\nEnter input fields that are exceptions to For Everything Except to pull from the other input source. To add multiple input fields, enter a comma-separated list.\nInclude Both Versions to include both input streams in the output, which may make the structure more complex.\nDecide whether to use Fuzzy Compare. When turned on, the comparison will tolerate small type differences when comparing fields. For example, the number 3 and the string 3 are treated as the same with Fuzzy Compare turned on, but wouldn't be treated the same with it turned off.\nUnderstand item comparison\nItem comparison is a two stage process:\nn8n checks if the values of the fields you selected to compare match across both inputs.\nIf the fields to compare match, n8n then compares all fields within the items, to determine if the items are the same or different.\nNode options\nUse the node Options to refine your comparison or tweak comparison behavior.\nFields to Skip Comparing\nEnter field names that you want to ignore in the comparison.\nFor example, if you compare the two datasets below using person.language as the Fields to Match, n8n returns them as different. If you add person.name to Fields to Skip Comparing, n8n returns them as matching.\nDisable Dot Notation\nWhether to disallow referencing child fields using parent.child in the field name (turned on) or allow it (turned off, default).\nMultiple Matches\nChoose how to handle duplicate data. The default is Include All Matches. You can choose Include First Match Only.\nFor example, given these two datasets:\nn8n returns three items in the Same Branch tab. The data is the same in both branches.\nIf you select Include First Match Only, n8n returns two items, in the Same Branch tab. The data is the same in both branches, but n8n only returns the first occurrence of the matching \"apple\" items.\nUnderstand the output\nThere are four output options:\nIn A only Branch: Contains data that occurs only in the first input.\nSame Branch: Contains data that's the same in both inputs.\nDifferent Branch: Contains data that's different between inputs.\nIn B only Branch: Contains data that occurs only in the second output.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.compression.md",
    "content": "Compression\nUse the Compression node to compress and decompress files. Supports Zip and Gzip formats.\nNode parameters\nThe node parameters depend on which Operation you select. Choose to:\nCompress: Create a compressed file from your input data.\nDecompress: Decompress an existing compressed file.\nRefer to the sections below for parameters specific to each Operation.\nCompress\nInput Binary Field(s): Enter the name of the fields in the input data that contain the binary files you want to compress. To compress more than one file, use a comma-separated list.\nOutput Format: Choose whether to format the compressed output as Zip or Gzip.\nFile Name: Enter the name of the zip file the node creates.\nPut Output File in Field: Enter the name of the field in the output data to contain the file.\nDecompress\nPut Output File in Field: Enter the name of the fields in the input data that contain the binary files you want to decompress. To decompress more than one file, use a comma-separated list.\nOutput Prefix: Enter a prefix to add to the output file name.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.converttofile.md",
    "content": "Convert to File\nUse the Convert to File node to take input data and output it as a file. This converts the input JSON data into a binary format.\nOperations\nConvert to CSV\nConvert to HTML\nConvert to ICS\nConvert to JSON\nConvert to ODS\nConvert to RTF\nConvert to Text File\nConvert to XLS\nConvert to XLSX\nMove Base64 String to File\nNode parameters and options depend on the operation you select.\nConvert to CSV\nConfigure the node for this operation with the Put Output File in Field parameter. Enter the name of the field in the output data to contain the file.\nConvert to CSV options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nIf the first row of the file contains header names, turn on the Header Row option.\nConvert to HTML\nConfigure the node for this operation with the Put Output File in Field parameter. Enter the name of the field in the output data to contain the file.\nConvert to HTML options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nIf the first row of the file contains header names, turn on the Header Row option.\nConvert to ICS\nPut Output File in Field. Enter the name of the field in the output data to contain the file.\nEvent Title: Enter the title for the event.\nStart: Enter the date and time the event will start. All-day events ignore the time.\nEnd: Enter the date and time the event will end. All-day events ignore the time. If unset, the node uses the start date.\nAll Day: Select whether the event is an all day event (turned on) or not (turned off).\nConvert to ICS options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nAttendees: Use this option to add attendees to the event. For each attendee, add:\nName\nEmail\nRSVP: Select whether the attendee needs to confirm attendance (turned on) or doesn't (turned off).\nBusy Status: Use this option to set the busy status for Microsoft applications like Outlook. Choose from:\nBusy\nTentative\nCalendar Name: For Apple and Microsoft calendars, enter the calendar name for the event.\nDescription: Enter an event description.\nGeolocation: Enter the Latitude and Longitude for the event's location.\nLocation: Enter the event's intended venue/location.\nRecurrence Rule: Enter a rule to define the repeat pattern of the event (RRULE). Generate rules using the iCalendar.org RRULE Tool.\nOrganizer: Enter the organizer's Name and Email.\nSequence: If you're sending an update for an event with the same universally unique ID (UID), enter the revision sequence number.\nStatus: Set the status of the event. Choose from:\nConfirmed\nCancelled\nTentative\nUID: Enter a universally unique ID (UID) for the event. The UID should be globally unique. The node automatically generates a UID if you don't enter one.\nURL: Enter a URL associated with the event.\nUse Workflow Timezone: Whether to use UTC time zone (turned off) or the workflow's timezone (turned on). Set the workflow's timezone in the Workflow Settings.\nConvert to JSON\nChoose the best output Mode for your needs from these options:\nAll Items to One File: Send all input items to a single file.\nEach Item to Separate File: Create a file for every input item.\nConvert to JSON options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nFormat: Choose whether to format the JSON for easier reading (turned on) or not (turned off).\nEncoding: Choose the character set to use to encode the data. The default is utf8.\nConvert to ODS\nConfigure the node for this operation with the Put Output File in Field parameter. Enter the name of the field in the output data to contain the file.\nConvert to ODS options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nCompression: Choose whether to compress and reduce the file's output size.\nHeader Row: Turn on if the first row of the file contains header names.\nSheet Name: Enter the Sheet Name to create in the spreadsheet.\nConvert to RTF\nConfigure the node for this operation with the Put Output File in Field parameter. Enter the name of the field in the output data to contain the file.\nConvert to RFT options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nIf the first row of the file contains header names, turn on the Header Row option.\nConvert to Text File\nEnter the name of the Text Input Field that contains a string to convert to a file. Use dot-notation for deep fields, for example level1.level2.currentKey.\nConvert to Text File options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nEncoding: Choose the character set to use to encode the data. The default is utf8.\nConvert to XLS\nConfigure the node for this operation with the Put Output File in Field parameter. Enter the name of the field in the output data to contain the file.\nConvert to XLS options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nHeader Row: Turn on if the first row of the file contains header names.\nSheet Name: Enter the Sheet Name to create in the spreadsheet.\nConvert to XLSX\nConfigure the node for this operation with the Put Output File in Field parameter. Enter the name of the field in the output data to contain the file.\nConvert to XLSX options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nCompression: Choose whether to compress and reduce the file's output size.\nHeader Row: Turn on if the first row of the file contains header names.\nSheet Name: Enter the Sheet Name to create in the spreadsheet.\nMove Base64 String to File\nEnter the name of the Base64 Input Field that contains the Base64 string to convert to a file. Use dot-notation for deep fields, for example level1.level2.currentKey.\nMove Base64 String to File options\nYou can also configure this operation with these Options:\nFile Name: Enter the file name for the generated output file.\nMIME Type: Enter the MIME type of the output file. Refer to Common MIME types for a list of common MIME types and the file extensions they relate to.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.crypto.md",
    "content": "Crypto\nUse the Crypto node to encrypt data in workflows.\nActions\nGenerate a random string\nHash a text or file in a specified format\nHmac a text or file in a specified format\nSign a string using a private key\nNode parameters\nNode parameters depend on the action you select.\nGenerate parameters\nProperty Name: Enter the name of the property to write the random string to.\nType: Select the encoding type to use to generate the string. Choose from:\nASCII\nBASE64\nHEX\nUUID\nHash parameters\nType: Select the hash type to use. Choose from:\nMD5\nSHA256\nSHA3-256\nSHA3-384\nSHA3-512\nSHA385\nSHA512\nBinary File: Turn this parameter on if the data you want to hash is from a binary file.\nValue: If you turn off Binary File, enter the value you want to hash.\nBinary Property Name: If you turn on Binary File, enter the name of the binary property that contains the data you want to hash.\nProperty Name: Enter the name of the property you want to write the hash to.\nEncoding: Select the encoding type to use. Choose from:\nBASE64\nHEX\nHmac parameters\nBinary File: Turn this parameter on if the data you want to encrypt is from a binary file.\nValue: If you turn off Binary File, enter the value you want to encrypt.\nBinary Property Name: If you turn on Binary File, enter the name of the binary property that contains the data you want to encrypt.\nType: Select the encryption type to use. Choose from:\nMD5\nSHA256\nSHA3-256\nSHA3-384\nSHA3-512\nSHA385\nSHA512\nProperty Name: Enter the name of the property you want to write the hash to.\nSecret: Enter the secret or secret key used for decoding.\nEncoding: Select the encoding type to use. Choose from:\nBASE64\nHEX\nSign parameters\nValue: Enter the value you want to sign.\nProperty Name: Enter the name of the property you want to write the signed value to.\nAlgorithm Name or ID: Choose an algorithm name from the list or specify an ID using an expression.\nEncoding: Select the encoding type to use. Choose from:\nBASE64\nHEX\nPrivate Key: Enter a private key to use when signing the string.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.datetime.md",
    "content": "Date & Time\nThe Date & Time node manipulates date and time data and convert it to different formats.\nOperations\nAdd to a Date: Add a specified amount of time to a date.\nExtract Part of a Date: Extract part of a date, such as the year, month, or day.\nFormat a Date: Transform a date's format to a new format using preset options or a custom expression.\nGet Current Date: Get the current date and choose whether to include the current time or not. Useful for triggering other flows and conditional logic.\nGet Time Between Dates: Calculate the amount of time in specific units between two dates.\nRound a Date: Round a date up or down to the nearest unit of your choice, such as month, day, or hour.\nSubtract From a Date: Subtract a specified amount of time from a date.\nRefer to the sections below for parameters and options specific to each operation.\nAdd to a Date\nConfigure the node for this operation using these parameters:\nDate to Add To: Enter the date you want to change.\nTime Unit to Add: Select the time unit for the Duration parameter.\nDuration: Enter the number of time units to add to the date.\nOutput Field Name: Enter the name of the field to output the new date to.\nAdd to a Date options\nThis operation has one option: Include Input Fields. If you'd like to include all of the input fields in the output, turn this option on. If turned off, only the Output Field Name and its contents are output.\nExtract Part of a Date\nConfigure the node for this operation using these parameters:\nDate: Enter the date you want to round or extract part of.\nPart: Select the part of the date you want to extract. Choose from:\nYear\nMonth\nWeek\nDay\nHour\nMinute\nSecond\nOutput Field Name: Enter the name of the field to output the extracted date part to.\nExtract Part of a Date options\nThis operation has one option: Include Input Fields. If you'd like to include all of the input fields in the output, turn this option on. If turned off, only the Output Field Name and its contents are output.\nFormat a Date\nConfigure the node for this operation using these parameters:\nDate: Enter the date you want to format.\nFormat: Select the format you want to change the date to. Choose from:\nCustom Format: Enter your own custom format using Luxon's special tokens. Tokens are case-sensitive.\nMM/DD/YYYY: For 4 September 1986, this formats the date as 09/04/1986.\nYYYY/MM/DD: For 4 September 1986, this formats the date as 1986/09/04.\nMMMM DD YYYY: For 4 September 1986, this formats the date as September 04 1986.\nMM-DD-YYYY: For 4 September 1986, this formats the date as 09-04-1986.\nYYYY-MM-DD: For 4 September 1986, this formats the date as 1986-09-04.\nOutput Field Name: Enter the name of the field to output the formatted date to.\nFormat a Date options\nThis operation includes these options:\nInclude Input Fields: If you'd like to include all of the input fields in the output, turn this option on. If turned off, only the Output Field Name and its contents are output.\nFrom Date Format: If the node isn't recognizing the Date format correctly, enter the format for that Date here so the node can process it properly. Use Luxon's special tokens to enter the format. Tokens are case-sensitive\nUse Workflow Timezone: Whether to use the input's time zone (turned off) or the workflow's timezone (turned on).\nGet Current Date\nConfigure the node for this operation using these parameters:\nInclude Current Time: Choose whether to include the current time (turned on) or to set the time to midnight (turned off).\nOutput Field Name: Enter the name of the field to output the current date to.\nGet Current Date options\nThis operation includes these options:\nInclude Input Fields: If you'd like to include all of the input fields in the output, turn this option on. If turned off, only the Output Field Name and its contents are output.\nTimezone: Set the timezone to use. If left blank, the node uses the n8n instance's timezone.\nGet Time Between Dates\nConfigure the node for this operation using these parameters:\nStart Date: Enter the earlier date you want to compare.\nEnd Date: Enter the later date you want to compare.\nUnits: Select the units you want to calculate the time between. You can include multiple units. Choose from:\nYear\nMonth\nWeek\nDay\nHour\nMinute\nSecond\nMillisecond\nOutput Field Name: Enter the name of the field to output the calculated time between to.\nGet Time Between Dates options\nThe Get Time Between Dates operation includes the Include Input Fields option as well as an Output as ISO String option. If you leave this option off, each unit you selected will return its own time difference calculation, for example:\ntimeDifference\nyears : 1\nmonths : 3\ndays : 13\nIf you turn on the Output as ISO String option, the node formats the output as a single ISO duration string, for example: P1Y3M13D.\nISO duration format displays a format as PYMDTHMS.  is the number for the unit after it.\nP = period (duration). It begins all ISO duration strings.\nY = years\nM = months\nW = weeks\nD = days\nT = delineator between dates and times, used to avoid confusion between months and minutes\nH = hours\nM = minutes\nS = seconds\nMilliseconds don't get their own unit, but instead are decimal seconds. For example, 2.1 milliseconds is 0.0021S.\nRound a Date\nConfigure the node for this operation using these parameters:\nDate: Enter the date you'd like to round.\nMode: Choose whether to Round Down or Round Up.\nTo Nearest: Select the unit you'd like to round to. Choose from:\nYear\nMonth\nWeek\nDay\nHour\nMinute\nSecond\nOutput Field Name: Enter the name of the field to output the rounded date to.\nRound a Date options\nThis operation has one option: Include Input Fields. If you'd like to include all of the input fields in the output, turn this option on. If turned off, only the Output Field Name and its contents are output.\nSubtract From a Date\nConfigure the node for this operation using these parameters:\nDate to Subtract From: Enter the date you'd like to subtract from.\nTime Unit to Subtract: Select the unit for the Duration amount you want to subtract.\nDuration: Enter the amount of the time units you want to subtract from the Date to Subtract From.\nOutput Field Name: Enter the name of the field to output the rounded date to.\nSubtract From a Date options\nThis operation has one option: Include Input Fields. If you'd like to include all of the input fields in the output, turn this option on. If turned off, only the Output Field Name and its contents are output.\nTemplates and examples\nRelated resources\nThe Date & Time node uses Luxon. You can also use Luxon in the Code node and expressions. Refer to Date and time with Luxon for more information.\nSupported date formats\nn8n supports all date formats supported by Luxon. Tokens are case-sensitive."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.debughelper.md",
    "content": "Debug Helper\nUse the Debug Helper node to trigger different error types or generate random datasets to help test n8n workflows.\nOperations\nDefine the operation by selecting the Category:\nDo Nothing: Don't do anything.\nThrow Error: Throw an error with the specified type and message.\nOut Of Memory: Generate a specific memory size to simulate being out of memory.\nGenerate Random Data: Generate some random data in a selected format.\nNode parameters\nThe node parameters depend on the Category selected. The Do Nothing Category has no other parameters.\nThrow Error\nError Type: Select the type of error to throw. Choose from:\nNodeApiError\nNodeOperationError\nError\nError Message: Enter the error message to throw.\nOut Of Memory\nThe Out of Memory Category adds one parameter, the Memory Size to Generate. Enter the approximate amount of memory to generate.\nGenerate Random Data\nData Type: Choose the type of random data you'd like to generate. Options include:\nAddress\nCoordinates\nCredit Card\nEmail\nIPv4\nIPv6\nMAC\nNanoids: If you select this data type, you'll also need to enter:\nNanoid Alphabet: The alphabet the generator will use to generate the nanoids.\nNanoid Length: The length of each nanoid.\nURL\nUser Data\nUUID\nVersion\nSeed: If you'd like to generate the data using a specific seed, enter it here. This ensures the data gets generated consistently. If you'd rather use random data generation, leave this field empty.\nNumber of Items to Generate: Enter the number of random items you'd like to generate.\nOutput as Single Array: Whether to generate the data as a single array (turned on) or multiple items (turned off).\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.editimage.md",
    "content": "Edit Image\nUse the Edit Image node to manipulate and edit images.\nOperations\nAdd a Blur to the image to reduce sharpness\nAdd a Border to the image\nComposite an image on top of another image\nCreate a new image\nCrop the image\nDraw on an image\nGet Information about the image\nMulti Step perform multiple operations on the image\nResize: Change the size of the image\nRotate the image\nShear image along the X or Y axis\nAdd Text to the image\nMake a color in image Transparent\nNode parameters\nThe parameters for this node depend on the operation you select.\nBlur parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nBlur: Enter a number to set how strong the blur should be, between 0 and 1000. Higher numbers create blurrier images.\nSigma: Enter a number to set the stigma for the blur, between 0 and 1000. Higher numbers create blurrier images.\nRefer to Node options for optional configuration options.\nBorder parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nBorder Width: Enter the width of the border.\nBorder Height: Enter the height of the border.\nBorder Color: Set the color for the border. You can either enter a hex or select the color swatch to open a color picker.\nRefer to Node options for optional configuration options.\nComposite parameters\nProperty Name: Enter the name of the binary property that stores the image data. This image is your base image.\nComposite Image Property: Enter the name of the binary property that stores image to composite on top of the Property Name image.\nOperator: Select composite operator, which determines how the composite works. Options include:\nAdd\nAtop\nBumpmap\nCopy\nCopy Black\nCopy Blue\nCopy Cyan\nCopy Green\nCopy Magenta\nCopy Opacity\nCopy Red\nCopy Yellow\nDifference\nDivide\nIn\nMinus\nMultiply\nOut\nOver\nPlus\nSubtract\nXor\nPosition X: Enter the x axis position (horizontal) of the composite image.\nPosition Y: Enter the y axis position (vertical) of the composite image.\nRefer to Node options for optional configuration options.\nCreate parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nBackground Color: Set the background color for the image. You can either enter a hex or select the color swatch to open a color picker.\nImage Width: Enter the width of the image.\nImage Height: Enter the height of the image.\nRefer to Node options for optional configuration options.\nCrop parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nWidth: Enter the width you'd like to crop to.\nHeight: Enter the height you'd like to crop to.\nPosition X: Enter the x axis position (horizontal) to start the crop from.\nPosition Y: Enter the y axis position (vertical) to start the crop from.\nRefer to Node options for optional configuration options.\nDraw parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nPrimitive: Select the primitive shape to draw. Choose from:\nCircle\nLine\nRectangle\nColor: Set the color for the primitive. You can either enter a hex or select the color swatch to open a color picker.\nStart Position X: Enter the x axis position (horizontal) to start drawing from.\nStart Position Y: Enter the y axis position (vertical) to start drawing from.\nEnd Position X: Enter the x axis position (horizontal) to stop drawing at.\nEnd Position Y: Enter the y axis position (vertical) to start drawing at.\nCorner Radius: Enter a number to set the corner radius. Adding a corner radius will round the corners of the drawn primitive.\nRefer to Node options for optional configuration options.\nGet Information parameters\nFor this operation, you only need to add the Property Name of the binary property that stores the image data.\nRefer to Node options for optional configuration options.\nMulti Step parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nOperations: Add the operations you want the multi step operation to perform. You can use any of the other operations.\nRefer to Node options for optional configuration options.\nResize parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nWidth: Enter the new width you'd like for the image.\nHeight: Enter the new height you'd like for the image.\nOption: Select how you'd like to resize the image. Choose from:\nIgnore Aspect Ratio: Ignore the aspect ratio and resize to the exact height and width you've entered.\nMaximum Area: The height and width you've entered is the maximum area/size for the image. The image maintains its aspect ratio and won't be larger than the height and/or width you've entered.\nMinimum Area: The height and width you've entered is the minimum area/size for the image. The image maintains its aspect ratio and won't be smaller than the height and/or width you've entered.\nOnly if Larger: Resize the image only if it's larger than the width and height you entered. The image maintains its aspect ratio.\nOnly if Smaller: Resize the image only if it's smaller than the width and height you entered. The image maintains its aspect ratio.\nPercent: Resize the image using the width and height as percentages of the original image.\nRefer to Node options for optional configuration options.\nRotate parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nRotate: Enter the number of degrees to rotate the image, from --360 to 360.\nBackground Color: Set the background color for the image. You can either enter a hex or select the color swatch to open a color picker. This color is used to fill in the empty background whenever the image is rotated by multiples of 90 degrees. If multipled of 90 degrees are used for the Rotate field, the background color isn't used.\nRefer to Node options for optional configuration options.\nShear parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nDegrees X: Enter the number of degrees to shear from the x axis.\nDegrees Y: Enter the number of degrees to shear from the y axis.\nRefer to Node options for optional configuration options.\nText parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nText: Enter the text you'd like to write on the image.\nFont Size: Select the font size for the text.\nFont Color: Set the font color. You can either enter a hex or select the color swatch to open a color picker.\nPosition X: Enter the x axis position (horizontal) to begin the text at.\nPosition Y: Enter the y axis position (vertical) to begin the text at.\nMax Line Length: Enter the maximum amount of characters in a line before adding a line break.\nRefer to Node options for optional configuration options.\nTransparent parameters\nProperty Name: Enter the name of the binary property that stores the image data.\nColor: Set the color to make transparent. You can either enter a hex or select the color swatch to open a color picker.\nRefer to Node options for optional configuration options.\nNode options\nFile Name: Enter the filename of the output file.\nFormat: Enter the image format of the output file. Choose from:\nbmp\ngif\njpeg\npng\ntiff\nWebP\nThe Text operation also includes the option for Font Name or ID. Select the text font from the dropdown or specify an ID using an expression.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.emailimap.md",
    "content": "Email Trigger (IMAP) node\nUse the IMAP Email node to receive emails using an IMAP email server. This node is a trigger node.\nOperations\nReceive an email\nNode parameters\nConfigure the node using the following parameters.\nCredential to connect with\nSelect or create an IMAP credential to connect to the server with.\nMailbox Name\nEnter the mailbox from which you want to receive emails.\nAction\nChoose whether you want an email marked as read when n8n receives it. None will leave it marked unread. Mark as Read will mark it as read.\nDownload Attachments\nThis toggle controls whether to download email attachments (turned on) or not (turned off). Only set this if necessary, since it increases processing.\nFormat\nChoose the format to return the message in from these options:\nRAW: This format returns the full email message data with body content in the raw field as a base64url encoded string. It doesn't use the payload field.\nResolved: This format returns the full email with all data resolved and attachments saved as binary data.\nSimple: This format returns the full email. Don't use it if you want to gather inline attachments.\nNode options\nYou can further configure the node using these Options.\nCustom Email Rules\nEnter custom email fetching rules to determine which emails the node fetches.\nRefer to node-imap's search function criteria for more information.\nForce Reconnect Every Minutes\nSet an interval in minutes to force reconnection.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.errortrigger.md",
    "content": "Error Trigger node\nYou can use the Error Trigger node to create error workflows. When another linked workflow fails, this node gets details about the failed workflow and the errors, and runs the error workflow.\nUsage\nNote the following:\nIf a workflow uses the Error Trigger node, you don't have to activate the workflow.\nIf a workflow contains the Error Trigger node, by default, the workflow uses itself as the error workflow.\nYou can't test error workflows when running workflows manually. The Error Trigger only runs when an automatic workflow errors.\nTemplates and examples\nRelated resources\nYou can use the Stop And Error node to send custom messages to the Error Trigger.\nRead more about Error workflows in n8n workflows.\nError data"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.evaluation.md",
    "content": "Evaluation node\nThe Evaluation node performs various operations related to evaluations to validate your AI workflow reliability. You can use the Evaluation node to conditionally execute logic based on whether the workflow is under evaluation, to write evaluation outcomes back to a Google Sheet dataset, or to log scoring metrics for your evaluation performance to n8n's evaluations tab.\nOperations\nThe Evaluation node offers the following operations:\nSet Outputs: Write the results of an evaluation back to a Google Sheet dataset.\nSet Metrics: Record metrics scoring the evaluation performance to n8n's Evaluations tab.\nCheck If Evaluating: Branches the workflow execution logic depending on whether the current execution is an evaluation.\nThe parameters and options available depend on the operation you select.\nSet Outputs\nThe Set Outputs operation has the following parameters:\nCredential to connect with: Create or select an existing Google Sheets credentials.\nDocument Containing Dataset: Choose the spreadsheet document you want to write the evaluation results to. Usually this is the same document you select in the Evaluation Trigger node.\nSelect From list to choose the spreadsheet title from the dropdown list, By URL to enter the url of the spreadsheet, or By ID to enter the spreadsheetId.\nYou can find the spreadsheetId in a Google Sheets URL:\nSheet Containing Dataset: Choose the sheet you want to write the evaluation results to. Usually this is the same sheet you select in the Evaluation Trigger node.\nSelect From list to choose the sheet title from the dropdown list, By URL to enter the url of the sheet, By ID to enter the sheetId, or By Name to enter the sheet title.\nYou can find the sheetId in a Google Sheets URL:\nYou define the items to write to the Google Sheet in the Outputs section. For each output, you set the following:\nName: The Google Sheet column name to write the evaluation results to.\nValue: The value to write to the Google Sheet.\nSet Metrics\nThe Set Metrics operation includes a Metrics to Return section where you define the metrics to record and track for your evaluations. You can see the metric results in your workflow's Evaluations tab.\nFor each metric you wish to record, you set the following details:\nName: The name to use for the metric.\nValue: The numeric value to record. Once you run your evaluation, you can drag and drop values from previous nodes here. Metric values must be numeric.\nCheck If Evaluating\nThe Check If Evaluating operation does not have any parameters. This operation provides branching output connectors so that you can conditionally execute logic depending on whether the current execution is an evaluation or not.\nTemplates and examples\nRelated resources\nTo learn more about n8n evaluations, check out the evaluations documentation\nn8n provides a trigger node for evaluations. You can find the node docs here.\nFor common questions or issues and suggested solutions, refer to the evaluations tips and common issues page."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.evaluationtrigger.md",
    "content": "Evaluation Trigger node\nUse the Evaluation Trigger node when setting up evaluations to validate your AI workflow reliability. During evaluation, the Evaluation Trigger node reads your evaluation dataset from Google Sheets, sending the items through the workflow one at a time, in sequence.\nOn this page, you'll find the Evaluation Trigger node parameters and\nParameters\nCredential to connect with: Create or select an existing Google Sheets credentials.\nDocument Containing Dataset: Choose the spreadsheet document with the sheet containing your test dataset.\nSelect From list to choose the spreadsheet title from the dropdown list, By URL to enter the url of the spreadsheet, or By ID to enter the spreadsheetId.\nYou can find the spreadsheetId in a Google Sheets URL:\nSheet Containing Dataset: Choose the sheet containing your test dataset.\nSelect From list to choose the sheet title from the dropdown list, By URL to enter the url of the sheet, By ID to enter the sheetId, or By Name to enter the sheet title.\nYou can find the sheetId in a Google Sheets URL:\nLimit Rows: Whether to limit the number of rows in the sheet to process.\nMax Rows to Process: When Limit Rows is enabled, the maximum number of rows to read and process during the evaluation.\nFilters\nOptionally filter the evaluation dataset based on column values.\nColumn: Choose a sheet column you want to filter by. Select From list to choose the column name from the dropdown list, or By ID to specify an ID using an expression.\nValue: The column value you want to filter by. The evaluation will only process rows with the given value for the selected column.\nTemplates and examples\nRelated resources\nTo learn more about n8n evaluations, check out the evaluations documentation\nn8n provides an app node for evaluations. You can find the node docs here.\nFor common questions or issues and suggested solutions, refer to the evaluations tips and common issues page."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.executeworkflow.md",
    "content": "Execute Sub-workflow\nUse the Execute Sub-workflow node to run a different workflow on the host machine that runs n8n.\nNode parameters\nSource\nSelect where the node should get the sub-workflow's information from:\nDatabase: Select this option to load the workflow from the database by ID. You must also enter either:\nFrom list: Select the workflow from a list of workflows available to your account.\nWorkflow ID: Enter the ID for the workflow. The URL of the workflow contains the ID after /workflow/. For example, if the URL of a workflow is  the Workflow ID is abCDE1f6gHiJKL7.\nLocal File: Select this option to load the workflow from a locally saved JSON file. You must also enter:\nWorkflow Path: Enter the path to the local JSON workflow file you want the node to execute.\nParameter: Select this option to load the workflow from a parameter. You must also enter:\nWorkflow JSON: Enter the JSON code you want the node to execute.\nURL: Select this option to load the workflow from a URL. You must also enter:\nWorkflow URL: Enter the URL you want to load the workflow from.\nWorkflow Inputs\nIf you select a sub-workflow using the database and From list options, the sub-workflow's input items will automatically display, ready for you to fill in or map values.\nYou can optionally remove requested input items, in which case the sub-workflow receives null as the item's value. You can also enable Attempt to convert types to try to automatically convert data to the sub-workflow item's requested type.\nInput items won't appear if the sub-workflow's Workflow Input Trigger node uses the \"Accept all data\" input data mode.\nMode\nUse this parameter to control the execution mode for the node. Choose from these options:\nRun once with all items: Pass all input items into a single execution of the node.\nRun once for each item: Execute the node once for each input item in turn.\nNode options\nThis node includes one option: Wait for Sub-Workflow Completion. This lets you control whether the main workflow should wait for the sub-workflow's completion before moving on to the next step (turned on) or whether the main workflow should continue without waiting (turned off).\nTemplates and examples\nSet up and use a sub-workflow\nThis section walks through setting up both the parent workflow and sub-workflow.\nHow data passes between workflows"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.executeworkflowtrigger.md",
    "content": "Execute Sub-workflow Trigger node\nUse this node to start a workflow in response to another workflow. It should be the first node in the workflow.\nn8n allows you to call workflows from other workflows. This is useful if you want to:\nReuse a workflow: for example, you could have multiple workflows pulling and processing data from different sources, then have all those workflows call a single workflow that generates a report.\nBreak large workflows into smaller components.\nUsage\nThis node runs in response to a call from the Execute Sub-workflow or Call n8n Workflow Tool nodes.\nTemplates and examples\nHow data passes between workflows"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.executiondata.md",
    "content": "Execution Data\nUse this node to save metadata for workflow executions. You can then search by this data in the Executions list.\nYou can retrieve custom execution data during workflow execution using the Code node. Refer to Custom executions data for more information.\nOperations\nSave Execution Data for Search\nData to Save\nAdd a Saved Field for each key/value pair of metadata you'd like to save.\nLimitations\nThe Execution Data node has the following restrictions when storing execution metadata:\nkey: limited to 50 characters\nvalue: limited to 512 characters\nIf either the key or value exceed the above limitations, n8n truncates to their maximum length and outputs a log entry.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.extractfromfile.md",
    "content": "Extract From File\nA common pattern in n8n workflows is to receive a file, either from and HTTP Request node (for files you are fetching from a website), a Webhook Node (for files which are sent to your workflow from elsewhere), or from a local source. Data obtained in this way is often in a binary format, for example a spreadsheet or PDF.\nThe Extract From File node extracts data from a binary format file and converts it to JSON, which can then be easily manipulated by the rest of your workflow. For converting JSON back into a binary file type, please see the Convert to File node.\nOperations\nUse the Operations drop-down to select the format of the source file to extract data from.\nExtract From CSV: The \"Comma Separated Values\" file type is commonly used for tabulated data.\nExtract From HTML: Extract fields from standard web page HTML format files.\nExtract From JSON: Extract JSON data from a binary file.\nExtract From ICS: Extract fields from iCalendar format files.\nExtract From ODS: Extract fields from ODS spreadsheet files.\nExtract From PDF: Extract fields from Portable Document Format files.\nExtract From RTF: Extract fields from Rich Text Format files.\nExtract From Text File: Extract fields from a standard text file format.\nExtract From XLS: Extract fields from a Microsoft Excel file (older format).\nExtract From XLSX: Extract fields from a Microsoft Excel file.\nMove File to Base64 String: Converts binary data to a text-friendly base64 format.\nExample workflow\nIn this example, a Webhook node is used to trigger the workflow. When a CSV file is sent to the webhook address, the file data is output and received by the Extract From File node.\nSet to operate as 'Extract from CSV', the node then outputs the data as a series of JSON 'row' objects:\nNode parameters\nInput Binary Field\nEnter the name of the field from the node input data that contains the binary file. The default is 'data'.\nDestination Output Field\nEnter the name of the field in the node output that will contain the extracted data.\nThis parameter is only available for these operations:\nExtract From JSON\nExtract From ICS\nExtract From Text File\nMove File to Base64 String\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.filter.md",
    "content": "Filter\nFilter items based on a condition. If the item meets the condition, the Filter node passes it on to the next node in the Filter node output. If the item doesn't meet the condition, the Filter node omits the item from its output.\nNode parameters\nCreate filter comparison Conditions to perform your filter.\nUse the data type dropdown to select the data type and comparison operation type for your condition. For example, to filter for dates after a particular date, select Date & Time > is after.\nThe fields and values to enter into the condition change based on the data type and comparison you select. Refer to Available data type comparisons for a full list of all comparisons by data type.\nSelect Add condition to create more conditions.\nCombining conditions\nYou can choose to keep items:\nWhen they meet all conditions: Create two or more conditions and select AND in the dropdown between them.\nWhen they meet any of the conditions: Create two or more conditions and select OR in the dropdown between them.\nYou can't create a mix of AND and OR rules.\nNode options\nIgnore Case: Whether to ignore letter case (turned on) or be case sensitive (turned off).\nLess Strict Type Validation: Whether you want n8n to attempt to convert value types based on the operator you choose (turned on) or not (turned off). Turn this on when facing a \"wrong type:\" error in your node.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.form.md",
    "content": "n8n Form node\nUse the n8n Form node to create user-facing forms with multiple steps. You can add other nodes with custom logic between to process user input. You must start the workflow with the n8n Form Trigger.\nSetting up the node\nSet default selections with query parameters\nYou can set the initial values for fields by using query parameters with the initial URL provided by the n8n Form Trigger. Every page in the form receives the same query parameters sent to the n8n Form Trigger URL.\nWhen using query parameters, percent-encode any field names or values that use special characters. This ensures n8n uses the initial values for the given fields. You can use tools like URL Encode/Decode to format your query parameters using percent-encoding.\nAs an example, imagine you have a form with the following properties:\nProduction URL:\nFields:\nname: Jane Doe\nemail: jane.doe@example.com\nWith query parameters and percent-encoding, you could use the following URL to set initial field values to the data above:\nHere, percent-encoding replaces the at-symbol (@) with the string %40 and the space character ( ) with the string %20. This will set the initial value for these fields no matter which page of the form they appear on.\nDisplaying custom HTML\nYou can display custom HTML on your form by adding a Custom HTML field to your form. This provides an HTML box where you can insert arbitrary HTML code to display as part of the form page.\nYou can use the HTML field to enrich your form page by including things like links, images, videos, and more. n8n will render the content with the rest of the form fields in the normal document flow.\nBecause custom HTML content is read-only, these fields aren't included in the form output data by default. To include the raw HTML content in the node output, provide a name for the data using the Element Name field.\nThe HTML field doesn't support"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.formtrigger.md",
    "content": "n8n Form Trigger node\nUse the n8n Form trigger to start a workflow when a user submits a form, taking the input data from the form. The node generates the form web page for you to use.\nYou can add more pages to continue the form with the n8n Form node.\nBuild and test workflows\nWhile building or testing a workflow, use the Test URL. Using a test URL ensures that you can view the incoming data in the editor UI, which is useful for debugging.\nThere are two ways to test:\nSelect Execute Step. n8n opens the form. When you submit the form, n8n runs the node, but not the rest of the workflow.\nSelect Execute Workflow. n8n opens the form. When you submit the form, n8n runs the workflow.\nProduction workflows\nWhen your workflow is ready, switch to using the Production URL. You can then activate your workflow, and n8n runs it automatically when a user submits the form.\nWhen working with a production URL, ensure that you have saved and activated the workflow. Data flowing through the Form trigger isn't visible in the editor UI with the production URL.\nSet default selections with query parameters\nYou can set the initial values for fields by using query parameters with the initial URL provided by the n8n Form Trigger. Every page in the form receives the same query parameters sent to the n8n Form Trigger URL.\nWhen using query parameters, percent-encode any field names or values that use special characters. This ensures n8n uses the initial values for the given fields. You can use tools like URL Encode/Decode to format your query parameters using percent-encoding.\nAs an example, imagine you have a form with the following properties:\nProduction URL:\nFields:\nname: Jane Doe\nemail: jane.doe@example.com\nWith query parameters and percent-encoding, you could use the following URL to set initial field values to the data above:\nHere, percent-encoding replaces the at-symbol (@) with the string %40 and the space character ( ) with the string %20. This will set the initial value for these fields no matter which page of the form they appear on.\nNode parameters\nThese are the main node configuration fields:\nAuthentication\nBasic Auth\nNone\nUsing basic auth\nTo configure this credential, you'll need:\nThe Username you use to access the app or service your HTTP Request is targeting.\nThe Password that goes with that username.\nForm URLs\nThe Form Trigger node has two URLs: Test URL and Production URL. n8n displays the URLs at the top of the node panel. Select Test URL or Production URL to toggle which URL n8n displays.\n!Screenshot of the form URLs\nTest URL: n8n registers a test webhook when you select Execute Step or Execute Workflow, if the workflow isn't active. When you call the URL, n8n displays the data in the workflow.\nProduction URL: n8n registers a production webhook when you activate the workflow. When using the production URL, n8n doesn't display the data in the workflow. You can still view workflow data for a production execution. Select the Executions tab in the workflow, then select the workflow execution you want to view.\nForm Path\nSet a custom slug for the form.\nForm Title\nEnter the title for your form. n8n displays the Form Title as the webpage title and main h1 title on the form.\nForm Description\nEnter the description for your form. n8n displays the Form Description as a subtitle below the main h1 title on the form. Use \\n or  to add a line break.\nForm Elements\nCreate the question fields for your form. Select Add Form Element to add a new field.\nEvery field has the following settings:\nField Label: Enter the label that appears above the input field.\nElement Type: Choose from Custom HTML, Date, Dropdown List, Email, File, Hidden Field, Number, Password, Text, or Textarea.\nSelect Custom HTML to insert arbitrary HTML.\nYou can include elements like links, images, video, and more. You can't include"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.ftp.md",
    "content": "FTP\nThe FTP node is useful to access and upload files to an FTP or SFTP server.\nTo connect to an SFTP server, use an SFTP credential. Refer to FTP credentials for more information.\nOperations\nDelete a file or folder\nDownload a file\nList folder content\nRename or move a file or folder\nUpload a file\nDelete\nThis operation includes one parameter: Path. Enter the remote path that you would like to connect to.\nDelete options\nThe delete operation adds one new option: Folder. If you turn this option on, the node can delete both folders and files. This configuration also displays one more option:\nRecursive: If you turn this option on and you delete a folder or directory, the node will delete all files and directories within the target directory.\nDownload\nConfigure this operation with these parameters:\nPath: Enter the remote path that you would like to connect to.\nPut Output File in Field: Enter the name of the output binary field to put the file in.\nList\nConfigure this operation with these parameters:\nPath: Enter the remote path that you would like to connect to.\nRecursive: Select whether to return an object representing all directories / objects recursively found within the FTP/SFTP server (turned on) or not (turned off).\nRename\nConfigure this operation with these parameters:\nOld Path: Enter the existing path of the file you'd like to rename in this field.\nNew Path: Enter the new path for the renamed file in this field.\nRename options\nThis operation adds one new option: Create Directories. If you turn this option on, the node will recursively create the destination directory when renaming an existing file or folder.\nUpload\nConfigure this operation with these parameters:\nPath: Enter the remote path that you would like to connect to.\nBinary File: Select whether you'll upload a binary file (turned on) or enter text content to be uploaded (turned off). Other parameters depend on your selection in this field.\nInput Binary Field: Displayed if you turn on Binary File. Enter the name of the input binary field that contains the file you'll upload in this field.\nFile Content: Displayed if you turn off Binary File Enter the text content of the file you'll upload in this field.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.git.md",
    "content": "Git\nGit is a free and open-source distributed version control system designed to handle everything from small to large projects with speed and efficiency.\nOperations\nAdd a file or folder to commit. Performs a git add.\nAdd Config: Add configuration property. Performs a git config set or add.\nClone a repository: Performs a git clone.\nCommit files or folders to git. Performs a git commit.\nFetch from remote repository. Performs a git fetch.\nList Config: Return current configuration. Performs a git config query.\nLog: Return git commit history. Performs a git log.\nPull from remote repository: Performs a git pull.\nPush to remote repository: Performs a git push.\nPush Tags to remote repository: Performs a git push --tags.\nReturn Status of current repository: Performs a git status.\nCreate a new Tag: Performs a git tag.\nUser Setup: Set the user.\nRefer to the sections below for more details on the parameters and options for each operation.\nAdd\nConfigure this operation with these parameters:\nRepository Path: Enter the local path of the git repository.\nPaths to Add: Enter a comma-separated list of paths of files or folders to add in this field. You can use absolute paths or relative paths from the Repository Path.\nAdd Config\nConfigure this operation with these parameters:\nRepository Path: Enter the local path of the git repository.\nKey: Enter the name of the key to set.\nValue: Enter the value of the key to set.\nAdd Config options\nThe add config operation adds the Mode option. Choose whether to Set or Append the setting in the local config.\nClone\nConfigure this operation with these parameters:\nRepository Path: Enter the local path of the git repository.\nAuthentication: Select Authenticate to pass credentials in. Select None to not use authentication.\nCredential for Git: If you select Authenticate, you must select or create credentials for the node to use. Refer to Git credential for more information.\nNew Repository Path: Enter the local path where you'd like to locate the cloned repository.\nSource Repository: Enter the URL or path of the repository you want to clone.\nCommit\nConfigure this operation with these parameters:\nRepository Path: Enter the local path of the git repository.\nMessage: Enter the commit message to use in this field.\nCommit options\nThe commit operation adds the Paths to Add option. To commit all \"added\" files and folders, leave this field blank. To commit specific \"added\" files and folders, enter a comma-separated list of paths of files or folders in this field.\nYou can use absolute paths or relative paths from the Repository Path.\nFetch\nThis operation only prompts you to enter the local path of the git repository in the Repository Path parameter.\nList Config\nThis operation only prompts you to enter the local path of the git repository in the Repository Path parameter.\nLog\nConfigure this operation with these parameters:\nRepository Path: Enter the local path of the git repository.\nReturn All: When turned on, the node will return all results. When turned off, the node will return results up to the set Limit.\nLimit: Only available when you turn off Return All. Enter the maximum number of results to return.\nLog options\nThe log operation adds the File option. Enter the path of a file or folder to get the history of in this field.\nYou can use absolute paths or relative paths from the Repository Path.\nPull\nThis operation only prompts you to enter the local path of the git repository in the Repository Path parameter.\nPush\nConfigure this operation with these parameters:\nRepository Path: Enter the local path of the git repository.\nAuthentication: Select Authenticate to pass credentials in or None to not use authentication.\nIf you select Authenticate, you must select or create Credential for Git for the node to use. Refer to Git credential for more information.\nPush options\nThe push operation adds the Target Repository option. Enter the URL or path of the repository to push to in this field.\nPush Tags\nThis operation only prompts you to enter the local path of the git repository in the Repository Path parameter.\nStatus\nThis operation only prompts you to enter the local path of the git repository in the Repository Path parameter.\nTag\nConfigure this operation with these parameters:\nRepository Path: Enter the local path of the git repository.\nName: Enter the name of the tag to create in this field.\nUser Setup\nThis operation only prompts you to enter the local path of the git repository in the Repository Path parameter.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.graphql.md",
    "content": "GraphQL\nGraphQL is an open-source data query and manipulation language for APIs, and a runtime for fulfilling queries with existing data. Use the GraphQL node to query a GraphQL endpoint.\nNode parameters\nAuthentication\nSelect the type of authentication to use.\nIf you select anything other than None, the Credential for  parameter appears for you to select an existing or create a new authentication credential for that authentication type.\nHTTP Request Method\nSelect the underlying HTTP Request method the node should use. Choose from:\nGET\nPOST: If you select this method, you'll also need to select the Request Format the node should use for the query payload. Choose from:\nGraphQL (Raw)\nJSON\nEndpoint\nEnter the GraphQL Endpoint you'd like to hit.\nIgnore SSL Issues\nWhen you turn on this control, n8n ignores SSL certificate validation failure.\nQuery\nEnter the GraphQL query you want to execute.\nRefer to Related Resources for information on writing your query.\nResponse Format\nSelect the format you'd like to receive query results in. Choose between:\nJSON\nString: If you select this format, enter a Response Data Property Name to define the property the string is written to.\nHeaders\nEnter any Headers you want to pass as part of the query as Name / Value pairs.\nTemplates and examples\nRelated resources\nTo use the GraphQL node, you need to understand GraphQL query language. GraphQL have their own Introduction to GraphQL tutorial."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.html.md",
    "content": "HTML\nThe HTML node provides operations to help you work with HTML in n8n.\nOperations\nGenerate HTML template: Use this operation to create an HTML template. This allows you to take data from your workflow and output it as HTML.\nExtract HTML content: Extract contents from an HTML-formatted source. The source can be in JSON or a binary file (.html).\nConvert to HTML Table: Convert content to an HTML table.\nThe node parameters and options depend on the operation you select. Refer to the sections below for more details on configuring each operation.\nGenerate HTML template\nCreate an HTML template. This allows you to take data from your workflow and output it as HTML.\nYou can include:\nStandard HTML\nCSS in"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.if.md",
    "content": "If\nUse the If node to split a workflow conditionally based on comparison operations.\nAdd conditions\nCreate comparison Conditions for your If node.\nUse the data type dropdown to select the data type and comparison operation type for your condition. For example, to filter for dates after a particular date, select Date & Time > is after.\nThe fields and values to enter into the condition change based on the data type and comparison you select. Refer to Available data type comparisons for a full list of all comparisons by data type.\nSelect Add condition to create more conditions.\nCombining conditions\nYou can choose to keep data:\nWhen it meets all conditions: Create two or more conditions and select AND in the dropdown between them.\nWhen it meets any of the conditions: Create two or more conditions and select OR in the dropdown between them.\nTemplates and examples\nBranch execution with If and Merge nodes\nRelated resources\nRefer to Splitting with conditionals for more information on using conditionals to create complex logic in n8n.\nIf you need more than two conditional outputs, use the Switch node."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.jwt.md",
    "content": "JWT\nWork with JSON web tokens in your n8n workflows.\nOperations\nDecode\nSign\nVerify\nNode parameters\nCredential to connect with: Select or create a JWT credential to connect with.\nToken: Enter the token to Verify or Decode.\nIf you select the Sign operation, you'll also have this parameter:\nUse JSON to Build Payload: When turned on, the node uses JSON to build the claims. The selection here influences what appears in the Payload Claims section.\nPayload Claims\nThe node only displays payload claims if you select the Sign operation. What you see depends on what you select for Use JSON to Build Payload:\nIf you select Use JSON to Build Payload, this section displays a JSON editor where you can construct the claims.\nIf you don't select Use JSON to Build Payload, this section prompts you to Add Claim.\nYou can add the following claims.\nAudience\nThe Audience or aud claim identifies the intended recipients of the JWT.\nRefer to \"aud\" (Audience) Claim for more information.\nExpires In\nThe Expires In or exp claim identifies the time after which the JWT expires and must not be accepted for processing.\nRefer to \"exp\" (Expiration Time) Claim for more information.\nIssuer\nThe Issuer or iss claim identifies the principal that issued the JWT.\nRefer to \"iss\" (Issuer) Claim for more information.\nJWT ID\nThe JWT ID or jti claim provides a unique identifier for the JWT.\nRefer to \"jti\" (JWT ID) Claim for more information.\nNot Before\nThe Not Before or nbf claim identifies the time before which the JWT must not be accepted for processing.\nRefer to \"nbf\" (Not Before) Claim for more information.\nSubject\nThe Subject or sub claim identifies the principal that's the subject of the JWT.\nRefer to \"sub\" (Subject) Claim for more information.\nNode options\nDecode node options\nThe Return Additional Info toggle controls how much information the node returns.\nWhen turned on, the node returns the complete decoded token with information about the header and signature. When turned off, the node only returns the payload.\nSign node options\nUse the Override Algorithm control to select the algorithm to use for verifying the token. This algorithm will override the algorithm selected in the credentials.\nVerify node options\nThis operation includes several node options:\nReturn Additional Info: This toggle controls how much information the node returns. When turned on, the node returns the complete decoded token with information about the header and signature. When turned off, the node only returns the payload.\nIgnore Expiration: This toggle controls whether the node should ignore the token's expiration time claim (exp). Refer to \"exp\" (Expiration Time) Claim for more information.\nIgnore Not Before Claim: This toggle controls whether to ignore the token's not before claim (nbf). Refer to \"nbf\" (Not Before) Claim for more information.\nClock Tolerance: Enter the number of seconds to tolerate when checking the nbf and exp claims. This allows you to deal with small clock differences among different servers. Refer to \"exp\" (Expiration Time) Claim for more information.\nOverride Algorithm: The algorithm to use for verifying the token. This algorithm will override the algorithm selected in the credentials.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.ldap.md",
    "content": "LDAP\nThis node allows you to interact with your LDAP servers to create, find, and update objects.\nOperations\nCompare an attribute\nCreate a new entry\nDelete an entry\nRename the DN of an existing entry\nSearch LDAP\nUpdate attributes\nRefer to the sections below for details on configuring the node for each operation.\nCompare\nConfigure this operation using these parameters:\nCredential to connect with: Select or create an LDAP credential to connect with.\nDN: Enter the Distinguished Name (DN) of the entry to compare.\nAttribute ID: Enter the ID of the attribute to compare.\nValue: Enter the value to compare.\nCreate\nConfigure this operation using these parameters:\nCredential to connect with: Select or create an LDAP credential to connect with.\nDN: Enter the Distinguished Name (DN) of the entry to create.\nAttributes: Add the Attribute ID/Value pairs you'd like to create.\nDelete\nConfigure this operation using these parameters:\nCredential to connect with: Select or create an LDAP credential to connect with.\nDN: Enter the Distinguished Name (DN) of the entry to be deleted.\nRename\nConfigure this operation using these parameters:\nCredential to connect with: Select or create an LDAP credential to connect with.\nDN: Enter the current Distinguished Name (DN) of the entry to rename.\nNew DN: Enter the new Distinguished Name (DN) for the entry in this field.\nSearch\nConfigure this operation using these parameters:\nCredential to connect with: Select or create an LDAP credential to connect with.\nBase DN: Enter the Distinguished Name (DN) of the subtree to search in.\nSearch For: Select the directory object class to search for.\nAttribute: Select the attribute to search for.\nSearch Text: Enter the text to search for. Use * for a wildcard.\nReturn All: When turned on, the node will return all results. When turned off, the node will return results up to the set Limit.\nLimit: Only available when you turn off Return All. Enter the maximum number of results to return.\nSearch options\nYou can also configure this operation using these options:\nAttribute Names or IDs: Enter a comma-separated list of attributes to return. Choose from the list or specify IDs using an expression.\nPage Size: Enter the maximum number of results to request at one time. Set to 0 to disable paging.\nScopes: The set of entries at or below the Base DN to search for potential matches. Select from:\nBase Tree: Often referred to as subordinateSubtree or just \"subordinates,\" selecting this option will search the subordinates of the Base DN entry but not the Base DN entry itself.\nSingle Level: Often referred to as \"one,\" selecting this option will search only the immediate children of the Base DN entry.\nWhole Subtree: Often referred to as \"sub,\" selecting this option will search the Base DN entry and all its subordinates to any depth.\nRefer to The LDAP Search Operation for more information on search scopes.\nUpdate\nConfigure this operation using these parameters:\nCredential to connect with: Select or create an LDAP credential to connect with.\nDN: Enter the Distinguished Name (DN) of the entry to update.\nUpdate Attributes*: Select whether to Add new, Remove existing, or Replace** existing attribute.\nThen enter the Attribute ID/Value pair you'd like to update.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.limit.md",
    "content": "Limit\nUse the Limit node to remove items beyond a defined maximum number. You can choose whether n8n takes the items from the beginning or end of the input data.\nNode parameters\nConfigure this node using the following parameters.\nMax Items\nEnter the maximum number of items that n8n should keep. If the input data contains more than this value, n8n removes the items.\nKeep\nIf the node has to remove items, select where it keeps the input items from:\nFirst Items: Keeps the Max Items number of items from the beginning of the input data.\nLast Items: Keeps the Max Items number of items from the end of the input data.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.localfiletrigger.md",
    "content": "Local File Trigger node\nThe Local File Trigger node starts a workflow when it detects changes on the file system. These changes involve a file or folder getting added, changed, or deleted.\nNode parameters\nYou can choose what event to watch for using the Trigger On parameter.\nChanges to a Specific File\nThe node triggers when the specified file changes.\nEnter the path for the file to watch in File to Watch.\nChanges Involving a Specific Folder\nThe node triggers when a change occurs in the selected folder.\nConfigure these parameters:\nFolder to Watch: Enter the path of the folder to watch.\nWatch for: Select the type of change to watch for.\nNode options\nUse the node Options to include or exclude files and folders.\nInclude Linked Files/Folders: also watch for changes to linked files or folders.\nIgnore: files or paths to ignore. n8n tests the whole path, not just the filename. Supports the Anymatch syntax.\nMax Folder Depth: how deep into the folder structure to watch for changes.\nExamples for Ignore\nIgnore a single file:\nIgnore a sub-directory of a directory you're watching:\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.manualworkflowtrigger.md",
    "content": "Manual Trigger node\nUse this node if you want to start a workflow by selecting Execute Workflow and don't want any option for the workflow to run automatically.\nWorkflows always need a trigger, or start point. Most workflows start with a trigger node firing in response to an external event or the Schedule Trigger firing on a set schedule.\nThe Manual Trigger node serves as the workflow trigger for workflows that don't have an automatic trigger.\nUse this trigger:\nTo test your workflow before you add an automatic trigger of some kind.\nWhen you don't want the workflow to run automatically.\nCommon issues\nHere are some common errors and issues with the Manual Trigger node and steps to resolve or troubleshoot them.\nOnly one 'Manual Trigger' node is allowed in a workflow\nThis error displays if you try to add a Manual Trigger node to a workflow which already includes a Manual Trigger node.\nRemove your existing Manual Trigger or edit your workflow to connect that trigger to a different node."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.markdown.md",
    "content": "Markdown\nThe Markdown node converts between Markdown and HTML formats.\nOperations\nThis node's operations are Modes:\nMarkdown to HTML: Use this mode to convert from Markdown to HTML.\nHTML to Markdown: Use this mode to convert from HTML to Markdown.\nNode parameters\nHTML or Markdown: Enter the data you want to convert. The field name changes based on which Mode you select.\nDestination Key: Enter the field you want to put the output in. Specify nested fields using dots, for example level1.level2.newKey.\nNode options\nThe node's Options depend on the Mode selected.\nMarkdown to HTML options\nTABLE_PLACEHOLDER_0\nTemplates and examples\nParsers\nn8n uses the following parsers:\nTo convert from HTML to Markdown: node-html-markdown.\nTo convert from Markdown to HTML: Showdown. Some options allow you to extend your Markdown with GitHub Flavored Markdown."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.merge.md",
    "content": "Merge\nUse the Merge node to combine data from multiple streams, once data of all streams is available.\nNode parameters\nYou can specify how the Merge node should combine data from different data streams by choosing a Mode:\nAppend\nKeep data from all inputs. Choose a Number of Inputs to output items of each input, one after another. The node waits for the execution of all connected inputs.\n!Sample Append mode inputs and output. Two separate data sources are on the left, one with items A, B, C and one with items D, E, F. The final data source combines both and lists A, B, C, D, E, F.\nAppend mode inputs and output\nCombine\nCombine data from two inputs. Select an option in Combine By to determine how you want to merge the input data.\nMatching Fields\nCompare items by field values. Enter the fields you want to compare in Fields to Match.\nn8n's default behavior is to keep matching items. You can change this using the Output Type setting:\nKeep Matches: Merge items that match. This is like an inner join.\nKeep Non-Matches: Merge items that don't match.\nKeep Everything: Merge items together that do match and include items that don't match. This is like an outer join.\nEnrich Input 1: Keep all data from Input 1, and add matching data from Input 2. This is like a left join.\nEnrich Input 2: Keep all data from Input 2, and add matching data from Input 1. This is like a right join.\n!Sample Combine mode inputs and output. Two separate data sources are on the left. The final data source combines these data sources by matching fields.\nCombine by Matching Fields mode inputs and output\nPosition\nCombine items based on their order. The item at index 0 in Input 1 merges with the item at index 0 in Input 2, and so on.\n!Sample Combine mode inputs and output. Two separate data sources are on the left. The final data source combines these data sources by index position.\nCombine by Position mode inputs and output\nAll Possible Combinations\nOutput all possible item combinations, while merging fields with the same name.\n!Sample Combine mode inputs and output. Two separate data sources are on the left. The final data source combines these data sources by all possible combinations.\nCombine by All Possible Combinations mode inputs and output\nCombine mode options\nWhen merging data by Mode > Combine, you can set these Options:\nClash Handling: Choose how to merge when data streams clash, or when there are sub-fields. Refer to Clash handling for details.\nFuzzy Compare: Whether to tolerate type differences when comparing fields (enabled), or not (disabled, default). For example, when you enable this, n8n treats \"3\" and 3 as the same.\nDisable Dot Notation: This prevents accessing child fields using parent.child in the field name.\nMultiple Matches: Choose how n8n handles multiple matches when comparing data streams.\nInclude All Matches: Output multiple items if there are multiple matches, one for each match.\nInclude First Match Only: Keep the first item per match and discard the remaining multiple matches.\nInclude Any Unpaired Items: Choose whether to keep or discard unpaired items when merging by position. The default behavior is to leave out the items without a match.\nClash Handling\nSQL Query\nWrite a custom SQL Query to merge the data.\nExample:\nData from previous nodes are available as tables and you can use them in the SQL query as input1, input2, input3, and so on, based on their order. Refer to AlaSQL GitHub page for a full list of supported SQL statements.\nChoose Branch\nChoose which input to keep. This option always waits until the data from both inputs is available. You can choose to Output:\nThe Input 1 Data\nThe Input 2 Data\nA Single, Empty Item\nThe node outputs the data from the chosen input, without changing it.\nTemplates and examples\nMerging data streams with uneven numbers of items\nThe items passed into Input 1 of the Merge node will take precedence. For example, if the Merge node receives five items in Input 1 and 10 items in Input 2, it only processes five items. The remaining five items from Input 2 aren't processed.\nBranch execution with If and Merge nodes\nTry it out: A step by step example\nCreate a workflow with some example input data to try out the Merge node.\nSet up sample data using the Code nodes\nAdd a Code node to the canvas and connect it to the Start node.\nPaste the following JavaScript code snippet in the JavaScript Code field:\nAdd a second Code node, and connect it to the Start node.\nPaste the following JavaScript code snippet in the JavaScript Code field:\nTry out different merge modes\nAdd the Merge node. Connect the first Code node to Input 1, and the second Code node to Input 2. Run the workflow to load data into the Merge node.\nThe final workflow should look like this:\nNow try different options in Mode to see how it affects the output data.\nAppend\nSelect Mode > Append, then select Execute step.\nYour output in table view should look like this:\nTABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.n8n.md",
    "content": "n8n\nA node to integrate with n8n itself. This node allows you to consume the n8n API in your workflows.\nRefer to the n8n REST API documentation for more information on using the n8n API. Refer to API endpoint reference for working with the API endpoints directly.\nOperations\nAudit\nGenerate a security audit\nCredential\nCreate a credential\nDelete a credential\nGet Schema: Use this operation to get credential data schema for type\nExecution\nGet an execution\nGet Many executions\nDelete an execution\nWorkflow\nActivate a workflow\nCreate a workflow\nDeactivate a workflow\nDelete a workflow\nGet a workflow\nGet Many workflows\nUpdate a workflow\nGenerate audit\nThis operation has no parameters. Configure it with these options:\nCategories: Select the risk categories you want the audit to include. Options include:\nCredentials\nDatabase\nFilesystem\nInstance\nNodes\nDays Abandoned Workflow: Use this option to set the number of days without execution after which a workflow should be considered abandoned. Enter a number of days. The default is 90.\nCreate credential\nConfigure this operation with these parameters:\nName: Enter the name of the credential you'd like to create.\nCredential Type: Enter the credential's type. The available types depend on nodes installed on the n8n instance. Some built-in types include githubApi, notionApi, and slackApi.\nData: Enter a valid JSON object with the required properties for this Credential Type. To see the expected format, use the Get Schema operation.\nDelete credential\nConfigure this operation with this parameter:\nCredential ID: Enter the ID of the credential you want to delete.\nGet credential schema\nConfigure this operation with this parameter:\nCredential Type: Enter the credential's type. The available types depend on nodes installed on the n8n instance. Some built-in types include githubApi, notionApi, and slackApi.\nGet execution\nConfigure this operation with this parameter:\nExecution ID: Enter the ID of the execution you want to retrieve.\nGet execution option\nYou can further configure this operation with this Option:\nInclude Execution Details: Use this control to set whether to include the detailed execution data (turned on) or not (turned off).\nGet many executions\nConfigure this operation with these parameters:\nReturn All: Set whether to return all results (turned on) or whether to limit the results to the entered Limit (turned on).\nLimit: Set the number of results to return if the Return All control is turned off.\nGet many executions filters\nYou can further configure this operation with these Filters:\nWorkflow: Filter the executions by workflow. Options include:\nFrom list: Select a workflow to use as a filter.\nBy URL: Enter a workflow URL to use as a filter.\nBy ID: Enter a workflow ID to use as a filter.\nStatus: Filter the executions by status. Options include:\nError\nSuccess\nWaiting\nGet many execution options\nYou can further configure this operation with this Option:\nInclude Execution Details: Use this control to set whether to include the detailed execution data (turned on) or not (turned off).\nDelete execution\nConfigure this operation with this parameter:\nExecution ID: Enter the ID of the execution you want to delete.\nActivate, deactivate, delete, and get workflow\nThe Activate, Deactivate, Delete, and Get workflow operations all include the same parameter for you to select the Workflow you want to perform the operation on. Options include:\nFrom list: Select the workflow from the list.\nBy URL: Enter the URL of the workflow.\nBy ID: Enter the ID of the workflow.\nCreate workflow\nConfigure this operation with this parameter:\nWorkflow Object: Enter a valid JSON object with the new workflow's details. The object requires these fields:\nname\nnodes\nconnections\nsettings\nRefer to the n8n API TABLE_PLACEHOLDER_0\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.n8ntrigger.md",
    "content": "n8n Trigger node\nThe n8n Trigger node triggers when the current workflow updates or activates, or when the n8n instance starts or restarts. You can use the n8n Trigger node to notify when these events occur.\nNode parameters\nThe node includes a single parameter to identify the Events that should trigger it. Choose from these events:\nActive Workflow Updated: If you select this event, the node triggers when this workflow is updated.\nInstance started: If you select this event, the node triggers when the n8n instance starts or restarts.\nWorkflow Activated: If you select this event, the node triggers when this workflow is activated.\nYou can select one or more of these events.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.noop.md",
    "content": "No Operation, do nothing\nUse the No Operation, do nothing node when you don't want to perform any operations. The purpose of this node is to make the workflow easier to read and understand where the flow of data stops. This can help others visually get a better understanding of the workflow.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.readwritefile.md",
    "content": "Read/Write Files from Disk\nUse the Read/Write Files from Disk node to read and write files from/to the machine where n8n is running.\nOperations\nRead File(s) From Disk: Use this operation to retrieve one or more files from the computer that runs n8n.\nWrite File to Disk: Use this operation to create a binary file on the computer that runs n8n.\nRefer to the sections below for more information on configuring the node for each operation.\nRead File(s) From Disk\nConfigure this operation with these parameters:\nFile(s) Selector: Enter the path of the file you want to read.\nTo enter multiple files, enter a page path pattern. You can use these characters to define a path pattern:\n*: Matches any character zero or more times, excluding path separators.\n**: Matches any character zero or more times, include path separators.\n?: Matches any character except for path separators one time.\n[]: Matches any characters inside the brackets. For example, [abc] would match the characters a, b, or c, and nothing else.\nRefer to Picomatch's Basic globbing documentation for more information on these characters and their expected behavior.\nRead File(s) From Disk options\nYou can also configure this operation with these Options:\nFile Extension: Enter the extension for the file in the node output.\nFile Name: Enter the name for the file in the node output.\nMIME Type: Enter the file's MIME type in the node output. Refer to Common MIME types for a list of file extensions and their MIME types.\nPut Output File in Field: Enter the name of the field in the output data to contain the file.\nWrite File to Disk\nConfigure this operation with these parameters:\nFile Path and Name: Enter the destination for the file, the file's name, and the file's extension.\nInput Binary Field: Enter the name of the field in the node input data that will contain the binary file.\nWrite File to Disk options\nYou can also configure this operation with these Options:\nThis operation includes a single option, whether to Append data to an existing file instead of creating a new one (turned on) or to create a new file instead of appending to existing (turned off).\nTemplates and examples\nFile locations\nIf you run n8n in Docker, your command runs in the n8n container and not the Docker host.\nThis node looks for files relative to the n8n install path. n8n recommends using absolute file paths to prevent any errors."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.renamekeys.md",
    "content": "Rename Keys\nUse the Rename Keys node to rename the keys of a key-value pair in n8n.\nNode parameters\nYou can rename one or multiple keys using the Rename Keys node. Select the Add new key button to rename a key.\nFor each key, enter the:\nCurrent Key Name: The current name of the key you want to rename.\nNew Key Name: The new name you want to assign to the key.\nNode options\nChoose whether to use a Regex regular expression to identify keys to rename. To use this option, you must also enter:\nThe Regular Expression you'd like to use.\nReplace With: Enter the new name you want to assign to the key(s) that match the Regular Expression.\nYou can also choose these Regex-specific options:\nCase Insensitive: Set whether the regular expression should match case (turned off) or be case insensitive (turned on).\nMax Depth: Enter the maximum depth to replace keys, using -1 for unlimited and 0 for top-level only.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.respondtowebhook.md",
    "content": "Respond to Webhook\nUse the Respond to Webhook node to control the response to incoming webhooks. This node works with the Webhook node.\nHow to use Respond to Webhook\nTo use the Respond to Webhook node:\nAdd a Webhook node as the trigger node for the workflow.\nIn the Webhook node, set Respond to Using 'Respond to Webhook' node.\nAdd the Respond to Webhook node anywhere in your workflow. If you want it to return data from other nodes, place it after those nodes.\nNode parameters\nConfigure the node behavior using these parameters.\nRespond With\nChoose what data to send in the webhook response.\nAll Incoming Items: Respond with all the JSON items from the input.\nBinary File: Respond with a binary file defined in Response Data Source.\nFirst Incoming Item: Respond with the first incoming item's JSON.\nJSON: Respond with a JSON object defined in Response Body.\nJWT Token: Respond with a JSON Web Token (JWT).\nNo Data: No response payload.\nRedirect: Redirect to a URL set in Redirect URL.\nText: Respond with text set in Response Body. This sends HTML by default (Content-Type: text/html).\nNode options\nSelect Add Option to view and set the options.\nResponse Code: Set the response code to use.\nResponse Headers: Define the response headers to send.\nPut Response in Field: Available when you respond with All Incoming Items or First Incoming Item. Set the field name for the field containing the response data.\nHow n8n secures HTML responses\nStarting with n8n version 1.103.0, n8n automatically wraps HTML responses to webhooks in  tags. This is a security mechanism to protect the instance users.\nThis has the following implications:\nHTML renders in a sandboxed iframe instead of directly in the parent document.\nJavaScript code that attempts to access the top-level window or local storage will fail.\nAuthentication headers aren't available in the sandboxed iframe (for example, basic auth). You need to use an alternative approach, like embedding a short-lived access token within the HTML.\nRelative URLs (for example, ) won't work. Use absolute URLs instead.\nTemplates and examples\nWorkflow behavior\nWhen using the Respond to Webhook node, workflows behave as follows:\nThe workflow finishes without executing the Respond to Webhook node: it returns a standard message with a 200 status.\nThe workflow errors before the first Respond to Webhook node executes: the workflow returns an error message with a 500 status.\nA second Respond to Webhook node executes after the first one: the workflow ignores it.\nA Respond to Webhook node executes but there was no webhook: the workflow ignores the Respond to Webhook node.\nOutput the response sent to the webhook\nBy default, the Respond to Webhook node has a single output branch that contains the node's input data.\nYou can optionally enable a second output branch containing the response sent to the webhook. To enable this secondary output, open the Respond to Webhook node on the canvas and select the Settings tab. Activate the Enable Response Output Branch option.\nThe node will now have two outputs:\nInput Data: The original output, passing on the node's input.\nResponse: The response object sent to the webhook.\nReturn more than one data item (deprecated)\nThe Respond to Webhook node runs once, using the first incoming data item. This includes when using expressions. You can't force looping using the Loop node: the workflow will run, but the webhook response will still only contain the results of the first execution.\nIf you need to return more than one data item, choose one of these options:\nInstead of using the Respond to Webhook node, use the When Last Node Finishes option in Respond in the Webhook node. Use this when you want to return the final data that the workflow outputs.\nUse the Aggregate node to turn multiple items into a single item before passing the data to the Respond to Webhook node. Set Aggregate to All Item Data (Into a Single List)."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.rssfeedread.md",
    "content": "RSS Read\nUse the RSS Read node to read data from RSS feeds published on the internet.\nNode parameters\nURL: Enter the URL for the RSS publication you want to read.\nNode options\nIgnore SSL Issues: Choose whether n8n should ignore SSL/TLS verification (turned on) or not (turned off).\nTemplates and examples\nRelated resources\nn8n provides a trigger node for RSS Read. You can find the trigger node docs here."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.rssfeedreadtrigger.md",
    "content": "RSS Feed Trigger node\nThe RSS Feed Trigger node allows you to start an n8n workflow when a new RSS feed item has been published.\nOn this page, you'll find a list of operations the RSS Feed Trigger node supports, and links to more resources.\nNode parameters\nPoll Times: Select a poll Mode to set how often to trigger the poll. Your Mode selection will add or remove relevant fields. Refer to the sections below to configure the parameters for each mode type.\nFeed URL: Enter the URL of the RSS feed to poll.\nTemplates and examples\nRelated resources\nn8n provides an app node for RSS Feeds. You can find the node docs here."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.sendemail.md",
    "content": "Send Email\nThe Send Email node sends emails using an SMTP email server.\nNode parameters\nConfigure this node using the following parameters.\nCredential to connect with\nSelect or create an SMTP account credential for the node to use.\nOperation\nThe Send Email node supports the following operations:\nSend: Send an email.\nSend and Wait for Response: Send an email and wait for a response from the receiver. This operation pauses the workflow execution until the user submits a response.\nChoosing Send and Wait for Response will activate parameters and options as discussed in waiting for a response.\nFrom Email\nEnter the email address you want to send the email from. You can also include a name using this format: Name Name email@sample.com, for example: Nathan Doe nate@n8n.io.\nTo Email\nEnter the email address you want to send the email to. You can also include a name using this format: Name Name email@sample.com, for example: Nathan Doe nate@n8n.io. Use a comma to separate multiple email addresses: first@sample.com, \"Name\" second@sample.com.\nSubject\nEnter the subject line for the email.\nEmail Format\nSelect the format to send the email in. This parameter is available when using the Send operation. Choose from:\nText: Send the email in plain-text format.\nHTML: Send the email in HTML format.\nBoth: Send the email in both formats. If you choose this option, the email recipient's client will set which format to display.\nNode options\nUse these Options to further refine the node's behavior.\nAppend n8n Attribution\nSet whether to include the phrase This email was sent automatically with n8n at the end of the email (turned on) or not (turned off).\nAttachments\nEnter the name of the binary properties that contain data to add as an attachment. Some tips on using this option:\nUse the Read/Write Files from Disk node or the HTTP Request node to upload the file to your workflow.\nAdd multiple attachments by entering a comma-separated list of binary properties.\nReference embedded images or other content within the body of an email message, for example .\nCC Email\nEnter an email address for the cc: field.\nBCC Email\nEnter an email address for the bcc: field.\nIgnore SSL Issues\nSet whether n8n should ignore failures with TLS/SSL certificate validation (turned on) or enforce them (turned off).\nReply To\nEnter an email address for the Reply To field.\nWaiting for a response\nBy choosing the Send and Wait for a Response operation, you can send an email message and pause the workflow execution until a person confirms the action or provides more information.\nResponse Type\nYou can choose between the following types of waiting and approval actions:\nApproval: Users can approve or disapprove from within the message.\nFree Text: Users can submit a response with a form.\nCustom Form: Users can submit a response with a custom form.\nDifferent options are available depending on which type you choose.\nApproval parameters and options\nWhen using the Approval response type, the following options are available:\nType of Approval: Whether to present only an approval button or both an approval and disapproval buttons.\nButton Label: The label for the approval or disapproval button. The default choice is Approve and Decline for approval and disapproval actions respectively.\nButton Style: The style (primary or secondary) for the button.\nThis mode also offers the following options:\nLimit Wait Time: Whether the workflow will automatically resume execution after a specified time limit. This can be an interval or a specific wall time.\nAppend n8n Attribution: Set whether to include the phrase This email was sent automatically with n8n at the end of the email (turned on) or not (turned off).\nFree Text parameters and options\nWhen using the Free Text response type, the following options are available:\nMessage Button Label: The label to use for message button. The default choice is Respond.\nResponse Form Title: The title of the form where users provide their response.\nResponse Form Description: A description for the form where users provide their response.\nResponse Form Button Label: The label for the button on the form to submit their response. The default choice is Submit.\nLimit Wait Time: Whether the workflow will automatically resume execution after a specified time limit. This can be an interval or a specific wall time.\nAppend n8n Attribution: Set whether to include the phrase This email was sent automatically with n8n at the end of the email (turned on) or not (turned off).\nCustom Form parameters and options\nWhen using the Custom Form response type, you build a form using the fields and options you want.\nYou can customize each form element with the settings outlined in the n8n Form trigger's form elements. To add more fields, select the Add Form Element button.\nThe following options are also available:\nMessage Button Label: The label to use for message button. The default choice is Respond.\nResponse Form Title: The title of the form where users provide their response.\nResponse Form Description: A description for the form where users provide their response.\nResponse Form Button Label: The label for the button on the form to submit their response. The default choice is Submit.\nLimit Wait Time: Whether the workflow will automatically resume execution after a specified time limit. This can be an interval or a specific wall time.\nAppend n8n Attribution: Set whether to include the phrase This email was sent automatically with n8n at the end of the email (turned on) or not (turned off).\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.set.md",
    "content": "Edit Fields (Set)\nUse the Edit Fields node to set workflow data. This node can set new data as well as overwrite data that already exists. This node is crucial in workflows which expect incoming data from previous nodes, such as when inserting values to Google Sheets or databases.\nNode parameters\nThese are the settings and options available in the Edit Fields node.\nMode\nYou can either use Manual Mapping to edit fields using the GUI or JSON Output to write JSON that n8n adds to the input data.\nFields to Set\nIf you select Mode > Manual Mapping, you can configure the fields by dragging and dropping values from INPUT.\nThe default behavior when you drag a value is:\nn8n sets the value's name as the field name.\nThe field value contains an expression which accesses the value.\nIf you don't want to use expressions:\nHover over a field. n8n displays the Fixed | Expressions toggle.\nSelect Fixed.\nYou can do this for both the name and value of the field.\n!A gif showing the drag and drop action, as well as changing a field to fixed\nKeep Only Set Fields\nEnable this to discard any input data that you don't use in Fields to Set.\nInclude in Output\nChoose which input data to include in the node's output data.\nNode options\nUse these options to customize the behavior of the node.\nInclude Binary Data\nIf the input data includes binary data, choose whether to include it in the Edit Fields node's output data.\nIgnore Type Conversion Errors\nManual Mapping only.\nEnabling this allows n8n to ignore some data type errors when mapping fields.\nSupport Dot Notation\nBy default, n8n supports dot notation.\nFor example, when using manual mapping, the node follows the dot notation for the Name field. That means if you set the name in the Name field as number.one and the value in the Value field as 20, the resulting JSON is:\nYou can prevent this behavior by selecting Add Option > Support Dot Notation, and setting the Dot Notion field to off. Now the resulting JSON is:\nTemplates and examples\nArrays and expressions in JSON Output mode\nYou can use arrays and expressions when creating your JSON Output.\nFor example, given this input data generated by the Customer Datastore node:\nAdd the following JSON in the JSON Output field, with Include in Output set to All Input Fields:\nYou get this output:"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.sort.md",
    "content": "Sort\nUse the Sort node to organize lists of items in a desired ordering, or generate a random selection.\nNode parameters\nConfigure this node using the Type parameter.\nUse the dropdown to select how you want to input the sorting from these options.\nSimple\nPerforms an ascending or descending sort using the selected fields.\nWhen you select this Type:\nUse the Add Field To Sort By button to input the Field Name.\nSelect whether to use Ascending or Descending order.\nSimple options\nWhen you select Simple as the Type, you have the option to Disable Dot Notation. By default, n8n enables dot notation to reference child fields in the format parent.child. Use this option to disable dot notation (turned on) or to continue using dot (turned off).\nRandom\nCreates a random order in the list.\nCode\nInput custom JavaScript code to perform the sort operation. This is a good option if a simple sort won't meet your needs.\nEnter your custom JavaScript code in the Code input field.\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.splitinbatches.md",
    "content": "Loop Over Items\nThe Loop Over Items node helps you loop through data when needed.\nThe node saves the original incoming data, and with each iteration, returns a predefined amount of data through the loop output.\nWhen the node execution completes, it combines all of the processed data and returns it through the done output.\nWhen to use the Loop Over Items node\nBy default, n8n nodes are designed to process a list of input items (with some exceptions, detailed below). Depending on what you're trying to achieve, you often don't need the Loop Over Items node in your workflow. You can learn more about how n8n processes multiple items on the looping in n8n page.\nThese links highlight some of the cases where the Loop Over Items node can be useful:\nLoop until all items are processed: describes how the Loop Over Items node differs from normal item processing and when you might want to incorporate this node.\nNode exceptions: outlines specific cases and nodes where you may need to use the Loop Over Items node to manually build looping logic.\nAvoiding rate limiting: demonstrates how to batch API requests to avoid rate limits from other services.\nNode parameters\nBatch Size\nEnter the number of items to return with each call.\nNode options\nReset\nIf turned on, the node will reset with the current input-data newly initialized with each loop. Use this when you want the Loop Over Items node to treat incoming data as a new set of data instead of a continuation of previous items.\nFor example, you can use the Loop Over Items node with the reset option and an If node to query a paginated service when you don't know how many pages you need in advance. The loop queries pages one at a time, performs any processing, and increments the page number. The loop reset ensures the loop recognizes each iteration as a new set of data. The If node evaluates an exit condition to decide whether to perform another iteration or not.\nWhen enabled, you can adjust the reset conditions by switching the parameter representation from Fixed to Expression. The results of your expression evaluation determine when the node will reset item processing.\nTemplates and examples\nRead RSS feed from two different sources\nThis workflow allows you to read an RSS feed from two different sources using the Loop Over Items node. You need the Loop Over Items node in the workflow as the RSS Feed Read node only processes the first item it receives. You can also find the workflow on n8n.io.\nThe example walks through building the workflow, but assumes you are already familiar with n8n. To build your first workflow, including learning how to add nodes to a workflow, refer to Try it out.\nThe final workflow looks like this:\nCopy the workflow file above and paste into your instance, or manually build it by following these steps:\nAdd the manual trigger.\nAdd the Code node.\nCopy this code into the Code node:\nAdd the Loop Over Items node.\nConfigure Loop Over Items: set the batch size to 1 in the Batch Size field.\nAdd the RSS Feed Read node.\nSelect Execute Workflow. This runs the workflow to load data into the RSS Feed Read node.\nConfigure RSS Feed Read: map url from the input to the URL field. You can do this by dragging and dropping from the INPUT panel, or using this expression: {{ $json.url }}.\nSelect Execute Workflow to run the workflow and see the resulting data.\nCheck that the node has processed all items\nTo check if the node still has items to process, use the following expression: {{$node[\"Loop Over Items\"].context[\"noItemsLeft\"]}}. This expression returns a boolean value. If the node still has data to process, the expression returns false, otherwise it returns true.\nGet the current running index of the node\nTo get the current running index of the node, use the following expression: {{$node[\"Loop Over Items\"].context[\"currentRunIndex\"];}}."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.splitout.md",
    "content": "Split Out\nUse the Split Out node to separate a single data item containing a list into multiple items. For example, a list of customers, and you want to split them so that you have an item for each customer.\nNode parameters\nConfigure this node using the following parameters.\nField to Split Out\nEnter the field containing the list you want to separate out into individual items.\nIf you're working with binary data inputs, use $binary in an expression to set the field to split out.\nInclude\nSelect whether and how you want n8n to keep any other fields from the input data with each new individual item.\nYou can select:\nNo Other Fields: No other fields will be included.\nAll Other Fields: All other fields will be included.\nSelected Other Fields: Only the selected fields will be included.\nFields to Include: Enter a comma separated list of the fields you want to include.\nNode options\nDisable Dot Notation\nBy default, n8n enables dot notation to reference child fields in the format parent.child. Use this option to disable dot notation (turned on) or to continue using dot (turned off).\nDestination Field Name\nEnter the field in the output where the split field contents should go.\nInclude Binary\nChoose whether to include binary data from the input in the new output (turned on) or not (turned off).\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.ssetrigger.md",
    "content": "SSE Trigger node\nServer-Sent Events (SSE) is a server push technology enabling a client to receive automatic updates from a server using HTTP connection. The SSE Trigger node is used to receive server-sent events.\nNode parameters\nThe SSE Trigger node has one parameter, the URL. Enter the URL from which to receive the server-sent events (SSE).\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.ssh.md",
    "content": "SSH\nThe SSH node is useful for executing commands using the Secure Shell Protocol.\nOperations\nExecute a command\nDownload a file\nUpload a file\nExecute Command\nConfigure this operation with these parameters:\nCredential to connect with: Select an existing or create a new SSH credential to connect with.\nCommand: Enter the command to execute on the remote device.\nWorking Directory: Enter the directory where n8n should execute the command.\nDownload File\nCredential to connect with: Select an existing or create a new SSH credential to connect with.\nPath: Enter the path for the file you want to download. This path must include the file name. The downloaded file will use this file name. To use a different name, use the File Name option. Refer to Download File options for more information.\nFile Property: Enter the name of the object property that holds the binary data you want to download.\nDownload File options\nYou can further configure this operation with the File Name option. Use this option to override the binary data file name to a name of your choice.\nUpload File\nCredential to connect with: Select an existing or create a new SSH credential to connect with.\nInput Binary Field: Enter the name of the input binary field that contains the file you want to upload.\nTarget Directory: The directory to upload the file to. The name of the file is taken from the binary data file name. To enter a different name, use the File Name option. Refer to Upload File options for more information.\nUpload File options\nYou can further configure this operation with the File Name option. Use this option to override the binary data file name to a name of your choice.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.stopanderror.md",
    "content": "Stop And Error\nUse the Stop And Error node to display custom error messages, cause executions to fail under certain conditions, and send custom error information to error workflows.\nOperations\nError Message\nError Object\nNode parameters\nBoth operations include one node parameter, the Error Type. Use this parameter to select the type of error to throw. Choose between the two operations: Error Message and Error Object.\nThe other parameters depend on which operation you select.\nError Message parameters\nThe Error Message Error Type adds one parameter, the Error Message field. Enter the message you'd like to throw.\nError Object parameters\nThe Error Object Error Type adds one parameter, the Error Object. Enter a JSON object that contains the error properties you'd like to throw.\nTemplates and examples\nRelated resources\nYou can use the Stop And Error node with the Error trigger node.\nRead more about Error workflows in n8n workflows."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.summarize.md",
    "content": "Summarize\nUse the Summarize node to aggregate items together, in a manner similar to Excel pivot tables.\nNode parameters\nFields to Summarize\nUse these fields to define how you want to summarize your input data.\nAggregation: Select the aggregation method to use on a given field. Options include:\nAppend: Append\nIf you select this option, decide whether you want to Include Empty Values or not.\nAverage: Calculate the numeric average of your input data.\nConcatenate: Combine together values in your input data.\nIf you select this option, decide whether you want to Include Empty Values or not.\nSeparator: Select the separator you want to insert between concatenated values.\nCount: Count the total number of values in your input data.\nCount Unique: Count the number of unique values in your input data.\nMax: Find the highest numeric value in your input data.\nMin: Find the lowest numeric value in your input data.\nSum: Add together the numeric values in your input data.\nField: Enter the name of the field you want to perform the aggregation on.\nFields to Split By\nEnter the name of the input fields that you want to split the summary by (similar to a group by statement). This allows you to get separate summaries based on values in other fields.\nFor example, if our input data contains columns for Sales Rep and Deal Amount and we're performing a Sum on the Deal Amount field, we could split by Sales Rep to get a Sum total for each Sales Rep.\nTo enter multiple fields to split by, enter a comma-separated list.\nNode options\nContinue if Field Not Found\nBy default, if a Field to Summarize isn't in any items, the node throws an error. Use this option to continue and return a single empty item (turned on) instead or keep the default error behavior (turned off).\nDisable Dot Notation\nBy default, n8n enables dot notation to reference child fields in the format parent.child. Use this option to disable dot notation (turned on) or to continue using dot (turned off).\nOutput Format\nSelect the format for your output format. This option is recommended if you're using Fields to Split By\nEach Split in a Separate Item: Use this option to generate a separate output item for each split out field.\nAll Splits in a Single Item: Use this option to generate a single item that lists the split out fields.\nIgnore items without valid fields to group by\nSet whether to ignore input items that don't contain the Fields to Split By (turned on) or not (turned off).\nTemplates and examples\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.switch.md",
    "content": "Switch\nUse the Switch node to route a workflow conditionally based on comparison operations. It's similar to the IF node, but supports multiple output routes.\nNode parameters\nSelect the Mode the node should use:\nRules: Select this mode to build a matching rule for each output.\nExpression: Select this mode to write an expression to return the output index programmatically.\nNode configuration depends on the Mode you select.\nRules\nTo configure the node with this operation, use these parameters:\nCreate Routing Rules to define comparison conditions.\nUse the data type dropdown to select the data type and comparison operation type for your condition. For example, to create a rules for dates after a particular date, select Date & Time > is after.\nThe fields and values to enter into the condition change based on the data type and comparison you select. Refer to Available data type comparisons for a full list of all comparisons by data type.\nRename Output: Turn this control on to rename the output field to put matching data into. Enter your desired Output Name.\nSelect Add Routing Rule to add more rules.\nRule options\nYou can further configure the node with this operation using these Options:\nFallback Output: Choose how to route the workflow when an item doesn't match any of the rules or conditions.\nNone: Ignore the item. This is the default behavior.\nExtra Output: Send items to an extra, separate output.\nOutput 0: Send items to the same output as those matching the first rule.\nIgnore Case: Set whether to ignore letter case when evaluating conditions (turned on) or enforce letter case (turned off).\nLess Strict Type Validation: Set whether you want n8n to attempt to convert value types based on the operator you choose (turned on) or not (turned off).\nSend data to all matching outputs: Set whether to send data to all outputs meeting conditions (turned on) or whether to send the data to the first output matching the conditions (turned off).\nExpression\nTo configure the node with this operation, use these parameters:\nNumber of Outputs: Set how many outputs the node should have.\nOutput Index: Create an expression to calculate which input item should be routed to which output. The expression must return a number.\nTemplates and examples\nRelated resources\nRefer to Splitting with conditionals for more information on using conditionals to create complex logic in n8n."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.totp.md",
    "content": "TOTP\nThe TOTP node provides a way to generate a TOTP (time-based one-time password).\nNode parameters\nConfigure this node with these parameters.\nCredential to connect with\nSelect or create a TOTP credential for the node to use.\nOperation\nGenerate Secret is the only operation currently supported.\nNode options\nUse these Options to further configure the node.\nAlgorithm\nSelect the HMAC hashing algorithm to use. Default is SHA1.\nDigits\nEnter the number of digits in the generated code. Default is 6.\nPeriod\nEnter how many seconds the TOTP is valid for. Default is 30.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.wait.md",
    "content": "Wait\nUse the Wait node pause your workflow's execution. When the workflow pauses it offloads the execution data to the database. When the resume condition is met, the workflow reloads the data and the execution continues.\nOperations\nThe Wait node can Resume on the following conditions:\nAfter Time Interval: The node waits for a certain amount of time.\nAt Specified Time: The node waits until a specific time.\nOn Webhook Call: The node waits until it receives an HTTP call.\nOn Form Submitted: The node waits until it receives a form submission.\nRefer to the more detailed sections below for more detailed instructions.\nAfter Time Interval\nWait for a certain amount of time.\nThis parameter includes two more fields:\nWait Amount: Enter the amount of time to wait.\nWait Unit: Select the unit of measure for the Wait Amount. Choose from:\nSeconds\nMinutes\nHours\nDays\nRefer to Time-based operations for more detail on how these intervals work and the timezone used.\nAt Specified Time\nWait until a specific date and time to continue. Use the date and time picker to set the Date and Time.\nRefer to Time-based operations for more detail on the timezone used.\nOn Webhook Call\nThis parameter enables your workflows to resume when the Wait node receives an HTTP call.\nThe webhook URL that resumes the execution when called is generated at runtime. The Wait node provides the $execution.resumeUrl variable so that you can reference and send the yet-to-be-generated URL wherever needed, for example to a third-party service or in an email.\nWhen the workflow executes, the Wait node generates the resume URL and the webhook(s) in your workflow using the $execution.resumeUrl. This generated URL is unique to each execution, so your workflow can contain multiple Wait nodes and as the webhook URL is called it will resume each Wait node sequentially.\nFor this Resume style, set more parameters listed below.\nAuthentication\nSelect if and how incoming resume-webhook-requests to $execution.resumeUrl should be authenticated. Options include:\nBasic Auth: Use basic authentication. Select or enter a new Credential for Basic Auth to use.\nHeader Auth: Use header authentication. Select or enter a new Credential for Header Auth to use.\nJWT Auth: Use JWT authentication. Select or enter a new Credential for JWT Auth to use.\nNone: Don't use authentication.\nHTTP Method\nSelect the HTTP method the webhook should use. Refer to the Webhook node TABLE_PLACEHOLDER_0 Webhook Suffix*: Enter a suffix to append to the resume URL. This is useful for creating unique webhook URLs for each Wait node when a workflow contains multiple Wait nodes. Note that the generated $resumeWebhookUrl won't automatically include this suffix, you must manually append it to the webhook URL before exposing it.\nOn Webhook Call limitations\nThere are some limitations to keep in mind when using On Webhook Call:\nPartial executions of your workflow changes the $resumeWebhookUrl, so be sure that the node sending this URL to your desired third-party runs in the same execution as the Wait node.\nOn Form Submitted\nWait for a form submission before continuing. Set up these parameters:\nForm Title\nEnter the title to display at the top of the form.\nForm Description\nEnter a form description to display beneath the title. This description can help prompt the user on how to complete the form.\nForm Fields\nSet up each field you want to appear on your form using these parameters:\nField Label: Enter the field label you want to appear in the form.\nField Type: Select the type of field to display in the form. Choose from:\nDate\nDropdown List: Enter each dropdown options in the Field Options.\nMultiple Choice: Select whether the user can select a single dropdown option (turned off) or multiple dropdown options (turned on)\nNumber\nPassword\nText\nTextarea\nRequired Field: Set whether the user must complete this field in order to submit the form (turned on) or if the user can submit the form without completing it (turned off).\nRespond When\nSet when to respond to the form submission. Choose from:\nForm Is Submitted: Respond as soon as this node receives the form submission.\nWorkflow Finishes: Respond when the last node of this workflow finishes.\nUsing 'Respond to Webhook' Node: Respond when the Respond to Webhook node executes.\nLimit Wait Time\nSet whether the workflow will automatically resume execution after a specific limit type (turned on) or not (turned off).\nIf turned on, also set:\n* Limit Type: Select what type of limit to enforce from these options:\n* After Time Interval: Wait for a certain amount of time.\n* Enter the limit's Amount of time.\n* Select the limit's Unit of time.\n* At Specified Time: Wait until a specific date and time to resume.\n* Max Date and Time: Use the date and time picker to set the specified time the node should resume.\nOn Form Response options\nForm Response: Choose how and what you want the form to Respond With from these options:\nForm Submitted Text: The form displays whatever text is entered in Text to Show after a user fills out the form. Use this option if you want to display a confirmation message.\nRedirect URL: The form will redirect the user to the URL to Redirect to after they fill out the form. This must be a valid URL.\nWebhook Suffix: Enter a suffix to append to the resume URL. This is useful for creating unique webhook URLs for each Wait node when a workflow contains multiple Wait nodes. Note that the generated $resumeWebhookUrl won't automatically include this suffix, you must manually append it to the webhook URL before exposing it.\nTemplates and examples\nTime-based operations\nFor the time-based resume operations, note that:\nFor wait times less than 65 seconds, the workflow doesn't offload execution data to the database. Instead, the process continues to run and the execution resumes after the specified interval passes.\nThe n8n server time is always used regardless of the timezone setting. Workflow timezone settings, and any changes made to them, don't affect the Wait node interval or specified time."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.workflowtrigger.md",
    "content": "Workflow Trigger node\nThe Workflow Trigger node gets triggered when a workflow is updated or activated.\nThe Workflow Trigger node gets triggered for the workflow that it gets added to. You can use the Workflow Trigger node to trigger a workflow to notify the state of the workflow.\nNode parameters\nThe node includes a single parameter to identify the Events that should trigger it. Choose from these events:\nActive Workflow Updated: If you select this event, the node triggers when this workflow is updated.\nWorkflow Activated: If you select this event, the node triggers when this workflow is activated.\nYou can select one or both of these events.\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.xml.md",
    "content": "XML\nUse the XML node to convert data from and to XML.\nNode parameters\nMode: The format the data should be converted from and to.\nJSON to XML: Converts data from JSON to XML.\nXML to JSON: Converts data from XML to JSON.\nProperty Name: Enter the name of the property which contains the data to convert.\nNode options\nThese options are available regardless of the Mode you select:\nAttribute Key: Enter the prefix used to access the attributes. Default is $.\nCharacter Key: Enter the prefix used to access the character content. Default is _.\nAll other options depend on the selected Mode.\nJSON to XML options\nThese options only appear if you select JSON to XML as the Mode:\nAllow Surrogate Chars: Set whether to allow using characters from the Unicode surrogate blocks (turned on) or not (turned off).\nCdata: Set whether to wrap text nodes in  ...  instead of escaping when it's required (turned on) or not (turned off).\nTurning this option on doesn't add  ...  if it's not required.\nHeadless: Set whether to omit the XML header (turned on) or include it (turned off).\nRoot Name: Enter the root element name to use.\nXML to JSON options\nThese options only appear if you select XML to JSON as the Mode:\nExplicit Array: Set whether to put child nodes in an array (turned on) or create an array only if there's more than one child node (turned off).\nExplicit Root: Set whether to get the root node in the resulting object (turned on) or not (turned off).\nIgnore Attributes: Set whether to ignore all XML attributes and only create text nodes (turned on) or not (turned off).\nMerge Attributes: Set whether to merge attributes and child elements as properties of the parent (turned on) or key attributes off a child attribute object (turned off). This option is ignored if Ignore Attribute is turned on.\nNormalize: Set whether to trim whitespaces inside the text nodes (turned on) or not to trim them (turned off).\nNormalize Tags: Set whether to normalize all tag names to lowercase (turned on) or keep tag names as-is (turned off).\nTrim: Set whether to trim the whitespace at the beginning and end of text nodes (turned on) or to leave the whitespace as-is (turned off).\nTemplates and examples"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-langchain.mcptrigger.md",
    "content": "MCP Server Trigger node\nUse the MCP Server Trigger node to allow n8n to act as a Model Context Protocol (MCP) server, making n8n tools and workflows available to MCP clients.\nHow the MCP Server Trigger node works\nThe MCP Server Trigger node acts as an entry point into n8n for MCP clients. It operates by exposing a URL that MCP clients can interact with to access n8n tools.\nUnlike conventional trigger nodes, which respond to events and pass their output to the next connected node, the MCP Server Trigger node only connects to and executes tool nodes. Clients can list the available tools and call individual tools to perform work.\nYou can expose n8n workflows to clients by attaching them with the Custom n8n Workflow Tool node.\nNode parameters\nUse these parameters to configure your node.\nMCP URL\nThe MCP Server Trigger node has two MCP URLs: test and production. n8n displays the URLs at the top of the node panel.\nSelect Test URL or Production URL to toggle which URL n8n displays.\nTest: n8n registers a test MCP URL when you select Listen for Test Event or Execute workflow, if the workflow isn't active. When you call the MCP URL, n8n displays the data in the workflow.\nProduction: n8n registers a production MCP URL when you activate the workflow. When using the production URL, n8n doesn't display the data in the workflow. You can still view workflow data for a production execution: select the Executions tab in the workflow, then select the workflow execution you want to view.\nAuthentication\nYou can require authentication for clients connecting to your MCP URL. Choose from these authentication methods:\nBearer auth\nHeader auth\nRefer to the HTTP request credentials for more information on setting up each credential type.\nPath\nBy default, this field contains a randomly generated MCP URL path, to avoid conflicts with other MCP Server Trigger nodes.\nYou can manually specify a URL path, including adding route parameters. For example, you may need to do this if you use n8n to prototype an API and want consistent endpoint URLs.\nTemplates and examples\nIntegrating with Claude Desktop\nYou can connect to the MCP Server Trigger node from Claude Desktop by running a gateway to proxy SSE messages to stdio-based servers.\nTo do so, add the following to your Claude Desktop configuration:\nBe sure to replace the  and  placeholders with the values from your MCP Server Trigger node parameters and credentials.\nLimitations\nConfiguring the MCP Server Trigger node with webhook replicas\nThe MCP Server Trigger node relies on Server-Sent Events (SSE) or streamable HTTP, which require the same server instance to handle persistent connections. This can cause problems when running n8n in queue mode depending on your webhook processor configuration:\nIf you use queue mode with a single webhook replica, the MCP Server Trigger node works as expected.\nIf you run multiple webhook replicas, you need to route all /mcp requests to a single, dedicated webhook replica. Create a separate replica set with one webhook container for MCP requests. Afterward, update your ingress or load balancer configuration to direct all /mcp traffic to that instance.\nRelated resources\nn8n also provides an MCP Client Tool node that allows you to connect your n8n AI agents to external tools.\nRefer to the MCP documentation and MCP specification for more details about the protocol, servers, and clients.\nCommon issues\nHere are some common errors and issues with the MCP Server Trigger node and steps to resolve or troubleshoot them.\nRunning the MCP Server Trigger node with a reverse proxy\nWhen running n8n behind a reverse proxy like nginx, you may experience problems if the MCP endpoint isn't configured for SSE or streamable HTTP.\nSpecifically, you need to disable proxy buffering for the endpoint. Other items you might want to adjust include disabling gzip compression (n8n handles this itself), disabling chunked transfer encoding, and setting the Connection to an empty string to remove it from the forwarded headers. Explicitly disabling these in the MCP endpoint ensures they're not inherited from other places in your nginx configuration.\nAn example nginx location block for serving MCP traffic with these settings may look like this:"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-langchain.respondtochat.md",
    "content": "Respond to Chat node\nUse the Respond to Chat node in correspondence with the Chat Trigger node to send a response into the chat and optionally wait for a response from the user. This allows you to have multiple chat interactions within a single execution and enables human-in-the-loop use cases in the chat.\nNode parameters\nMessage\nThe message to send to the chat.\nWait for User Reply\nSet whether the workflow execution should wait for a response from the user (enabled) or continue immediately after sending the message (disabled).\nNode options\nAdd Memory Input Connection\nChoose whether you want to commit the messages from the Respond to Chat node to a connected memory. Using a shared memory between an agent or chain root node and the Respond to Chat node attaches the same session key to these messages and lets you capture the full message history.\nLimit Wait Time\nWhen you enable Wait for User Reply, this option decides whether the workflow automatically resumes execution after a specific limit (enabled) or not (disabled).\nRelated resources\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.code\\common-issues.md",
    "content": "Code node common issues\nHere are some common errors and issues with the Code node and steps to resolve or troubleshoot them.\nCode doesn't return items properly\nThis error occurs when the code in your Code node doesn't return data in the expected format.\nIn n8n, all data passed between nodes is an array of objects. Each of these objects wraps another object with the json key:\nTo troubleshoot this error, check the following:\nRead the data structure to understand the data you receive in the Code node and the requirements for outputting data from the node.\nUnderstand how data items work and how to connect data items from previous nodes with item linking.\nA 'json' property isn't an object\nThis error occurs when the Code node returns data where the json key isn't pointing to an object.\nThis may happen if you set json to a different data structure, like an array:\nTo resolve this, ensure that the json key references an object in your return data:\nCode doesn't return an object\nThis error may occur when your Code node doesn't return anything or if it returns an unexpected result.\nTo resolve this, ensure that your Code node returns the expected data structure:\nThis error may also occur if the code you provided returns 'undefined' instead of the expected result. In that case, ensure that the data you are referencing in your Code node exists in each execution and that it has the structure your code expects.\n'import' and 'export' may only appear at the top level\nThis error occurs if you try to use import or export in the Code node. These aren't supported by n8n's JavaScript sandbox. Instead, use the require function to load modules.\nTo resolve this issue, try changing your import statements to use require:\nCannot find module '<module>'\nThis error occurs if you try to use require in the Code node and n8n can't find the module.\nIf you're self-hosting n8n, follow these steps:\nInstall the module into your n8n environment.\nIf you are running n8n with npm, install the module in the same environment as n8n.\nIf you are running n8n with Docker, you need to extend the official n8n image with a custom image that includes your module.\nSet the NODE_FUNCTION_ALLOW_BUILTIN and NODE_FUNCTION_ALLOW_EXTERNAL environment variables to allow importing modules.\nUsing global variables\nSometimes you may wish to set and retrieve simple global data related to a workflow across and within executions. For example, you may wish to include the date of the previous report when compiling a report with a list of project updates.\nTo set, update, and retrieve data directly to a workflow, use the static data functions within your code. You can manage data either globally or tied to specific nodes."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.code\\index.md",
    "content": "Code node\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.code\\keyboard-shortcuts.md",
    "content": "Keyboard shortcuts when using the Code editor\nThe Code node editing environment supports a range of keyboard shortcuts to speed up and enhance your experience. Select the appropriate tab to see the relevant shortcuts for your operating system.\nCursor Movement\n=== \"Windows\"\nAction: Move cursor left   Shortcut: ++left++\nAction: Move cursor right   Shortcut: ++right++\nAction: Move cursor up   Shortcut: ++up++\nAction: Move cursor down   Shortcut: ++down++\nAction: Move cursor by word left   Shortcut: ++control+left++\nAction: Move cursor by word right   Shortcut: ++control+right++\nAction: Move to line start   Shortcut: ++home++ **or** ++control+left++\nAction: Move to line end   Shortcut: ++end++ or ++control+right++\nAction: Move to document start   Shortcut: ++control+home++\nAction: Move to document end   Shortcut: ++control+end++\nAction: Move page up   Shortcut: ++page-up++\nAction: Move page down   Shortcut: ++page-down++\nAction: Action   Shortcut: Shortcut\nAction: Move cursor left   Shortcut: ++left++ **or** ++control+b++\nAction: Move cursor right   Shortcut: ++right++ **or** ++control+f++\nAction: Move cursor up   Shortcut: ++up++ **or** ++control+p++\nAction: Move cursor down   Shortcut: ++down++ **or** ++control+n++\nAction: Move cursor by word left   Shortcut: ++option+left++\nAction: Move cursor by word right   Shortcut: ++option+right++\nAction: Move to line start   Shortcut: ++command+left++ **or** ++control+a++\nAction: Move to line end   Shortcut: ++command+right++ **or** ++control+e++\nAction: Move to document start   Shortcut: ++command+up++\nAction: Move to document end   Shortcut: ++command+down++\nAction: Move page up   Shortcut: ++page-up++ **or** ++option+v++\nAction: Move page down   Shortcut: ++page-down++ **or** ++control+v++\nAction: Action   Shortcut: Shortcut\nAction: Move cursor left   Shortcut: ++left++\nAction: Move cursor right   Shortcut: ++right++\nAction: Move cursor up   Shortcut: ++up++\nAction: Move cursor down   Shortcut: ++down++\nAction: Move cursor by word left   Shortcut: ++control+left++\nAction: Move cursor by word right   Shortcut: ++control+right++\nAction: Move to line start   Shortcut: ++home++ **or** ++control+left++\nAction: Move to line end   Shortcut: ++end++ or ++control+right++\nAction: Move to document start   Shortcut: ++control+home++\nAction: Move to document end   Shortcut: ++control+end++\nAction: Move page up   Shortcut: ++page-up++\nAction: Move page down   Shortcut: ++page-down++\nAction: Action   Shortcut: Shortcut\nAction: Selection with any movement key   Shortcut: ++shift++ + [Movement Key]\nAction: Select all   Shortcut: ++control+a++\nAction: Select line   Shortcut: ++control+l++\nAction: Select next occurrence   Shortcut: ++control+d++\nAction: Select all occurrences   Shortcut: ++shift+control+l++\nAction: Go to matching bracket   Shortcut: ++shift+control+backslash++\nAction: Action   Shortcut: Shortcut\nAction: Selection with any movement key   Shortcut: ++shift++ + [Movement Key]\nAction: Select all   Shortcut: ++command+a++\nAction: Select line   Shortcut: ++command+l++\nAction: Select next occurrence   Shortcut: ++command+d++\nAction: Go to matching bracket   Shortcut: ++shift+command+backslash++\nAction: Action   Shortcut: Shortcut\nAction: Selection with any movement key   Shortcut: ++shift++ + [Movement Key]\nAction: Select all   Shortcut: ++control+a++\nAction: Select line   Shortcut: ++control+l++\nAction: Select next occurrence   Shortcut: ++control+d++\nAction: Select all occurrences   Shortcut: ++shift+control+l++\nAction: Go to matching bracket   Shortcut: ++shift+control+backslash++\nAction: Action   Shortcut: Shortcut\nAction: New line with indentation   Shortcut: ++enter++\nAction: Undo   Shortcut: ++control+z++\nAction: Redo   Shortcut: ++control+y++ **or** ++control+shift+z++\nAction: Undo selection   Shortcut: ++control+u++\nAction: Copy   Shortcut: ++control+c++\nAction: Cut   Shortcut: ++control+x++\nAction: Paste   Shortcut: ++control+v++\nAction: Action   Shortcut: Shortcut\nAction: New line with indentation   Shortcut: ++enter++\nAction: Undo   Shortcut: ++command+z++\nAction: Redo   Shortcut: ++command+y++ **or** ++command+shift+z++\nAction: Undo selection   Shortcut: ++command+u++\nAction: Copy   Shortcut: ++command+c++\nAction: Cut   Shortcut: ++command+x++\nAction: Paste   Shortcut: ++command+v++\nAction: Action   Shortcut: Shortcut\nAction: New line with indentation   Shortcut: ++enter++\nAction: Undo   Shortcut: ++control+z++\nAction: Redo   Shortcut: ++control+y++ **or** ++control+shift+z++\nAction: Undo selection   Shortcut: ++control+u++\nAction: Copy   Shortcut: ++control+c++\nAction: Cut   Shortcut: ++control+x++\nAction: Paste   Shortcut: ++control+v++\nAction: Action   Shortcut: Shortcut\nAction: Delete character left   Shortcut: ++backspace++\nAction: Delete character right   Shortcut: ++delete++\nAction: Delete word left   Shortcut: ++control+backspace++\nAction: Delete word right   Shortcut: ++control+delete++\nAction: Delete line   Shortcut: ++shift+control+k++\nAction: Action   Shortcut: Shortcut\nAction: Delete character left   Shortcut: ++backspace++\nAction: Delete character right   Shortcut: ++delete++\nAction: Delete word left   Shortcut: ++option+backspace++ **or** ++control+command+h++\nAction: Delete word right   Shortcut: ++option+delete++  **or** ++function+option+backspace++\nAction: Delete line   Shortcut: ++shift+command+k++\nAction: Delete to line start   Shortcut: ++command+backspace++\nAction: Delete to line end   Shortcut: ++command+delete++ **or** ++control+k++\nAction: Action   Shortcut: Shortcut\nAction: Delete character left   Shortcut: ++backspace++\nAction: Delete character right   Shortcut: ++delete++\nAction: Delete word left   Shortcut: ++control+backspace++\nAction: Delete word right   Shortcut: ++control+delete++\nAction: Delete line   Shortcut: ++shift+control+k++\nAction: Action   Shortcut: Shortcut\nAction: Move line up   Shortcut: ++alt+up++\nAction: Move line down   Shortcut: ++alt+down++\nAction: Copy line up   Shortcut: ++shift+alt+up++\nAction: Copy line down   Shortcut: ++shift+alt+down++\nAction: Toggle line comment   Shortcut: ++control+slash++\nAction: Add line comment   Shortcut: ++control+k++ **then** ++control+c++\nAction: Remove line comment   Shortcut: ++control+k++ **then** ++control+u++\nAction: Toggle block comment   Shortcut: ++shift+alt+a++\nAction: Action   Shortcut: Shortcut\nAction: Move line up   Shortcut: ++option+up++\nAction: Move line down   Shortcut: ++option+down++\nAction: Copy line up   Shortcut: ++shift+option+up++\nAction: Copy line down   Shortcut: ++shift+option+down++\nAction: Toggle line comment   Shortcut: ++command+slash++\nAction: Add line comment   Shortcut: ++command+k++ **then** ++command+c++\nAction: Remove line comment   Shortcut: ++command+k++ **then** ++command+u++\nAction: Toggle block comment   Shortcut: ++shift+option+a++\nAction: Split line   Shortcut: ++control+o++\nAction: Transpose characters   Shortcut: ++control+t++\nAction: Action   Shortcut: Shortcut\nAction: Move line up   Shortcut: ++alt+up++\nAction: Move line down   Shortcut: ++alt+down++\nAction: Copy line up   Shortcut: ++shift+alt+up++\nAction: Copy line down   Shortcut: ++shift+alt+down++\nAction: Toggle line comment   Shortcut: ++control+slash++\nAction: Add line comment   Shortcut: ++control+k++ **then** ++control+c++\nAction: Remove line comment   Shortcut: ++control+k++ **then** ++control+c++\nAction: Toggle block comment   Shortcut: ++shift+alt+a++\nAction: Action   Shortcut: Shortcut\nAction: Start completion   Shortcut: ++control+space++\nAction: Accept completion   Shortcut: ++enter++ **or** ++tab++\nAction: Close completion   Shortcut: ++escape++\nAction: Navigate completion options   Shortcut: ++up++ **or** ++down++\nAction: Action   Shortcut: Shortcut\nAction: Start completion   Shortcut: ++control+space++\nAction: Accept completion   Shortcut: ++enter++ **or** ++tab++\nAction: Close completion   Shortcut: ++escape++\nAction: Navigate completion options   Shortcut: ++up++ **or** ++down++\nAction: Action   Shortcut: Shortcut\nAction: Start completion   Shortcut: ++control+space++\nAction: Accept completion   Shortcut: ++enter++ **or** ++tab++\nAction: Close completion   Shortcut: ++escape++\nAction: Navigate completion options   Shortcut: ++up++ **or** ++down++\nAction: Action   Shortcut: Shortcut\nAction: Indent more   Shortcut: ++tab++ **or** ++control+bracket-right++\nAction: Indent less   Shortcut: ++shift+tab++ **or** ++control+bracket-left++\nAction: Action   Shortcut: Shortcut\nAction: Indent more   Shortcut: ++command+bracket-right++\nAction: Indent less   Shortcut: ++command+bracket-left++\nAction: Action   Shortcut: Shortcut\nAction: Indent more   Shortcut: ++tab++ **or** ++control+bracket-right++\nAction: Indent less   Shortcut: ++shift+tab++ **or** ++control+bracket-left++\nAction: Action   Shortcut: Shortcut\nAction: Fold code   Shortcut: ++control+shift+bracket-left++\nAction: Unfold code   Shortcut: ++control+shift+bracket-right++\nAction: Fold all   Shortcut: ++control+k++ **then** ++control+0++\nAction: Unfold all   Shortcut: ++control+k++ **then** ++control+j++\nAction: Action   Shortcut: Shortcut\nAction: Fold code   Shortcut: ++command+option+bracket-left++\nAction: Unfold code   Shortcut: ++command+option+bracket-right++\nAction: Fold all   Shortcut: ++command+k++ **then** ++command+0++\nAction: Unfold all   Shortcut: ++command+k++ **then** ++command+j++\nAction: Action   Shortcut: Shortcut\nAction: Fold code   Shortcut: ++control+shift+bracket-left++\nAction: Unfold code   Shortcut: ++control+shift+bracket-right++\nAction: Fold all   Shortcut: ++control+k++ **then** ++control+0++\nAction: Unfold all   Shortcut: ++control+k++ **then** ++control+j++\nAction: Action   Shortcut: Shortcut\nAction: Add cursor at click position   Shortcut: ++alt+left-button++\nAction: Add cursor above   Shortcut: ++control+alt+up++\nAction: Add cursor below   Shortcut: ++control+alt+down++\nAction: Add cursors to line ends   Shortcut: ++shift+alt+i++\nAction: Clear multiple cursors   Shortcut: ++escape++\nAction: Action   Shortcut: Shortcut\nAction: Add cursor at click position   Shortcut: ++option+left-button++\nAction: Add cursor above   Shortcut: ++control+option+up++\nAction: Add cursor below   Shortcut: ++control+option+down++\nAction: Add cursors to line ends   Shortcut: ++shift+option+i++\nAction: Clear multiple cursors   Shortcut: ++escape++\nAction: Action   Shortcut: Shortcut\nAction: Add cursor at click position   Shortcut: ++alt+left-button++\nAction: Add cursor above   Shortcut: ++shift+alt+up++\nAction: Add cursor below   Shortcut: ++shift+alt+down++\nAction: Add cursors to line ends   Shortcut: ++shift+alt+i++\nAction: Clear multiple cursors   Shortcut: ++escape++\nAction: Action   Shortcut: Shortcut\nAction: Format document   Shortcut: ++shift+alt+f++\nAction: Action   Shortcut: Shortcut\nAction: Format document   Shortcut: ++shift+command+f++\nAction: Action   Shortcut: Shortcut\nAction: Format document   Shortcut: ++control+shift+i++\nAction: Action   Shortcut: Shortcut\nAction: Open Search   Shortcut: ++control+f++\nAction: Select All   Shortcut: ++alt+enter++\nAction: Replace All   Shortcut: ++control+alt+enter++\nAction: Go To Line   Shortcut: ++control+g++\nAction: Next Diagnostic   Shortcut: ++f8++\nAction: Previous Diag.   Shortcut: ++shift+f8++\nAction: Open Lint Panel   Shortcut: ++control+shift+m++\nAction: Action   Shortcut: Shortcut\nAction: Open Search   Shortcut: ++command+f++\nAction: Select All   Shortcut: ++command+enter++\nAction: Replace All   Shortcut: ++command+option+enter++\nAction: Go To Line   Shortcut: ++command+g++\nAction: Next Diagnostic   Shortcut: ++f8++\nAction: Previous Diag.   Shortcut: ++shift+f8++\nAction: Open Lint Panel   Shortcut: ++command+shift+m++\nAction: Action   Shortcut: Shortcut\nAction: Open Search   Shortcut: ++control+f++\nAction: Select All   Shortcut: ++alt+enter++\nAction: Replace All   Shortcut: ++control+alt+enter++\nAction: Go To Line   Shortcut: ++control+g++\nAction: Next Diagnostic   Shortcut: ++f8++\nAction: Previous Diag.   Shortcut: ++shift+f8++    | Open Lint Panel | ++control+shift+m++   |"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.executecommand\\common-issues.md",
    "content": "Execute Command node common issues\nHere are some common errors and issues with the Execute Command node and steps to resolve or troubleshoot them.\nCommand failed: <command> /bin/sh: <command>: not found\nThis error occurs when the shell environment can't find one of the commands in the Command parameter.\nTo fix this error, review the following:\nCheck that the command and its arguments don't have typos in the Command parameter.\nCheck that the command is in the PATH of the user running n8n.\nIf you are running n8n with Docker, check if the command is available within the container by trying to run it manually. If your command isn't included in the container, you might have to extend the official n8n image with a custom image that includes your command.\nIf n8n is already running:\nIf n8n isn't running:\nError: stdout maxBuffer length exceeded\nThis error happens when your command returns more output than the Execute Command node is able to process at one time.\nTo avoid this error, reduce output your command produces. Check your command's manual page or documentation to see if there are flags to limit or filter output. If not, you may need to pipe the output to another command to remove unneeded info."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.executecommand\\index.md",
    "content": "Execute Command\nThe Execute Command node runs shell commands on the host machine that runs n8n.\nNode parameters\nConfigure the node using the following parameters.\nExecute Once\nChoose whether you want the node to execute only once (turned on) or once for every item it receives as input (turned off).\nCommand\nEnter the command to execute on the host machine. Refer to sections below for examples of running multiple commands and cURL commands.\nRun multiple commands\nUse one of two methods to run multiple commands in one Execute Command node:\nEnter each command on one line separated by &&. For example, you can combine the change directory (cd) command with the list (ls) command using &&.\nEnter each command on a separate line. For example, you can write the list (ls) command on a new line after the change directory (cd) command.\nRun cURL command\nYou can also use the HTTP Request node to make a cURL request.\nIf you want to run the curl command in the Execute Command node, you will have to build a Docker image based on the existing n8n image. The default n8n Docker image uses Alpine Linux. You will have to install the curl package.\nCreate a file named Dockerfile.\nAdd the below code snippet to the Dockerfile.\nIn the same folder, execute the command below to build the Docker image.\nReplace the Docker image you used before. For example, replace docker.n8n.io/n8nio/n8n with n8n-curl.\nRun the newly created Docker image. You'll now be able to execute ssh using the Execute Command Node.\nTemplates and examples\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.httprequest\\common-issues.md",
    "content": "HTTP Request node common issues\nHere are some common errors and issues with the HTTP Request node and steps to resolve or troubleshoot them.\nBad request - please check your parameters\nThis error displays when the node receives a 400 error indicating a bad request. This error most often occurs because:\nYou're using an invalid name or value in a Query Parameter.\nYou're passing array values in a Query Parameter but the array isn't formatted correctly. Try using the Array Format in Query Parameters option.\nReview the API documentation for your service to format your query parameters.\nThe resource you are requesting could not be found\nThis error displays when the endpoint URL you entered is invalid.\nThis may be due to a typo in the URL or a deprecated API. Refer to your service's API documentation to verify you have a valid endpoint.\nJSON parameter need to be an valid JSON\nThis error displays when you've passed a parameter as JSON and it's not formatted as valid JSON.\nTo resolve, review the JSON you've entered for these issues:\nTest your JSON in a JSON checker or syntax parser to find errors like missing quotation marks, extra or missing commas, incorrectly formatted arrays, extra or missing square brackets or curly brackets, and so on.\nIf you've used an Expression in the node, be sure you've wrapped the entire JSON in double curly brackets, for example:\nForbidden - perhaps check your credentials\nThis error displays when the node receives a 403 error indicating authentication failed.\nTo resolve, review the selected credentials and make sure you can authenticate with them. You may need to:\nUpdate permissions or scopes so that your API key or account can perform the operation you've selected.\nFormat your generic credential in a different way.\nGenerate a new API key or token with the appropriate permissions or scopes.\n429 - The service is receiving too many requests from you\nThis error displays when the node receives a 429 error from the service that you're calling. This often means that you have hit the rate limits of that service. You can find out more on the Handling API rate limits page.\nTo resolve the error, you can use one of the built-in options of the HTTP request node:\nBatching\nUse this option to send requests in batches and introduce a delay between them.\nIn the HTTP Request node, select Add Option > Batching.\nSet Items per Batch to the number of input items to include in each request.\nSet Batch Interval (ms) to introduce a delay between requests in milliseconds. For example, to send one request to an API per second, set Batch Interval (ms) to 1000.\nRetry on Fail\nUse this option to retry the node after a failed attempt.\nIn the HTTP Request node, go to Settings and enable Retry on Fail.\nSet Max Tries to the maximum number of times n8n should retry the node.\nSet Wait Between Tries (ms) to the desired delay in milliseconds between retries. For example, to wait one second before retrying the request again, set Wait Between Tries (ms) to 1000."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.httprequest\\index.md",
    "content": "HTTP Request node\nThe HTTP Request node is one of the most versatile nodes in n8n. It allows you to make HTTP requests to query data from any app or service with a REST API. You can use the HTTP Request node a regular node or attached to an AI agent to use as a tool{ data-preview }.\nWhen using this node, you're creating a REST API call. You need some understanding of basic API terminology and concepts.\nThere are two ways to create an HTTP request: configure the node parameters or import a curl command.\nNode parameters\nMethod\nSelect the method to use for the request:\nDELETE\nGET\nHEAD\nOPTIONS\nPATCH\nPOST\nPUT\nURL\nEnter the endpoint you want to use.\nAuthentication\nn8n recommends using the Predefined Credential Type option when it's available. It offers an easier way to set up and manage credentials, compared to configuring generic credentials.\nPredefined credentials\nCredentials for integrations supported by n8n, including both built-in and community nodes. Use Predefined Credential Type for custom operations without extra setup. Refer to Custom API operations for more information.\nGeneric credentials\nCredentials for integrations not supported by n8n. You'll need to manually configure the authentication process, including specifying the required API endpoints, necessary parameters, and the authentication method.\nYou can select one of the following methods:\nBasic auth\nCustom auth\nDigest auth\nHeader auth\nOAuth1 API\nOAuth2 API\nQuery auth\nRefer to HTTP request credentials for more information on setting up each credential type.\nSend Query Parameters\nQuery parameters act as filters on HTTP requests. If the API you're interacting with supports them and the request you're making needs a filter, turn this option on.\nSpecify your query parameters using one of the available options:\nUsing Fields Below: Enter Name/Value pairs of Query Parameters. To enter more query parameter name/value pairs, select Add Parameter. The name is the name of the field you're filtering on, and the value is the filter value.\nUsing JSON: Enter JSON to define your query parameters.\nRefer to your service's API documentation for detailed guidance.\nSend Headers\nUse this parameter to send headers with your request. Headers contain metadata or context about your request.\nSpecify Headers using one of the available options:\nUsing Fields Below: Enter Name/Value pairs of Header Parameters. To enter more header parameter name/value pairs, select Add Parameter. The name is the header you wish to set, and the value is the value you want to pass for that header.\nUsing JSON: Enter JSON to define your header parameters.\nRefer to your service's API documentation for detailed guidance.\nSend Body\nIf you need to send a body with your API request, turn this option on.\nThen select the Body Content Type that best matches the format for the body content you wish to send.\nForm URLencoded\nUse this option to send your body as application/x-www-form-urlencoded.\nSpecify Body using one of the available options:\nUsing Fields Below: Enter Name/Value pairs of Body Parameters. To enter more body parameter name/value pairs, select Add Parameter. The name should be the form field name, and the value is what you wish to set that field to.\nUsing Single Field: Enter your name/value pairs in a single Body parameter with format fieldname1=value1&fieldname2=value2.\nRefer to your service's API documentation for detailed guidance.\nForm-Data\nUse this option to send your body as multipart/form-data.\nConfigure your Body Parameters by selecting the Parameter Type:\nChoose Form Data to enter Name/Value pairs.\nChoose n8n Binary File to pull the body from a file the node has access to.\nName: Enter the ID of the field to set.\nInput Data Field Name: Enter the name of the incoming field containing the binary file data you want to process.\nSelect Add Parameter to enter more parameters.\nRefer to your service's API documentation for detailed guidance.\nJSON\nUse this option to send your body as JSON.\nSpecify Body using one of the available options:\nUsing Fields Below: Enter Name/Value pairs of Body Parameters. To enter more body parameter name/value pairs, select Add Parameter.\nUsing JSON: Enter JSON to define your body.\nRefer to your service's API documentation for detailed guidance.\nn8n Binary File\nUse this option to send the contents of a file stored in n8n as the body.\nEnter the name of the incoming field that contains the file as the Input Data Field Name.\nRefer to your service's API documentation for detailed guidance on how to format the file.\nRaw\nUse this option to send raw data in the body.\nContent Type: Enter the Content-Type header to use for the raw body content. Refer to the IANA Media types documentation for a full list of MIME content types.\nBody: Enter the raw body content to send.\nRefer to your service's API documentation for detailed guidance.\nNode options\nSelect Add Option to view and select these options. Options are available to all parameters unless otherwise noted.\nArray Format in Query Parameters\nUse this option to control the format for arrays included in query parameters. Choose from these options:\nNo Brackets: Arrays will format as the name=value for each item in the array, for example: foo=bar&foo=qux.\nBrackets Only: The node adds square brackets after each array name, for example: foo[]=bar&foo[]=qux.\nBrackets with Indices: The node adds square brackets with an index value after each array name, for example: foo[0]=bar&foo[1]=qux.\nRefer to your service's API documentation for guidance on which option to use.\nBatching\nControl how to batch large numbers of input items:\nItems per Batch: Enter the number of input items to include in each batch.\nBatch Interval: Enter the time to wait between each batch of requests in milliseconds. Enter 0 for no batch interval.\nIgnore SSL Issues\nBy default, n8n only downloads the response if SSL certificate validation succeeds. If you'd like to download the response even if SSL certificate validation fails, turn this option on.\nLowercase Headers\nChoose whether to lowercase header names (turned on, default) or not (turned off).\nRedirects\nChoose whether to follow redirects (turned on by default) or not (turned off). If turned on, enter the maximum number of redirects the request should follow in Max Redirects.\nResponse\nUse this option to set some details about the expected API response, including:\nInclude Response Headers and Status: By default, the node returns only the body. Turn this option on to return the full response (headers and response status code) as well as the body.\nNever Error: By default, the node returns success only when the response returns with a 2xx code. Turn this option on to return success regardless of the code returned.\nResponse Format: Select the format in which the data gets returned. Choose from:\nAutodetect (default): The node detects and formats the response based on the data returned.\nFile: Select this option to put the response into a file. Enter the field name where you want the file returned in Put Output in Field.\nJSON: Select this option to format the response as JSON.\nText: Select this option to format the response as plain text. Enter the field name where you want the file returned in Put Output in Field.\nPagination\nUse this option to paginate results, useful for handling query results that are too big for the API to return in a single call.\n??? Details \"Understand pagination\"\nPagination means splitting a large set of data into multiple pages. The amount of data on each page depends on the limit you set.\nFor example, you make an API call to an endpoint called /users. The API wants to send back information on 300 users, but this is too much data for the API to send in one response.\nIf the API supports pagination, you can incrementally fetch the data. To do this, you call /users with a pagination limit, and a page number or URL to tell the API which page to send. In this example, say you use a limit of 10, and start from page 0. The API sends the first 10 users in its response. You then call the API again, increasing the page number by 1, to get the next 10 results.\nConfigure the pagination settings:\nPagination Mode:\nOff: Turn off pagination.\nUpdate a Parameter in Each Request: Use this when you need to dynamically set parameters for each request.\nResponse Contains Next URL: Use this when the API response includes the URL of the next page. Use an expression to set Next URL.\nFor example setups, refer to HTTP Request node cookbook | Pagination.\nn8n provides built-in variables for working with HTTP node requests and responses when using pagination:\nProxy\nUse this option if you need to specify an HTTP proxy.\nEnter the Proxy the request should use. This takes precedence over global settings defined with the HTTP_PROXY, HTTPS_PROXY, or ALL_PROXY environment variables.\nTimeout\nUse this option to set how long the node should wait for the server to send response headers (and start the response body). The node aborts requests that exceed this value for the initial response.\nEnter the Timeout time to wait in milliseconds.\nTool-only options\nThe following options are only available when attached to an AI agent as a tool{ data-preview }.\nOptimize Response\nWhether to optimize the tool response to reduce the amount of data passed to the LLM. Optimizing the response can reduce costs and can help the LLM ignore unimportant details, often leading to better results.\nWhen optimizing responses, you select an expected response type, which determines other options you can configure. The supported response types are:\nJSON\nWhen expecting a JSON response, you can configure which parts of the JSON data to use as a response with the following choices:\nField Containing Data: This field identifies a specific part of the JSON object that contains your relevant data. You can leave this blank to use the entire response.\nInclude Fields: This is how you choose which fields you want in your response object. There are three choices:\nAll: Include all fields in the response object.\nSelected: Include only the fields specified below.\nFields: A comma-separated list of fields to include in the response. You can use dot notation to specify nested fields. You can drag fields from the Input panel to add them to the field list.\nExclude: Include all fields except the fields specified below.\nFields: A comma-separated list of fields to exclude from the response. You can use dot notation to specify nested fields. You can drag fields from the Input panel to add them to the field list.\nHTML\nWhen expecting HTML, you can identify the part of an HTML document relevant to the LLM and optimize the response with the following options:\nSelector (CSS): A specific element or element type to include in the response HTML. Uses the body element by default.\nReturn Only Content: Whether to strip HTML tags and attributes from the response, leaving only the actual content. This uses fewer tokens and may be easier for the model to understand.\nElements To Omit: A comma-separated list of CSS selectors to exclude when extracting content.\nTruncate Response: Whether to limit the response size to save tokens.\nMax Response Characters: The maximum number of characters to include in the HTML response. The default value is 1000.\nText\nWhen expecting a generic Text response, you can optimize the results with the following options:\nTruncate Response: Whether to limit the response size to save tokens.\nMax Response Characters: The maximum number of characters to include in the HTML response. The default value is 1000.\nImport curl command\ncurl is a command line tool and library for transferring data with URLs.\nYou can use curl to call REST APIs. If the API documentation of the service you want to use provides curl examples, you can copy them out of the documentation and into n8n to configure the HTTP Request node.\nImport a curl command:\nFrom the HTTP Request node's Parameters tab, select Import cURL. The Import cURL command modal opens.\nPaste your curl command into the text box.\nSelect Import. n8n loads the request configuration into the node fields. This overwrites any existing configuration.\nTemplates and examples\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.removeduplicates\\index.md",
    "content": "Remove Duplicates node\nUse the Remove Duplicates node to identify and delete items that are:\nidentical across all fields or a subset of fields in a single execution\nidentical to or surpassed by items seen in previous executions\nThis is helpful in situations where you can end up with duplicate data, such as a user creating multiple accounts, or a customer submitting the same order multiple times. When working with large datasets it becomes more difficult to spot and remove these items.\nBy comparing against data from previous executions, the Remove Duplicates node can  delete items seen in earlier executions. It can also ensure that new items have a later date or a higher value than previous values.\nOperation modes\nThe remove duplication node works differently depending on the value of the operation parameter:\nRemove Items Repeated Within Current Input: Identify and remove duplicate items in the current input across all fields or a subset of fields.\nRemove Items Processed in Previous Executions: Compare items in the current input to items from previous executions and remove duplicates.\nClear Deduplication History: Wipe the memory of items from previous executions.\nRemove Items Repeated Within Current Input\nWhen you set the \"Operations\" field to Remove Items Repeated Within Current Input, the Remove Duplicate node identifies and removes duplicate items in the current input. It can do this across all fields, or within a subset of fields.\nRemove Items Repeated Within Current Input parameters\nWhen using the Remove Items Repeated Within Current Input operation, the following parameter is available:\nCompare: Select which fields of the input data n8n should compare to check if they're the same. The following options are available:\nAll Fields: Compares all fields of the input data.\nAll Fields Except: Enter which input data fields n8n should exclude from the comparison. You can provide multiple values separated by commas.\nSelected Fields: Enter which input data fields n8n should include in the comparison. You can provide multiple values separated by commas.\nRemove Items Repeated Within Current Input options\nIf you choose All Fields Except or Selected Fields as your compare type, you can add these options:\nDisable Dot Notation: Set whether to use dot notation to reference child fields in the format parent.child (turned off) or not (turn on).\nRemove Other Fields: Set whether to remove any fields that aren't used in the comparison (turned on) or not (turned off).\nRemove Items Processed in Previous Executions\nWhen you set the \"Operation\" field to Remove Items Processed in Previous Executions, the Remove Duplicate node compares items in the current input to items from previous executions.\nRemove Items Processed in Previous Executions parameters\nWhen using the Remove Items Processed in Previous Executions operation, the following parameters are available:\nKeep Items Where: Select how n8n decides which items to keep. The following options are available:\nValue Is New: n8n removes items if their value matches items from earlier executions.\nValue Is Higher than Any Previous Value: n8n removes items if the current value isn't higher than previous values.\nValue Is a Date Later than Any Previous Date: n8n removes date items if the current date isn't later than previous dates.\nValue to Dedupe On: The input field or fields to compare. The option you select for the Keep Items Where parameter determines the exact format you need:\nWhen using Value Is New, this must be an input field or combination of fields with a unique ID.\nWhen using Value Is Higher than Any Previous Value, this must be an input field or combination of fields that has an incremental value.\nWhen using Value Is a Date Later than Any Previous Date, this must be an input field that has a date value in ISO format.\nRemove Items Processed in Previous Executions options\nWhen using the Remove Items Processed in Previous Executions operation, the following option is available:\nScope: Sets how n8n stores and uses the deduplication data for comparisons. The following options are available:\nNode: (default) Stores the data for this node independently from other Remove Duplicates instances in the workflow. When you use this scope, you can clear the duplication history for this node instance without affecting other nodes.\nWorkflow: Stores the duplication data at the workflow level. This shares duplication data with any other Remove Duplicate nodes set to use \"workflow\" scope.  n8n will still manage the duplication data for other Remove Duplicate nodes set to \"node\" scope independently.\nWhen you select Value Is New as your Keep Items Where choice, this option is also available:\nHistory Size: The number of items for n8n to store to track duplicates across executions. The value of the Scope option determines whether this history size is specific to this individual Remove Duplicate node instance or shared with other instances in the workflow. By default, n8n stores 10,000 items.\nClear Deduplication History\nWhen you set the \"Operation\" field to Clear Deduplication History, the Remove Duplicates node manages and clears the stored items from previous executions. This operation doesn't affect any items in the current input. Instead, it manages the database of items that the \"Remove Items Processed in Previous Executions\" operation uses.\nClear Deduplication History parameters\nWhen using the Clear Deduplication History operation, the following parameter is available:\nMode: How you want to manage the key / value items stored in the database. The following option is available:\nClean Database: Deletes all duplication data stored in the database. This resets the duplication database to its original state.\nClear Deduplication History options\nWhen using the Clear Deduplication History operation, the following option is available:\nScope: Sets the scope n8n uses when managing the duplication database.\nNode: (default) Manages the duplication database specific to this Remove Duplicates node instance.\nWorkflow: Manages the duplication database shared by all Remove Duplicate node instances that use workflow scope.\nTemplates and examples\nFor templates using the Remove Duplicates node and examples of how to use it, refer to Templates and examples.\nRelated resources"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.removeduplicates\\templates-and-examples.md",
    "content": "Templates and examples\nHere are some templates and examples for the Remove Duplicates node.\nTemplates\nSet up sample data using the Code node\nCreate a workflow with some example input data to try out the Remove Duplicates node.\nAdd a Code node to the canvas and connect it to the Manual Trigger node.\nIn the Code node, set Mode to Run Once for Each Item and Language to JavaScript.\nPaste the following JavaScript code snippet in the JavaScript field:\nAdd a Split Out node to the canvas and connect it to the Code node.\nIn the Split Out node, enter data in the Fields To Split Out field.\nRemoving duplicates from the current input\nAdd a Remove Duplicates node to the canvas and connect it to the Split Out node. Choose Remove items repeated within current input as the Action to start.\nOpen the Remove Duplicates node and ensure that the Operation is set to Remove Items Repeated Within Current Input.\nChoose All fields in the Compare field.\nSelect Execute step to run the Remove Duplicates node, removing duplicated data in the current input.\nn8n removes the items that have the same data across all fields. Your output in table view should look like this:\nTABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.scheduletrigger\\common-issues.md",
    "content": "Schedule Trigger node common issues\nHere are some common errors and issues with the Schedule Trigger node and steps to resolve or troubleshoot them.\nInvalid cron expression\nThis error occurs when you set Trigger Interval to Custom (Cron) and n8n doesn't understand your cron expression. This may mean that there is a mistake in your cron expression or that you're using an incompatible syntax.\nTo debug, check that the following:\nThat your cron expression follows the syntax used in the cron examples\nThat your cron expression (after removing the seconds column) validates on crontab guru\nScheduled workflows run at the wrong time\nIf the Schedule Trigger node runs at the wrong time, it may mean that you need to adjust the time zone n8n uses.\nAdjust the timezone globally\nIf you're using n8n Cloud, follow the instructions on the set the Cloud instance timezone page to ensure that n8n executes in sync with your local time.\nIf you're self hosting, set your global timezone using the GENERIC_TIMEZONE environment variable.\nAdjust the timezone for an individual workflow\nTo set the timezone for an individual workflow:\nOpen the workflow on the canvas.\nSelect the !three dots menu Three dots icon in the upper-right corner.\nSelect Settings.\nChange the Timezone setting.\nSelect Save.\nVariables not working as expected\nWhile variables can be used in the scheduled trigger, their values only get evaluated when the workflow is activated. After activating the worfklow, you can alter a variable's value in the settings but it won't change how often the workflow runs. To work around this, you must stop and then re-activate the workflow to apply the updated variable value.\nChanging the trigger interval\nYou can update the scheduled trigger interval at any time but it only gets updated when the workflow is activated. If you change the trigger interval after the workflow is active, the changes won't take effect until you stop and then re-activate the workflow.\nAlso, the schedule begins from the time when you activate the workflow. For example, if you had originally set a schedule of every 1 hour and it should execute at 12:00, if you changed it to a 2 hour schedule and re-activated the workflow at 11:30, the next execution will be at 13:30, 2 hours from when you activated it."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.scheduletrigger\\index.md",
    "content": "Schedule Trigger node\nUse the Schedule Trigger node to run workflows at fixed intervals and times. This works in a similar way to the Cron software utility in Unix-like systems.\nNode parameters\nAdd Trigger Rules to determine when the trigger should run.\nUse the Trigger Interval to select the time interval unit of measure to schedule the trigger for. All other parameters depend on the interval you select. Choose from:\nSeconds trigger interval\nMinutes trigger interval\nHours trigger interval\nDays trigger interval\nWeeks trigger interval\nMonths trigger interval\nCustom (Cron) interval\nYou can add multiple Trigger Rules to run the node on different schedules.\nRefer to the sections below for more detail on configuring each Trigger Interval. Refer to Templates and examples for further examples.\nSeconds trigger interval\nSeconds Between Triggers: Enter the number of seconds between each workflow trigger. For example, if you enter 30 here, the trigger will run every 30 seconds.\nMinutes trigger interval\nMinutes Between Triggers: Enter the number of minutes between each workflow trigger. For example, if you enter 5 here, the trigger will run every 5 minutes.\nHours trigger interval\nHours Between Triggers: Enter the number of hours between each workflow trigger.\nTrigger at Minute: Enter the minute past the hour to trigger the node when it runs, from 0 to 59.\nFor example, if you enter 6 Hours Between Triggers and 30 Trigger at Minute, the node will run every six hours at 30 minutes past the hour.\nDays trigger interval\nDays Between Triggers: Enter the number of days between each workflow trigger.\nTrigger at Hour: Select the hour of the day to trigger the node.\nTrigger at Minute: Enter the minute past the hour to trigger the node when it runs, from 0 to 59.\nFor example, if you enter 2 Days Between Triggers, 9am for Trigger at Hour, and 15 Trigger at Minute, the node will run every two days at 9:15am.\nWeeks trigger interval\nWeeks Between Triggers: Enter the number of weeks between each workflow trigger.\nTrigger on Weekdays: Select the day(s) of the week you want to trigger the node.\nTrigger at Hour: Select the hour of the day to trigger the node.\nTrigger at Minute: Enter the minute past the hour to trigger the node when it runs, from 0 to 59.\nFor example, if you enter 2 Weeks Between Triggers, Monday for Trigger on Weekdays, 3pm for Trigger at Hour, and 30 Trigger at Minute, the node will run every two weeks on Monday at 3:30 PM.\nMonths trigger interval\nMonths Between Triggers: Enter the number of months between each workflow trigger.\nTrigger at Day of Month: Enter the day of the month the day should trigger at, from 1 to 31. If a month doesn't have this day, the node won't trigger. For example, if you enter 30 here, the node won't trigger in February.\nTrigger at Hour: Select the hour of the day to trigger the node.\nTrigger at Minute: Enter the minute past the hour to trigger the node when it runs, from 0 to 59.\nFor example, if you enter 3 Months Between Triggers, 28 Trigger at Day of Month, 9am for Trigger at Hour, and 0 Trigger at Minute, the node will run each quarter on the 28th day of the month at 9:00 AM.\nCustom (Cron) interval\nEnter a custom cron Expression to set the schedule for the trigger.\nTo generate a Cron expression, you can use crontab guru. Paste the Cron expression that you generated using crontab guru in the Expression field in n8n.\nExamples\nTABLE_PLACEHOLDER_0\nTemplates and examples\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.webhook\\common-issues.md",
    "content": "Common issues and questions\nHere are some common issues and questions for the Webhook node and suggested solutions.\nListen for multiple HTTP methods\nBy default, the Webhook node accepts calls that use a single method. For example, it can accept GET or POST requests, but not both. If you want to accept calls using multiple methods:\nOpen the node Settings.\nTurn on Allow Multiple HTTP Methods.\nReturn to Parameters. By default, the node now accepts GET and POST calls. You can add other methods in the HTTP Methods field.\nThe Webhook node has an output for each method, so you can perform different actions depending on the method.\nUse the HTTP Request node to trigger the Webhook node\nThe HTTP Request node makes HTTP requests to the URL you specify.\nCreate a new workflow.\nAdd the HTTP Request node to the workflow.\nSelect a method from the Request Method dropdown list. For example, if you select GET as the HTTP method in your Webhook node, select GET as the request method in the HTTP Request node.\nCopy the URL from the Webhook node, and paste it in the URL field in the HTTP Request node.\nIf using the test URL for the webhook node: execute the workflow with the Webhook node.\nExecute the HTTP Request node.\nUse curl to trigger the Webhook node\nYou can use curl to make HTTP requests that trigger the Webhook node.\nMake an HTTP request without any parameters:\nMake an HTTP request with a body parameter:\nMake an HTTP request with header parameter:\nMake an HTTP request to send a file:\nReplace /path/to/file with the path of the file you want to send.\nSend a response of type string\nBy default, the response format is JSON or an array. To send a response of type string:\nSelect Response Mode > When Last Node Finishes.\nSelect Response Data > First Entry JSON.\nSelect Add Option > Property Name.\nEnter the name of the property that contains the response. This defaults to data.\nConnect an Edit Fields node to the Webhook node.\nIn the Edit Fields node, select Add Value > String.\nEnter the name of the property in the Name field. The name should match the property name from step 4.\nEnter the string value in the Value field.\nToggle Keep Only Set to on (green).\nWhen you call the Webhook, it sends the string response from the Edit Fields node.\nTest URL versus Production URL\nn8n generates two Webhook URLs for each Webhook node: a Test URL and a Production URL.\nWhile building or testing a workflow, use the Test URL. Once you're ready to use your Webhook URL in production, use the Production URL.\nTABLE_PLACEHOLDER_0\nRefer to Workflow development for more information.\nIP addresses in whitelist are failing to connect\nIf you're unable to connect from IP addresses in your IP whitelist, check if you are running n8n behind a reverse proxy.\nIf so, set the N8N_PROXY_HOPS environment variable to the number of reverse-proxies n8n is running behind.\nOnly one webhook per path and method\nn8n only permits registering one webhook for each path and HTTP method combination (for example, a GET request for /my-request). This avoids ambiguity over which webhook should receive requests.\nIf you receive a message that the path and method you chose are already in use, you can either:\nDeactivate the workflow with the conflicting webhook.\nChange the webhook path and/or method for one of the conflicting webhooks."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.webhook\\index.md",
    "content": "Webhook node\nUse the Webhook node to create webhooks, which can receive data from apps and services when an event occurs. It's a trigger node, which means it can start an n8n workflow. This allows services to connect to n8n and run a workflow.\nYou can use the Webhook node as a trigger for a workflow when you want to receive data and run a workflow based on the data. The Webhook node also supports returning the data generated at the end of a workflow. This makes it useful for building a workflow to process data and return the results, like an API endpoint.\nThe webhook allows you to trigger workflows from services that don't have a dedicated app trigger node.\nWorkflow development process\nn8n provides different Webhook URLs for testing and production. The testing URL includes an option to Listen for test event. Refer to Workflow development for more information on building, testing, and shifting your Webhook node to production.\nNode parameters\nUse these parameters to configure your node.\nWebhook URLs\nThe Webhook node has two Webhook URLs: test and production. n8n displays the URLs at the top of the node panel.\nSelect Test URL or Production URL to toggle which URL n8n displays.\n!Sample Webhook URLs in the Webhook node's Parameters tab display a Test URL and Production URL\nSample Webhook URLs in the Webhook node's Parameters tab\nTest: n8n registers a test webhook when you select Listen for Test Event or Execute workflow, if the workflow isn't active. When you call the webhook URL, n8n displays the data in the workflow.\nProduction: n8n registers a production webhook when you activate the workflow. When using the production URL, n8n doesn't display the data in the workflow. You can still view workflow data for a production execution: select the Executions tab in the workflow, then select the workflow execution you want to view.\nHTTP Method\nThe Webhook node supports standard HTTP Request Methods:\nDELETE\nGET\nHEAD\nPATCH\nPOST\nPUT\nPath\nBy default, this field contains a randomly generated webhook URL path, to avoid conflicts with other webhook nodes.\nYou can manually specify a URL path, including adding route parameters. For example, you may need to do this if you use n8n to prototype an API and want consistent endpoint URLs.\nThe Path field can take the following formats:\n/:variable\n/path/:variable\n/:variable/path\n/:variable1/path/:variable2\n/:variable1/:variable2\nSupported authentication methods\nYou can require authentication for any service calling your webhook URL. Choose from these authentication methods:\nBasic auth\nHeader auth\nJWT auth\nNone\nRefer to Webhook credentials for more information on setting up each credential type.\nRespond\nImmediately: The Webhook node returns the response code and the message Workflow got started.\nWhen Last Node Finishes: The Webhook node returns the response code and the data output from the last node executed in the workflow.\nUsing 'Respond to Webhook' Node: The Webhook node responds as defined in the Respond to Webhook node.\nResponse Code\nCustomize the HTTP response code that the Webhook node returns upon successful execution. Select from common response codes or create a custom code.\nResponse Data\nChoose what data to include in the response body:\nAll Entries: The Webhook returns all the entries of the last node in an array.\nFirst Entry JSON: The Webhook returns the JSON data of the first entry of the last node in a JSON object.\nFirst Entry Binary: The Webhook returns the binary data of the first entry of the last node in a binary file.\nNo Response Body: The Webhook returns without a body.\nApplies only to Respond > When Last Node Finishes.\nNode options\nSelect Add Option to view more configuration options. The available options depend on your node parameters. Refer to the table for option availability.\nAllowed Origins (CORS): Set the permitted cross-origin domains. Enter a comma-separated list of URLs allowed for cross-origin non-preflight requests. Use * (default) to allow all origins.\nBinary Property: Enabling this setting allows the Webhook node to receive binary data, such as an image or audio file. Enter the name of the binary property to write the data of the received file to.\nIgnore Bots: Ignore requests from bots like link previewers and web crawlers.\nIP(s) Whitelist: Enable this to limit who (or what) can invoke a Webhook trigger URL. Enter a comma-separated list of allowed IP addresses. Access from IP addresses outside the whitelist throws a 403 error. If left blank, all IP addresses can invoke the webhook trigger URL.\nNo Response Body: Enable this to prevent n8n sending a body with the response.\nRaw Body: Specify that the Webhook node will receive data in a raw format, such as JSON or XML.\nResponse Content-Type: Choose the format for the webhook body.\nResponse Data: Send custom data with the response.\nResponse Headers: Send extra headers in the Webhook response. Refer to MDN Web Docs TABLE_PLACEHOLDER_0\nHow n8n secures HTML responses\nStarting with n8n version 1.103.0, n8n automatically wraps HTML responses to webhooks in  tags. This is a security mechanism to protect the instance users.\nThis has the following implications:\nHTML renders in a sandboxed iframe instead of directly in the parent document.\nJavaScript code that attempts to access the top-level window or local storage will fail.\nAuthentication headers aren't available in the sandboxed iframe (for example, basic auth). You need to use an alternative approach, like embedding a short-lived access token within the HTML.\nRelative URLs (for example, ) won't work. Use absolute URLs instead.\nTemplates and examples\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-base.webhook\\workflow-development.md",
    "content": "Workflow development\nThe Webhook node works a bit differently from other core nodes. n8n recommends following these processes for building, testing, and using your Webhook node in production.\nn8n generates two Webhook URLs for each Webhook node: a Test URL and a Production URL.\nBuild and test workflows\nWhile building or testing a workflow, use the Test webhook URL.\nUsing a test webhook ensures that you can view the incoming data in the editor UI, which is useful for debugging. Select Listen for test event to register the webhook before sending the data to the test webhook. The test webhook stays active for 120 seconds.\nWhen using the Webhook node on localhost on a self-hosted n8n instance, run n8n in tunnel mode:\nnpm with tunnel\nDocker with tunnel\nProduction workflows\nWhen your workflow is ready, switch to using the Production webhook URL. You can then activate your workflow, and n8n runs it automatically when an external service calls the webhook URL.\nWhen working with a Production webhook, ensure that you have saved and activated the workflow. Data flowing through the webhook isn't visible in the editor UI with the production webhook.\nRefer to Create a workflow for more information on activating workflows."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-langchain.chattrigger\\common-issues.md",
    "content": "Chat Trigger node common issues\nHere are some common errors and issues with the Chat Trigger node and steps to resolve or troubleshoot them.\nPass data from a website to an embedded Chat Trigger node\nWhen embedding the Chat Trigger node in a website, you might want to pass extra information to the Chat Trigger. For example, passing a user ID stored in a site cookie.\nTo do this, use the metadata field in the JSON object you pass to the createChat function in your embedded chat window:\nThe metadata field can contain arbitrary data that will appear in the Chat Trigger output alongside other output data. From there, you can query and process the data from downstream nodes as usual using  n8n's data processing features.\nChat Trigger node doesn't fetch previous messages\nWhen you configure a Chat Trigger node, you might experience problems fetching previous messages if you aren't careful about how you configure session loading. This often manifests as a workflow could not be started! error.\nIn Chat Triggers, the Load Previous Session option retrieves previous chat messages for a session using the sessionID. When you set the Load Previous Session option to From memory, it's almost always best to connect the same memory node to both the Chat Trigger and the Agent in your workflow:\nIn your Chat Trigger node, set the Load Previous Session option to From Memory. This is only visible if you've made the chat publicly available.\nAttach a Simple Memory node to the Memory connector.\nAttach the same Simple Memory node to Memory connector of your Agent.\nIn the Simple Memory node, set Session ID to Connected Chat Trigger Node.\nOne instance where you may want to attach separate memory nodes to your Chat Trigger and the Agent is if you want to set the Session ID in your memory node to Define below.\nIf you're retrieving the session ID from an expression, the same expression must work for each of the nodes attached to it. If the expression isn't compatible with each of the nodes that need memory, you might need to use separate memory nodes so you can customize the expression for the session ID on a per-node basis."
  },
  {
    "file_path": "integrations\\builtin\\core-nodes\\n8n-nodes-langchain.chattrigger\\index.md",
    "content": "Chat Trigger node\nUse the Chat Trigger node when building AI workflows for chatbots and other chat interfaces. You can configure how users access the chat, using one of n8n's provided interfaces, or your own. You can add authentication.\nYou must connect either an agent or chain root node.\nNode parameters\nMake Chat Publicly Available\nSet whether the chat should be publicly available (turned on) or only available through the manual chat interface (turned off).\nLeave this turned off while you're building the workflow. Turn it on when you're ready to activate the workflow and allow users to access the chat.\nMode\nChoose how users access the chat. Select from:\nHosted Chat: Use n8n's hosted chat interface. Recommended for most users because you can configure the interface using the node options and don't have to do any other setup.\nEmbedded Chat: This option requires you to create your own chat interface. You can use n8n's chat widget or build your own. Your chat interface must call the webhook URL shown in Chat URL in the node.\nAuthentication\nChoose whether and how to restrict access to the chat. Select from:\nNone: The chat doesn't use authentication. Anyone can use the chat.\nBasic Auth: The chat uses basic authentication.\nSelect or create a Credential for Basic Auth with a username and password. All users must use the same username and password.\nn8n User Auth: Only users logged in to an n8n account can use the chat.\nInitial Message(s)\nThis parameter's only available if you're using Hosted Chat. Use it to configure the message the n8n chat interface displays when the user arrives on the page.\nNode options\nAvailable options depend on the chat mode.\nHosted chat options\nAllowed Origin (CORS)\nSet the origins that can access the chat URL. Enter a comma-separated list of URLs allowed for cross-origin non-preflight requests.\nUse * (default) to allow all origins.\nInput Placeholder, Title, and Subtitle\nEnter the text for these elements in the chat interface.\n??? Details \"View screenshot\"\n!Customizable text elements\nLoad Previous Session\nSelect whether to load chat messages from a previous chat session.\nIf you select any option other than Off, you must connect the Chat trigger and the Agent you're using to a memory sub-node. The memory connector on the Chat trigger appears when you set Load Previous Session to From Memory. n8n recommends connecting both the Chat trigger and Agent to the same memory sub-node, as this ensures a single source of truth for both nodes.\n??? Details \"View screenshot\"\n!Connect nodes to memory\nResponse Mode\nUse this option when building a workflow with steps after the agent or chain that's handling the chat. Choose from:\nWhen Last Node Finishes: The Chat Trigger node returns the response code and the data output from the last node executed in the workflow.\nUsing Response Nodes: The Chat Trigger node responds as defined in a Respond to Chat node or Respond to Webhook node. In this response mode, the Chat Trigger will solely show messages as defined in these nodes and not output the data from the last node executed in the workflow.\nRequire Button Click to Start Chat\nSet whether to display a New Conversation button on the chat interface (turned on) or not (turned off).\n??? Details \"View screenshot\"\n!New Conversation button\nEmbedded chat options\nAllowed Origin (CORS)\nSet the origins that can access the chat URL. Enter a comma-separated list of URLs allowed for cross-origin non-preflight requests.\nUse * (default) to allow all origins.\nLoad Previous Session\nSelect whether to load chat messages from a previous chat session.\nIf you select any option other than Off, you must connect the Chat trigger and the Agent you're using to a memory sub-node. The memory connector on the Chat trigger appears when you set Load Previous Session to From Memory. n8n recommends connecting both the Chat trigger and Agent to the same memory sub-node, as this ensures a single source of truth for both nodes.\n??? Details \"View screenshot\"\n!Connect nodes to memory\nResponse Mode\nUse this option when building a workflow with steps after the agent or chain that's handling the chat. Choose from:\nWhen Last Node Finishes: The Chat Trigger node returns the response code and the data output from the last node executed in the workflow.\nUsing Response Nodes: The Chat Trigger node responds as defined in a Respond to Chat node or Respond to Webhook node. In this response mode, the Chat Trigger will solely show messages as defined in these nodes and not output the data from the last node executed in the workflow.\nTemplates and examples\nRelated resources\nSet the chat response manually\nYou need to manually set the chat response when you don't want to directly send the output of an Agent or Chain node to the user. Instead, you want to take the output of an Agent or Chain node and modify it or do something else with it before sending it back to the user.\nIn a basic workflow, the Agent and Chain nodes output a parameter named either output or text, and the Chat trigger sends the value of this parameter to the user as the chat response.\nIf you need to manually create the response sent to the user, you must create a parameter named either text or output. If you use a different parameter name, the Chat trigger sends the entire object as its response, not just the value.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common Issues."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\actionnetwork.md",
    "content": "Action Network credentials\nYou can use these credentials to authenticate the following nodes:\nAction Network\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Action Network's API documentation for more information about working with the service.\nUsing API key\nTo configure this credential, you'll need an Action Network account with API key access enabled and:\nAn API Key\nTo get an API key:\nLog in to your Action Network account.\nFrom the Start Organizing menu, select Details > API & Sync.\nSelect the list you want to generate an API key for.\nGenerate an API key for that list.\nCopy the API Key and enter it in your n8n credential.\nRefer to the Action Network API Authentication instructions for more information.\nRequest API access\nEach user account and group on the Action Network has a separate API key to access that user or group's data.\nYou must explicitly request API access from Action Network, which you can do in one of two ways:\nIf you're already a paying customer, contact them to request partner access. Partner access includes API key access.\nIf you're a developer, request a developer account. Once your account request is granted, you'll have API key access."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\activecampaign.md",
    "content": "ActiveCampaign credentials\nYou can use these credentials to authenticate the following nodes:\nActiveCampaign\nActive Campaign Trigger\nSupported authentication methods\nAPI key\nRelated resources\nRefer to ActiveCampaign's API documentation for more information about working with the service.\nUsing API key\nTo configure this credential, you'll need an ActiveCampaign account and:\nAn API URL\nAn API Key\nTo get both and set up the credential:\nIn ActiveCampaign, select Settings (the gear cog icon) from the left menu.\nSelect Developer.\nCopy the API URL and enter it in your n8n credential.\nCopy the API Key and enter it in your n8n credential.\nRefer to How to obtain your ActiveCampaign API URL and Key for more information or for instructions on resetting your API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\acuityscheduling.md",
    "content": "Acuity Scheduling credentials\nYou can use these credentials to authenticate the following nodes:\nAcuity Scheduling Trigger\nPrerequisites\nCreate an Acuity Scheduling account.\nSupported authentication methods\nAPI key\nOAuth2\nRelated resources\nRefer to Acuity's API documentation for more information about working with the service.\nUsing API key\nTo configure this credential, you'll need:\nA numeric User ID\nAn API Key\nRefer to the Acuity API Quick Start authentication instructions to generate an API key and view your User ID.\nUsing OAuth2\nIf you need to set this up from scratch, complete the Acuity OAuth2 Account Registration page. Use the Client ID and Client Secret provided from that registration."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\adalo.md",
    "content": "Adalo credentials\nYou can use these credentials to authenticate the following nodes:\nAdalo\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Adalo's API collections documentation for more information about working with the service.\nUsing API key\nTo configure this credential, you'll need an Adalo account and:\nAn API Key\nAn App ID\nTo get these, create an Adalo app:\nFrom the app dropdown in the top navigation, select CREATE NEW APP.\nSelect the App Layout type that makes sense for you and select Next.\nIf you're new to using the product, Adalo recommend using Mobile Only.\nSelect a template to get started with or select Blank, then select Next.\nEnter an App Name, like n8n integration.\nIf applicable, select the Team for the app.\nSelect branding colors.\nSelect Create. The app editor opens.\nIn the left menu, select Settings (the gear cog icon).\nSelect App Access.\nIn the API Key section, select Generate Key.\nIf you don't have the correct plan level, you'll see a prompt to upgrade instead.\nCopy the key and enter it as the API Key in your n8n credential.\nThe URL includes the App ID after  For example, if the URL for your app is  b78bdfcf-48dc-4550-a474-dd52c19fc371 is the App ID. Copy this value and enter it in your n8n credential.\nRefer to Creating an app for more information on creating apps in Adalo. Refer to The Adalo API for more information on generating API keys."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\affinity.md",
    "content": "Affinity credentials\nYou can use these credentials to authenticate the following nodes:\nAffinity\nAffinity Trigger\nPrerequisites\nCreate an Affinity account at the Scale, Advanced, or Enterprise subscription tiers.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Affinity's API documentation for more information about working with the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to How to obtain your Affinity API key documentation to get your API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\agilecrm.md",
    "content": "Agile CRM credentials\nYou can use these credentials to authenticate the following nodes:\nAgile CRM\nPrerequisites\nCreate an Agile CRM account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Agile CRM's API documentation for more information about working with the service.\nUsing API key\nTo configure this credential, you'll need:\nAn Email Address registered with AgileCRM\nA REST API Key: Access your Agile CRM API key through Admin Settings > Developers & API > REST API key.\nAn Agile CRM Subdomain (for example, n8n)"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\airtable.md",
    "content": "Airtable credentials\nYou can use these credentials to authenticate the following nodes:\nAirtable\nAirtable Trigger\nPrerequisites\nCreate an Airtable account.\nSupported authentication methods\nPersonal Access Token (PAT)\nOAuth2\nRelated resources\nRefer to Airtable's API documentation for more information about the service.\nUsing personal access token\nTo configure this credential, you'll need:\nA Personal Access Token (PAT)\nTo create your PAT:\nGo to the Airtable Builder Hub Personal access tokens page.\nSelect + Create new token. Airtable opens the Create personal access token page.\nEnter a Name for your token, like n8n credential.\nAdd Scopes to your token. Refer to Airtable's Scopes guide for more information. n8n recommends using these scopes:\ndata.records:read\ndata.records:write\nschema.bases:read\nSelect the Access for your token. Choose from a single base, multiple bases (even bases from different workspaces), all of the current and future bases in a workspace you own, or all of the bases from any workspace that you own including bases/workspace added in the future.\nSelect Create token.\nAirtable opens a modal with your token displayed. Copy this token and enter it in your n8n credential as the Access Token.\nRefer to Airtable's Find/create PATs documentation for more information.\nUsing OAuth2\nTo configure this credential, you'll need:\nAn OAuth Redirect URL\nA Client ID\nA Client Secret\nTo generate all this information, register a new Airtable integration:\nOpen your Airtable Builder Hub OAuth integrations page.\nSelect the Register new OAuth integration button.\nEnter a name for your OAuth integration.\nCopy the OAuth Redirect URL from your n8n credential.\nPaste that redirect URL in Airtable as the OAuth redirect URL.\nSelect Register integration.\nOn the following page, copy the Client ID from Airtable and paste it into the Client ID in your n8n credential.\nIn Airtable, select Generate client secret.\nCopy the client secret and paste it into the Client Secret in your n8n credential.\nSelect the following scopes in Airtable:\ndata.records:read\ndata.records:write\nschema.bases:read\nSelect Save changes in Airtable.\nIn your n8n credential, select the Connect my account. A Grant access modal opens.\nFollow the instructions and select the base you want to work on (or all bases).\nSelect Grant access to complete the connection.\nRefer to the Airtable Register a new integration documentation for steps on registering a new Oauth integration."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\airtop.md",
    "content": "Airtop credentials\nYou can use these credentials to authenticate the following nodes:\nAirtop\nPrerequisites\nCreate an Airtop account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Airtop's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need an Airtop account and an API key. To generate a new key:\nLog in to the Airtop Portal.\nGo to API Keys.\nSelect the + Create new key button.\nEnter a name for the API key.\nSelect the generated key to copy the key.\nEnter this as the API Key in your n8n credential.\nRefer to Airtop's Support for assistance if you have any issues creating your API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\alienvault.md",
    "content": "AlienVault credentials\nPrerequisites\nCreate an AlienVault account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to AlienVault's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn OTX Key: Once you have an AlienVault account, the OTX Key displays in your Settings."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\amqp.md",
    "content": "AMQP credentials\nYou can use these credentials to authenticate the following nodes:\nAMQP Sender\nAMQP Trigger\nPrerequisites\nInstall an AMQP 1.0-compatible message broker like ActiveMQ. Refer to AMQP Products for a list of options.\nSupported authentication methods\nAMQP connection\nRelated resources\nAdvanced Message Queuing Protocol (AMQP) is an open standard application layer protocol for message-oriented middleware. The defining features of AMQP are message orientation, queuing, routing, reliability and security. Refer to the OASIS AMQP Version 1.0 Standard for more information.\nRefer to your provider's documentation for more information about the service. Refer to ActiveMQ's API documentation as one example.\nUsing AMQP connection\nTo configure this credential, you'll need:\nA Hostname: Enter the hostname of your AMQP message broker.\nA Port: Enter the port number the connection should use.\nA User: Enter the name of the user to establish the connection as.\nFor example, the default username in ActiveMQ is admin.\nA Password: Enter the user's password.\nFor example, the default password in ActiveMQ is admin.\nOptional: Transport Type: Enter either tcp or tls.\nRefer to your provider's documentation for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\anthropic.md",
    "content": "Anthropic credentials\nYou can use these credentials to authenticate the following nodes:\nAnthropic\nAnthropic Chat Model\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Anthropic's documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need an Anthropic Console account with access to Claude.\nThen:\nIn the Anthropic Console, open Settings > API Keys.\nSelect + Create Key.\nGive your key a Name, like n8n-integration.\nSelect Copy Key to copy the key.\nEnter this as the API Key in your n8n credential.\nRefer to Anthropic's Intro to Claude and Quickstart for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\apitemplateio.md",
    "content": "APITemplate.io credentials\nYou can use these credentials to authenticate the following nodes:\nAPITemplate.io\nPrerequisites\nCreate an APITemplate.io account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to APITemplate.io's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Once you've created an APITemplate.io account, go to API Integration to copy the API Key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\asana.md",
    "content": "Asana credentials\nYou can use these credentials to authenticate the following nodes:\nAsana\nAsana Trigger\nSupported authentication methods\nAccess token\nOAuth2\nRelated resources\nRefer to Asana's Developer Guides for more information about working with the service.\nUsing Access token\nTo configure this credential, you'll need an Asana account and:\nA Personal Access Token (PAT)\nTo get your PAT:\nOpen the Asana developer console.\nIn the Personal access tokens section, select Create new token.\nEnter a Token name, like n8n integration.\nCheck the box to agree to the Asana API terms.\nSelect Create token.\nCopy the token and enter it as the Access Token in your n8n credential.\nRefer to the Asana Quick start guide for more information.\nUsing OAuth2\nTo configure this credential, you'll need an Asana account.\nIf you're self-hosting n8n, you'll need to register an application to set up OAuth:\nOpen the Asana developer console.\nIn the My apps section, select Create new app.\nEnter an App name for your application, like n8n integration.\nSelect a purpose for your app.\nCheck the box to agree to the Asana API terms.\nSelect Create app. The page opens to the app's Basic Information.\nSelect OAuth from the left menu.\nIn n8n, copy the OAuth Redirect URL.\nIn Asana, select Add redirect URL and enter the URL you copied from n8n.\nCopy the Client ID from Asana and enter it in your n8n credential.\nCopy the Client Secret from Asana and enter it in your n8n credential.\nRefer to the Asana OAuth register an application documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\auth0management.md",
    "content": "Auth0 Management credentials\nPrerequisites\nCreate an Auth0 account.\nSupported authentication methods\nAPI client secret\nRelated resources\nRefer to Auth0 Management's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API client secret\nTo configure this credential, you'll need:\nAn Auth0 Domain\nA Client ID\nA Client Secret\nRefer to the Auth0 Management API Get Access Tokens documentation for instructions on obtaining the Client ID and Client Secret from the application's Settings tab."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\automizy.md",
    "content": "Automizy credentials\nYou can use these credentials to authenticate the following nodes:\nAutomizy\nPrerequisites\nCreate an Automizy account.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Automizy's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API Token: Create new or access existing API tokens from your Automizy dashboard > Settings > API Token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\autopilot.md",
    "content": "Autopilot credentials\nYou can use these credentials to authenticate the following nodes:\nAutopilot\nAutopilot Trigger\nPrerequisites\nCreate an Autopilot account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Autopilot's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate an API key in Settings > Autopilot API. Refer to Autopilot API authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\aws.md",
    "content": "AWS credentials\nYou can use these credentials to authenticate the following nodes:\nAWS Bedrock Chat Model\nAWS Certificate Manager\nAWS Cognito\nAWS Comprehend\nAWS DynamoDB\nAWS Elastic Load Balancing\nAWS Lambda\nAWS Rekognition\nAWS S3\nAWS SES\nAWS SNS\nAWS SNS Trigger\nAWS SQS\nAWS Textract\nAWS Transcribe\nEmbeddings AWS Bedrock\nSupported authentication methods\nAPI access key\nRelated resources\nRefer to AWS's Identity and Access Management documentation for more information about the service.\nUsing API access key\nTo configure this credential, you'll need an AWS account and:\nYour AWS Region\nThe Access Key ID: Generated when you create an access key.\nThe Secret Access Key: Generated when you create an access key.\nTo create an access key and set up the credential:\nIn your n8n credential, select your AWS Region.\nLog in to the IAM console.\nIn the navigation bar on the upper right, select your user name and then select Security credentials.\nIn the Access keys section, select Create access key.\nOn the Access key best practices & alternatives page, choose your use case. If it doesn't prompt you to create an access key, select Other.\nSelect Next.\nSet a description tag value for the access key to make it easier to identify, for example n8n integration.\nSelect Create access key.\nReveal the Access Key ID and Secret Access Key and enter them in n8n.\nTo use a Temporary security credential, turn that option on and add a Session token. Refer to the AWS Temporary security credential documentation for more information on working with temporary security credentials.\nIf you use Amazon Virtual Private Cloud (VPC) to host n8n, you can establish a connection between your VPC and some apps. Use Custom Endpoints to enter relevant custom endpoint(s) for this connection. This setup works with these apps:\nRekognition\nLambda\nSNS\nSES\nSQS\nS3\nYou can also generate access keys through the AWS CLI and AWS API. Refer to the AWS Managing Access Keys documentation for instructions on generating access keys using these methods."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\azurecosmosdb.md",
    "content": "Azure Cosmos DB credentials\nYou can use these credentials to authenticate the following nodes:\nAzure Cosmos DB\nPrerequisites\nCreate an Azure subscription.\nCreate an Azure Cosmos DB account.\nSupported authentication methods\nAPI Key\nRelated resources\nRefer to Azure Cosmos DB's API documentation for more information about the service.\nUsing API Key\nTo configure this credential, you'll need:\nAn Account: The name of your Azure Cosmos DB account.\nA Key: A key for your Azure Cosmos DB account. Select Overview > Keys in the Azure portal for your Azure Cosmos DB. You can use either of the two account keys for this purpose.\nA Database: The name of the Azure Cosmos DB database to connect to.\nRefer to Get your primary key | Microsoft for more detailed steps.\nCommon issues\nHere are the known common errors and issues with Azure Cosmos DB credentials."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\azureopenai.md",
    "content": "Azure OpenAI credentials\nYou can use these credentials to authenticate the following nodes:\nChat Azure OpenAI\nEmbeddings Azure OpenAI\nPrerequisites\nCreate an Azure subscription.\nAccess to Azure OpenAI within that subscription. You may need to request access if your organization doesn't yet have it.\nSupported authentication methods\nAPI key\nAzure Entra ID (OAuth2)\nRelated resources\nRefer to Azure OpenAI's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Resource Name: the Name you give the resource\nAn API key: Key 1 works well. This can be accessed before deployment in Keys and Endpoint.\nThe API Version the credentials should use. See the Azure OpenAI API preview lifecycle documentation for more information about API versioning in Azure OpenAI.\nTo get the information above, create and deploy an Azure OpenAI Service resource.\nUsing Azure Entra ID (OAuth2)\nFor self-hosted users, there are two main steps to configure OAuth2 from scratch:\nRegister an application with the Microsoft Identity Platform.\nGenerate a client secret for that application.\nFollow the detailed instructions for each step below. For more detail on the Microsoft OAuth2 web flow, refer to Microsoft authentication and authorization basics.\nRegister an application\nRegister an application with the Microsoft Identity Platform:\nOpen the Microsoft Application Registration Portal.\nSelect Register an application.\nEnter a Name for your app.\nIn Supported account types, select Accounts in any organizational directory (Any Azure AD directory - Multi-tenant) and personal Microsoft accounts (for example, Skype, Xbox).\nIn Register an application:\nCopy the OAuth Callback URL from your n8n credential.\nPaste it into the Redirect URI (optional) field.\nSelect Select a platform > Web.\nSelect Register to finish creating your application.\nCopy the Application (client) ID and paste it into n8n as the Client ID.\nRefer to Register an application with the Microsoft Identity Platform for more information.\nGenerate a client secret\nWith your application created, generate a client secret for it:\nOn your Microsoft application page, select Certificates & secrets in the left navigation.\nIn Client secrets, select + New client secret.\nEnter a Description for your client secret, such as n8n credential.\nSelect Add.\nCopy the Secret in the Value column.\nPaste it into n8n as the Client Secret.\nSelect Connect my account in n8n to finish setting up the connection.\nLog in to your Microsoft account and allow the app to access your info.\nRefer to Microsoft's Add credentials for more information on adding a client secret.\nSetting custom scopes\nAzure Entra ID credentials use the following scopes by default:\nopenid\noffline_access\nAccessReview.ReadWrite.All\nDirectory.ReadWrite.All\nNetworkAccessPolicy.ReadWrite.All\nDelegatedAdminRelationship.ReadWrite.All\nEntitlementManagement.ReadWrite.All\nUser.ReadWrite.All\nDirectory.AccessAsUser.All\nSites.FullControl.All\nGroupMember.ReadWrite.All\nTo select different scopes for your credentials, enable the Custom Scopes slider and edit the Enabled Scopes list. Keep in mind that some features may not work as expected with more restrictive scopes."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\azurestorage.md",
    "content": "Azure Storage credentials\nYou can use these credentials to authenticate the following nodes:\nAzure Storage\nPrerequisites\nCreate an Azure subscription.\nCreate an Azure storage account.\nSupported authentication methods\nOAuth2\nShared Key\nRelated resources\nRefer to Azure Storage's API documentation for more information about the service.\nUsing OAuth2\nFor self-hosted users, there are two main steps to configure OAuth2 from scratch:\nRegister an application with the Microsoft Identity Platform.\nGenerate a client secret for that application.\nFollow the detailed instructions for each step below. For more detail on the Microsoft OAuth2 web flow, refer to Microsoft authentication and authorization basics.\nRegister an application\nRegister an application with the Microsoft Identity Platform:\nOpen the Microsoft Application Registration Portal.\nSelect Register an application.\nEnter a Name for your app.\nIn Supported account types, select Accounts in any organizational directory (Any Azure AD directory - Multi-tenant) and personal Microsoft accounts (for example, Skype, Xbox).\nIn Register an application:\nCopy the OAuth Callback URL from your n8n credential.\nPaste it into the Redirect URI (optional) field.\nSelect Select a platform > Web.\nSelect Register to finish creating your application.\nCopy the Application (client) ID and paste it into n8n as the Client ID.\nRefer to Register an application with the Microsoft Identity Platform for more information.\nGenerate a client secret\nWith your application created, generate a client secret for it:\nOn your Microsoft application page, select Certificates & secrets in the left navigation.\nIn Client secrets, select + New client secret.\nEnter a Description for your client secret, such as n8n credential.\nSelect Add.\nCopy the Secret in the Value column.\nPaste it into n8n as the Client Secret.\nSelect Connect my account in n8n to finish setting up the connection.\nLog in to your Microsoft account and allow the app to access your info.\nRefer to Microsoft's Add credentials for more information on adding a client secret.\nUsing Shared Key\nTo configure this credential, you'll need:\nAn Account: The name of your Azure Storage account.\nA Key: A shared key for your Azure Storage account. Select Security + networking and then Access keys. You can use either of the two account keys for this purpose.\nRefer to Manage storage account access keys | Microsoft for more detailed steps.\nCommon issues\nHere are the known common errors and issues with Azure Storage credentials."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\bamboohr.md",
    "content": "BambooHR credentials\nYou can use these credentials to authenticate the following node:\nBambooHR\nPrerequisites\nCreate a BambooHR account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to BambooHR's API documentation for more information about the service.\nUsing API Key\nTo configure this credential, you'll need:\nYour BambooHR Subdomain: the part between https:// and .bamboohr.com\nA BambooHR API Key: Refer to the Authentication section of BambooHR's Getting Started API documentation for instructions on generating an API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\bannerbear.md",
    "content": "Bannerbear credentials\nYou can use these credentials to authenticate the following nodes:\nBannerbear\nPrerequisites\nCreate a Bannerbear account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Bannerbear's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Project API Key: To generate an API key, first create a Bannerbear project. Go to Settings > API Key to view the API key. Refer to the Bannerbear API Authentication documentation for more detailed steps."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\baserow.md",
    "content": "Baserow credentials\nYou can use these credentials to authenticate the following node:\nBaserow\nPrerequisites\nCreate a Baserow account on any hosted Baserow instance or a self-hosted instance.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Baserow's documentation for more information about the service.\nRefer to Baserow's auto-generated API documentation for more information about the API specifically.\nUsing basic auth\nTo configure this credential, you'll need:\nYour Baserow Host\nA Username and Password to log in with\nFollow these steps:\nEnter the Host for the Baserow instance:\nFor a Baserow-hosted instance: leave as\nFor a self-hosted instance: set to your self-hosted instance API URL.\nEnter the Username for the user account n8n should use.\nEnter the Password for that user account.\nRefer to Baserow's API Authentication documentation for information on creating user accounts."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\beeminder.md",
    "content": "Beeminder credentials\nYou can use these credentials to authenticate the following node:\nBeeminder\nPrerequisites\nCreate a Beeminder account.\nSupported authentication methods\nAPI user token\nRelated resources\nRefer to Beeminder's API documentation for more information about the service.\nUsing API user token\nTo configure this credential, you'll need:\nA User name: Should match the user who the Auth Token is generated for.\nA personal Auth Token for that user. Generate this using either method below:\nIn the GUI: From the Apps & API option within Account Settings\nIn the API: From hitting the auth_token API endpoint"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\bitbucket.md",
    "content": "Bitbucket credentials\nYou can use these credentials to authenticate the following nodes:\nBitbucket Trigger\nPrerequisites\nCreate a Bitbucket account.\nSupported authentication methods\nAPI username and app password\nRelated resources\nRefer to Bitbucket's API documentation for more information about the service.\nUsing API username/app password\nTo configure this credential, you'll need:\nA Username: Visible in your Bitbucket profile settings Personal settings > Account settings.\nAn App Password: Refer to the Bitbucket instructions to Create an app password.\nApp password permissions\nBitbucket API credentials will only work if the user account you generated the app password for has the appropriate privilege scopes for the selected app password permissions. The n8n credentials dialog will throw an error if the user account lacks the appropriate permissions for the selected scope, like Your credentials lack one or more required privilege scopes.\nSee the Bitbucket App password permissions documentation for more information on working with these permissions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\bitly.md",
    "content": "Bitly credentials\nYou can use these credentials to authenticate the following node:\nBitly\nPrerequisites\nCreate a Bitly account.\nSupported authentication methods\nAPI token\nOAuth2\nRelated resources\nRefer to Bitly's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn Access Token: Once logged in, visit Settings > Developer Settings > API to generate an Access Token.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch or need more detail on what's happening in the OAuth web flow, refer to the Bitly API Authentication documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\bitwarden.md",
    "content": "Bitwarden credentials\nYou can use these credentials to authenticate the following node:\nBitwarden\nPrerequisites\nCreate a Bitwarden Teams organization or Enterprise organization account. (Bitwarden only makes the Bitwarden Public API available for these organization plans.)\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Bitwarden's Public API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Client ID: Provided when you generate an API key\nA Client Secret: Provided when you generate an API key\nThe Environment:\nChoose Cloud-hosted if you don't self-host Bitwarden. No further configuration required.\nChoose Self-hosted if you host Bitwarden on your own server. Enter your Self-hosted domain in the appropriate field.\nThe Client ID and Client Secret must be for an Organization API Key, not a Personal API Key. Refer to the Bitwarden Public API Authentication documentation for instructions on generating an Organization API Key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\box.md",
    "content": "Box credentials\nYou can use these credentials to authenticate the following nodes:\nBox\nBox Trigger\nPrerequisites\nCreate a Box account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Box's API documentation for more information about the service.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch or need more detail on what's happening in the OAuth web flow, you'll need to create a Custom App. Refer to the Box OAuth2 Setup documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\brandfetch.md",
    "content": "Brandfetch credentials\nYou can use these credentials to authenticate the following node:\nBrandfetch\nPrerequisites\nCreate a Brandfetch developer developer account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Brandfetch's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Brandfetch Create an Account documentation to generate an API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\brevo.md",
    "content": "Brevo credentials\nYou can use these credentials to authenticate the following nodes:\nBrevo node\nBrevo Trigger node\nPrerequisites\nCreate a Brevo developer account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Brevo's API documentation for more information about authenticating with the service.\nAPI key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Brevo API Quick Start documentation for instructions on creating a new API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\bubble.md",
    "content": "Bubble credentials\nYou can use these credentials to authenticate the following nodes:\nBubble\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Bubble's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a paid Bubble account and:\nAn API Token\nAn App Name\nYour Domain, if you're using a custom domain\nTo set it up, you'll need to create an app:\nGo to the Apps page in Bubble.\nSelect Create an app.\nEnter a Name for your app, like n8n-integration.\nSelect Get started. The app's details open.\nIn the left navigation, select Settings (the gear cog icon).\nSelect the API tab.\nIn the Public API Endpoints section, check the box to Enable Data API.\nThe page displays the Data API root URL, for example:\nCopy the part of the URL after https:// and before .bubbleapps.io and enter it in n8n as the App Name. In the above example, you'd enter n8n-integration.\nSelect Generate a new API token.\nEnter an API Token Label, like n8n integration.\nCopy the Private key and enter it as the API Token in your n8n credential.\nRefer to Data API | Authentication for more information on generating API tokens.\nIn n8n, select the Environment that best matches your app:\nSelect Development for an app that you haven't deployed, accessed at  or\nSelect Live for an app that you've deployed, accessed at  or\nIn n8n, select your Hosting:\nIf you haven't set up a custom domain, select Bubble Hosting.\nIf you've set up a custom domain, select Self Hosted and enter your custom Domain.\nRefer to Bubble's Creating and managing apps documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\cal.md",
    "content": "Cal.com credentials\nYou can use these credentials to authenticate the following nodes:\nCal.com Trigger\nPrerequisites\nCreate a Cal.com account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Cal.com's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Cal API Quick Start documentation for information on how to generate a new API key.\nA Host: If you're using the cloud version of Cal.com, leave the Host as  If you're self-hosting Cal.com, enter the Host for your Cal.com instance."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\calendly.md",
    "content": "Calendly credentials\nYou can use these credentials to authenticate the following nodes:\nCalendly Trigger\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to Calendly's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need a Calendly account and:\nAn API Key or Personal Access Token\nTo get your access token:\nGo to the Calendly Integrations & apps page.\nSelect API & Webhooks.\nIn Your Personal Access Tokens, select Generate new token.\nEnter a Name for your access token, like n8n integration.\nSelect Create token.\nSelect Copy token and enter it in your n8n credential.\nRefer to Calendly's API authentication documentation for more information.\nUsing OAuth2\nTo configure this credential, you'll need a Calendly developer account and:\nA Client ID\nA Client Secret\nTo get both, create a new OAuth app in Calendly:\nLog in to Calendly's developer portal and go to My apps.\nSelect Create new app.\nEnter a Name of app, like n8n integration.\nIn Kind of app, select Web.\nIn Environment type, select the environment that corresponds to your usage, either Sandbox or Production.\nCalendly recommends starting with Sandbox for development and creating a second application for Production when you're ready to go live.\nCopy the OAuth Redirect URL from n8n and enter it as a Redirect URI in the OAuth app.\nSelect Save & Continue. The app details display.\nCopy the Client ID and enter this as your n8n Client ID.\nCopy the Client secret and enter this as your n8n Client Secret.\nSelect Connect my account in n8n and follow the on-screen prompts to finish authorizing the credential.\nRefer to Registering your application with Calendly for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\carbonblack.md",
    "content": "Carbon Black credentials\nPrerequisites\nCreate a Carbon Black subscription.\nCreate a Carbon Black developer account.\nAuthentication methods\nAPI key\nRelated resources\nRefer to Carbon Black's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nA URL: This URL is determined by the environment/product URL you use. You can find it by looking at the web address of your Carbon Black Cloud console. Refer to Carbon Black's URL Parts documentation for more information.\nAn Access Token: Refer to the Carbon Black Create an API key documentation to create an API key. Add the API Secret Key as the Access Token in n8n."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\chargebee.md",
    "content": "Chargebee credentials\nYou can use these credentials to authenticate the following nodes:\nChargebee\nChargebee Trigger\nPrerequisites\nCreate a Chargebee account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Chargebee's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn Account Name: This is your Chargebee Site Name or subdomain, for example if  is the full site name, the Account Name is n8n.\nAn API Key: Refer to the Chargebee Creating an API key documentation for steps on how to generate an API key.\nRefer to their more general API authentication documentation for further clarification."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\circleci.md",
    "content": "CircleCI credentials\nYou can use these credentials to authenticate the following nodes:\nCircleCI\nPrerequisites\nCreate a CircleCI account.\nSupported authentication methods\nPersonal API token\nRelated resources\nRefer to CircleCI's API documentation for more information about the service.\nUsing personal API token\nTo configure this credential, you'll need:\nA Personal API Token: Refer to the CircleCI Creating a Personal API token documentation for instructions on creating your token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ciscomeraki.md",
    "content": "Cisco Meraki credentials\nPrerequisites\nCreate a Cisco DevNet developer account.\nAccess to a Cisco Meraki account.\nAuthentication methods\nAPI key\nRelated resources\nRefer to Cisco Meraki's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Cisco Meraki Obtaining your Meraki API Key documentation for instructions on getting your API Key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ciscosecureendpoint.md",
    "content": "Cisco Secure Endpoint credentials\nPrerequisites\nCreate a Cisco DevNet developer account.\nAccess to a Cisco Secure Endpoint license.\nAuthentication methods\nOAuth2\nRelated resources\nRefer to Cisco Secure Endpoint's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing OAuth2\nTo configure this credential, you'll need:\nThe Region for your Cisco Secure Endpoint. Options are:\nAsia Pacific, Japan, and China\nEurope\nNorth America\nA Client ID: Provided when you register a SecureX API Client\nA Client Secret: Provided when you register a SecureX API Client\nTo get a Client ID and Client Secret, you'll need to Register a SecureX API Client. Refer to Cisco Secure Endpoint's authentication documentation for detailed instructions. Use the SecureX Client Password as the Client Secret within the n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ciscoumbrella.md",
    "content": "Cisco Umbrella credentials\nPrerequisites\nCreate a Cisco DevNet developer account.\nA Cisco Umbrella user account with Full Admin role.\nAuthentication methods\nAPI key\nRelated resources\nRefer to Cisco Umbrella's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nA Secret: Provided when you generate an API key\nRefer to the Cisco Umbrella Manage API Keys documentation for instructions on creating an Umbrella API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ciscowebex.md",
    "content": "Webex by Cisco credentials\nYou can use these credentials to authenticate the following nodes:\nWebex by Cisco\nWebex by Cisco Trigger\nPrerequisites\nCreate a Webex by Cisco account (this should automatically get you developer account access).\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Webex's API documentation for more information about the service.\nUsing OAuth2\nShould you need to configure OAuth2 from scratch, you'll need to create an integration to use this credential. Refer to the instructions in the Webex Registering your Integration documentation to begin.\nn8n recommends using the following Scopes for your integration:\nspark:rooms_read\nspark:messages_write\nspark:messages_read\nspark:memberships_read\nspark:memberships_write\nmeeting:recordings_write\nmeeting:recordings_read\nmeeting:preferences_read\nmeeting:schedules_write\nmeeting:schedules_read"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\clearbit.md",
    "content": "Clearbit credentials\nYou can use these credentials to authenticate the following node:\nClearbit\nPrerequisites\nCreate a Clearbit account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Clearbit's API documentation for more information about authenticating with the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to Clearbit's API Authentication documentation for more information on creating and viewing API keys."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\clickup.md",
    "content": "ClickUp credentials\nYou can use these credentials to authenticate the following nodes:\nClickUp\nClickUp Trigger\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to ClickUp's documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need a ClickUp account and:\nA Personal API Access Token\nTo get your personal API token:\nIf you're using ClickUp 2.0, select your avatar in the lower-left corner and select Apps. If you're using ClickUp 3.0, select your avatar in the upper-right corner, select Settings, and scroll down to select Apps in the sidebar.\nUnder API Token, select Generate.\nCopy your Personal API token and enter it in your n8n credential as the Access Token.\nRefer to ClickUp's Personal Token documentation for more information.\nUsing OAuth2\nIf you're self-hosting n8n, you'll need to create an OAuth app:\nIn ClickUp, select your avatar and select Integrations.\nSelect ClickUp API.\nSelect Create an App.\nEnter a Name for your app.\nIn n8n, copy the OAuth Redirect URL. Enter this as your ClickUp app's Redirect URL.\nOnce you create your app, copy the client_id and secret and enter them in your n8n credential.\nSelect Connect my account and follow the on-screen prompts to finish connecting the credential.\nRefer to the ClickUp Oauth flow documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\clockify.md",
    "content": "Clockify credentials\nYou can use these credentials to authenticate the following nodes:\nClockify\nClockify Trigger\nPrerequisites\nCreate a Clockify account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Clockify's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Access your API key from your Clockify Profile Settings."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\cloudflare.md",
    "content": "Cloudflare credentials\nYou can use these credentials to authenticate the following nodes:\nCloudflare node\nPrerequisites\nCreate a Cloudflare account.\nAdd a domain.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Cloudflare's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API token: Follow the Cloudflare documentation to create an API token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\cockpit.md",
    "content": "Cockpit credentials\nYou can use these credentials to authenticate the following nodes:\nCockpit\nPrerequisites\nCreate a Cockpit account.\nSet up a self-hosted instance of Cockpit.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Cockpit's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nYour Cockpit URL: The URL you use to access your Cockpit instance\nAn Access Token: Refer to the Cockpit Managing tokens documentation for instructions on creating an API token. Use the API token as the n8n Access Token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\coda.md",
    "content": "Coda credentials\nYou can use these credentials to authenticate the following nodes:\nCoda\nPrerequisites\nCreate a Coda account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Coda's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn API Access Token: Generate an API access token in your Coda Account settings."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\cohere.md",
    "content": "Cohere credentials\nYou can use these credentials to authenticate the following nodes:\nCohere\nCohere Chat\nReranker Cohere\nEmbeddings Cohere\nPrerequisites\nCreate a Cohere account.\nYou'll need an account with the following access:\nFor the Trial API, you need User or Owner permissions.\nFor Production API, you need Owner permissions.\nRefer to Cohere Teams and Roles documentation for more information.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Cohere's documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To generate a Cohere API key, go to the API Keys section of your Cohere dashboard."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\contentful.md",
    "content": "Contentful credentials\nYou can use these credentials to authenticate the following nodes:\nContentful\nPrerequisites\nCreate a Contentful account.\nCreate a Contentful space.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Contentful's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nYour Contentful Space ID: The Space ID displays as you generate the tokens;  You can also refer to the Contentful Find space ID documentation to view the Space ID.\nA Content Delivery API Access Token: Required if you want to use the Content Delivery API. Leave blank if you don't intend to use this API.\nA Content Preview API Access Token: Required if you want to use the Content Preview API. Leave blank if you don't intend to use this API.\nView and generate access tokens in Contentful in Settings > API keys. Contentful generates tokens for both Content Delivery API and Content Preview API as part of a single key. Refer to Contentful Creating and managing API keys for detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\convertapi.md",
    "content": "ConvertAPI credentials\nSupported authentication methods\nAPI Token\nRelated resources\nRefer to ConvertAPI's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API Token\nTo configure this credential, you'll need a ConvertAPI account and:\nAn API Token to authenticate requests to the service.\nRefer to ConvertAPI's API documentation for more information about authenticating to the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\convertkit.md",
    "content": "ConvertKit credentials\nYou can use these credentials to authenticate the following nodes:\nConvertKit\nConvertKit Trigger\nPrerequisites\nCreate a ConvertKit account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to ConvertKit's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Secret: Access your ConvertKit API key in Account Settings > Advanced. Add this key as the API Secret in n8n."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\copper.md",
    "content": "Copper credentials\nYou can use these credentials to authenticate the following nodes:\nCopper\nCopper Trigger\nPrerequisites\nCreate a Copper account at the Professional or Business plan level.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Copper's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Copper Generating an API key documentation for information on generating an API key.\nAn Email address: Use the API key creator's email address"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\cortex.md",
    "content": "Cortex credentials\nYou can use these credentials to authenticate the following nodes:\nCortex\nPrerequisites\nInstall Cortex on your server.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Cortex's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Cortex API Authentication documentation for detailed instructions on generating API keys.\nThe URL/Server Address for your Cortex Instance (defaults to )"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\cratedb.md",
    "content": "CrateDB credentials\nYou can use these credentials to authenticate the following nodes:\nCrateDB\nPrerequisites\nAn available instance of CrateDB.\nSupported authentication methods\naccount connection\nRelated resources\nRefer to CrateDB's documentation for more information about the service.\nUsing account connection\nTo configure this credential, you'll need:\nYour Host name\nYour Database name\nA User name\nA user Password\nTo set the SSL parameter. Refer to the CrateDB Secured Communications (SSL/TLS) documentation for more information. The options n8n supports are:\nAllow\nDisable\nRequire\nA Port number\nRefer to the Connect to a CrateDB cluster documentation for detailed instructions on these fields and their default values."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\crowddev.md",
    "content": "crowd.dev credentials\nYou can use these credentials to authenticate the following nodes:\ncrowd.dev\ncrowd.dev Trigger\nPrerequisites\nCreate a working instance of crowd.dev.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to crowd.dev's documentation for more information about the service, and their API documentation for working with the API.\nUsing API key\nTo configure this credential, you'll need:\nA URL:\nIf your crowd.dev instance is hosted on crowd.dev, keep the default of\nIf your crowd.dev instance is self-hosted, use the URL you use to access your crowd.dev instance.\nYour crowd.dev Tenant ID: Displayed in the Settings section of the crowd.dev app\nAn API Token: Displayed in the Settings section of the crowd.dev app\nRefer to the crowd.dev API documentation for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\crowdstrike.md",
    "content": "CrowdStrike credentials\nPrerequisites\nCreate a CrowdStrike account.\nAuthentication methods\nOAuth2\nRelated resources\nRefer to CrowdStrike's documentation for more information about the service. Their documentation is behind a log in, so you must log in to your account on their website to access the API documentation.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing OAuth2\nTo configure this credential, you'll need:\nThe URL of your CrowdStrike instance\nA Client ID: Generated by creating a new API Client in Crowdstrike in Support > API Clients and Keys.\nA Client Secret: Generated by creating a new API Client in Crowdstrike in Support > API Clients and Keys.\nA broad outline of the appropriate steps is available publicly at the CrowdStrike blog: Getting Access to the CrowdStrike API. CrowdStrike's full documentation is behind a log in, so you must log in to your account to access the full API documentation."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\customerio.md",
    "content": "Customer.io credentials\nYou can use these credentials to authenticate the following nodes with Customer.io.\nCustomer.io\nCustomer.io Trigger\nPrerequisites\nCreate a Customer.io account.\nSupported authentication methods\nAPI Key\nRelated resources\nRefer to Customer.io's summary API documentation for more information about the service.\nFor detailed API reference documentation for each API, refer to the Track API documentation and the App API documentation.\nUsing API key\nTo configure this credential, you'll need:\nA Tracking API Key: For use with the Track API at  See the FAQs below for more details.\nYour Region: Customer.io uses different API subdomains depending on the region you select. Options include:\nGlobal region: Keeps the default URLs for both APIs; for use in all non-EU countries/regions.\nEU region: Adjusts the Track API subdomain to track-eu and the App API subdomain to api-eu; only use this if you are in the EU.\nA Tracking Site ID: Required with your Tracking API Key\nAn App API Key: For use with the App API at  See the FAQs below for more details.\nRefer to the Customer.io Finding and managing your API credentials documentation for instructions on creating both Tracking API and App API keys.\nWhy you need a Tracking API Key and an App API Key\nCustomer.io has two different API endpoints and generates and stores the keys for each slightly differently:\nThe Track API at\nThe App API at\nThe Track API requires a Tracking Site ID; the App API doesn't.\nBased on the operation you want to perform, n8n uses the correct API key and its corresponding endpoint."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\datadog.md",
    "content": "Datadog credentials\nPrerequisites\nCreate a Datadog account.\nRelated resources\nRefer to Datadog's API documentation for more information about authenticating with the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API Key\nTo configure this credential, you'll need:\nYour Datadog instance Host\nAn API Key\nAn App Key\nRefer to Authentication on Datadog's website for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\deepl.md",
    "content": "DeepL credentials\nYou can use these credentials to authenticate the following nodes:\nDeepL\nPrerequisites\nCreate a DeepL developer account. n8n works with both Free and Pro API Plans.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to DeepL's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to DeepL's Authentication documentation for more information on getting your API key.\nTo identify which API Plan you're on. DeepL has different API endpoints for each plan, so be sure you select the correct one:\nPro Plan\nFree Plan"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\deepseek.md",
    "content": "DeepSeek credentials\nYou can use these credentials to authenticate the following nodes:\nChat DeepSeek\nPrerequisites\nCreate a DeepSeek account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to DeepSeek's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nTo generate your API Key:\nLogin to your DeepSeek account or create an account.\nOpen your API keys page.\nSelect Create new secret key to create an API key, optionally naming the key.\nCopy your key and add it as the API Key in n8n.\nRefer to the Your First API Call page for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\demio.md",
    "content": "Demio credentials\nYou can use these credentials to authenticate the following nodes:\nDemio\nPrerequisites\nCreate a Demio account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Demio's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nAn API Secret\nYou must have Owner status in Demio to generate API keys and secrets. To view and generate API keys and secrets, go to Account Settings > API. Refer to the Demio Account Owner Settings documentation for more detailed steps."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\dfiriris.md",
    "content": "DFIR-IRIS credentials\nPrerequisites\nAn accessible instance of DFIR-IRIS.\nRelated resources\nRefer to DFIR-IRIS's API documentation for more information about authenticating with the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API Key\nTo configure this credential, you'll need:\nAn API Key: Refer to DFIR-IRIS's API documentation for instructions on getting your API key.\nThe Base URL of your DFIR-IRIS instance."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\dhl.md",
    "content": "DHL credentials\nYou can use these credentials to authenticate the following nodes:\nDHL\nSupported authentication methods\nAPI key\nRelated resources\nRefer to DHL's Developer documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a DHL Developer account and:\nAn API Key\nTo get an API key, create an app:\nIn the DHL Developer portal, select the user icon to open your User Apps.\nSelect + Create App.\nEnter an App name, like n8n integration.\nEnter a Machine name, like n8n_integration.\nIn SELECT APIs, select Shipment Tracking - Unified. The API is added to the Add API to app section.\nIn the Add API to app section, select the + next to the Shipment Tracking - Unified API.\nSelect Create App. The Apps page opens, displaying the app you just created.\nSelect the app you just created to view its details.\nSelect Show key next to API Key.\nCopy the API Key and enter it in your n8n credential.\nRefer to How to create an app? for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\discord.md",
    "content": "Discord credentials\nYou can use these credentials to authenticate the following nodes:\nDiscord\nPrerequisites\nCreate a Discord account.\nFor Bot and OAuth2 credentials:\nSet up your local developer environment.\nCreate an application and a bot user.\nFor webhook credentials, create a webhook.\nSupported authentication methods\nBot\nOAuth2\nWebhook\nNot sure which method to use? Refer to Choose an authentication method for more guidance.\nRelated resources\nRefer to Discord's Developer documentation for more information about the service.\nUsing bot\nUse this method if you want to add the bot to your Discord server using a bot token rather than OAuth2.\nTo configure this credential, you'll need:\nA Bot Token: Generated once you create an application with a bot.\nTo create an application with a bot and generate the Bot Token:\nIf you don't have one already, create an app in the developer portal.\nEnter a Name for your app.\nSelect Create.\nSelect Bot from the left menu.\nUnder Token, select Reset Token to generate a new bot token.\nCopy the token and add it to your n8n credential.\nIn Bot > Privileged Gateway Intents, add any privileged intents you want your bot to have. Refer to Configuring your bot for more information on privileged intents.\nn8n recommends activating SERVER MEMBERS INTENT: Required for your bot to receive events listed under GUILD_MEMBERS.\nIn Installation > Installation Contexts, select the installation contexts you want your bot to use:\nSelect Guild Install for server-installed apps. (Most common for n8n users.)\nSelect User Install for user-installed apps. (Less common for n8n users, but may be useful for testing.)\nRefer to Discord's Choosing installation contexts documentation for more information about these installation contexts.\nIn Installation > Install Link, select Discord Provided Link if it's not already selected.\nStill on the Installation page, in the Default Install Settings section, select applications.commands and bot scopes. Refer to Discord's Scopes documentation for more information about these and other scopes.\nAdd permissions on the Bot > Bot Permissions page. Refer to Discord's Permissions documentation for more information. n8n recommends selecting these permissions for the Discord node:\nManage Roles\nManage Channels\nRead Messages/View Channels\nSend Messages\nCreate Public Threads\nCreate Private Threads\nSend Messages in Threads\nSend TTS Messages\nManage Messages\nManage Threads\nEmbed Links\nAttach Files\nRead Message History\nAdd Reactions\nAdd the app to your server or test server:\nGo to Installation > Install Link and copy the link listed there.\nPaste the link in your browser and hit Enter.\nSelect Add to server in the installation prompt.\nOnce your app's added to your server, you'll see it in the member list.\nThese steps outline the basic functionality needed to set up your n8n credential. Refer to the Discord Creating an App guide for more information on creating an app, especially:\nFetching your credentials for getting your app's credentials into your local developer environment.\nHandling interactivity for information on setting up public endpoints for interactive /slash commands.\nUsing OAuth2\nUse this method if you want to add the bot to Discord servers using the OAuth2 flow, which simplifies the process for those installing your app.\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nChoose whether to send Authentication in the Header or Body\nA Bot Token\nFor details on creating an application with a bot and generating the token, follow the same steps as in Using bot above.\nThen:\nCopy the Bot Token you generate and add it into the n8n credential.\nOpen the OAuth2 page in your Discord application to access your Client ID and generate a Client Secret. Add these to your n8n credential.\nFrom n8n, copy the OAuth Redirect URL and add it into the Discord application in OAuth2 > Redirects. Be sure you save these changes.\nUsing webhook\nTo configure this credential, you'll need:\nA Webhook URL: Generated once you create a webhook.\nTo get a Webhook URL, you create a webhook and copy the URL that gets generated:\nOpen your Discord Server Settings and open the Integrations tab.\nSelect Create Webhook to create a new webhook.\nGive your webhook a Name that makes sense.\nSelect the avatar next to the Name to edit or upload a new avatar.\nIn the CHANNEL dropdown, select the channel the webhook should post to.\nSelect Copy Webhook URL to copy the Webhook URL. Enter this URL in your n8n credential.\nRefer to the Discord Making a Webhook documentation for more information.\nChoose an authentication method\nThe simplest installation is a webhook. You create and add webhooks to a single channel on a Discord server. Webhooks can post messages to a channel. They don't require a bot user or authentication. But they can't listen or respond to user requests or commands. If you need a straightforward way to send messages to a channel without the need for interaction or feedback, use a webhook.\nA bot is an interactive step up from a webhook. You add bots to the Discord server (referred to as a guild in the Discord API documentation) or to user accounts. Bots added to the server can interact with users on all the server's channels. They can manage channels, send and retrieve messages, retrieve the list of all users, and change their roles. If you need to build an interactive, complex, or multi-step workflow, use a bot.\nOAuth2 is basically a bot that uses an OAuth2 flow rather than just the bot token. As with bots, you add these to the Discord server or to user accounts. These credentials offer the same functionalities as bots, but they can simplify the installation of the bot on your server."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\discourse.md",
    "content": "Discourse credentials\nYou can use these credentials to authenticate the following nodes:\nDiscourse\nPrerequisites\nHost an instance of Discourse\nCreate an account on your hosted instance and make sure that you are an admin\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Discourse's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nThe URL of your Discourse instance, for example\nAn API Key: Create an API key through the Discourse admin panel. Refer to the Discourse create and configure an API key documentation for instructions on creating an API key and specifying a username.\nA Username: Use your own name, system, or another user.\nRefer to the Authentication section of the Discourse API documentation for examples."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\disqus.md",
    "content": "Disqus credentials\nYou can use these credentials to authenticate the following nodes:\nDisqus\nPrerequisites\nCreate a Disqus account.\nRegister an API application.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Disqus's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn Access Token: Once you've registered an API application, copy the API Key and add it to n8n as the Access Token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\drift.md",
    "content": "Drift credentials\nYou can use these credentials to authenticate the following nodes:\nDrift\nPrerequisites\nCreate a Drift account.\nCreate a Drift app.\nSupported authentication methods\nAPI personal access token\nOAuth2\nRelated resources\nRefer to Drift's API documentation for more information about the service.\nUsing API personal access token\nTo configure this credential, you'll need:\nA Personal Access Token: To get a token, create a Drift app. Install the app to generate an OAuth Access token. Add this to the n8n credential as your Personal Access Token.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch or need more detail on what's happening in the OAuth web flow, refer to the instructions in the Drift Authentication and Scopes documentation to set up OAuth for your app."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\dropbox.md",
    "content": "Dropbox credentials\nYou can use these credentials to authenticate the following nodes:\nDropbox\nSupported authentication methods\nAPI access token: Dropbox recommends this method for testing with your user account and granting a limited number of users access.\nOAuth2: Dropbox recommends this method for production or for testing with more than 50 users.\nRelated resources\nRefer to Dropbox's Developer documentation for more information about the service.\nUsing access token\nTo configure this credential, you'll need a Dropbox developer account and:\nAn Access Token: Generated once you create a Dropbox app.\nAn App Access Type\nTo set up the credential, create a Dropbox app:\nOpen the App Console within the Dropbox developer portal.\nSelect Create app.\nIn Choose an API, select Scoped access.\nIn Choose the type of access you need, choose whichever option best fits your use of the Dropbox node:\nApp Folder grants access to a single folder created specifically for your app.\nFull Dropbox grants access to all files and folders in your user's Dropbox.\nRefer to the DBX Platform developer guide for more information.\nIn Name your app, enter a name for your app, like n8n integration.\nCheck the box to agree to the Dropbox API Terms and Conditions.\nSelect Create app. The app's Settings open.\nIn the OAuth 2 section, in Generated access token, select Generate.\nCopy the access token and enter it as the Access Token in your n8n credential.\nIn n8n, select the same App Access Type you selected for your app.\nRefer to the Dropbox App Console Settings documentation for more information.\nUsing OAuth2\nCloud users need to select the App Access Type:\nApp Folder grants access to a single folder created specifically for your app.\nFull Dropbox grants access to all files and folders in your user's Dropbox.\nRefer to the DBX Platform developer guide for more information.\nIf you're self-hosting n8n, you'll need to configure OAuth2 manually:\nOpen the App Console within the Dropbox developer portal.\nSelect Create app.\nIn Choose an API, select Scoped access.\nIn Choose the type of access you need, choose whichever option best fits your use of the Dropbox node:\nApp Folder grants access to a single folder created specifically for your app.\nFull Dropbox grants access to all files and folders in your user's Dropbox.\nRefer to the DBX Platform developer guide for more information.\nIn Name your app, enter a name for your app, like n8n integration.\nCheck the box to agree to the Dropbox API Terms and Conditions.\nSelect Create app. The app's Settings open.\nCopy the App key and enter it as the Client ID in your n8n credential.\nCopy the Secret and enter it as the Client Secret in your n8n credential.\nIn n8n, copy the OAuth Redirect URL and enter it in the Dropbox Redirect URIs.\nIn n8n, select the same App Access Type you selected for your app.\nRefer to the instructions in the Dropbox Implementing OAuth documentation for more information.\nFor internal tools and limited usage, you can keep your app private. But if you'd like your app to be used by more than 50 users or you want to distribute it, you'll need to complete Dropbox's production approval process. Refer to Production Approval in the DBX Platform developer guide for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\dropcontact.md",
    "content": "Dropcontact credentials\nYou can use these credentials to authenticate the following nodes:\nDropcontact\nPrerequisites\nCreate a developer account in Dropcontact.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Dropcontact's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To view your API key in Dropcontact, go to API. Refer to the Dropcontact API key documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\dynatrace.md",
    "content": "Dynatrace credentials\nPrerequisites\nCreate a Dynatrace account.\nRelated resources\nRefer to Dynatrace's API documentation for more information about authenticating with the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing Access Token\nTo configure this credential, you'll need:\nAn Access Token\nRefer to Access Tokens on Dynatrace's website for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\egoi.md",
    "content": "E-goi credentials\nYou can use these credentials to authenticate the following nodes:\nE-goi\nPrerequisites\nCreate an E-goi account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to E-goi's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to E-goi's API key documentation for instructions on generating and viewing an API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\elasticsearch.md",
    "content": "Elasticsearch credentials\nYou can use these credentials to authenticate the following nodes:\nElasticsearch\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Elasticsearch's documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need an Elasticsearch account with a deployment and:\nA Username\nA Password\nYour Elasticsearch application's Base URL (also known as the Elasticsearch application endpoint)\nTo set up the credential:\nEnter your Elasticsearch Username.\nEnter your Elasticsearch Password.\nIn Elasticsearch, go to Deployments.\nSelect your deployment.\nSelect Manage this deployment.\nIn the Applications section, copy the endpoint of the Elasticsearch application.\nEnter this in n8n as the Base URL.\nBy default, n8n connects only if SSL certificate validation succeeds. If you'd like to connect even if SSL certificate validation fails, turn on Ignore SSL Issues."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\elasticsecurity.md",
    "content": "Elastic Security credentials\nYou can use these credentials to authenticate the following nodes:\nElastic Security\nPrerequisites\nCreate an Elastic Security account.\nDeploy an application.\nSupported authentication methods\nBasic auth\nAPI Key\nRelated resources\nRefer to Elastic Security's documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA Username: For the user account you log into Elasticsearch with.\nA Password: For the user account you log into Elasticsearch with.\nYour Elasticsearch application's Base URL (also known as the Elasticsearch application endpoint):\nIn Elasticsearch, select the option to Manage this deployment.\nIn the Applications section, copy the endpoint of the Elasticsearch application.\nAdd this in n8n as the Base URL.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: For the user account you log into Elasticsearch with. Refer to Elasticsearch's Create API key documentation for more information.\nYour Elasticsearch application's Base URL (also known as the Elasticsearch application endpoint):\nIn Elasticsearch, select the option to Manage this deployment.\nIn the Applications section, copy the endpoint of the Elasticsearch application.\nAdd this in n8n as the Base URL."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\emelia.md",
    "content": "Emelia credentials\nYou can use these credentials to authenticate the following nodes:\nEmelia\nEmelia Trigger\nPrerequisites\nCreate an Emelia account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Emelia's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To generate an API Key in Emelia, access your API Keys by selecting the avatar in the top right (your Settings). Refer to the Authentication section of Emelia's API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\erpnext.md",
    "content": "ERPNext credentials\nYou can use these credentials to authenticate the following nodes:\nERPNext\nPrerequisites\nCreate an ERPNext account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to ERPNext's documentation for more information about the service.\nRefer to ERPNext's developer documentation for more information about working with the framework.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate this from your own ERPNext user account in Settings > My Settings > API Access.\nAn API Secret: Generated with the API key.\nYour ERPNext Environment:\nFor Cloud-hosted:\nYour ERPNext Subdomain: Refer to the FAQs\nYour Domain: Choose between erpnext.com and frappe.cloud.\nFor Self-hosted:\nThe fully qualified Domain where you host ERPNext\nChoose whether to Ignore SSL Issues: When selected, n8n will connect even if SSL certificate validation is unavailable.\nIf you are an ERPNext System Manager, you can also generate API keys and secrets for other users. Refer to the ERPNext Adding Users documentation for more information.\nHow to find the subdomain of an ERPNext cloud-hosted account\nYou can find your ERPNext subdomain by reviewing the address bar of your browser. The string between https:// and either .erpnext.com or frappe.cloud is your subdomain.\nFor example, if the URL in the address bar is  the subdomain is n8n."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\eventbrite.md",
    "content": "Eventbrite credentials\nYou can use these credentials to authenticate the following nodes:\nEventbrite Trigger\nPrerequisites\nCreate an Eventbrite account.\nSupported authentication methods\nAPI private key\nOAuth2\nRelated resources\nRefer to Eventbrite's API documentation for more information about the service.\nUsing API private key\nTo configure this credential, you'll need:\nA Private Key: Refer to the Eventbrite API Authentication Get a Private Token documentation for detailed steps to generate a Private Token. Use this private token as the Private Key in the n8n credential.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch or need more detail on what's happening in the OAuth web flow, refer to the instructions in the Eventbrite API authentication For App Partners documentation-authorize-your-users) to set up OAuth."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\f5bigip.md",
    "content": "F5 Big-IP credentials\nPrerequisites\nCreate an F5 Big-IP account.\nAuthentication methods\nAccount login\nRelated resources\nRefer to F5 Big-IP's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing account login\nTo configure this credential, you'll need:\nA Username: Use the username you use to log in to F5 Big-IP.\nA Password: Use the user password you use to log in to F5 Big-IP."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\facebookapp.md",
    "content": "Facebook App credentials\nYou can use these credentials to authenticate the following nodes:\nFacebook Trigger\nSupported authentication methods\nApp access token\nRelated resources\nRefer to Meta's Graph API documentation for more information about the service.\nUsing app access token\nTo configure this credential, you'll need a Meta for Developers account and:\nAn app Access Token\nAn optional App Secret: Used to verify the integrity and origin of the payload.\nThere are five steps in setting up your credential:\nCreate a Meta app with the Webhooks product.\nGenerate an App Access Token for that app.\nConfigure the Facebook trigger.\nOptional: Add an app secret.\nApp Review: Only required if your app's users don't have roles on the app itself. If you're creating the app for your own internal purposes, this isn't necessary.\nRefer to the detailed instructions below for each step.\nCreate a Meta app\nTo create a Meta app:\nGo to the Meta Developer App Dashboard and select Create App.\nIf you have a business portfolio and you're ready to connect the app to it, select the business portfolio. If you don't have a business portfolio or you're not ready to connect the app to the portfolio, select I don‚Äôt want to connect a business portfolio yet and select Next. The Use cases page opens.\nSelect Other, then select Next.\nSelect Business and Next.\nComplete the essential information:\nAdd an App name.\nAdd an App contact email.\nHere again you can connect to a business portfolio or skip it.\nSelect Create app.\nThe Add products to your app page opens.\nSelect App settings > Basic from the left menu.\nEnter a Privacy Policy URL. (Required to take the app \"Live.\")\nSelect Save changes.\nAt the top of the page, toggle the App Mode from Development to Live.\nIn the left menu, select Add Product.\nThe Add products to your app page appears. Select Webhooks.\nThe Webhooks product opens.\nRefer to Meta's Create an app documentation for more information on creating an app, required fields like the Privacy Policy URL, and adding products.\nFor more information on the app modes and switching to Live mode, refer to App Modes and Publish | App Types.\nGenerate an App Access Token\nNext, create an app access token to be used by your n8n credential and the Webhooks product:\nIn a separate tab or window, open the Graph API explorer.\nSelect the Meta App you just created in the Access Token section.\nIn User or Page, select Get App Token.\nSelect Generate Access Token.\nThe page prompts you to log in and grant access. Follow the on-screen prompts.\nCopy the token and enter it in your n8n credential as the Access Token. Save this token somewhere else, too, since you'll need it for the Webhooks configuration.\nSave your n8n credential.\nRefer to the Meta instructions for Your First Request for more information on generating the token.\nConfigure the Facebook Trigger\nNow that you have a token, you can configure the Facebook Trigger node:\nIn your Meta app, copy the App ID from the top navigation bar.\nIn n8n, open your Facebook Trigger node.\nPaste the App ID into the APP ID field.\nSelect Execute step to shift the trigger into listening mode.\nReturn to the tab or window where your Meta app's Webhooks product configuration is open.\nSubscribe to the objects you want to receive Facebook Trigger notifications about. For each subscription:\nCopy the Webhook URL from n8n and enter it as the Callback URL in your Meta App.\nEnter the Access Token you copied above as the Verify token.\nSelect Verify and save. (This step fails if you don't have your n8n trigger listening.)\nSome webhook subscriptions, like User, prompt you to subscribe to individual events. Subscribe to the events you're interested in.\nYou can send some Test events from Meta to confirm things are working. If you send a test event, verify its receipt in n8n.\nRefer to the Facebook Trigger node documentation for more information.\nOptional: Add an App Secret\nFor added security, Meta recommends adding an App Secret. This signs all API calls with the appsecret_proof parameter. The app secret proof is a sha256 hash of your access token, using your app secret as the key.\nTo generate an App Secret:\nIn Meta while viewing your app, select App settings > Basic from the left menu.\nSelect Show next to the App secret field.\nThe page prompts you to re-enter your Facebook account credentials. Once you do so, Meta shows the App Secret.\nHighlight it to select it, copy it, and paste this into your n8n credential as the App Secret.\nSave your n8n credential.\nRefer to the App Secret documentation for more information.\nApp review\nApp Review requires Business Verification.\nYour app must go through App Review if it will be used by someone who:\nDoesn't have a role on the app itself.\nDoesn't have a role in the Business that has claimed the app.\nIf your only app users are users who have a role on the app itself, App Review isn't required.\nAs part of the App Review process, you may need to request advanced access for your webhook subscriptions.\nRefer to Meta's App Review and Advanced Access documentation for more information.\nCommon issues\nUnverified apps limit\nFacebook only lets you have a developer or administrator role on a maximum of 15 apps that aren't already linked to a Meta Verified Business Account.\nRefer to Limitations | Create an app if you're over that limit."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\facebookgraph.md",
    "content": "Facebook Graph API credentials\nYou can use these credentials to authenticate the following nodes:\nFacebook Graph API\nSupported authentication methods\nApp access token\nRelated resources\nRefer to Meta's Graph API documentation for more information about the service.\nUsing app access token\nTo configure this credential, you'll need a Meta for Developers account and:\nAn app Access Token\nThere are two steps in setting up your credential:\nCreate a Meta app with the products you need to access.\nGenerate an App Access Token for that app.\nRefer to the detailed instructions below for each step.\nCreate a Meta app\nTo create a Meta app:\nGo to the Meta Developer App Dashboard and select Create App.\nIf you have a business portfolio and you're ready to connect the app to it, select the business portfolio. If you don't have a business portfolio or you're not ready to connect the app to the portfolio, select I don‚Äôt want to connect a business portfolio yet and select Next. The Use cases page opens.\nSelect the Use case that aligns with how you wish to use the Facebook Graph API. For example, for products in Meta's Business suite (like Messenger, Instagram, WhatsApp, Marketing API, App Events, Audience Network, Commerce API, Fundraisers, Jobs, Threat Exchange, and Webhooks), select Other, then select Next.\nSelect Business and Next.\nComplete the essential information:\nAdd an App name.\nAdd an App contact email.\nHere again you can connect to a business portfolio or skip it.\nSelect Create app.\nThe Add products to your app page opens.\nSelect App settings > Basic from the left menu.\nEnter a Privacy Policy URL. (Required to take the app \"Live.\")\nSelect Save changes.\nAt the top of the page, toggle the App Mode from Development to Live.\nIn the left menu, select Add Product.\nThe Add products to your app page appears. Select the products that make sense for your app and configure them.\nRefer to Meta's Create an app documentation for more information on creating an app, required fields like the Privacy Policy URL, and adding products.\nFor more information on the app modes and switching to Live mode, refer to App Modes and Publish | App Types.\nGenerate an App Access Token\nNext, create an app access token to use with your n8n credential and the products you selected:\nIn a separate tab or window, open the Graph API explorer.\nSelect the Meta App you just created in the Access Token section.\nIn User or Page, select Get App Token.\nSelect Generate Access Token.\nThe page prompts you to log in and grant access. Follow the on-screen prompts.\nCopy the token and enter it in your n8n credential as the Access Token.\nRefer to the Meta instructions for Your First Request for more information on generating the token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\facebookleadads.md",
    "content": "Facebook Lead Ads credentials\nYou can use these credentials to authenticate the following nodes:\nFacebook Lead Ads trigger\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Facebook Lead Ads' documentation for more information about the service.\nView example workflows and related content on n8n's website.\nUsing OAuth2\nTo configure this credential, you'll need a Meta for Developers account and:\nA Client ID\nA Client Secret\nTo get both, create a Meta app with either the Facebook Login product or the Facebook Login for Business product.\nTo create your app and set up the credential with Facebook Login for Business:\nGo to the Meta Developer App Dashboard and select Create App.\nIf you have a business portfolio and you're ready to connect the app to it, select the business portfolio. If you don't have a business portfolio or you're not ready to connect the app to the portfolio, select I don‚Äôt want to connect a business portfolio yet and select Next. The Use cases page opens.\nSelect Other, then select Next.\nSelect Business and Next.\nComplete the essential information:\nAdd an App name.\nAdd an App contact email.\nHere again you can connect to a business portfolio or skip it.\nSelect Create app. The Add products to your app page opens.\nSelect Facebook Login for Business. The Settings page for this product opens.\nCopy the OAuth Redirect URL from your n8n credential.\nIn your Meta app settings in Client OAuth settings, paste that URL as the Valid OAuth Redirect URIs.\nSelect App settings > Basic from the left menu.\nCopy the App ID and enter it as the Client ID within your n8n credential.\nCopy the App Secret and enter it as the Client Secret within your n8n credential.\nYour credential should successfully connect now, but you'll need to go through the steps to take your Meta app live before you can use it with the Facebook Lead Ads trigger. Here's a summary of what you'll need to do:\nIn your Meta app, select App settings > Basic from the left menu.\nEnter a Privacy Policy URL. (Required to take the app \"Live.\")\nSelect Save changes.\nAt the top of the page, toggle the App Mode from Development to Live.\nFacebook Login for Business requires Advanced Access for public_profile. To add it, go to App Review > Permissions and Features.\nSearch for public_profile and select Request advanced access.\nComplete the steps for business verification.\nUse the Lead Ads Testing Tool to trigger some demo form submissions and test your workflow.\nRefer to Meta's Create an app documentation for more information on creating an app, required fields like the Privacy Policy URL, and adding products.\nFor more information on the app modes and switching to Live mode, refer to App Modes and Publish | App Types."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\figma.md",
    "content": "Figma credentials\nYou can use these credentials to authenticate the following nodes:\nFigma Trigger (Beta)\nPrerequisites\nCreate a Figma account. You need an admin or owner level account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Figma's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Personal Access Token (PAT): Refer to the Figma API Access Tokens documentation for instructions on generating a Personal Access Token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\filemaker.md",
    "content": "FileMaker credentials\nYou can use these credentials to authenticate the following nodes:\nFileMaker\nPrerequisites\nCreate a user account on a FileMaker Server with the fmrest extended privilege to Access the FileMaker Data API.\nEnsure the FileMaker Server can use the FileMaker Data API:\nPrepare your database for FileMaker Data API access using FileMaker Pro. You can create a database or prepare an existing database.\nRefer to Prepare databases for FileMaker Data API access for more information.\nWrite code that calls FileMaker Data API methods to find, create, edit, duplicate, and delete records in a hosted database.\nRefer to Write FileMaker Data API calls for more information.\nHost your solution with FileMaker Data API access enabled.\nRefer to Host a FileMaker Data API solution for more information.\nTest that FileMaker Data API access is working.\nRefer to Test the FileMaker Data API solution for more information.\nMonitor your hosted solution using Admin Console.\nRefer to Monitor FileMaker Data API solutions for more information.\nSupported authentication methods\nDatabase connection\nRelated resources\nRefer to FileMaker's Data API Guide for more information about the service.\nUsing database connection\nTo configure this credential:\nEnter the Host name or IP address of your FileMaker Server.\nEnter the Database name. This should match the database name as it appears in the Databases list within FileMaker.\nEnter the user account Login for the account with the fmrest extended privilege. Refer to the previous Prerequisites section for more information.\nEnter the Password for that user account."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\filescan.md",
    "content": "Filescan credentials\nPrerequisites\nCreate a Filescan account.\nRelated resources\nRefer to Filescan's API documentation for more information about authenticating with the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate your API key from your profile settings > API Key. Refer to the Filescan FAQ for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\flow.md",
    "content": "Flow credentials\nYou can use these credentials to authenticate the following nodes:\nFlow\nFlow Trigger\nPrerequisites\nCreate a Flow account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Flow's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nYour numeric Organization ID\nAn Access Token\nRefer to the Flow API Getting Started documentation for instructions on generating your Access Token and viewing your Organization ID."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\formiotrigger.md",
    "content": "Form.io Trigger credentials\nYou can use these credentials to authenticate the following nodes:\nForm.io Trigger\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Form.io's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need a Form.io account and:\nYour Environment\nYour login Email address\nYour Password\nTo set up the credential:\nSelect your Environment:\nChoose Cloud hosted if you aren't hosting Form.io yourself.\nChoose Self-hosted if you're hosting Form.io yourself. Then add:\nYour Self-Hosted Domain. Use only the domain itself. For example, if you view a form at  the Self-Hosted Domain is\nEnter the Email address you use to log in to Form.io.\nEnter the Password you use to log in to Form.io."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\formstacktrigger.md",
    "content": "Formstack Trigger credentials\nYou can use these credentials to authenticate the following nodes:\nFormstack Trigger\nPrerequisites\nCreate a Formstack account.\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to Formstack's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn API Access Token: To generate an Access Token, create a new application in Formstack using the following details:\nRedirect URI: For cloud n8n instances, enter\nFor self-hosted n8n instances, enter the OAuth callback URL for your n8n instance in the format  For example\nPlatform: Select Website.\nOnce you've created the application, copy the access token either from the applications list or by selecting the application to view its details.\nRefer to Formstack's API Authorization documentation for more detailed instructions.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nTo generate both of these, create a new application in Formstack using the following details:\nRedirect URI: Copy the OAuth Redirect URL from the n8n credential to enter here.\nFor self-hosted n8n instances, enter the OAuth callback URL for your n8n instance in the format  For example\nPlatform: Select Website.\nOnce you've created the application, select it from the applications list to view the Application Details. Copy the Client ID and Client Secret and add them to n8n. Once you've added both, select the Connect my account button to begin the OAuth2 flow and authorization process.\nRefer to Formstack's API Authorization documentation for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\fortigate.md",
    "content": "Fortinet FortiGate credentials\nPrerequisites\nCreate a Fortinet FortiGate account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Fortinet FortiGate's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API access token\nTo configure this credential, you'll need:\nAn API Access Token: To generate an access token, create a REST API administrator.\nRefer to the Fortinet FortiGate Using APIs documentation for more information about token-based authentication in FortiGate."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\freshdesk.md",
    "content": "Freshdesk credentials\nYou can use these credentials to authenticate the following nodes:\nFreshdesk\nPrerequisites\nCreate a Freshdesk account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Freshdesk's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Freshdesk API authenticaton documentation for detailed instructions on getting your API key.\nA Freshdesk Domain: Use the subdomain of your Freshdesk account. This is part of the URL, for example  So if you access Freshdesk through  enter n8n as your Domain."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\freshservice.md",
    "content": "Freshservice credentials\nYou can use these credentials to authenticate the following nodes:\nFreshservice\nPrerequisites\nCreate a Freshservice account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Freshservice's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Freshservice API authenticaton documentation for detailed instructions on getting your API key.\nYour Freshservice Domain: Use the subdomain of your Freshservice account. This is part of the URL, for example  So if you access Freshservice through  enter n8n as your Domain."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\freshworkscrm.md",
    "content": "Freshworks CRM credentials\nYou can use these credentials to authenticate the following nodes:\nFreshworks CRM\nPrerequisites\nCreate a Freshworks CRM account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Freshworks CRM's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Freshworks CRM API authenticaton documentation for detailed instructions on getting your API key.\nYour Freshworks CRM Domain: Use the subdomain of your Freshworks CRM account. This is part of the URL, for example  So if you access Freshworks CRM through  enter n8n as your Domain."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ftp.md",
    "content": "FTP credentials\nYou can use these credentials to authenticate the following nodes:\nFTP\nPrerequisites\nCreate an account on a File Transfer Protocol (FTP) server like JSCAPE, OpenSSH, or FileZilla Server.\nSupported authentication methods\nFTP account: Use this method if your FTP server doesn't support SSH tunneling or encrypted connections.\nSFTP account: Use this method if your FTP server supports SSH tunneling and encrypted connections.\nRelated resources\nFile Transfer Protocol (FTP) and Secure Shell File Transfer Protocol (SFTP) are protocols for transferring files directly between an FTP/SFTP client and server.\nUsing FTP account\nUse this method if your FTP server doesn't support SSH tunneling or encrypted connections.\nTo configure this credential, you'll need to:\nEnter the name or IP address of your FTP server's Host.\nEnter the Port number the connection should use.\nEnter the Username the credential should connect as.\nEnter the user's Password.\nReview your FTP server provider's documentation for instructions on getting the information you need.\nUsing SFTP account\nUse this method if your FTP server supports SSH tunneling and encrypted connections.\nTo configure this credential, you'll need to:\nEnter the name or IP address of your FTP server's Host.\nEnter the Port number the connection should use.\nEnter the Username the credential should connect as.\nEnter the user's Password.\nFor the Private Key, enter a string for either key-based or host-based user authentication\nEnter your Private Key in OpenSSH format. This is most often generated using the ssh-keygen -o parameter, for example: ssh-keygen -o -a 100 -t ed25519.\nIf the Private Key is encrypted, enter the Passphrase used to decrypt it.\nIf the Private Key doesn't use a passphrase, leave this field blank.\nReview your FTP server provider's documentation for instructions on getting the information you need."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\getresponse.md",
    "content": "GetResponse credentials\nYou can use these credentials to authenticate the following nodes:\nGetResponse\nGetResponse Trigger\nPrerequisites\nCreate a GetResponse account.\nSupported authentication methods\nAPI key\nOAuth2\nRelated resources\nRefer to GetResponse's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To view or generate an API key, go to Integrations and API > API. Refer to the GetResponse Help Center for more detailed instructions.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you register your application.\nA Client Secret: Generated when you register your application as the Client Secret Key.\nWhen you register your application, copy the OAuth Redirect URL from n8n and add it as the Redirect URL in GetResponse.\nConfigure OAuth2 credentials for a local environment\nGetResponse doesn't accept the localhost callback URL. Follow the steps below to configure the OAuth credentials for a local environment:\n1. Use ngrok to expose the local server running on port 5678 to the internet. In your terminal, run the following command:\nRun the following command in a new terminal. Replace  with the URL that you got from the previous step.\nFollow the Using OAuth2 instructions to configure your credentials, using this URL as your Redirect URL."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ghost.md",
    "content": "Ghost credentials\nYou can use these credentials to authenticate the following nodes:\nGhost\nPrerequisites\nCreate a Ghost account.\nSupported authentication methods\nAdmin API key\nContent API key\nThe keys are generated following the same steps, but the authorization flows and key format are different, so n8n stores the credentials separately. The Content API uses an API key; the Admin API uses an API key to generate a token for authentication.\nRelated resources\nRefer to Ghost's Admin API documentation for more information about the Admin API service. Refer to Ghost's Content API documentation for more information about the Content API service.\nUsing Admin API key\nTo configure this credential, you'll need:\nThe URL of your Ghost admin domain. Your admin domain can be different to your main domain and may include a subdirectory. All Ghost(Pro) blogs have a *.ghost.io domain as their admin domain and require https.\nAn API Key: To generate a new API key, create a new Custom Integration. Refer to the Ghost Admin API Token Authentication Key documentation for more detailed instructions. Copy the Admin API Key and use this as the API Key in the Ghost Admin n8n credential.\nUsing Content API key\nTo configure this credential, you'll need:\nThe URL of your Ghost admin domain. Your admin domain can be different to your main domain and may include a subdirectory. All Ghost(Pro) blogs have a *.ghost.io domain as their admin domain and require https.\nAn API Key: To generate a new API key, create a new Custom Integration. Refer to the Ghost Content API Key documentation for more detailed instructions. Copy the Content API Key and use this as the API Key in the Ghost Content n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\git.md",
    "content": "Git credentials\nYou can use these credentials to authenticate the following nodes:\nGit\nPrerequisites\nCreate an account on GitHub, GitLab, or similar platforms for use with Git.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Git's documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA Username for GitHub, GitLab, or a similar platform\nA Password for GitHub, GitLab, or a similar platform"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\github.md",
    "content": "GitHub credentials\nYou can use these credentials to authenticate the following nodes:\nGitHub\nGitHub Trigger\nGitHub Document Loader: this node doesn't support OAuth.\nPrerequisites\nCreate a GitHub account.\nSupported authentication methods\nAPI access token: Use this method with any GitHub nodes.\nOAuth2: Use this method with GitHub and GitHub Trigger nodes only; don't use with GitHub Document Loader.\nRelated resources\nRefer to GitHub's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need a GitHub account.\nThere are two steps to setting up this credential:\nGenerate a GitHub personal access token.\nSet up the credential.\nRefer to the sections below for detailed instructions.\nGenerate personal access token\nTo generate your personal access token:\nIf you haven't done so already, verify your email address with GitHub. Refer to Verifying your email address for more information.\nOpen your GitHub profile Settings.\nIn the left navigation, select Developer settings.\nIn the left navigation, under Personal access tokens, select Tokens (classic).\nSelect Generate new token > Generate new token (classic).\nEnter a descriptive name for your token in the Note field, like n8n integration.\nSelect the Expiration you'd like for the token, or select No expiration.\nSelect Scopes for your token. For most of the n8n GitHub nodes, add the repo scope.\nA token without assigned scopes can only access public information.\nRefer to\nSelect Generate token.\nCopy the token.\nRefer to Creating a personal access token (classic) for more information. Refer to Scopes for OAuth apps for more information on GitHub scopes.\nSet up the credential\nThen, in your n8n credential:\nIf you aren't using GitHub Enterprise Server, don't change the GitHub server URL.\nIf you're using GitHub Enterprise Server, update GitHub server to match the URL for your server.\nEnter your User name as it appears in your GitHub profile.\nEnter the Access Token you generated above.\nUsing OAuth2\nIf you're self-hosting n8n, create a new GitHub OAuth app:\nOpen your GitHub profile Settings.\nIn the left navigation, select Developer settings.\nIn the left navigation, select OAuth apps.\nSelect New OAuth App.\nIf you haven't created an app before, you may see Register a new application instead. Select it.\nEnter an Application name, like n8n integration.\nEnter the Homepage URL for your app's website.\nIf you'd like, add the optional Application description, which GitHub displays to end-users.\nFrom n8n, copy the OAuth Redirect URL and paste it into the GitHub Authorization callback URL.\nSelect Register application.\nCopy the Client ID and Client Secret this generates and add them to your n8n credential.\nRefer to the GitHub Authorizing OAuth apps documentation for more information on the authorization process."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\gitlab.md",
    "content": "GitLab credentials\nYou can use these credentials to authenticate the following nodes:\nGitLab\nGitLab Trigger\nSupported authentication methods\nAPI access token\nOAuth2 (Recommended)\nRelated resources\nRefer to GitLab's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need a GitLab account and:\nThe URL of your GitLab Server\nAn Access Token\nTo set up the credential:\nIn GitLab, select your avatar, then select Edit profile.\nIn the left sidebar, select Access tokens.\nSelect Add new token.\nEnter a Name for the token, like n8n integration.\nEnter an expiry date for the token. If you don't enter an expiry date, GitLab automatically sets it to 365 days later than the current date.\nThe token expires on that expiry date at midnight UTC.\nSelect the desired Scopes. For the GitLab node, use the api scope to easily grant access for all the node's functionality. Or refer to Personal access token scopes to select scopes for the functions you want to use.\nSelect Create personal access token.\nCopy the access token this creates and enter it in your n8n credential as the Access Token.\nEnter the URL of your GitLab Server in your n8n credential.\nRefer to GitLab's Create a personal access token documentation for more information.\nUsing OAuth2\nIf you're self-hosting n8n, you'll need a GitLab account. Then create a new GitLab application:\nIn GitLab, select your avatar, then select Edit profile.\nIn the left sidebar, select Applications.\nSelect Add new application.\nEnter a Name for your application, like n8n integration.\nIn n8n, copy the OAuth Redirect URL. Enter it as the GitLab Redirect URI.\nSelect the desired Scopes. For the GitLab node, use the api scope to easily grant access for all the node's functionality. Or refer to Personal access token scopes to select scopes for the functions you want to use.\nSelect Save application.\nCopy the Application ID and enter it as the Client ID in your n8n credential.\nCopy the Secret and enter it as the Client Secret in your n8n credential.\nRefer to GitLab's Configure GitLab as an OAuth 2.0 authentication identity provider documentation for more information. Refer to the GitLab OAuth 2.0 identity provider API documentation for more information on OAuth2 and GitLab."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\gong.md",
    "content": "Gong credentials\nYou can use these credentials to authenticate the following nodes:\nGong\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to Gong's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need a Gong account and:\nAn Access Key\nAn Access Key Secret\nYou can create both of these items on the Gong API Page (you must be a technical administrator in Gong to access this resource).\nRefer to Gong's API documentation for more information about authenticating to the service.\nUsing OAuth2\nTo configure this credential, you'll need a Gong account, a Gong developer account and:\nA Client ID: Generated when you create an Oauth app for Gong.\nA Client Secret: Generated when you create an Oauth app for Gong.\nIf you're self-hosting n8n, you'll need to create an app to configure OAuth2. Refer to Gong's OAuth documentation for more information about setting up OAuth2."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\googleai.md",
    "content": "Google Gemini(PaLM) credentials\nYou can use these credentials to authenticate the following nodes:\nEmbeddings Google Gemini\nGoogle Gemini\nGoogle Gemini Chat Model\nEmbeddings Google PaLM\nPrerequisites\nCreate a Google Cloud account.\nCreate a Google Cloud Platform project.\nSupported authentication methods\nGemini(PaLM) API key\nRelated resources\nRefer to Google's Gemini API documentation for more information about the service.\nUsing Gemini(PaLM) API key\nTo configure this credential, you'll need:\nThe API Host URL: Both PaLM and Gemini use the default\nAn API Key: Create a key in Google AI Studio.\nTo create an API key:\nGo to the API Key page in Google AI Studio: [).\nSelect Create API Key.\nYou can choose whether to Create API key in new project or search for an existing Google Cloud project to Create API key in existing project.\nCopy the generated API key and add it to your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\gotify.md",
    "content": "Gotify credentials\nYou can use these credentials to authenticate the following nodes:\nGotify\nPrerequisites\nInstall Gotify on your server.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Gotify's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn App API Token: Only required if you'll use this credential to create messages. To generate an App API token, create an application from the Apps menu. Refer to Gotify's Push messages documentation for more information.\nA Client API Token: Required for all actions other than creating messages (such as deleting or retrieving messages). To generate a Client API token, create a client from the Clients menu.\nThe URL of the Gotify host"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\gotowebinar.md",
    "content": "GoTo Webinar credentials\nYou can use these credentials to authenticate the following nodes:\nGoToWebinar\nPrerequisites\nCreate a GoToWebinar account with Developer Center access.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to GoToWebinar's API documentation for more information about authenticating with the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Provided once you create an OAuth client\nA Client Secret: Provided once you create an OAuth client\nRefer to the Create an OAuth client documentation for detailed instructions on creating an OAuth client. Copy the OAuth Callback URL from n8n to use as the Redirect URI in your OAuth client. The Client ID and Client secret are provided once you've finished setting up your client."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\grafana.md",
    "content": "Grafana credentials\nYou can use these credentials to authenticate the following nodes:\nGrafana\nPrerequisites\nCreate a Grafana account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Grafana's API documentation for more information about authenticating with the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Create an API key documentation for detailed instructions on creating an API key.\nThe Base URL for your Grafana instance, for example:"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\grist.md",
    "content": "Grist credentials\nYou can use these credentials to authenticate the following nodes:\nGrist\nPrerequisites\nCreate a Grist account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Grist's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Grist API authentication documentation for instructions on creating an API key.\nTo select your Grist Plan Type. Options include:\nFree\nPaid: If selected, provide your Grist Custom Subdomain. This is the portion that comes before .getgrist.com. For example, if our full Grist domain was n8n.getgrist.com, we'd enter n8n here.\nSelf-Hosted: If selected, provide your Grist Self-Hosted URL. This should be the full URL."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\groq.md",
    "content": "Groq credentials\nYou can use these credentials to authenticate the following nodes:\nGroq Chat Model\nPrerequisites\nCreate a Groq account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Groq's documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nTo get your API key:\nGo to the API Keys page of your Groq console.\nSelect Create API Key.\nEnter a display name for the key, like n8n integration, and select Submit.\nCopy the key and paste it into your n8n credential.\nRefer to Groq's API Keys documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\gumroad.md",
    "content": "Gumroad credentials\nYou can use these credentials to authenticate the following nodes:\nGumroad Trigger\nPrerequisites\nCreate a Gumroad account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Gumroad's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn API Access Token: Create an application to generate an access token. Refer to the Gumroad Create an application for the API documentation for detailed instructions on creating a new application and generating an access token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\halopsa.md",
    "content": "HaloPSA credentials\nYou can use these credentials to authenticate the following nodes:\nHaloPSA\nPrerequisites\nCreate a HaloPSA account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to HaloPSA's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nTo select your Hosting Type:\nOn Premise Solution: Choose this option if you're hosting the Halo application on your own server\nHosted Solution Of Halo: Choose this option if your application is hosted by Halo. If this option is selected, you'll need to provide your Tenant.\nThe HaloPSA Authorisation Server URL: Your Authorisation Server URL is displayed within HaloPSA in Configuration > Integrations > Halo API in API Details.\nThe Resource Server URL: Your Resource Server is displayed within HaloPSA in Configuration > Integrations > Halo API in API Details.\nA Client ID: Obtained by registering the application in the Halo API settings. Refer to HaloPSA's Authorisation documentation for detailed instructions. n8n recommends using these settings:\nChoose Client Credentials as your Authentication Method.\nUse the all permission.\nA Client Secret: Obtained by registering the application in the Halo API settings.\nYour Tenant name: If Hosted Solution of Halo is selected as the Hosting Type, you must provide your tenant name. Your tenant name is displayed within HaloPSA in Configuration > Integrations > Halo API in API Details.\nHaloPSA uses both the application permissions and the agent's permissions to determine API access."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\harvest.md",
    "content": "Harvest credentials\nYou can use these credentials to authenticate the following nodes:\nHarvest\nPrerequisites\nCreate a Harvest account.\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to Harvest's API documentation for more information about the service.\nUsing API Access Token\nTo configure this credential, you'll need:\nA Personal Access Token: Refer to the Harvest Personal Access Token Authentication documentation for instructions on creating a personal access token.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch or need more detail on what's happening in the OAuth web flow, refer to the instructions in the Harvest OAuth2 documentation to set up OAuth."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\helpscout.md",
    "content": "Help Scout credentials\nYou can use these credentials to authenticate the following nodes:\nHelp Scout\nHelp Scout Trigger\nPrerequisites\nCreate a Help Scout account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Help Scout's API documentation for more information about the service.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch or need more detail on what's happening in the OAuth web flow, you'll need to create a Help Scout app. Refer to the instructions in the Help Scout OAuth documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\highlevel.md",
    "content": "HighLevel credentials\nYou can use these credentials to authenticate the following nodes:\nHighLevel node\nPrerequisites\nCreate a HighLevel developer account.\nSupported authentication methods\nAPI key: Use with API v1\nOAuth2: Use with API v2\nRelated resources\nRefer to HighLevel's API 2.0 documentation for more information about the service.\nFor existing integrations with the API v1.0, refer to HighLevel's API 1.0 documentation.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the HighLevel API 1.0 Welcome documentation for instructions on getting your API key.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nTo generate both, create an app in My Apps > Create App. Use these settings:\nSet Distribution Type to Sub-Account.\nAdd these Scopes:\nlocations.readonly\ncontacts.readonly\ncontacts.write\nopportunities.readonly\nopportunities.write\nusers.readonly\nCopy the OAuth Redirect URL from n8n and add it as a Redirect URL in your HighLevel app.\nCopy the Client ID and Client Secret from HighLevel and add them to your n8n credential.\nAdd the same scopes added above to your n8n credential in a space-separated list. For example:\nRefer to HighLevel's API Authorization documentation for more details. Refer to HighLevel's API Scopes documentation for more information about available scopes."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\homeassistant.md",
    "content": "Home Assistant credentials\nYou can use these credentials to authenticate the following nodes:\nHome Assistant\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Home Assistant's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need to Install Home Assistant, create a Home Assistant account, and have:\nYour Host\nThe Port\nA Long-Lived Access Token\nTo generate an access token and set up the credential:\nTo generate your Access Token, log in to Home Assistant and open your User profile.\nIn the Long-Lived Access Tokens section, generate a new token.\nCopy this token and enter it in n8n as your Access Token.\nEnter the URL or IP address of your Home Assistant Host, without the http:// or https:// protocol, for example your.awesome.home.\nFor the Port, enter the appropriate port:\nIf you've made no port changes and access Home Assistant at  keep the default of 8123.\nIf you've made no port changes and access Home Assistant at  enter 443.\nIf you've configured Home Assistant to use a specific port, enter that port.\nIf you've enabled SSL in Home Assistant in the config.yml map key, turn on the SSL toggle in n8n. If you're not sure, it's best to turn this setting on if you access your home assistant UI using https:// instead of"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\httprequest.md",
    "content": "HTTP Request credentials\nYou can use these credentials to authenticate the following nodes:\nHTTP Request\nHTTP Request Tool (legacy)\nPrerequisites\nYou must use the authentication method required by the app or service you want to query.\nIf you need to secure the authentication with an SSL certificate, refer to Provide an SSL certificate for the information you'll need.\nSupported authentication methods\nPredefined credential type\nBasic auth (generic credential type)\nCustom auth (generic credential type)\nDigest auth (generic credential type)\nHeader auth (generic credential type)\nBearer auth (generic credential type)\nOAuth1 (generic credential type)\nOAuth2 (generic credential type)\nQuery auth (generic credential type)\nRefer to HTTP authentication for more information relating to generic credential types.\nUsing predefined credential type\nRefer to Custom API operations for more information.\nUsing OAuth1\nUse this generic authentication if your app or service supports OAuth1 authentication.\nTo configure this credential, enter:\nAn Authorization URL: Also known as the Resource Owner Authorization URI. This URL typically ends in /oauth1/authorize. The temporary credentials are sent here to prompt a user to complete authorization.\nAn Access Token URL: This is the URI used for the initial request for temporary credentials. This URL typically ends in /oauth1/request or /oauth1/token.\nA Consumer Key: Also known as the client key, like a username. This specifies the oauth_consumer_key to use for the call.\nA Consumer Secret: Also known as the client secret, like a password.\nA Request Token URL: This is the URI used to switch from temporary credentials to long-lived credentials after authorization. This URL typically ends in /oauth1/access.\nSelect the Signature Method the auth handshake uses. This specifies the oauth_signature_method to use for the call. Options include:\nHMAC-SHA1\nHMAC-SHA256\nHMAC-SHA512\nFor most OAuth1 integrations, you'll need to configure an app, service, or integration to generate the values for most of these fields. Use the OAuth Redirect URL in n8n as the redirect URL or redirect URI for such a service.\nRead more about OAuth1 and the OAuth1 authorization flow.\nUsing OAuth2\nUse this generic authentication if your app or service supports OAuth2 authentication.\nRequirements to configure this credential depend on the Grant Type selected. Refer to OAuth Grant Types for more information on each grant type.\nFor most OAuth2 integrations, you'll need to configure an app, service, or integration. Use the OAuth Redirect URL in n8n as the redirect URL or redirect URI for such a service.\nRead more about OAuth2.\nAuthorization Code grant type\nUse Authorization Code grant type to exchange an authorization code for an access token. The auth flow uses the redirect URL to return the user to the client. Then the application gets the authorization code from the URL and uses it to request an access token. Refer to Authorization Code Request for more information.\nTo configure this credential, select Authorization Code as the Grant Type.\nThen enter:\nAn Authorization URL\nAn Access Token URL\nA Client ID: The ID or username to log in with.\nA Client Secret: The secret or password used to log in with.\nOptional: Enter one or more Scopes for the credential. If unspecified, the credential will request all scopes available to the client.\nOptional: Some services require more query parameters. If your service does, add them as Auth URI Query Parameters.\nAn Authentication type: Select the option that best suits your use case. Options include:\nHeader: Send the credentials as a basic auth header.\nBody: Send the credentials in the body of the request.\nOptional: Choose whether to Ignore SSL Issues. If turned on, n8n will connect even if SSL validation fails.\nClient Credentials grant type\nUse the Client Credentials grant type when applications request an access token to access their own resources, not on behalf of a user. Refer to Client Credentials for more information.\nTo configure this credential, select Client Credentials as the Grant Type.\nThen enter:\nAn Access Token URL: The URL to hit to begin the OAuth2 flow. Typically this URL ends in /token.\nA Client ID: The ID or username to use to log in to the client.\nA Client Secret: The secret or password used to log in to the client.\nOptional: Enter one or more Scopes for the credential. Most services don't support scopes for Client Credentials grant types; only enter scopes here if yours does.\nAn Authentication type: Select the option that best suits your use case. Options include:\nHeader: Send the credentials as a basic auth header.\nBody: Send the credentials in the body of the request.\nOptional: Choose whether to Ignore SSL Issues. If turned on, n8n will connect even if SSL validation fails.\nPKCE grant type\nProof Key for Code Exchange (PKCE) grant type is an extension to the Authorization Code flow to prevent CSRF and authorization code injection attacks.\nTo configure this credential, select PKCE as the Grant Type.\nThen enter:\nAn Authorization URL\nAn Access Token URL\nA Client ID: The ID or username to log in with.\nA Client Secret: The secret or password used to log in with.\nOptional: Enter one or more Scopes for the credential. If unspecified, the credential will request all scopes available to the client.\nOptional: Some services require more query parameters. If your service does, add them as Auth URI Query Parameters.\nAn Authentication type: Select the option that best suits your use case. Options include:\nHeader: Send the credentials as a basic auth header.\nBody: Send the credentials in the body of the request.\nOptional: Choose whether to Ignore SSL Issues. If turned on, n8n will connect even if SSL validation fails.\nUsing query auth\nUse this generic authentication if your app or service supports passing authentication as a single key/value query parameter. (For multiple query parameters, use Custom Auth.)\nTo configure this credential, enter:\nA query parameter key or Name\nA query parameter Value\nUsing custom auth\nUse this generic authentication if your app or service supports passing authentication as multiple key/value query parameters or you need more flexibility than the other generic auth options.\nThe Custom Auth credential expects JSON data to define your credential. You can use headers, qs, body or a mix. Review the examples below to get started.\nSending two headers\nBody\nQuery string\nSending header and query string\nProvide an SSL certificate\nYou can send an SSL certificate with your HTTP request. Create the SSL certificate as a separate credential for use by the node:\nIn the HTTP Request node Settings, turn on SSL Certificates.\nOn the Parameters tab, add an existing SSL Certificate credential to Credential for SSL Certificates or create a new one.\nTo configure your SSL Certificates credential, you'll need to add:\nThe Certificate Authority CA bundle\nThe Certificate (CRT): May also appear as a Public Key, depending on who your issuing CA was and how they format the cert\nThe Private Key (KEY)\nOptional: If the Private Key is encrypted, enter a Passphrase for the private key.\nIf your SSL certificate is in a single file (such as a .pfx file), you'll need to open the file to copy details from it to paste into the appropriate fields:\nEnter the Public Key/CRT as the Certificate\nEnter the Private Key/KEY in that field"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\hubspot.md",
    "content": "HubSpot credentials\nYou can use these credentials to authenticate the following nodes:\nHubSpot\nHubSpot Trigger\nSupported authentication methods\nApp token: Use with the HubSpot node.\nDeveloper API key: Use with the HubSpot Trigger node.\nOAuth2: Use with the HubSpot node.\nRelated resources\nRefer to HubSpot's API documentation for more information about the service. The HubSpot Trigger node uses the Webhooks API; refer to HubSpot's Webhooks API documentation for more information about that service.\nUsing App token\nTo configure this credential, you'll need a HubSpot account or HubSpot developer account and:\nAn App Token\nTo generate an app token, create a private app in HubSpot:\nIn your HubSpot account, select the settings icon in the main navigation bar.\nIn the left sidebar menu, go to Integrations > Private Apps.\nSelect Create private app.\nOn the Basic Info tab, enter your app's Name.\nHover over the placeholder logo and select the upload icon to upload a square image that will serve as the logo for your app.\nEnter a Description for your app.\nOpen the Scopes tab and add the appropriate scopes. Refer to Required scopes for HubSpot node for a complete list of scopes you should add.\nSelect Create app to finish the process.\nIn the modal, review the info about your app's access token, then select Continue creating.\nOnce your app's created, open the Access token card and select Show token to reveal the token.\nCopy this token and enter it in your n8n credential.\nRefer to the HubSpot Private Apps documentation for more information.\nUsing Developer API key\nTo configure this credential, you'll need a HubSpot developer account and:\nA Client ID: Generated once you create a public app.\nA Client Secret: Generated once you create a public app.\nA Developer API Key: Generated from your Developer Apps dashboard.\nAn App ID: Generated once you create a public app.\nTo create the public app and set up the credential:\nLog into your HubSpot app developer account.\nSelect Apps from the main navigation bar.\nSelect Get HubSpot API key. You may need to select the option to Show key.\nCopy the key and enter it in n8n as the Developer API Key.\nStill on the HubSpot Apps page, select Create app.\nOn the App Info tab, add an App name, Description, Logo, and any support contact info you want to provide. Anyone encountering the app would see these.\nOpen the Auth tab.\nCopy the App ID and enter it in n8n.\nCopy the Client ID and enter it in n8n.\nCopy the Client Secret and enter it in n8n.\nIn the Scopes section, select Add new scope.\nAdd all the scopes listed in Required scopes for HubSpot Trigger node to your app.\nSelect Update.\nCopy the n8n OAuth Redirect URL and enter it as the Redirect URL in your HubSpot app.\nSelect Create app to finish creating the HubSpot app.\nRefer to the HubSpot Public Apps documentation for more detailed instructions.\nRequired scopes for HubSpot Trigger node\nIf you're creating an app for use with the HubSpot Trigger node, n8n recommends starting with these scopes:\nTABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\huggingface.md",
    "content": "Hugging Face credentials\nYou can use these credentials to authenticate the following nodes:\nHugging Face Inference\nEmbeddings Hugging Face Inference\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Hugging Face's documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a Hugging Face account and:\nAn API Key: Hugging Face calls these API tokens.\nTo get your API token:\nOpen your Hugging Face profile and go to the Tokens section.\nCopy the token listed there. It should begin with hf_.\nEnter this API token as your n8n credential API Key.\nRefer to Get your API token for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\humanticai.md",
    "content": "Humantic AI credentials\nYou can use these credentials to authenticate the following nodes:\nHumantic AI\nPrerequisites\nCreate a Humantic AI account.\nYou can also try out an API key as a free trial at the Humantic AI API page.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Humantic AI's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Get an API key from the Humantic AI API page."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\hunter.md",
    "content": "Hunter credentials\nYou can use these credentials to authenticate the following nodes:\nHunter\nPrerequisites\nCreate a Hunter account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Hunter's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate an API key from your profile in the dashboard. Refer to the Hunter API Authentication documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\hybridanalysis.md",
    "content": "Hybrid Analysis credentials\nPrerequisites\nCreate a Hybrid Analysis account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Hybrid Analysis' API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Hybrid Analysis' API documentation for instructions on generating an API key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\impervawaf.md",
    "content": "Imperva WAF credentials\nPrerequisites\nCreate an Imperva WAF account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Imperva WAF's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API ID\nAn API Key\nRefer to Imperva WAF's API Key Management documentation for instructions on generating and viewing API Keys and IDs."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\index.md",
    "content": "Credentials library\nThis section contains step-by-step information about authenticating the different nodes in n8n.\nTo learn more about creating, managing, and sharing credentials, refer to Manage credentials."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\intercom.md",
    "content": "Intercom credentials\nYou can use these credentials to authenticate the following nodes:\nIntercom\nPrerequisites\nCreate an Intercom developer account.\nCreate an app in your developer hub.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Intercom's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Intercom automatically generates an Access Token when you create an app. Use this Access Token as your n8n API Key. Refer to How to get your Access Token for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\invoiceninja.md",
    "content": "Invoice Ninja credentials\nYou can use these credentials to authenticate the following nodes:\nInvoice Ninja\nInvoice Ninja Trigger\nPrerequisites\nCreate an Invoice Ninja account. Only the Pro and Enterprise plans support API integrations.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Invoice Ninja's v4 API documentation and v5 API documentation for more information about the APIs.\nUsing API key\nTo configure this credential, you'll need:\nA URL: If Invoice Ninja hosts your installation, use either of the default URLs mentioned. If you're self-hosting your installation, use the URL of your Invoice Ninja instance.\nAn API Token: Generate an API token in Settings > Account Management > API Tokens.\nAn optional Secret, available only for v5 API users"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\iterable.md",
    "content": "Iterable credentials\nYou can use these credentials to authenticate the following nodes:\nIterable\nPrerequisites\nCreate an Iterable account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Iterable's API documentation for more information about the service:\nUS-based Iterable projects\nEurope-based Iterable projects\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to Iterable's Creating API keys documentation for instructions on creating API keys."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\jenkins.md",
    "content": "Jenkins credentials\nYou can use these credentials to authenticate the following nodes:\nJenkins\nPrerequisites\nCreate an account on a Jenkins instance.\nSupported authentication methods\nAPI token\nRelated resources\nJenkins doesn't provide public API documentation; API documentation for each page is available from the user interface in the bottom right. Refer to those detailed pages for more information about the service. Refer to Jenkins Remote Access API for information on the API and API wrappers.\nUsing API token\nTo configure this credential, you'll need:\nThe Jenkins Username: For the user whom the token belongs to\nA Personal API Token: Generate this from the user's profile details > Configure > Add new token. Refer to these Stack Overflow instructions for more detail.\nThe Jenkins Instance URL\nJenkins rebuilt their API token setup in 2018. If you're working with an older Jenkins instance, be sure you're using a non-legacy API token. Refer to Security Hardening: New API token system in Jenkins 2.129+ for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\jinaai.md",
    "content": "Jina AI credentials\nYou can use these credentials to authenticate the following nodes:\nJina AI\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Jina AI's reader API documentation and Jina AI's search API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAPI key: A Jina AI API key. You can get your free API key without creating an account by doing the following:\nVisit the Jina AI website.\nSelect API on the page.\nSelect API KEY & BILLING in the API app widget.\nCopy the key labeled \"This is your unique key. Store it securely!\".\nJina AI API keys start with 10 million free tokens that you can use non-commercially. To top up your key or use commercially, scroll on the API KEY & BILLING tab of the API widget and select the top up option that best fits your needs."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\jira.md",
    "content": "Jira credentials\nYou can use these credentials to authenticate the following nodes:\nJira\nJira Trigger\nPrerequisites\nCreate a Jira Software Cloud or Server account.\nSupported authentication methods\nSW Cloud API token: Use this method with Jira Software Cloud.\nSW Server account: Use this method with Jira Software Server.\nRelated resources\nRefer to Jira's API documentation for more information about the service.\nUsing SW Cloud API token\nTo configure this credential, you'll need an account on Jira Software Cloud.\nThen:\nLog in to your Atlassian profile > Security > API tokens page, or jump straight there using this link.\nSelect Create API Token.\nEnter a good Label for your token, like n8n integration.\nSelect Create.\nCopy the API token.\nIn n8n, enter the Email address associated with your Jira account.\nPaste the API token you copied as your API Token.\nEnter the Domain you access Jira on, for example\nRefer to Manage API tokens for your Atlassian account for more information.\nUsing SW Server account\nTo configure this credential, you'll need an account on Jira Software Server.\nThen:\nEnter the Email address associated with your Jira account.\nEnter your Jira account Password.\nEnter the Domain you access Jira on."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\jotform.md",
    "content": "JotForm credentials\nYou can use these credentials to authenticate the following nodes:\nJotForm Trigger\nSupported authentication methods\nAPI key\nRelated resources\nRefer to JotForm's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a JotForm account and:\nAn API Key\nThe API Domain\nTo set it up:\nGo to Settings > API.\nSelect Create New Key.\nSelect the Name in JotForm to update the API key name to something meaningful, like n8n integration.\nCopy the API Key and enter it in your n8n credential.\nIn n8n, select the API Domain that applies to you based on the forms you're using:\napi.jotform.com: Use this unless the other form types apply to you.\neu-api.jotform.com: Select this if you're using JotForm EU Safe Forms.\nhipaa-api.jotform.com: Select this if you're using JotForm HIPAA forms.\nRefer to the JotForm API documentation for more information on creating keys and API domains."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\jwt.md",
    "content": "JWT credentials\nYou can use these credentials to authenticate the following nodes:\nJWT\nWebhook\nSupported authentication methods\nPassphrase: Signed with a secret with HMAC algorithm\nPrivate key (PEM key): For use with Private Key JWT with RSA or ECDSA algorithm\nRelated resources\nRefer to the JSON Web Token spec for more details.\nFor a more verbose introduction, refer to the JWT website Introduction to JSON Web Tokens. Refer to JSON Web Token (JWT) Signing Algorithms Overview for more information on selecting between the two types and the algorithms involved.\nUsing Passphrase\nTo configure this credential:\nSelect the Key Type of Passphrase.\nEnter the Passphrase Secret\nSelect the Algorithm used to sign the assertion. Refer to Available algorithms below for a list of supported algorithms.\nUsing private key (PEM key)\nTo configure this credential:\n1. Select the Key Type of PEM Key.\n2. A Private Key: Obtained from generating a Key Pair. Refer to Generate RSA Key Pair for an example.\n3. A Public Key: Obtained from generating a Key Pair. Refer to Generate RSA Key Pair for an example.\n4. Select the Algorithm used to sign the assertion. Refer to Available algorithms below for a list of supported algorithms.\nAvailable algorithms\nThis n8n credential supports the following algorithms:\nHS256\nHS384\nHS512\nRS256\nRS384\nRS512\nES256\nES384\nES512\nPS256\nPS384\nPS512\nnone"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\kafka.md",
    "content": "Kafka credentials\nYou can use these credentials to authenticate the following nodes:\nKafka\nKafka Trigger\nSupported authentication methods\nClient ID\nRelated resources\nRefer to Kafka's documentation for more information about using the service.\nIf you're new to Kafka, refer to the Apache Kafka Quickstart for initial setup.\nRefer to Encryption and Authentication using SSL for working with SSL in Kafka.\nUsing client ID\nTo configure this credential, you'll need a running Kafka environment and:\nA Client ID\nA list of relevant Brokers\nUsername/password authentication details if your Kafka environment uses authentication\nTo set it up:\nEnter the CLIENT-ID of the client or consumer group in the Client ID field in your credential.\nEnter a comma-separated list of relevant Brokers for the credential to use in the format :. Use the name you gave the broker when you defined it in the services list. For example, kafka-1:9092,kafka-2:9092 would add the brokers kafka-1 and kafka-2 on port 9092.\nIf your Kafka environment doesn't use SSL, turn off the SSL toggle.\nIf you've enabled authentication using SASL in your Kafka environment, turn on the Authentication toggle. Then add:\nThe Username\nThe Password\nSelect the broker's configured SASL Mechanism. Refer to SASL configuration for more information. Options include:\nPlain\nscram-sha-256\nscram-sha-512"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\keap.md",
    "content": "Keap credentials\nYou can use these credentials to authenticate the following nodes:\nKeap\nKeap Trigger\nPrerequisites\nCreate a Keap developer account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Keap's REST API documentation for more information about the service.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch or need more detail on what's happening in the OAuth web flow, refer to the instructions in the Getting Started with OAuth2 documentation."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\kibana.md",
    "content": "Kibana credentials\nPrerequisites\nCreate an Elasticsearch account.\nIf you're creating a new account to test with, load some sample data into Kibana. Refer to the Kibana quick start for more information.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Kibana's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing basic auth\nTo configure this credential, you'll need:\nThe URL you use to access Kibana, for example\nA Username: Use the same username that you use to log in to Elastic.\nA Password: Use the same password that you use to log in to Elastic."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\kitemaker.md",
    "content": "Kitemaker credentials\nYou can use these credentials to authenticate the following nodes:\nKitemaker\nPrerequisites\nCreate a Kitemaker account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Kitemaker's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nA Personal Access Token: Generate a personal access token from Manage > Developer settings. Refer to API Authentication for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\kobotoolbox.md",
    "content": "KoboToolbox credentials\nYou can use these credentials to authenticate the following nodes:\nKoboToolbox trigger\nKoboToolbox\nPrerequisites\nCreate a KoboToolbox account.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to KoboToolbox's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API Root URL: Enter the URL of the KoboToolbox server where you created your account. For the Global KoboToolbox Server, use  For the European Union KoboToolbox Server, use\nAn API Token: Displayed in your Account Settings. Refer to Getting your API token for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ldap.md",
    "content": "LDAP credentials\nYou can use these credentials to authenticate the following nodes:\nLDAP\nPrerequisites\nCreate a server directory using Lightweight Directory Access Protocol (LDAP).\nSome common LDAP providers include:\nJumpcloud\nAzure ADDS\nOkta\nSupported authentication methods\nLDAP server details\nRelated resources\nRefer to your LDAP provider's own documentation for detailed information.\nFor general LDAP information, refer to Basic LDAP concepts for a basic overview and The LDAP Bind Operation for information on how the bind operation and authentication work.\nUsing LDAP server details\nTo configure this credential, you'll need:\nThe LDAP Server Address: Use the IP address or domain of your LDAP server.\nThe LDAP Server Port: Use the number of the port used to connect to the LDAP server.\nThe Binding DN: Use the Binding Distinguished Name (Bind DN) for your LDAP server. This is the user account the credential should log in as. If you're using Active Directory, this may look something like cn=administrator, cn=Users, dc=n8n, dc=io. Refer to your LDAP provider's documentation for more information on identifying this DN and the related password.\nThe Binding Password: Use the password for the Binding DN user.\nSelect the Connection Security: Options include:\nNone\nTLS\nSTARTTLS\nOptional: Enter a numeric value in seconds to set a Connection Timeout."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\lemlist.md",
    "content": "Lemlist credentials\nYou can use these credentials to authenticate the following nodes:\nLemlist\nLemlist Trigger\nPrerequisites\nCreate an account on a Lemlist instance.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Lemlist's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Access your API key in Settings > Integrations. Refer to the API Authentication documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\line.md",
    "content": "Line credentials\nYou can use these credentials to authenticate the following nodes:\nLine\nSupported authentication methods\nNotify OAuth2\nRelated resources\nRefer to Line Notify's API documentation for more information about the service.\nUsing Notify OAuth2\nTo configure this credential, you'll need a Line account and:\nA Client ID\nA Client Secret\nTo generate both, connect Line with Line Notify. Then:\nOpen the Line Notify page to add a new service.\nEnter a Service name. This name displays when someone tries to connect to the service.\nEnter a Service description.\nEnter a Service URL\nEnter your Company/Enterprise.\nSelect your Country/region.\nEnter your name or team name as the Representative.\nEnter a valid Email address. Line will verify this email address before the service is fully registered. Use an email address you have ready access to.\nCopy the OAuth Redirect URL from your n8n credential and enter it as the Callback URL in Line Notify.\nSelect Agree and continue to agree to the terms of service.\nVerify the information you entered is correct and select Add.\nCheck your email and open the Line Notify Registration URL to verify your email address.\nOnce verification is complete, open My services.\nSelect the service you just added.\nCopy the Client ID and enter it in your n8n credential.\nSelect the option to Display the Client Secret. Copy the Client Secret and enter it in your n8n credential.\nIn n8n, select Connect my account and follow the on-screen prompts to finish the credential.\nRefer to the Authentication section of Line Notify's API documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\linear.md",
    "content": "Linear credentials\nYou can use these credentials to authenticate the following nodes:\nLinear Trigger\nLinear\nPrerequisites\nCreate a Linear account.\nSupported authentication methods\nAPI key\nOAuth2\nRelated resources\nRefer to Linear's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA personal API Key: Create an API key in your Settings > API. Refer to the Linear Personal API keys documentation for more information.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you create a new OAuth2 application.\nA Client Secret: Generated when you create a new OAuth2 application.\nSelect the Actor: The actor defines how the OAuth2 application should create issues, comments and other changes. Options include:\nUser (Linear's default): The application creates resources as the authorizing user. Use this option if you want each user to do their own authentication.\nApplication: The application creates resources as itself. Use this option if you have only one user (like an admin) authorizing the application.\nTo use this credential with the Linear Trigger node, you must enable the Include Admin Scope toggle.\nRefer to the Linear OAuth2 Authentication documentation for more detailed instructions and explanations. Use the n8n OAuth Redirect URL as the Redirect callback URL in your Linear OAuth2 application."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\lingvanex.md",
    "content": "LingvaNex credentials\nYou can use these credentials to authenticate the following nodes:\nLingvaNex\nPrerequisites\nCreate a LingvaNex account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Lingvanex's Cloud API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate an API key from your Account page. Refer to Where can I get the authorization key? for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\linkedin.md",
    "content": "LinkedIn credentials\nYou can use these credentials to authenticate the following nodes:\nLinkedIn\nPrerequisites\nCreate a LinkedIn account.\nCreate a LinkedIn Company Page.\nSupported authentication methods\nCommunity Management OAuth2: Use this method if you're a new LinkedIn user or creating a new LinkedIn app.\nOAuth2: Use this method for older LinkedIn apps and user accounts.\nRelated Resources\nRefer to LinkedIn's Community Management API documentation for more information about the service.\nThis credential works with API version 202404.\nUsing Community Management OAuth2\nUse this method if you're a new LinkedIn user or creating a new LinkedIn app.\nTo configure this credential, you'll need a LinkedIn account, a LinkedIn Company Page, and:\nA Client ID: Generated after you create a new developer app.\nA Client Secret: Generated after you create a new developer app.\nTo create a new developer app and set up the credential:\nLog into LinkedIn and select this link to create a new developer app.\nEnter an App name for your app, like n8n integration.\nFor the LinkedIn Page, enter a LinkedIn Company Page or use the Create a new LinkedIn Page link to create one on-the-fly. Refer to Associate an App with a LinkedIn Page for more information.\nAdd an App logo.\nCheck the box to agree to the Legal agreement.\nSelect Create app.\nThis should open the Products tab. Select the products/APIs you want to enable for your app. For the LinkedIn node to work properly, you must include:\nShare on LinkedIn\nSign In with LinkedIn using OpenID Connect\nOnce you've requested access to the products you need, open the Auth tab.\nCopy the Client ID and enter it in your n8n credential.\nSelect the icon to Copy the Primary Client Secret. Enter this in your n8n credential as the Client Secret.\nRefer to Getting Access to LinkedIn APIs for more information on scopes and permissions.\nUsing OAuth2\nOnly use this method for older LinkedIn apps and user accounts.\nAll users must select:\nOrganization Support: If turned on, the credential requests permission to post as an organization using the w_organization_social scope.\nTo use this option, you must put your app through LinkedIn's Community Management App Review process.\nLegacy: If turned on, the credential uses legacy scopes for r_liteprofile and r_emailaddress instead of the newer profile and email scopes.\nIf you're self-hosting n8n, you'll need to configure OAuth2 from scratch by creating a new developer app:\nLog into LinkedIn and select this link to create a new developer app.\nEnter an App name for your app, like n8n integration.\nFor the LinkedIn Page, enter a LinkedIn Company Page or use the Create a new LinkedIn Page link to create one on-the-fly. Refer to Associate an App with a LinkedIn Page for more information.\nAdd an App logo.\nCheck the box to agree to the Legal agreement.\nSelect Create app.\nThis should open the Products tab. Select the products/APIs you want to enable for your app. For the LinkedIn node to work properly, you must include:\nShare on LinkedIn\nSign In with LinkedIn using OpenID Connect\nOnce you've requested access to the products you need, open the Auth tab.\nCopy the Client ID and enter it in your n8n credential.\nSelect the icon to Copy the Primary Client Secret. Enter this in your n8n credential as the Client Secret.\nRefer to Getting Access to LinkedIn APIs for more information on scopes and permissions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\lonescale.md",
    "content": "LoneScale credentials\nYou can use these credentials to authenticate the following nodes:\nLoneScale\nLoneScale Trigger\nPrerequisites\nCreate a LoneScale account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to LoneScale's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Refer to LoneScale's Generate an API key documentation to generate your key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\magento2.md",
    "content": "Magento 2 credentials\nYou can use these credentials to authenticate the following node:\nMagento 2\nPrerequisites\nCreate a Magento account.\nSet your store to Allow OAuth Access Tokens to be used as standalone Bearer tokens.\nGo to Admin > Stores > Configuration > Services > OAuth > Consumer Settings.\nSet the Allow OAuth Access Tokens to be used as standalone Bearer tokens option to Yes.\nYou can also enable this setting from the CLI by running the following command:\nThis step is necessary until n8n updates the Magento 2 credentials to use OAuth. Refer to Integration Tokens for more information.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Magento's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nA Host: Enter the address of your Magento store.\nAn Access Token: Get an access token from the Admin Panel:\nGo to System > Extensions > Integrations.\nAdd a new Integration.\nGo to the API tab and select the Magento resources you'd like the n8n integration to access.\nFrom the Integrations page, Activate the new integration.\nSelect Allow to display your access token so you can copy it and enter it in n8n."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mailcheck.md",
    "content": "Mailcheck credentials\nYou can use these credentials to authenticate the following nodes:\nMailcheck\nPrerequisites\nCreate a Mailcheck account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Mailcheck's API documentation for more information about the service.\nUsing API Key\nTo configure this credential, you'll need:\nAn API Key: Generate an API Key in the API section of your dashboard. Refer to Mailcheck's How to create an API key documentation for detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mailchimp.md",
    "content": "Mailchimp credentials\nYou can use these credentials to authenticate the following nodes:\nMailchimp\nMailchimp Trigger\nPrerequisites\nCreate a Mailchimp account.\nSupported authentication methods\nAPI key\nOAuth2\nRefer to Selecting an authentication method for guidance on which method to use.\nRelated resources\nRefer to Mailchimp's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate an API key in the API keys section of your Mailchimp account. Refer to Mailchimp's Generate your API key documentation for more detailed instructions.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch, register an application. Refer to the Mailchimp OAuth2 documentation for more information.\nSelecting an authentication method\nMailchimp suggests using an API key if you're only accessing your own Mailchimp account's data:\nUse an API key if you're writing code that tightly couples your application's data to your Mailchimp account's data. If you ever need to access someone else's Mailchimp account's data, you should be using OAuth 2 (source)"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mailerlite.md",
    "content": "MailerLite credentials\nYou can use these credentials to authenticate the following nodes:\nMailerLite\nMailerLite Trigger\nPrerequisites\nCreate a MailerLite account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to MailerLite's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate an API key from the Integrations menu. Refer to the API Authentication documentation for more detailed instructions.\nEnable the Classic API toggle if the API key is for a MailerLite Classic account instead of the newer MailerLite experience."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mailgun.md",
    "content": "Mailgun credentials\nYou can use these credentials to authenticate the following nodes:\nMailgun\nPrerequisites\nCreate a Mailgun account.\nAdd and verify a domain in Mailgun or use the provided sandbox domain for testing.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Mailgun's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Domain: If your Mailgun account is based in Europe, select api.eu.mailgun.net; otherwise, select api.mailgun.net. Refer to Mailgun Base URLs for more information.\nAn Email Domain: Enter the email sending domain you're working with. If you have multiple sending domains, refer to Working with multiple email domains for more information.\nAn API Key: View your API key in Settings > API Keys. Refer to Mailgun's API Authentication documentation for more detailed instructions.\nWorking with multiple email domains\nIf your Mailgun account includes multiple sending domains, create a separate credential for each email domain you're working with."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mailjet.md",
    "content": "Mailjet credentials\nYou can use these credentials to authenticate the following nodes:\nMailjet\nMailjet Trigger\nPrerequisites\nCreate a Mailjet account.\nSupported authentication methods\nEmail API key: For use with Mailjet's Email API\nSMS token: For use with Mailjet's SMS API\nRelated resources\nRefer to Mailjet's Email API documentation and Mailjet's SMS API documentation for more information about each service.\nUsing Email API key\nTo configure this credential, you'll need:\nAn API Key: View and generate API keys in your Mailjet API Key Management page.\nA Secret Key: View your API Secret Keys in your Mailjet API Key Management page.\nOptional: Select whether to use Sandbox Mode for calls made using this credential. When turned on, all API calls use Sandbox mode: the API will still validate the payloads but won't deliver the actual messages. This can be useful to troubleshoot any payload error messages without actually sending messages. Refer to Mailjet's Sandbox Mode documentation for more information.\nFor this credential, you can use either:\nMailjet's primary API key and secret key\nA subaccount API key and secret key\nRefer to Mailjet's How to create a subaccount (or additional API key) documentation for detailed instructions on creating more API keys. Refer to What are subaccounts and how does it help me? page for more information on Mailjet subaccounts and when you might want to use one.\nUsing SMS Token\nTo configure this credential, you'll need:\nAn access Token: Generate a new token from Mailjet's SMS Dashboard. Refer to the SMS API Getting Started guide for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\malcore.md",
    "content": "Malcore credentials\nPrerequisites\nCreate a Malcore account.\nRelated resources\nRefer to Malcore's API documentation for more information about authenticating with the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Get an API Key from your Account > API.\nRefer to Using the Malcore API for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mandrill.md",
    "content": "Mandrill credentials\nYou can use these credentials to authenticate the following nodes:\nMandrill\nPrerequisites\nCreate a Mailchimp Transactional email account\nLog in to Mandrill with your Mailchimp account.\nIf you already have a Mailchimp account with a Standard plan or higher, enable Transactional Emails within that account to use Mandrill.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Mailchimp's Transactional API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate an API key from the Mandrill Settings. Refer to Mailchimp's Generate your API key documentation for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\marketstack.md",
    "content": "Marketstack credentials\nYou can use these credentials to authenticate the following nodes:\nMarketstack\nPrerequisites\nCreate a Marketstack account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Marketstack's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: View and generate API keys in your Marketstack account dashboard.\nSelect whether to Use HTTPS: Make this selection based on your Marketstack account plan level:\nFree plan: Turn off Use HTTPS\nAll other plans: Turn on Use HTTPS"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\matrix.md",
    "content": "Matrix credentials\nYou can use these credentials to authenticate the following nodes:\nMatrix\nPrerequisites\nCreate an account on a Matrix server. Refer to Creating an account for more information.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to the Matrix Specification for more information about the service.\nRefer to the documentation for the specific client you're using to access the Matrix server.\nUsing API access token\nTo configure this credential, you'll need:\nAn Access Token: This token is tied to the account you use to log into Matrix with.\nA Homeserver URL: This is the URL of the homeserver you entered when you created your account. n8n prepopulates this with matrix.org's own server; adjust this if you're using a server hosted elsewhere.\nInstructions for getting these details vary depending on the client you're using to access the server. Both the Access Token and the Homeserver URL can most commonly be found in Settings > Help & About > Advanced, but refer to your client's documentation for more details."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mattermost.md",
    "content": "Mattermost credentials\nYou can use these credentials to authenticate the following nodes:\nMattermost\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Mattermost's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need a Mattermost account and:\nA personal Access Token\nYour Mattermost Base URL.\nTo set it up:\nIn Mattermost, go to Profile > Security > Personal Access Tokens.\nSelect Create Token.\nEnter a Token description, like n8n integration.\nSelect Save.\nCopy the Token ID and enter it as the Access Token in your n8n credential.\nEnter your Mattermost URL as the Base URL.\nBy default, n8n connects only if SSL certificate validation succeeds. To connect even if SSL certificate validation fails, turn on Ignore SSL Issues.\nRefer to the Mattermost Personal access tokens documentation for more information.\nEnable personal access tokens\nNot seeing the Personal Access Tokens option has two possible causes:\nMattermost doesn't have the personal access tokens integration enabled.\nYou're trying to generate a personal access token as a non-admin user who doesn't have permission to generate personal access tokens.\nTo identify the root cause and resolve it:\nLog in to Mattermost as an admin.\nGo to System Console > Integrations > Integration Management.\nConfirm that Enable personal access tokens is set to true. If it's not, change.\nGo to System Console > User Management > Users.\nSearch for the user account you want to allow to generate personal access tokens.\nSelect the Actions dropdown for the user and select Manage roles.\nCheck the box for Allow this account to generate personal access tokens and Save.\nRefer to the Mattermost Personal access tokens documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mautic.md",
    "content": "Mautic credentials\nYou can use these credentials to authenticate the following nodes:\nMautic\nMautic Trigger\nSupported authentication methods\nBasic auth\nOAuth2\nRelated resources\nRefer to Mautic's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need an account on a Mautic instance and:\nYour URL\nA Username\nA Password\nTo set it up:\nIn Mautic, go to Configuration > API Settings.\nIf Enable HTTP basic auth? is set to No, change it to Yes and save. Refer to the API Settings documentation for more information.\nIn n8n, enter the Base URL of your Mautic instance.\nEnter your Mautic Username.\nEnter your Mautic Password.\nUsing OAuth2\nTo configure this credential, you'll need an account on a Mautic instance and:\nA Client ID: Generated when you create new API credentials.\nA Client Secret: Generated when you create new API credentials.\nYour URL\nTo set it up:\nIn Mautic, go to Configuration > Settings.\nSelect API Credentials.\nSelect the option to Create new client.\nSelect OAuth 2 as the Authorization Protocol.\nEnter a Name for your credential, like n8n integration.\nIn n8n, copy the OAuth Callback URL and enter it as the Redirect URI in Mautic.\nSelect Apply.\nCopy the Client ID from Mautic and enter it in your n8n credential.\nCopy the Client Secret from Mautic and enter it in your n8n credential.\nEnter the Base URL of your Mautic instance.\nRefer to What is Mautic's API? for more information.\nEnable the API\nTo enable the API in your Mautic instance:\nGo to Settings > Configuration.\nSelect API Settings.\nSet API enabled? to Yes.\nSave your changes.\nRefer to How to use the Mautic API for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\medium.md",
    "content": "Medium credentials\nYou can use these credentials to authenticate the following nodes:\nMedium\nPrerequisites\nCreate an account on Medium.\nFor OAuth2, request access to credentials by emailing yourfriends@medium.com.\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to Medium's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn API Access Token: Generate a token in Settings > Security and apps > Integration tokens. Use the integration token this generates as your n8n Access Token.\nRefer to the Medium API Self-issued access tokens documentation for more information.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nTo generate a Client ID and Client Secret, you'll need access to the Developers menu. From there, create a new application to generate the Client ID and Secret.\nUse these settings for your new application:\nSelect OAuth 2 as the Authorization Protocol\nCopy the OAuth Callback URL from n8n and use this as the Callback URL in Medium."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\messagebird.md",
    "content": "MessageBird credentials\nYou can use these credentials to authenticate the following nodes:\nMessageBird\nPrerequisites\nCreate a Bird account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to MessageBird's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To generate an appropriate key, visit the Access keys page in MessageBird. Refer to the API authorization documentation for detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\metabase.md",
    "content": "Metabase credentials\nYou can use these credentials to authenticate the following nodes:\nMetabase node\nPrerequisites\nCreate a Metabase account with access to a Metabase instance.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Metabase's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA URL: Enter the base URL of your Metabase instance. If you're using a custom domain, use that URL.\nA Username: Enter your Metabase username.\nA Password: Enter your Metabase password."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\microsoft.md",
    "content": "Microsoft credentials\nYou can use these credentials to authenticate the following nodes:\nMicrosoft Dynamics CRM\nMicrosoft Excel\nMicrosoft Graph Security\nMicrosoft OneDrive\nMicrosoft Outlook\nMicrosoft SharePoint\nMicrosoft Teams\nMicrosoft Teams Trigger\nMicrosoft To Do\nPrerequisites\nCreate a Microsoft Azure account.\nCreate at least one user account with access to the appropriate service.\nIf a corporate Microsoft Entra account manages the user account, the administrator account has enabled the option ‚ÄúUser can consent to apps accessing company data on their behalf‚Äù for this user (see the Microsoft Entra documentation).\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to the linked Microsoft API documentation below for more information about each service's API:\nDynamics CRM: Web API\nExcel: Graph API\nGraph Security: Graph API\nOneDrive: Graph API\nOutlook: Graph API and Outlook API\nTeams: Graph API\nTo Do: Graph API\nUsing OAuth2\nSome Microsoft services require extra information for OAuth2. Refer to Service-specific settings for more guidance on those services.\nFor self-hosted users, there are two main steps to configure OAuth2 from scratch:\nRegister an application with the Microsoft Identity Platform.\nGenerate a client secret for that application.\nFollow the detailed instructions for each step below. For more detail on the Microsoft OAuth2 web flow, refer to Microsoft authentication and authorization basics.\nRegister an application\nRegister an application with the Microsoft Identity Platform:\nOpen the Microsoft Application Registration Portal.\nSelect Register an application.\nEnter a Name for your app.\nIn Supported account types, select Accounts in any organizational directory (Any Azure AD directory - Multi-tenant) and personal Microsoft accounts (for example, Skype, Xbox).\nIn Register an application:\nCopy the OAuth Callback URL from your n8n credential.\nPaste it into the Redirect URI (optional) field.\nSelect Select a platform > Web.\nSelect Register to finish creating your application.\nCopy the Application (client) ID and paste it into n8n as the Client ID.\nRefer to Register an application with the Microsoft Identity Platform for more information.\nGenerate a client secret\nWith your application created, generate a client secret for it:\nOn your Microsoft application page, select Certificates & secrets in the left navigation.\nIn Client secrets, select + New client secret.\nEnter a Description for your client secret, such as n8n credential.\nSelect Add.\nCopy the Secret in the Value column.\nPaste it into n8n as the Client Secret.\nIf you see other fields in the n8n credential, refer to Service-specific settings below for guidance on completing those fields.\nSelect Connect my account in n8n to finish setting up the connection.\nLog in to your Microsoft account and allow the app to access your info.\nRefer to Microsoft's Add credentials for more information on adding a client secret.\nService-specific settings\nThe following services require extra information for OAuth2:\nDynamics\nDynamics OAuth2 requires information about your Dynamics domain and region. Follow these extra steps to complete the credential:\nEnter your Dynamics Domain.\nSelect the Dynamics data center Region you're within.\nRefer to the Microsoft Datacenter regions documentation for more information on the region options and corresponding URLs.\nMicrosoft (general)\nThe general Microsoft OAuth2 also requires you to provide a space-separated list of Scopes for this credential.\nRefer to Scopes and permissions in the Microsoft identity platform for a list of possible scopes.\nOutlook\nOutlook OAuth2 supports the credential accessing a user's primary email inbox or a shared inbox. By default, the credential will access a user's primary email inbox. To change this behavior:\nTurn on Use Shared Inbox.\nEnter the target user's UPN or ID as the User Principal Name.\nSharePoint\nSharePoint OAuth2 requires information about your SharePoint Subdomain.\nTo complete the credential, enter the Subdomain part of your SharePoint URL. For example, if your SharePoint URL is  the subdomain is tenant123.\nSharePoint requires the following permissions:\nApplication permissions:\nSites.Read.All\nSites.ReadWrite.All\nDelegated permissions:\nSearchConfiguration.Read.All\nSearchConfiguration.ReadWrite.All\nCommon issues\nHere are the known common errors and issues with Microsoft OAuth2 credentials."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\microsoftazuremonitor.md",
    "content": "Microsoft Azure Monitor credentials\nPrerequisites\nCreate a Microsoft Azure account or subscription\nAn app registered in Microsoft Entra ID\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Microsoft Azure Monitor's API documentation for more information about the service.\nUsing OAuth2\nTo configure this credential, you'll need a Microsoft Azure account and:\nA Client ID\nA Client Secret\nA Tenant ID\nThe Resource you plan to access\nRefer to Microsoft Azure Monitor's API documentation for more information about authenticating to the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\microsoftentra.md",
    "content": "Microsoft Entra ID credentials\nYou can use these credentials to authenticate the following nodes:\nMicrosoft Entra ID\nPrerequisites\nCreate a Microsoft Entra ID account or subscription.\nIf the user account is managed by a corporate Microsoft Entra account, the administrator account has enabled the option ‚ÄúUser can consent to apps accessing company data on their behalf‚Äù for this user (see the Microsoft Entra documentation).\nMicrosoft includes an Entra ID free plan when you create a Microsoft Azure account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Microsoft Entra ID's documentation for more information about the service.\nUsing OAuth2\nFor self-hosted users, there are two main steps to configure OAuth2 from scratch:\nRegister an application with the Microsoft Identity Platform.\nGenerate a client secret for that application.\nFollow the detailed instructions for each step below. For more detail on the Microsoft OAuth2 web flow, refer to Microsoft authentication and authorization basics.\nRegister an application\nRegister an application with the Microsoft Identity Platform:\nOpen the Microsoft Application Registration Portal.\nSelect Register an application.\nEnter a Name for your app.\nIn Supported account types, select Accounts in any organizational directory (Any Azure AD directory - Multi-tenant) and personal Microsoft accounts (for example, Skype, Xbox).\nIn Register an application:\nCopy the OAuth Callback URL from your n8n credential.\nPaste it into the Redirect URI (optional) field.\nSelect Select a platform > Web.\nSelect Register to finish creating your application.\nCopy the Application (client) ID and paste it into n8n as the Client ID.\nRefer to Register an application with the Microsoft Identity Platform for more information.\nGenerate a client secret\nWith your application created, generate a client secret for it:\nOn your Microsoft application page, select Certificates & secrets in the left navigation.\nIn Client secrets, select + New client secret.\nEnter a Description for your client secret, such as n8n credential.\nSelect Add.\nCopy the Secret in the Value column.\nPaste it into n8n as the Client Secret.\nSelect Connect my account in n8n to finish setting up the connection.\nLog in to your Microsoft account and allow the app to access your info.\nRefer to Microsoft's Add credentials for more information on adding a client secret.\nSetting custom scopes\nMicrosoft Entra ID credentials use the following scopes by default:\nopenid\noffline_access\nAccessReview.ReadWrite.All\nDirectory.ReadWrite.All\nNetworkAccessPolicy.ReadWrite.All\nDelegatedAdminRelationship.ReadWrite.All\nEntitlementManagement.ReadWrite.All\nUser.ReadWrite.All\nDirectory.AccessAsUser.All\nSites.FullControl.All\nGroupMember.ReadWrite.All\nTo select different scopes for your credentials, enable the Custom Scopes slider and edit the Enabled Scopes list. Keep in mind that some features may not work as expected with more restrictive scopes.\nCommon issues\nHere are the known common errors and issues with Microsoft Entra credentials."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\microsoftsql.md",
    "content": "Microsoft SQL credentials\nYou can use these credentials to authenticate the following nodes:\nMicrosoft SQL\nPrerequisites\nCreate a user account on a Microsoft SQL server database.\nSupported authentication methods\nSQL database connection\nRelated resources\nRefer to Microsoft's Connect to SQL Server documentation for more information about connecting to the service.\nUsing SQL database connection\nTo configure this credential, you'll need:\nThe Server name\nThe Database name\nYour User account/ID\nYour Password\nThe Port to use for the connection\nThe Domain name\nWhether to use TLS\nWhether to Ignore SSL Issues\nThe Connect Timeout\nThe Request Timeout\nThe TDS Version the connection should use\nTo set up the database connection:\nEnter the SQL Server Host Name as the Server. In an existing SQL Server connection, the host name comes before the instance name in the format HOSTNAME\\INSTANCENAME. Find the host name:\nIn the Object Explorer pane as the top-level object for your database.\nIn the footer of a query window.\nViewing the current connection Properties and looking for Name or Display Name.\nRefer to Find SQL Server Instance Name | When you're connected to SQL Server for more information. You can also find the information in the Error logs.\nEnter the SQL Server Instance Name as the Database name. Find this name using the same steps listed above for finding the host name.\nIf you don't see an instance name in any of these places, then your database uses the default MSSQLSERVER instance name.\nEnter your User account name or ID.\nEnter your Password.\nFor the Port:\nSQL Server defaults to 1433.\nIf you can't connect over port 1433, check the Error logs for the phrase Server is listening on to identify the port number you should enter.\nYou only need to enter the Domain name if users in multiple domains access your database. Run this SQL query to get the domain name:\nSelect whether to use TLS.\nSelect whether to Ignore SSL Issues: If turned on, the credential will connect even if SSL certificate validation fails.\nEnter the number of milliseconds n8n should attempt the initial connection to complete before disconnecting as the Connect Timeout. Refer to the SqlConnection.ConnectionTimeout property documentation for more information.\nSQL Server stores this timeout as seconds, while n8n stores it as milliseconds. If you're copying your SQL Server defaults, multiple by 100 before entering the number here.\nEnter the number of milliseconds n8n should wait on a given request before timing out as the Request Timeout. This is basically a query timeout parameter. Refer to Troubleshoot query time-out errors for more information.\nSelect the Tabular Data Stream (TDS) protocol to use from the TDS Version dropdown. If the server doesn't support the version you select here, the connection uses a negotiated alternate version. Refer to Appendix A: Product Behavior for a more detailed breakdown of the TDS versions' compatibility with different SQL Server versions and .NET frameworks. Options include:\n7_4 (SQL Server 2012 ~ 2019): TDS version 7.4.\n7_3_B (SQL Server 2008R2): TDS version 7.3.B.\n7_3_A (SQL Server 2008): TDS version 7.3.A.\n7_2 (SQL Server 2005): TDS version 7.2.\n7_1 (SQL Server 2000): TDS version 7.1."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\milvus.md",
    "content": "Milvus credentials\nYou can use these credentials to authenticate the following nodes:\nMilvus Vector Store\nPrerequisites\nCreate and run an Milvus instance. Refer to the Install Milvus for more information.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Milvus's Authentication documentation for more information about setting up authentication.\nUsing basic auth\nTo configure this credential, you'll need:\nBase URL: The base URL of your Milvus instance. The default is\nUsername: The username to authenticate to your Milvus instance. The default value is root.\nPassword: The password to authenticate to your Milvus instance. The default value is Milvus."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mindee.md",
    "content": "Mindee credentials\nYou can use these credentials to authenticate the following nodes:\nMindee\nPrerequisites\nCreate a Mindee account.\nSupported authentication methods\nInvoice API key: For use with the Invoice OCR API\nReceipt API key: For use with the Receipt OCR API\nRelated resources\nRefer to Mindee's Invoice OCR API documentation and Mindee's Receipt OCR API documentation for more information about each service.\nUsing invoice API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Mindee Create & Manage API Keys documentation for instructions on creating API keys.\nUsing receipt API key\nTo configure this credential, you'll need:\nAn API Key: Refer to the Mindee Create & Manage API Keys documentation for instructions on creating API keys."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\miro.md",
    "content": "Miro credentials\nPrerequisites\nCreate a Miro account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Miro's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing OAuth2\nTo configure this credential, you'll need a Miro account and app, as well as:\nA Client ID: Generated when you create a new OAuth2 application.\nA Client Secret: Generated when you create a new OAuth2 application.\nRefer to Miro's API documentation for more information about authenticating to the service.\nIf you're self-hosting n8n, you'll need to create an app to configure OAuth2. Refer to Miro's OAuth documentation for more information about setting up OAuth2."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\misp.md",
    "content": "MISP credentials\nYou can use these credentials to authenticate the following nodes:\nMISP\nPrerequisites\nInstall and run a MISP instance.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to MISP's Automation API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: In MISP, these are called Automation keys. Get an automation key from Event Actions > Automation. Refer to MISP's automation keys documentation for instructions on generating more keys.\nA Base URL: Your MISP URL.\nSelect whether to Allow Unauthorized Certificates: If turned on, the credential will connect even if SSL certificate validation fails."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mist.md",
    "content": "Mist credentials\nPrerequisites\nCreate a Mist account and organization. Refer to Create a Mist account and Organization for detailed instructions.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Mist's documentation for more information about the service. If you're logged in to your Mist account, go to ) to view the full API documentation.\nThis is a credential-only node. Refer to [Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API token\nTo configure this credential, you'll need:\nAn API Token: You can use either a User API token or an Org API token. Refer to How to generate a user API token for instructions on generating a User API token. Refer to Org API token for instructions on generating an Org API token.\nSelect the Region you're in. Options include:\nEurope: Select this option if your cloud environment is in any of the EMEA regions.\nGlobal: Select this option if your cloud environment is in any of the global regions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mistral.md",
    "content": "Mistral Cloud credentials\nYou can use these credentials to authenticate the following nodes:\nMistral AI\nMistral Cloud\nEmbeddings Mistral Cloud\nPrerequisites\nCreate a Mistral La Plateforme account.\nYou must add payment information in Workspace > Billing and activate payments to enable API keys. Refer to Account setup for more information.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Mistral's API documentation for more information about the APIs.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nOnce you've added payment information to your Mistral Cloud account:\nSign in to your Mistral account.\nGo to the API Keys page.\nSelect Create new key.\nCopy the API key and enter it in your n8n credential.\nRefer to Account setup for more information.\n/// note | Paid account required\nMistral requires you to add payment information and activate payments to use API keys. Refer to the Prerequisites section above for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mocean.md",
    "content": "Mocean credentials\nYou can use these credentials to authenticate the following nodes:\nMocean\nPrerequisites\nCreate a Mocean account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Mocean's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nAn API Secret\nBoth the key and secret are accessible in your Mocean Dashboard. Refer to API Authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mondaycom.md",
    "content": "monday.com credentials\nYou can use these credentials to authenticate the following nodes:\nmonday.com\nSupported authentication methods\nAPI token\nOAuth2\nRelated resources\nRefer to monday.com's API documentation for more information about authenticating with the service.\nUsing API token\nTo configure this credential, you'll need a monday.com account and:\nAn API Token V2\nTo get your token:\nIn your monday.com account, select your profile picture in the top right corner.\nSelect Developers. The Developer Center opens in a new tab.\nIn the Developer Center, select My Access Tokens > Show.\nCopy your personal token and enter it in your n8n credential as the Token V2.\nRefer to monday.com API Authentication for more information.\nUsing OAuth2\nTo configure this credential, you'll need a monday.com account and:\nA Client ID\nA Client Secret\nTo generate both these fields, register a new monday.com application:\nIn your monday.com account, select your profile picture in the top right corner.\nSelect Developers. The Developer Center opens in a new tab.\nIn the Developer Center, select Build app. The app details open.\nEnter a Name for your app, like n8n integration.\nCopy the Client ID and enter it in your n8n credential.\nShow the Client Secret, copy it, and enter it in your n8n credential.\nIn the left menu, select OAuth.\nFor Scopes, select boards:write and boards:read.\nSelect Save Scopes.\nSelect the Redirect URLs tab.\nCopy the OAuth Redirect URL from n8n and enter it as the Redirect URL.\nSave your changes in monday.com.\nIn n8n, select Connect my account to finish the setup.\nRefer to Create an app for more information on creating apps.\nRefer to OAuth and permissions for more information on the available scopes and setting up the Redirect URL."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mongodb.md",
    "content": "MongoDB credentials\nYou can use these credentials to authenticate the following nodes:\nMongoDB\nMongoDB Atlas Vector Store\nMongoDB Chat Memory\nPrerequisites\nCreate a user account with the appropriate permissions on a MongoDB server.\nAs a Project Owner, add all the n8n IP addresses to the IP Access List Entries in the project's Network Access. Refer to Add IP Access List entries for detailed instructions.\nIf you are setting up MongoDB from scratch, create a cluster and a database. Refer to the MongoDB Atlas documentation for more detailed instructions on these steps.\nSupported authentication methods\nDatabase connection - Connection string\nDatabase connection - Values\nRelated resources\nRefer to the MongoDBs Atlas documentation for more information about the service.\nUsing database connection - Connection string\nTo configure this credential, you'll need the Prerequisites listed above. Then:\nSelect Connection String as the Configuration Type.\nEnter your MongoDB Connection String. To get your connection string in MongoDB, go to Database > Connect.\nSelect Drivers.\nCopy the code you see in Add your connection string into your application code. It will be something like: mongodb+srv://yourName:yourPassword@clusterName.mongodb.net/?retryWrites=true&w=majority.\nReplace the  and  in the connection string with the database user's credentials you'll be using.\nEnter that connection string into n8n.\nRefer to Connection String for information on finding and formatting your connection string.\nEnter your Database name. This is the name of the database that the user whose details you added to the connection string is logging into.\nSelect whether to Use TLS: Turn on to use TLS. You must have your MongoDB database configured to use TLS and have an x.509 certificate generated. Add information for these certificate fields in n8n:\nCA Certificate\nPublic Client Certificate\nPrivate Client Key\nPassphrase\nRefer to MongoDB's x.509 documentation for more information on working with x.509 certificates.\nUsing database connection - Values\nTo configure this credential, you'll need the Prerequisites listed above. Then:\nSelect Values as the Configuration Type.\nEnter the database Host name or address.\nEnter the Database name.\nEnter the User you'd like to log in as.\nEnter the user's Password.\nEnter the Port to connect over. This is the port number your server uses to listen for incoming connections.\nSelect whether to Use TLS: Turn on to use TLS. You must have your MongoDB database configured to use TLS and have an x.509 certificate generated. Add information for these certificate fields in n8n:\nCA Certificate\nPublic Client Certificate\nPrivate Client Key\nPassphrase\nRefer to MongoDB's x.509 documentation for more information on working with x.509 certificates."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\monicacrm.md",
    "content": "Monica CRM credentials\nYou can use these credentials to authenticate the following nodes:\nMonica CRM\nPrerequisites\nSign up for a Monica CRM account or self-host an instance.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Monica's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nYour Environment:\nSelect Cloud-Hosted if you access your Monica instance through Monica.\nSelect Self-Hosted if you have self-hosted Monica on your own server. Provide your Self-Hosted Domain.\nAn API Token: Generate a token in Settings > API."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\motorhead.md",
    "content": "Motorhead credentials\nYou can use these credentials to authenticate the following nodes:\nMotorhead\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Motorhead's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a Motorhead account and:\nYour Host URL\nAn API Key\nA Client ID\nTo set it up, you'll generate an API key:\nIf you're self-hosting Motorhead, update the Host URL to match your Motorhead URL.\nIn Motorhead, go to Settings > Organization.\nIn the API Keys section, select Create.\nEnter a Name for your API Key, like n8n integration.\nSelect Generate.\nCopy the apiKey and enter it in your n8n credential.\nReturn to the API key list.\nCopy the clientID for the key and enter it as the Client ID in your n8n credential.\nRefer to Generate an API key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mqtt.md",
    "content": "MQTT credentials\nYou can use these credentials to authenticate the following nodes:\nMQTT\nMQTT Trigger\nPrerequisites\nInstall an MQTT broker.\nMQTT provides a list of Servers/Brokers at MQTT Software.\nSupported authentication methods\nBroker connection\nRelated resources\nRefer to MQTT's documentation for more information about the MQTT protocol.\nRefer to your broker provider's documentation for more detailed configuration and details.\nUsing broker connection\nTo configure this credential, you'll need:\nYour MQTT broker's Protocol\nThe Host\nThe Port\nA Username and Password to authenticate with\nIf you're using SSL, the relevant certificates and keys\nTo set things up:\nSelect the broker's Protocol, which determines the URL n8n uses. Options include:\nMqtt: Begin the URL with the standard mqtt: protocol.\nMqtts: Begin the URL with the secure mqtts: protocol.\nWs: Begin the URL with the WebSocket ws: protocol.\nEnter your broker Host.\nEnter the Port number n8n should use to connect to the broker host.\nEnter the Username to log into the broker as.\nEnter that user's Password.\nIf you want to receive QoS 1 and 2 messages while offline, turn off the Clean Session toggle.\nEnter a Client ID you'd like the credential to use. If you leave this blank, n8n will generate one for you. You can use a fixed or expression-based Client ID.\nClient IDs can be useful to identify and track connection access. n8n recommends using something with n8n in it for easier auditing.\nIf your MQTT broker uses SSL, turn the SSL toggle on. Once you turn it on:\nSelect whether to use Passwordless connection with certificates, which is like the SASL mechanism EXTERNAL. If turned on:\nSelect whether to Reject Unauthorized Certificate: If turned off, n8n will connect even if the certificate validation fails.\nAdd an SSL Client Certificate.\nAdd an SSL Client Key for the Client Certificate.\nOne or more SSL CA Certificates.\nRefer to your MQTT broker provider's documentation for more detailed configuration instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\msg91.md",
    "content": "MSG91 credentials\nYou can use these credentials to authenticate the following nodes:\nMSG91\nPrerequisites\nCreate a MSG91 account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to MSG91's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn Authentication Key: To get your Authentication Key, go to the user menu and select Authkey. Refer to MSG91's Where can I find my authentication key? documentation for more information.\nIP Security\nMSG91 enables IP Security by default for authkeys.\nFor the n8n credentials to function with this setting enabled, add all the n8n IP addresses as whitelisted IPs in MSG91. You can add them in one of two places, depending on your desired security level:\nTo allow any/all authkeys in the account to work with n8n, add the n8n IP addresses in the Company's whitelisted IPs section of the Authkey page.\nTo allow only specific authkeys to work with n8n, add the n8n IP addresses in the Whitelisted IPs section of an authkey's details."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\mysql.md",
    "content": "MySQL credentials\nYou can use these credentials to authenticate the following nodes:\nMySQL\nAgent\nPrerequisites\nCreate a user account on a MySQL server database.\nSupported authentication methods\nDatabase connection\nRelated resources\nRefer to MySQL's documentation for more information about the service.\nUsing database connection\nTo configure this credential, you'll need:\nThe server Host: The database's host name or IP address.\nThe Database name.\nA User name.\nA Password for that user.\nThe Port number used by the MySQL server.\nConnect Timeout: The number of milliseconds during the initial database connection before a timeout occurs.\nSSL: If your database is using SSL, turn this on and add details for the SSL certificate.\nSSH Tunnel: Choose whether to connect over an SSH tunnel. An SSH tunnel lets un-encrypted traffic pass over an encrypted connection and enables authorized remote access to servers protected from outside connections by a firewall.\nTo set up your database connection credential:\nEnter your database's hostname as the Host in your n8n credential. Run this query to confirm the hostname:\nEnter your database's name as the Database in your n8n credential. Run this query to confirm the database name:\nEnter the username of a User in the database. This user should have appropriate permissions for whatever actions you want n8n to perform.\nEnter the Password for that user.\nEnter the Port number used by the MySQL server (default is 3306). Run this query to confirm the port number:\nEnter the Connect Timeout you'd like the node to use. The Connect Timeout is the number of milliseconds during the initial database connection the node should wait before timing out. n8n defaults to 10000 which is the default used by MySQL of 10 seconds. If you want to match your database's connect_timeout, run this query to get it, then multiply by 1000 before entering it in n8n:\nIf your database uses SSL and you'd like to use SSL for the connection, turn this option on in the credential. If you turn it on, enter the information from your MySQL SSL certificate in these fields:\nEnter the ca.pem file contents in the CA Certificate field.\nEnter the client-key.pem file contents in the Client Private Key field.\nEnter the client-cert.pem file contents in the Client Certificate field.\nIf you want to use SSH Tunnel for the connection, turn this option on in the credential. Otherwise, skip it. If you turn it on:\nSelect the SSH Authenticate with to set the SSH Tunnel type to build:\nSelect Password if you want to connect to SSH using a password.\nSelect Private Key if you want to connect to SSH using an identity file (private key) and a passphrase.\nEnter the SSH Host. n8n uses this host to create the SSH URI formatted as: [user@]host:port.\nEnter the SSH Port. n8n uses this port to create the SSH URI formatted as: [user@]host:port.\nEnter the SSH User to connect with. n8n uses this user to create the SSH URI formatted as: [user@]host:port.\nIf you selected Password for SSH Authenticate with, add the SSH Password.\nIf you selected Private Key for SSH Authenticate with:\nAdd the contents of the Private Key or identity file used for SSH. This is the same as using the ssh-identity-file option with the shell-connect() command in MySQL.\nIf the Private Key was created with a passphrase, enter that Passphrase. This is the same as using the ssh-identity-pass option with the shell-connect() command in MySQL. If the Private Key has no passphrase, leave this field blank.\nRefer to MySQL | Creating SSL and RSA Certificates and Keys for more information on working with SSL certificates in MySQL. Refer to MySQL | Using an SSH Tunnel for more information on working with SSH tunnels in MySQL."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\nasa.md",
    "content": "NASA credentials\nYou can use these credentials to authenticate the following nodes:\nNASA\nSupported authentication methods\nAPI key\nRelated resources\nRefer to the Browse APIs section of the NASA Open APIs for more information about the service.\nUsing an API key\nTo configure this credential, you'll need:\nAn API Key\nTo generate an API key:\nGo to the NASA Open APIs page.\nComplete the fields in the Generate API Key section.\nCopy the API Key and enter it in your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\netlify.md",
    "content": "Netlify credentials\nYou can use these credentials to authenticate the following nodes:\nNetlify\nNetlify Trigger\nPrerequisites\nCreate a Netlify account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Netlify's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn Access Token: Generate an Access Token in Applications > Personal Access Tokens. Refer to Netlify API Authentication for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\netscaleradc.md",
    "content": "Netscaler ADC credentials\nYou can use these credentials to authenticate the following nodes:\nNetscaler ADC node\nPrerequisites\nInstall a NetScaler/Citrix ADC appliance.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Netscaler ADC's 14.1 NITRO API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA URL: Enter the URL of your NetScaler/Citrix ADC instance.\nA Username: Enter your NetScaler/Citrix ADC username.\nA Password: Enter your NetScaler/Citrix ADC password.\nRefer to Performing Basic Netscaler ADC Operations for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\nextcloud.md",
    "content": "Nextcloud credentials\nYou can use these credentials to authenticate the following nodes:\nNextcloud\nSupported authentication methods\nBasic auth\nOAuth2\nRelated resources\nRefer to Nextcloud's API documentation for more information about the service.\nRefer to Nextcloud's user manual for more information on installing and configuring Nextcloud.\nUsing basic auth\nTo configure this credential, you'll need a Nextcloud account and:\nYour Web DAV URL\nYour User name\nYour Password or an app password\nTo set it up:\nTo create your Web DAV URL: If Nextcloud is in the root of your domain: Enter the URL you use to access Nextcloud and add /remote.php/webdav/. For example, if you access Nextcloud at  your WebDAV URL is\nIf you have Nextcloud installed in a subdirectory, enter the URL you use to access Nextcloud and add //remote.php/webdav/. Replace  with the subdirectory Nextcloud's installed in.\nRefer to Nextcloud's Third-party WebDAV clients documentation for more information on constructing your WebDAV URL.\nEnter your User name.\nFor the Password, Nextcloud recommends using an app password rather than your user password. To create an app password:\nIn the Nextcloud Web interface, select your avatar in the top right and select Personal settings.\nIn the left menu, choose Security.\nScroll to the bottom to the App Password section and create a new app password.\nCopy that app password and enter it in n8n as your Password.\nUsing OAuth2\nTo configure this credential, you'll need a Nextcloud account and:\nAn Authorization URL and Access Token URL: These depend on the URL you use to access Nextcloud.\nA Client ID: Generated once you add an OAuth2 client application in Administrator Security Settings.\nA Client Secret: Generated once you add an OAuth2 client application in Administrator Security Settings.\nA Web DAV URL: This depends on the URL you use to access Nextcloud.\nTo set it up:\nIn Nextcloud, open your Administrator Security Settings.\nFind the Add client section under OAuth 2.0 clients.\nEnter a Name for your client, like n8n integration.\nCopy the OAuth Callback URL from n8n and enter it as the Redirection URI.\nThen select Add in Nextcloud.\nIn n8n, update the Authorization URL to replace  with the URL you use to access Nextcloud. For example, if you access Nextcloud at  the Authorization URL is\nIn n8n, update the Access Token URL to replace  with the URL you use to access Nextcloud. For example, if you access Nextcloud at  the Access Token URL is\nCopy the Nextcloud Client Identifier for your OAuth2 client and enter it as the Client ID in n8n.\nCopy the Nextcloud Secret and enter it as the Client Secret in n8n.\nIn n8n, to create your Web DAV URL: If Nextcloud is in the root of your domain, enter the URL you use to access Nextcloud and add /remote.php/webdav/. For example, if you access Nextcloud at  your WebDAV URL is\nIf you have Nextcloud installed in a subdirectory, enter the URL you use to access Nextcloud and add //remote.php/webdav/. Replace  with the subdirectory Nextcloud's installed in.\nRefer to Nextcloud's Third-party WebDAV clients documentation for more information on constructing your WebDAV URL.\nRefer to the Nextcloud OAuth2 Configuration documentation for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\nocodb.md",
    "content": "NocoDB credentials\nYou can use these credentials to authenticate the following nodes:\nNocoDB\nSupported authentication methods\nAPI token (recommended)\nUser auth token\nRelated resources\nRefer to NocoDB's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need a NocoDB instance and:\nAn API Token\nYour database Host\nTo generate an API token:\nLog into NocoDB and select the User menu in the bottom left sidebar.\nSelect Account Settings.\nOpen the Tokens tab.\nSelect Add new API token.\nEnter a Name for your token, like n8n integration.\nSelect Save.\nCopy the API Token and enter it in your n8n credential.\nEnter the Host of your NocoDB instance in your n8n credential, for example\nRefer to the NocoDB API Tokens documentation for more detailed instructions.\nUsing user auth token\nBefore NocoDB deprecated it, user auth token was a temporary token designed for quick experiments with the API, valid for a session until the user logs out or for 10 hours.\nTo configure this credential, you'll need a NocoDB instance and:\nA User Token\nYour database Host\nTo generate a user auth token:\nLog into NocoDB and select the User menu in the bottom left sidebar.\nSelect Copy Auth token.\nEnter that auth token as the User Token in n8n.\nEnter the Host of your NocoDB instance, for example\nRefer to the NocoDB Auth Tokens documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\notion.md",
    "content": "Notion credentials\nYou can use these credentials to authenticate the following nodes:\nNotion\nNotion Trigger\nPrerequisites\nCreate a Notion account with admin level access.\nSupported authentication methods\nAPI integration token: Used for internal integrations.\nOAuth2: Used for public integrations.\nRelated resources\nRefer to Notion's API documentation for more information about the service.\nUsing API integration token\nTo configure this credential, you'll need:\nAn Internal Integration Secret: Generated once you create a Notion integration.\nTo generate an integration secret, create a Notion integration and grab the integration secret from the Secrets tab:\nGo to your Notion integration dashboard.\nSelect the + New integration button.\nEnter a Name for your integration, for example n8n integration. If desired, add a Logo.\nSelect Submit to create your integration.\nOpen the Capabilities tab. Select these capabilities:\nRead content\nUpdate content\nInsert content\nUser information without email addresses\nBe sure to Save changes.\nSelect the Secrets tab.\nCopy the Internal Integration Token and add it as your n8n Internal Integration Secret.\nRefer to the Internal integration auth flow setup documentation for more information about authenticating to the service.\nShare Notion page(s) with the integration\nFor your integration to interact with Notion, you must give your integration page permission to interact with page(s) in your Notion workspace:\nVisit the page in your Notion workspace.\nSelect the triple dot menu at the top right of a page.\nIn Connections, select Connect to.\nUse the search bar to find and select your integration from the dropdown list.\nOnce you share at least one page with the integration, you can start making API requests. If the page isn't shared, any API requests made will respond with an error.\nRefer to Integration permissions for more information.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated once you configure a public integration.\nA Client Secret: Generated once you configure a public integration.\nYou must create a Notion integration and set it to public distribution:\nGo to your Notion integration dashboard.\nSelect the + New integration button.\nEnter a Name for your integration, for example n8n integration. If desired, add a Logo.\nSelect Submit to create your integration.\nOpen the Capabilities tab. Select these capabilities:\nRead content\nUpdate content\nInsert content\nUser information without email addresses\nSelect Save changes.\nGo to the Distribution tab.\nTurn on the Do you want to make this integration public? control.\nEnter your company name and website in the Organization Information section.\nCopy the n8n OAuth Redirect URL and add it to as a Redirect URI in the Notion integration's OAuth Domain & URLs section.\nGo to the Secrets tab.\nCopy the Client ID and Client Secret and add them to your n8n credential.\nRefer to Notion's public integration auth flow setup for more information about authenticating to the service.\nInternal vs. public integrations\nInternal integrations are:\nSpecific to a single workspace.\nAccessible only to members of that workspace.\nIdeal for custom workspace enhancements.\nInternal integrations use a simpler authentication process (the integration secret) and don't require any security review before publishing.\nPublic integrations are:\nUsable across multiple, unrelated Notion workspaces.\nAccessible by any Notion user, regardless of their workspace.\nIdeal for catering to broad use cases.\nPublic integrations use the OAuth 2.0 protocol for authentication. They require a Notion security review before publishing.\nFor a more detailed breakdown of the two integration types, refer to Notion's Internal vs. Public Integrations documentation."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\npm.md",
    "content": "npm credentials\nYou can use these credentials to authenticate the following nodes:\nnpm\nPrerequisites\nCreate an npm account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to npm's external integrations documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn Access Token: Create an access token by selecting Access Tokens from your profile menu. Refer to npm's Creating and viewing access tokens documentation for more detailed instructions.\nA Registry URL: If you're using a custom npm registry, update the Registry URL to that custom registry. Otherwise, keep the public registry value."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\odoo.md",
    "content": "Odoo credentials\nYou can use these credentials to authenticate the following nodes:\nOdoo\nSupported authentication methods\nAPI key (Recommended)\nPassword\nRelated resources\nRefer to Odoo's External API documentation for more information about the service.\nRefer to the Odoo Getting Started tutorial if you're new to Odoo.\nUsing API key\nTo configure this credential, you'll need a user account on an Odoo database and:\nYour Site URL\nYour Username\nAn API key\nYour Database name\nTo set up the credential with an API key:\nEnter your Odoo server or site URL as the Site URL.\nEnter your Username as it's displayed on your Change password screen in Odoo.\nTo use an API key, go to Your Profile > Preferences > Account Security > Developer API Keys.\nIf you don't have this option, you may need to upgrade your Odoo plan. Refer to Required plan type for more information.\nSelect New API Key.\nEnter a Description for the key, like n8n integration.\nSelect Generate Key.\nCopy the key and enter it as the Password or API key in your n8n credential.\nEnter your Odoo Database name, also known as the instance name.\nRefer to Odoo API Keys for more information.\nUsing password\nTo configure this credential, you'll need a user account on an Odoo database and:\nYour Site URL\nYour Username\nYour Password\nYour Database name\nTo set up the credential with a password:\nEnter your Odoo server or site URL as the Site URL.\nEnter your Username as it's displayed on your Change password screen in Odoo.\nTo use a password, enter your user password in the Password or API key field.\nEnter your Odoo Database name, also known as the instance name.\nRequired plan type"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\okta.md",
    "content": "Okta credentials\nYou can use these credentials to authenticate the following nodes:\nOkta\nPrerequisites\nCreate an Okta free trial or create an admin account on an existing Okta org.\nSupported authentication methods\nSSWS API Access token\nRelated resources\nRefer to Okta's documentation for more information about the service.\nUsing SSWS API access token\nTo configure this credential, you'll need:\nThe URL: The base URL of your Okta org, also referred to as your unique subdomain. There are two quick ways to access it:\nIn the Admin Console, select your Profile, hover over the domain listed below your username, and select the Copy icon. Paste this into n8n, but be sure to add https:// before it.\nCopy the base URL of your Admin Console URL, for example  Paste it into n8n and remove -admin, for example:\nAn SSWS Access Token: Create a token by going to Security > API > Tokens > Create token. Refer to Create Okta API tokens for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ollama.md",
    "content": "Ollama credentials\nYou can use these credentials to authenticate the following nodes:\nOllama\nChat Ollama\nEmbeddings Ollama\nPrerequisites\nCreate and run an Ollama instance with one user. Refer to the Ollama Quick Start for more information.\nSupported authentication methods\nInstance URL\nRelated resources\nRefer to Ollama's API documentation for more information about the service.\nUsing instance URL\nTo configure this credential, you'll need:\nThe Base URL of your Ollama instance.\nThe default Base URL is  but if you've set the OLLAMA_HOST environment variable, enter that value. If you have issues connecting to a local n8n server, try 127.0.0.1 instead of localhost.\nRefer to How do I configure Ollama server? for more information.\nOllama and self-hosted n8n\nIf you're self-hosting n8n on the same machine as Ollama, you may run into issues if they're running in different containers.\nFor this setup, open a specific port for n8n to communicate with Ollama by setting the OLLAMA_ORIGINS variable or adjusting OLLAMA_HOST to an address the other container can access.\nRefer to Ollama's How can I allow additional web origins to access Ollama? for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\onesimpleapi.md",
    "content": "One Simple API credentials\nYou can use these credentials to authenticate the following nodes:\nOne Simple API\nPrerequisites\nCreate a One Simple API account.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to One Simple API's documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API token: Create a new API token on the API Tokens page. Be sure you select appropriate permissions for the token.\nYou can also access the API Tokens page by selecting your Profile > API Tokens."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\onfleet.md",
    "content": "Onfleet credentials\nYou can use these credentials to authenticate the following nodes:\nOnfleet\nOnfleet Trigger\nPrerequisites\nCreate an Onfleet administrator account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Onfleet's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API key: To create an API key, log into your organization's administrator account. Select Settings > API & Webhooks, then select + to create a new key. Refer to Onfleet's Creating an API key documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\openai.md",
    "content": "OpenAI credentials\nYou can use these credentials to authenticate the following nodes:\nOpenAI\nChat OpenAI\nEmbeddings OpenAI\nLM OpenAI\nPrerequisites\nCreate an OpenAI account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to OpenAI's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nAn Organization ID: Required if you belong to multiple organizations; otherwise, leave this blank.\nTo generate your API Key:\nLogin to your OpenAI account or create an account.\nOpen your API keys page.\nSelect Create new secret key to create an API key, optionally naming the key.\nCopy your key and add it as the API Key in n8n.\nRefer to the API Quickstart Account Setup documentation for more information.\nTo find your Organization ID:\nGo to your Organization Settings page.\nCopy your Organization ID and add it as the Organization ID in n8n.\nRefer to Setting up your organization for more information. Note that API requests made using an Organization ID will count toward the organization's subscription quota."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\opencti.md",
    "content": "OpenCTI credentials\nPrerequisites\nCreate an OpenCTI developer account.\nAuthentication methods\nAPI key\nRelated resources\nRefer to OpenCTI's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To get your API key, go to your Profile > API access. Refer to the OpenCTI Integrations Authentication documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\openrouter.md",
    "content": "OpenRouter credentials\nYou can use these credentials to authenticate the following nodes:\nChat OpenRouter\nPrerequisites\nCreate a OpenRouter account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to OpenRouter's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nTo generate your API Key:\nLogin to your OpenRouter account or create an account.\nOpen your API keys page.\nSelect Create new secret key to create an API key, optionally naming the key.\nCopy your key and add it as the API Key in n8n.\nRefer to the OpenRouter Quick Start page for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\openweathermap.md",
    "content": "OpenWeatherMap credentials\nYou can use these credentials to authenticate the following nodes:\nOpenWeatherMap\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to OpenWeatherMap's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need an OpenWeatherMap account and:\nAn Access Token\nTo get your Access Token:\nAfter you verify your email address, OpenWeatherMap includes an API Key in your welcome email.\nCopy that key and enter it in your n8n credential.\nIf you'd prefer to create a new key:\nTo create a new key, go to Account > API Keys.\nIn the Create Key section, enter an API Key Name, like n8n integration.\nSelect Generate to generate your key.\nCopy the generated key and enter it in your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\oura.md",
    "content": "Oura credentials\nYou can use these credentials to authenticate the following nodes:\nOura\nPrerequisites\nCreate an Oura account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Oura's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nA Personal Access Token: To generate a personal access token, go to the Personal Access Tokens page and select Create A New Personal Access Token.\nRefer to How to Generate Personal Access Tokens for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\paddle.md",
    "content": "Paddle credentials\nYou can use these credentials to authenticate the following nodes:\nPaddle\nPrerequisites\nCreate a Paddle account.\nSupported authentication methods\nAPI access token (Classic)\nRelated resources\nRefer to Paddle Classic's API documentation for more information about the service.\nUsing API access token (Classic)\nTo configure this credential, you'll need:\nA Vendor Auth Code: Created when you generate an API key.\nA Vendor ID: Displayed when you generate an API key.\nUse Sandbox Environment API: When turned on, nodes using this credential will hit the Sandbox API endpoint instead of the live API endpoint.\nTo generate an auth code and view your Vendor ID, go to Paddle > Developer Tools > Authentication > Generate Auth Code. Select Reveal Auth Code to display the Auth Code. Refer to API Authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\pagerduty.md",
    "content": "PagerDuty credentials\nYou can use these credentials to authenticate the following nodes:\nPagerDuty\nPrerequisites\nCreate a PagerDuty account.\nSupported authentication methods\nAPI token\nOAuth2\nRelated resources\nRefer to PagerDuty's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nA general access API Token: To generate an API token, go to Integrations > Developer Tools > API Access Keys > Create New API Key. Refer to Generate a General Access REST API key for more information.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch, register a new Pagerduty app.\nUse these settings for registering your app:\nIn the Category dropdown list, select Infrastructure Automation.\nIn the Functionality section, select OAuth 2.0.\nOnce you Save your app, open the app details and edit your app configuration to use these settings:\nWithin the OAuth 2.0 section, select Add.\nCopy the OAuth Callback URL from n8n and paste it into the Redirect URL field.\nCopy the Client ID and Client Secret from PagerDuty and add these to your n8n credentials.\nSelect Read/Write from the Set Permission Scopes dropdown list.\nRefer to the instructions in App functionality for more information on available functionality. Refer to the PagerDuty OAuth Functionality documentation for more information on the OAuth flow."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\paypal.md",
    "content": "PayPal credentials\nYou can use these credentials to authenticate the following nodes:\nPayPal\nPayPal Trigger\nPrerequisites\nCreate a PayPal developer account.\nSupported authentication methods\nAPI client and secret\nRelated resources\nRefer to Paypal's API documentation for more information about the service.\nUsing API client and secret\nTo configure this credential, you'll need:\nA Client ID: Generated when you create an app.\nA Secret: Generated when you create an app.\nAn Environment: Select Live or Sandbox.\nTo generate the Client ID and Secret, log in to your Paypal developer dashboard. Select Apps & Credentials > Rest API apps > Create app. Refer to Get client ID and client secret for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\peekalink.md",
    "content": "Peekalink credentials\nYou can use these credentials to authenticate the following nodes:\nPeekalink\nPrerequisites\nCreate a Peekalink account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Peekalink's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To get your API key, access your Peekalink dashboard and copy the key in the Your API Key section. Refer to Get your API key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\perplexity.md",
    "content": "Perplexity credentials\nYou can use these credentials to authenticate the following nodes:\nPerplexity\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Perplexity's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a Perplexity account and:\na Perplexity API key: You can find out how to create a Perplexity API key in the Perplexity API getting started guide.\nRefer to Perplexity's API documentation for more information about authenticating to the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\phantombuster.md",
    "content": "PhantomBuster credentials\nYou can use these credentials to authenticate the following nodes:\nPhantomBuster\nPrerequisites\nCreate a PhantomBuster account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to PhantomBuster's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To get an API key, go to Workspace settings > Third party API keys and select + Add API Key. Refer to How to find my API key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\philipshue.md",
    "content": "Philips Hue credentials\nYou can use these credentials to authenticate the following nodes:\nPhilips Hue\nPrerequisites\nCreate a Philips Hue account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Philips Hue's CLIP API documentation for more information about the service.\nUsing OAuth2\nIf you're using the built-in OAuth connection, you don't need to enter an APP ID.\nIf you need to configure OAuth2 from scratch, you'll need a Philips Hue developer account\nCreate a new remote app on the Add new Hue Remote API app page.\nUse these settings for your app:\nCopy the OAuth Callback URL from n8n and add it as a Callback URL.\nCopy the AppId, ClientId, and ClientSecret and enter these in the corresponding fields in n8n."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\pinecone.md",
    "content": "Pinecone credentials\nYou can use these credentials to authenticate the following nodes:\nPinecone Vector Store\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Pinecone's documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a Pinecone account and:\nAn API Key\nTo get an API key:\nOpen your Pinecone console.\nSelect the project you want to create an API key for. If you don't have any existing projects, create one. Refer to Pinecone's Quickstart for more information.\nGo to API Keys.\nCopy the API Key displayed there and enter it in your n8n credential.\nRefer to Pinecone's API Authentication documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\pipedrive.md",
    "content": "Pipedrive credentials\nYou can use these credentials to authenticate the following nodes:\nPipedrive\nPipedrive Trigger\nSupported authentication methods\nAPI token\nOAuth2\nRelated resources\nRefer to Pipedrive's developer documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need a Pipedrive account and:\nAn API Token\nTo get your API token:\nOpen your API Personal Preferences.\nCopy Your personal API token and enter it in your n8n credential.\nIf you have multiple companies, you'll need to select the correct company first:\nSelect your account name and be sure you're viewing the correct company.\nThen select Company Settings.\nSelect Personal Preferences.\nSelect the API tab.\nCopy Your personal API token and enter it in your n8n credential.\nRefer to How to find the API token for more information.\nUsing OAuth2\nTo configure this credential, you'll need a Pipedrive developer sandbox account and:\nA Client ID\nA Client Secret\nTo get both, you'll need to register a new app:\nSelect your profile name in the upper right corner.\nFind the company name of your sandbox account and select Developer Hub.\nSelect Create an app.\nSelect Create public app. The app's Basic info tab opens.\nEnter an App name for your app, like n8n integration.\nCopy the OAuth Redirect URL from n8n and add it as the app's Callback URL.\nSelect Save. The app's OAuth & access scopes tab opens.\nTurn on appropriate Scopes for your app. Refer to Pipedrive node scopes and Pipedrive Trigger node scopes below for more guidance.\nCopy the Client ID and enter it in your n8n credential.\nCopy the Client Secret and enter it in your n8n credential.\nRefer to Registering a public app for more information.\nPipedrive node scopes\nThe scopes you add to your app depend on which node(s) you want to use it for in n8n and what actions you want to complete with those.\nScopes you may need for the Pipedrive node:\nTABLE_PLACEHOLDER_0\nThe Pipedrive node also supports Custom API calls. Add relevant scopes for whatever custom API calls you intend to make.\nRefer to Scopes and permissions explanations for more information.\nPipedrive Trigger node scopes\nThe Pipedrive Trigger node requires the Webhooks: Full access (webhooks:full) scope."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\plivo.md",
    "content": "Plivo credentials\nYou can use these credentials to authenticate the following nodes:\nPlivo\nPrerequisites\nCreate a Plivo account.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Plivo's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nAn Auth ID: Acts like your username. Copy yours from the Overview page of the Plivo console.\nAn Auth Token: Acts like a password. Copy yours from the Overview page of the Plivo console.\nRefer to How can I change my Auth ID or Auth Token? for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\postgres.md",
    "content": "Postgres credentials\nYou can use these credentials to authenticate the following nodes:\nPostgres\nAgent\nPostgres Chat Memory\nPGVector Vector Store\nPrerequisites\nCreate a user account on a Postgres server.\nSupported authentication methods\nDatabase connection\nRelated resources\nRefer to Postgres's documentation for more information about the service.\nUsing database connection\nTo configure this credential, you'll need:\nThe Host or domain name for the server.\nThe Database name.\nA User name.\nA user Password.\nIgnore SSL Issues: Set whether the credential connects if SSL validation fails.\nSSL: Choose whether to use SSL in your connection.\nThe Port number to use for the connection.\nSSH Tunnel: Choose if you want to use SSH to encrypt the network connection with the Postgres server.\nTo set up the database connection:\nEnter the Host or domain name for the Postgres server. You can either run the /conninfo command to confirm the host name or run this query:\nEnter the Database name. Run the /conninfo command to confirm the database name.\nEnter the User name of the user you wish to connect as.\nEnter the user's Password.\nIgnore SSL Issues: If you turn this on, the credential will connect even if SSL validation fails.\nSSL: Choose whether to use SSL in your connection. Refer to Postgres SSL Support for more information. Options include:\nAllow: Sets the ssl-mode parameter to allow. First try a non-SSL connection; if that fails, try an SSL connection.\nDisable: Sets the ssl-mode parameter to disable. Only try a non-SSL connection.\nRequire: Sets the ssl-mode parameter to require. Only try an SSL connection. If a root CA file is present, verify that a trusted certificate authority (CA) issued the server certificate.\nEnter the Port number to use for the connection. You can either run the /conninfo command to confirm the host name or run this query:\nSSH Tunnel: Turn this setting on to connect to the database over SSH. Refer to SSH tunnel limitations for some guidance around using SSH. Once turned on, you'll need:\nSelect SSH Authenticate with to set the SSH Tunnel type to build:\nSelect Password if you want to connect to SSH using a password.\nSelect Private Key if you want to connect to SSH using an identity file (private key) and a passphrase.\nEnter the remote bind address you're connecting to as the SSH Host.\nSSH Port: Enter the local port number for the SSH tunnel.\nSSH Postgres Port: Enter the remote end of the tunnel, the port number the database server is using.\nSSH User: Enter the username to log in as.\nIf you selected Password for SSH Authenticate with, add the user's SSH Password.\nIf you selected Private Key for SSH Authenticate with:\nAdd the contents of the Private Key or identity file used for SSH.\nIf the Private Key was created with a passphrase, enter that Passphrase. If the Private Key has no passphrase, leave this field blank.\nRefer to Secure TCP/IP Connections with SSH Tunnels for more information.\nSSH tunnel limitations\nOnly use the SSH Tunnel setting if:\nYou're using the credential with the Postgres node (Agent node doesn't support SSH tunnels).\nYou have an SSH server running on the same machine as the Postgres server.\nYou have a user account that can log in using ssh."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\posthog.md",
    "content": "PostHog credentials\nYou can use these credentials to authenticate the following nodes:\nPostHog\nPrerequisites\nCreate a PostHog account or host PostHog on your server.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to PostHog's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nThe API URL: Enter the correct domain for your API requests:\nOn US Cloud, use  for public POST-only endpoints or  for private endpoints.\nOn EU Cloud, use  for public POST-only endpoints or  for private endpoints.\nFor self-hosted instances, use your self-hosted domain.\nConfirm yours by checking your PostHog instance URL.\nAn API Key: The API key you use depends on whether you're accessing public or private endpoints:\nFor public POST-only endpoints, use a Project API key from your project's General Settings.\nFor private endpoints, use a Personal API key from your User account's Personal API Keys Settings. Refer to How to obtain a personal API key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\postmark.md",
    "content": "Postmark credentials\nYou can use these credentials to authenticate the following nodes:\nPostmark Trigger\nPrerequisites\nCreate a Postmark account on a Postmark server.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Postmark's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nA Server API Token: The Server API token is accessible by Account Owners, Account Admins, and users who have Server Admin privileges on a server. Get yours from the API Tokens tab under your Postmark server. Refer to API Authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\profitwell.md",
    "content": "ProfitWell credentials\nYou can use these credentials to authenticate the following nodes:\nProfitWell\nPrerequisites\nCreate a ProfitWell account.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Profitwell's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API Token: To get an API key or token, go to Account Settings > Integrations and select ProfitWell API."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\pushbullet.md",
    "content": "Pushbullet credentials\nYou can use these credentials to authenticate the following nodes:\nPushbullet\nPrerequisites\nCreate a Pushbullet account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Pushbullet's API documentation for more information about the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you create a Pushbullet app, also known as an OAuth client.\nA Client Secret: Generated when you create a Pushbullet app, also known as an OAuth client.\nTo generate the Client ID and Client Secret, go to the create client page. Copy the OAuth Redirect URL from n8n and add this as your redirect_uri for the app/client. Use the client_id and client_secret from the OAuth Client in your n8n credential.\nRefer to Pushbullet's OAuth2 Guide for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\pushcut.md",
    "content": "Pushcut credentials\nYou can use these credentials to authenticate the following nodes:\nPushcut\nPushcut Trigger\nPrerequisites\nDownload the Pushcut app.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Pushcut's Guides documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To generate an API key, go to Account > Integrations > Add API Key. Refer to Create an API key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\pushover.md",
    "content": "Pushover credentials\nYou can use these credentials to authenticate the following nodes:\nPushover\nPrerequisites\nCreate a Pushover account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Pushover's API documentation for more information about authenticating with the service.\nUsing API Key\nTo configure this credential, you'll need:\nAn API Key: Generated when you register an application. Refer to Application Registration for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\qdrant.md",
    "content": "Qdrant credentials\nYou can use these credentials to authenticate the following nodes:\nQdrant Vector Store\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Qdrant's documentation for more information.\nUsing API key\nTo configure this credential, you'll need a Qdrant cluster and:\nAn API Key\nYour Qdrant URL\nTo set it up:\nGo to the Cloud Dashboard.\nSelect Access Management to display available API keys (or go to the API Keys section of the Cluster detail page).\nSelect Create.\nSelect the cluster you want the key to have access to in the dropdown.\nSelect OK.\nCopy the API Key and enter it in your n8n credential.\nEnter the URL for your Qdrant cluster in the Qdrant URL. Refer to Qdrant Web UI for more information.\nRefer to Qdrant's authentication documentation for more information on creating and using API keys."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\qradar.md",
    "content": "QRadar credentials\nPrerequisites\nCreate a Qradar account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to QRadar's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Also known as an authorized service token. Use the Manage Authorized Services window on the Admin tab to create an authentication token. Refer to Creating an authentication token for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\qualys.md",
    "content": "Qualys credentials\nPrerequisites\nCreate a Qualys user account with any user role except Contact.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Qualys's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing basic auth\nTo configure this credential, you'll need:\nA Username\nA Password\nA Requested With string: Enter a user description, like a user agent, or keep the default n8n application. This sets the required X-Requested-With header."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\questdb.md",
    "content": "QuestDB credentials\nYou can use these credentials to authenticate the following nodes:\nQuestDB\nPrerequisites\nCreate a user account on an instance of QuestDB.\nSupported authentication methods\nDatabase connection\nRelated resources\nRefer to QuestDB's documentation for more information about the service.\nUsing database connection\nTo configure this credential, you'll need:\nThe Host: Enter the host name or IP address for the server.\nThe Database: Enter the database name, for example qdb.\nA User: Enter the username for the user account as configured in pg.user or pg.readonly.user property in server.conf. Default value is admin.\nA Password: Enter the password for the user account as configured in pg.password or pg.readonly.password property in server.conf. Default value is quest.\nSSL: Select whether the connection should use SSL, which sets the sslmode parameter. Options include:\nAllow\nDisable\nRequire\nThe Port: Enter the port number to use for the connection. Default is 8812.\nRefer to List of supported connection properties for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\quickbase.md",
    "content": "Quick Base credentials\nYou can use these credentials to authenticate the following nodes:\nQuick Base\nPrerequisites\nCreate a Quick Base account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Quick Base's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Hostname: The string of characters located between https:// and /db in your Quick Base URL.\nA User Token: To generate a token, select your Profile > My preferences > My User Information > Manage my user tokens. Refer to Creating and using user tokens for detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\quickbooks.md",
    "content": "QuickBooks credentials\nYou can use these credentials to authenticate the following nodes:\nQuickBooks\nPrerequisites\nCreate an Intuit developer account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Intuit's API documentation for more information about the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you create an app.\nA Client Secret: Generated when you create an app.\nAn Environment: Select whether this credential should access your Production or Sandbox environment.\nTo generate your Client ID and Client Secret, create an app.\nUse these settings when creating your app:\nSelect appropriate scopes for your app. Refer to Learn about scopes for more information.\nEnter the OAuth Redirect URL from n8n as a Redirect URI in the app's Development > Keys & OAuth section.\nCopy the Client ID and Client Secret from the app's Development > Keys & OAuth section to enter in n8n. Refer to Get the Client ID and Client Secret for your app for more information.\nRefer to Intuit's Set up OAuth 2.0 documentation for more information on the entire process."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\rabbitmq.md",
    "content": "RabbitMQ credentials\nYou can use these credentials to authenticate the following nodes:\nRabbitMQ\nRabbitMQ Trigger\nSupported authentication methods\nUser connection\nRelated resources\nRefer to RabbitMQ's Connections documentation for more information about the service.\nUsing user connection\nTo configure this credential, you'll need to have a RabbitMQ broker installed and:\nEnter the Hostname for the RabbitMQ broker.\nEnter the Port the connection should use.\nEnter a User the connection should use to log in as.\nThe default is guest. RabbitMQ recommends using a different user in production environments. Refer to Access Control | The Basics for more information. If you're using the guest account with a non-localhost connection, refer to guest user issues below for troubleshooting tips.\nEnter the user's Password.\nThe default password for the guest user is guest.\nEnter the virtual host the connection should use as the Vhost. The default virtual host is /.\nSelect whether the connection should use SSL. If turned on, also set:\nPasswordless: Select whether the SSL certificate connection users SASL mechanism EXTERNAL (turned off) or doesn't use a password (turned on). If turned on, you'll also need to enter:\nThe Client Certificate: Paste the text of the SSL client certificate to use.\nThe Client Key: Paste the SSL client key to use.\nThe Passphrase: Paste the SSL passphrase to use.\nCA Certificates: Paste the text of the SSL CA certificates to use.\nguest user issues\nIf you use the guest user for the credential and you try to access a remote host, you may see a connection error. The RabbitMQ logs show an error like this:\n[error] <0.918.0> PLAIN login refused: user 'guest' can only connect via localhost\nThis happens because RabbitMQ prohibits the default guest user from connecting from remote hosts. It can only connect over the localhost.\nTo resolve this error, you can:\nUpdate the guest user to allow it remote host access.\nCreate or use a different user to connect to the remote host. The guest user is the only user limited by default.\nRefer to \"guest\" user can only connect from localhost for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\raindrop.md",
    "content": "Raindrop credentials\nYou can use these credentials to authenticate the following nodes:\nRaindrop\nPrerequisites\nCreate a Raindrop account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Raindrop's API documentation for more information about the service.\nUsing OAuth\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nGenerate both by creating a Raindrop app.\nTo create an app, go to Settings > Integrations and select + Create new app in the For Developers section.\nUse these settings for your app:\nCopy the OAuth Redirect URL from n8n and add it as a Redirect URI in your app.\nCopy the Client ID and Client Secret from the Raindrop app and enter them in your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\rapid7insightvm.md",
    "content": "Rapid7 InsightVM credentials\nPrerequisites\nCreate a Rapid7 InsightVM account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Rapid7 InsightVM's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need a Rapid7 InsightVM account and:\nA URL: The API endpoint URL where the resource or data you are requesting lives. You can find more information about the expected format in the endpoint section of the Rapid7's API overview.\nAn API Key: Refer to Rapid7's Managing Platform API Keys documentation to create an API key.\nRefer to Rapid7 InsightVM's API documentation for more information about authenticating to the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\recordedfuture.md",
    "content": "Recorded Future credentials\nPrerequisites\nCreate a Recorded Future account.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Recorded Future's documentation for more information about the service. The rest of Recorded Future's help center requires a paid account.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API access token\nTo configure this credential, you'll need:\nAn API Access Token\nRefer to the Recorded Future APIs documentation for more information on getting your API access token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\reddit.md",
    "content": "Reddit credentials\nYou can use these credentials to authenticate the following nodes:\nReddit\nPrerequisites\nCreate a Reddit account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Reddit's developer documentation for more information about the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nGenerate both by creating a third-party app. Visit the previous link or go to your profile > Settings > Safety & Privacy > Manage third-party app authorization > are you a developer? create an app.\nUse these settings for your app:\nCopy the OAuth Callback URL from n8n and use it as your app's redirect uri.\nThe app's client ID displays underneath your app name. Copy that and add it as your n8n Client ID.\nCopy the app's secret and add it as your n8n Client Secret."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\redis.md",
    "content": "Redis credentials\nYou can use these credentials to authenticate the following nodes:\nRedis\nRedis Chat Memory\nSupported authentication methods\nDatabase connection\nRelated resources\nRefer to Redis's developer documentation for more information about the service.\nUsing database connection\nYou'll need a user account on a Redis server and:\nA Password\nThe Host name\nThe Port number\nA Database Number\nSSL\nTo configure this credential:\nEnter your user account Password.\nEnter the Host name of the Redis server. The default is localhost.\nEnter the Port number the connection should use. The default is 6379.\nThis number should match the tcp_port listed when you run the INFO command.\nEnter the Database Number. The default is 0.\nIf the connection should use SSL, turn on the SSL toggle. If this toggle is off, the connection uses TCP only.\nRefer to Connecting to Redis | Generic client for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\rocketchat.md",
    "content": "Rocket.Chat credentials\nYou can use these credentials to authenticate the following nodes:\nRocket.Chat\nPrerequisites\nCreate a Rocket.Chat account.\nYour account must have the create-personal-access-tokens permission to generate personal access tokens.\nSupported authentication methods\nAPI access token\nRelated resources\nRefer to Rocket.Chat's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nYour User ID: Displayed when you generate an access token.\nAn Auth Key: Your personal access token. To generate an access token, go to your avatar > Account > Personal Access Tokens. Copy the token and add it as the n8n Auth Key.\nYour Rocket.Chat Domain: Also known as your default URL or workspace URL.\nRefer to Personal Access Tokens for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\rundeck.md",
    "content": "Rundeck credentials\nYou can use these credentials to authenticate the following nodes:\nRundeck\nPrerequisites\nCreate a user account on a Rundeck server.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Rundeck's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nYour URL: Enter the base URL of your Rundeck server, for example  Refer to URLs for more information.\nA user API Token: To generate a user API token, go to your Profile > User API Tokens. Refer to User API tokens for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\s3.md",
    "content": "S3 credentials\nYou can use these credentials to authenticate the following nodes:\nS3\nPrerequisites\nCreate an account on an S3-compatible server. Use the S3 node for generic or non-AWS S3 like:\nDigitalOcean Spaces\nMinIO\nWasabi\nSupported authentication methods\nS3 endpoint\nRelated resources\nRefer to your S3-compatible provider's documentation for more information on the services. For example, refer to Wasabi's REST API documentation or DigitalOcean's Spaces API Reference Documentation.\nUsing S3 endpoint\nTo configure this credential, you'll need:\nAn S3 Endpoint: Enter the URL endpoint for the S3 storage backend.\nA Region: Enter the region for your S3 storage. Some providers call this the \"region slug.\"\nAn Access Key ID: Enter the S3 access key your S3 provider uses to access the bucket or space. Some providers call this API keys.\nA Secret Access Key: Enter the secret access key for the Access Key ID.\nForce Path Style: When turned on, the connection uses path-style addressing for buckets.\nIgnore SSL Issues: When turned on, n8n will connect even if SSL certificate validation fails.\nMore detailed instructions for DigitalOcean Spaces and Wasabi follow. If you're using a different provider, refer to their documentation for more information.\nUsing DigitalOcean Spaces\nTo configure the credential for use with DigitalOcean spaces:\nIn DigitalOceans, go to the control panel and open Settings. Your endpoint should be listed there. Prepend https:// to that endpoint and enter it as the S3 Endpoint in n8n.\nYour DigitalOceans endpoint depends on the data center region your bucket's in.\nFor the Region, enter the region your bucket's located in, for example, nyc3.\nIf you plan to use this credential to create new Spaces, enter us-east-1 instead.\nFrom your DigitalOceans control panel, go to API.\nOpen the Spaces Keys tab.\nSelect Generate New Key.\nEnter a Name for your key, like n8n integration and select the checkmark.\nCopy the Key displayed next to the name and enter this as the Access Key ID in n8n.\nCopy the Secret value and enter this as the Secret Access Key in n8n.\nRefer to Sharing Access to Buckets with Access Keys for more information on generating the key and secret.\nKeep the Force Path Style toggle turned off unless you want to use subdomain/virtual calling format.\nDecide how you want the n8n credential to handle SSL:\nTo respect SSL certificate validation, keep the default of Ignore SSL Issues turned off.\nTo connect even if SSL certificate validation fails, turn on Ignore SSL Issues.\nRefer to DigitalOcean's Spaces API Reference Documentation for more information.\nUsing Wasabi\nTo configure the credential for use with Wasabi:\nFor the S3 Endpoint, enter the service URL for your bucket's region. Start it with\nRefer to Service URLs for Wasabi's Storage Regions to identify the correct URL.\nFor the Region, enter the region slug portion of the service URL. For example, if you entered  as the S3 Endpoint, us-east-2 is the region.\nLog into you Wasabi Console as the root user.\nOpen the Menu and select Access Keys.\nSelect CREATE NEW ACCESS KEY.\nSelect whether the key is for the Root User or a Sub-User and select CREATE.\nCopy the Access Key and enter it in n8n as the Access Key ID.\nCopy the Secret Key and enter it in n8n as the Secret Access Key.\nRefer to Creating a New Access Key for more information on generating the key and secret.\nWasabi recommends turning on the Force Path Style toggle \"because the path-style offers the greatest flexibility in bucket names, avoiding domain name issues.\" Refer to the Wasabi REST API Introduction for more information.\nDecide how you want the n8n credential to handle SSL:\nTo respect SSL certificate validation, keep the default of Ignore SSL Issues turned off.\nTo connect even if SSL certificate validation fails, turn on Ignore SSL Issues."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\salesforce.md",
    "content": "Salesforce credentials\nYou can use these credentials to authenticate the following nodes:\nSalesforce\nSalesforce trigger\nSupported authentication methods\nJWT\nOAuth2\nRelated resources\nRefer to Salesforce's developer documentation for more information about the service.\nUsing JWT\nTo configure this credential, you'll need a Salesforce account and:\nYour Environment Type (Production or Sandbox)\nA Client ID: Generated when you create a connected app.\nYour Salesforce Username\nA Private Key for a self-signed digital certificate\nTo set things up, first you'll create a private key and certificate, then a connected app:\nIn n8n, select the Environment Type for your connection. Choose the option that best describes your environment from Production or Sandbox.\nEnter your Salesforce Username.\nLog in to your org in Salesforce.\nYou'll need a private key and certificate issued by a certification authority. Use your own key/cert or use OpenSSL to create a key and a self-signed digital certificate. Refer to the Salesforce Create a Private Key and Self-Signed Digital Certificate documentation for instructions on creating your own key and certificate.\nFrom Setup in Salesforce, enter App Manager in the Quick Find box, then select App Manager.\nOn the App Manager page, select New Connected App.\nEnter the required Basic Info for your connected app, including a Name and Contact Email address. Refer to Salesforce's Configure Basic Connected App Settings documentation for more information.\nCheck the box to Enable OAuth Settings.\nFor the Callback URL, enter\nCheck the box to Use digital signatures.\nSelect Choose File and upload the file that contains your digital certificate, such as server.crt.\nAdd these OAuth scopes:\nFull access (full)\nPerform requests at any time (refresh_token, offline_access)\nSelect Save, then Continue. The Manage Connected Apps page should open to the app you just created.\nIn the API (Enable OAuth Settings) section, select Manage Consumer Details.\nCopy the Consumer Key and add it to your n8n credential as the Client ID.\nEnter the contents of the private key file in n8n as Private Key.\nUse the multi-line editor in n8n.\nEnter the private key in standard PEM key format:\nThese steps are what's required on the n8n side. Salesforce recommends setting refresh token policies, session policies, and OAuth policies too:\nIn Salesforce, select Back to Manage Connected Apps.\nSelect Manage.\nSelect Edit Policies.\nReview the Refresh Token Policy field. Salesforce recommends using expire refresh token after 90 days.\nIn the Session Policies section, Salesforce recommends setting Timeout Value to 15 minutes.\nIn the OAuth Policies section, select Admin approved users are pre-authorized for permitted users for Permitted Users, and select OK.\nSelect Save.\nSelect Manage Profiles, select the profiles that are pre-authorized to use this connected app, and select Save.\nSelect Manage Permission Sets to select the permission sets. Create permission sets if necessary.\nRefer to Salesforce's Create a Connected App in Your Org documentation for more information.\nUsing OAuth2\nTo configure this credential, you'll need a Salesforce account.\nCloud and hosted users will need to select your Environment Type. Choose between Production and Sandbox.\nIf you're self-hosting n8n, you'll need to configure OAuth2 from scratch by creating a connected app:\nIn n8n, select the Environment Type for your connection. Choose the option that best describes your environment from Production or Sandbox.\nEnter your Salesforce Username.\nLog in to your org in Salesforce.\nFrom Setup in Salesforce, enter App Manager in the Quick Find box, then select App Manager.\nOn the App Manager page, select New Connected App.\nEnter the required Basic Info for your connected app, including a Name and Contact Email address. Refer to Salesforce's Configure Basic Connected App Settings documentation for more information.\nCheck the box to Enable OAuth Settings.\nFor the Callback URL, enter\nAdd these OAuth scopes:\nFull access (full)\nPerform requests at any time (refresh_token, offline_access)\nMake sure the following settings are unchecked:\nRequire Proof Key for Code Exchange (PKCE) Extension for Supported Authorization Flows\nRequire Secret for Web Server Flow\nRequire Secret for Refresh Token Flow\nSelect Save, then Continue. The Manage Connected Apps page should open to the app you just created.\nIn the API (Enable OAuth Settings) section, select Manage Consumer Details.\nCopy the Consumer Key and add it to your n8n credential as the Client ID.\nCopy the Consumer Secret and add it to your n8n credential as the Client Secret.\nThese steps are what's required on the n8n side. Salesforce recommends setting refresh token policies and session policies, too:\nIn Salesforce, select Back to Manage Connected Apps.\nSelect Manage.\nSelect Edit Policies.\nReview the Refresh Token Policy field. Salesforce recommends using expire refresh token after 90 days.\nIn the Session Policies section, Salesforce recommends setting Timeout Value to 15 minutes.\nRefer to Salesforce's Create a Connected App in Your Org documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\salesmate.md",
    "content": "Salesmate credentials\nYou can use these credentials to authenticate the following nodes:\nSalesmate\nPrerequisites\nCreate a Salesmate account.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Salesmate's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nA Session Token: An Access Key. Generate an access key in My Account > Access Key. Refer to Access Rights and Keys for more information.\nA URL: Your Salesmate domain name/base URL, for example n8n.salesmate.io."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\searxng.md",
    "content": "SearXNG credentials\nYou can use these credentials to authenticate the following nodes:\nSearXNG Tool\nSupported authentication methods\nAPI URL\nRelated resources\nRefer to SearXNG's documentation for more information about the service.\nUsing API URL\nTo configure this credential, you'll need an instance of SearXNG running at an URL that's accessible from n8n:\nAPI URL: The URL of the SearXNG instance you want to connect to.\nRefer to SearXNG's Administrator documentation for more information about running the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\seatable.md",
    "content": "SeaTable credentials\nYou can use these credentials to authenticate the following nodes:\nSeaTable\nSeaTable Trigger\nPrerequisites\nCreate a SeaTable account on either a cloud or self-hosted SeaTable server.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to SeaTable's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn Environment: Select the environment that matches your SeaTable instance:\nCloud-Hosted\nSelf-Hosted\nAn API Token (of a Base): Generate a Base-Token in SeaTable from the base options > Advanced > API Token.\nUse Read-Write permission for your token.\nRefer to Creating an API token for more information.\nA Timezone: Select the timezone of your SeaTable server."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\securityscorecard.md",
    "content": "SecurityScorecard credentials\nYou can use these credentials to authenticate the following nodes:\nSecurityScorecard\nPrerequisites\nCreate a SecurityScorecard account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to SecurityScorecard's Developer documentation and API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Generate an API key in one of two ways:\nAs a user in My Settings > API. Refer to Get an API key for more information.\nAs a bot user: View the bot user and select create token. Refer to Authenticate with a bot user for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\segment.md",
    "content": "Segment credentials\nYou can use these credentials to authenticate the following nodes:\nSegment\nPrerequisites\nCreate a Segment account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Segment's Sources documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Write Key: To get a Write Key, go to Sources > Add Source. Add a Node.js source and copy that write key to add to your n8n credential.\nRefer to Locate your Write Key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sekoia.md",
    "content": "Sekoia credentials\nPrerequisites\nCreate a Sekoia SOC platform account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Sekoia's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: To generate an API key, select + API Key. Refer to Create an API key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sendgrid.md",
    "content": "SendGrid credentials\nYou can use these credentials to authenticate the following nodes:\nSendGrid\nSupported authentication methods\nAPI key\nRelated resources\nRefer to SendGrid's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a SendGrid account and:\nAn API Key\nTo create an API key:\nIn the Twilio SendGrid app, go to Settings > API Keys.\nSelect Create API Key.\nEnter a Name for your API key, like n8n integration.\nSelect Full Access.\nSelect Create & View.\nCopy the key and enter it in your n8n credential.\nRefer to Create API Keys for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sendy.md",
    "content": "Sendy credentials\nYou can use these credentials to authenticate the following nodes:\nSendy\nPrerequisites\nHost a Sendy application.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Sendy's API documentation for more information about the service.\nUsing API Key\nTo configure this credential, you'll need:\nA URL: The URL of your Sendy application.\nAn API Key: Get your API key from your user profile > Settings > Your API Key."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sentryio.md",
    "content": "Sentry.io credentials\nYou can use these credentials to authenticate the following nodes:\nSentry.io\nPrerequisites\nCreate a Sentry.io account.\nSupported authentication methods\nAPI token\nOAuth2\nServer API token: Use for self-hosted Sentry.\nRelated resources\nRefer to Sentry.io's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API Token: Generate a User Auth Token in Account > Settings > User Auth Tokens. Refer to User Auth Tokens for more information.\nUsing OAuth\nIf you need to configure OAuth2 from scratch, create an integration with these settings:\nCopy the n8n OAuth Callback URL and add it as an Authorized Redirect URI.\nCopy the Client ID and Client Secret and add them to your n8n credential.\nRefer to Public integrations for more information on creating the integration.\nUsing Server API token\nTo configure this credential, you'll need:\nAn API Token: Generate a User Auth Token in Account > Settings > User Auth Tokens. Refer to User Auth Tokens for more information.\nThe URL of your self-hosted Sentry instance."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\serp.md",
    "content": "Serp credentials\nYou can use these credentials to authenticate the following nodes:\nSerp\nPrerequisites\nCreate a SerpApi account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Serp's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nTo get your API key:\nGo to Your Account > API Key.\nCopy Your Private API Key and enter it as the API Key in your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\servicenow.md",
    "content": "ServiceNow credentials\nYou can use these credentials to authenticate the following nodes:\nServiceNow\nPrerequisites\nCreate a ServiceNow developer account.\nSupported authentication methods\nBasic auth\nOAuth2\nRelated resources\nRefer to ServiceNow's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA User name: Enter your ServiceNow username.\nA Password: Enter your ServiceNow password.\nA Subdomain: The subdomain for your servicenow instance is in your instance URL:  For example, if the full URL is  then the subdomain is dev99890.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated once you register a new app.\nA Client Secret: Generated once you register a new app.\nA Subdomain: The subdomain for your servicenow instance is in your instance URL:  For example, if the full URL is  then the subdomain is dev99890.\nTo generate your Client ID and Client Secret, register a new app in System OAuth > Application Registry > New > Create an OAuth API endpoint for external clients. Use these settings for your app:\nCopy the Client ID and add it to your n8n credential.\nEnter a Client Secret or leave it blank to automatically generate a random secret. Add this secret to your n8n credential.\nCopy the n8n OAuth Redirect URL and add it as a Redirect URL.\nRefer to How to setup OAuth2 authentication for RESTMessageV2 integrations for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\shopify.md",
    "content": "Shopify credentials\nYou can use these credentials to authenticate the following nodes with Shopify.\nShopify\nShopify Trigger\nSupported authentication methods\nAccess token (recommended): For private apps/single store use. Can be created by regular admins.\nOAuth2: For public apps. Must be created by partner accounts.\nAPI key: Deprecated.\nRelated resources\nRefer to Shopify's authentication documentation for more information about the service.\nUsing access token\nTo configure this credential, you'll need a Shopify admin account and:\nYour Shop Subdomain\nAn Access Token: Generated when you create a custom app.\nAn APP Secret Key: Generated when you create a custom app.\nTo set up the credential, you'll need to create and install a custom app:\nEnter your Shop Subdomain.\nYour subdomain is within the URL:  For example, if the full URL is  the Shop Subdomain is n8n.\nIn Shopify, go to Admin > Settings > Apps and sales channels.\nSelect Develop apps.\nSelect Create a custom app.\nIn the modal window, enter the App name.\nSelect an App developer. The app developer can be the store owner or any account with the Develop apps permission.\nSelect Create app.\nSelect Select scopes. In the Admin API access scopes section, select the API scopes you want for your app.\nTo use all functionality in the Shopify node, add the read_orders, write_orders, read_products, and write_products scopes.\nRefer to Shopify API Access Scopes for more information on the available scopes.\nSelect Save.\nSelect Install app.\nIn the modal window, select Install app.\nOpen the app's API Credentials section.\nCopy the Admin API Access Token. Enter this in your n8n credential as the Access Token.\nCopy the API Secret Key. Enter this in your n8n credential as the APP Secret Key.\nRefer to Creating a custom app and Generate access tokens for custom apps in the Shopify admin for more information on these steps.\nUsing OAuth2\nTo configure this credential, you'll need a Shopify partner account and:\nA Client ID: Generated when you create a custom app.\nA Client Secret: Generated when you create a custom app.\nYour Shop Subdomain\nTo set up the credential, you'll need to create and install a custom app:\nOpen your Shopify Partner dashboard.\nSelect Apps from the left navigation.\nSelect Create app.\nIn the Use Shopify Partners section, enter an App name.\nSelect Create app.\nWhen the app details open, copy the Client ID. Enter this in your n8n credential.\nCopy the Client Secret. Enter this in your n8n credential.\nIn the left menu, select Configuration.\nIn n8n, copy the OAuth Redirect URL and paste it into the Allowed redirection URL(s) in the URLs section.\nIn the URLs section, enter an App URL for your app. The host entered here needs to match the host for the Allowed redirection URL(s), like the base URL for your n8n instance.\nSelect Save and release.\nSelect Overview from the left menu. At this point, you can choose to Test your app by installing it to one of your stores, or Choose distribution to distribute it publicly.\nIn n8n, enter the Shop Subdomain of the store you installed the app to, either as a test or as a distribution.\nYour subdomain is within the URL:  For example, if the full URL is  the Shop Subdomain is n8n.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nA Password\nYour Shop Subdomain: Your subdomain is within the URL:  For example, if the full URL is  the Shop Subdomain is n8n.\nOptional: A Shared Secret\nCommon issues\nHere are some common issues setting up the Shopify credential and steps to resolve or troubleshoot them.\nEnable custom app development\nIf you don't see the option to Create a custom app, no one's enabled custom app development for your store.\nTo enable custom app development, you must log in either as a store owner or as a user with the Enable app development permission:\nIn Shopify, go to Admin > Settings > Apps and sales channels.\nSelect Develop apps.\nSelect Allow custom app development.\nRead the warning and information provided and select Allow custom app development.\nForbidden credentials error\nIf you get a Couldn't connect with these settings / Forbidden - perhaps check your credentials warning when you test the credentials, this may be due to your app's access scope dependencies. For example, the read_orders scope also requires read_products scope. Review the scopes you have assigned and the action you're trying to complete."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\shuffler.md",
    "content": "Shuffler credentials\nPrerequisites\nCreate a Shuffler account on either a cloud or self-hosted instance.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Shuffler's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Get your API key from the Settings page."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\signl4.md",
    "content": "SIGNL4 credentials\nYou can use these credentials to authenticate the following nodes:\nSIGNL4\nPrerequisites\nCreate a SIGNL4 account.\nSupported authentication methods\nWebhook secret\nRelated resources\nRefer to SIGNL4's Inbound Webhook documentation for more information about the service.\nUsing webhook secret\nTo configure this credential, you'll need:\nA Team Secret: SIGNL4 includes this secret in the \"‚úÖ Sign up complete\" email as the last part of the webhook URL. If your webhook URL is  your team secret would be helloworld."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\slack.md",
    "content": "Slack credentials\nYou can use these credentials to authenticate the following nodes:\nSlack\nSlack Trigger\nSupported authentication methods\nAPI access token:\nRequired for the Slack Trigger node.\nWorks with the Slack node, but not recommended.\nOAuth2:\nRecommended method for the Slack node.\nDoesn't work with the Slack Trigger node.\nRelated resources\nRefer to Slack's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need a Slack account and:\nAn Access Token\nTo generate an access token, create a Slack app:\nOpen your Slack API Apps page.\nSelect Create New App > From scratch.\nEnter an App Name.\nSelect the Workspace where you'll be developing your app.\nSelect Create App. The app details open.\nIn the left menu under Features, select OAuth & Permissions.\nIn the Scopes section, select appropriate scopes for your app. Refer to Scopes for a list of recommended scopes.\nAfter you've added scopes, go up to the OAuth Tokens section and select Install to Workspace. You must be a Slack workspace admin to complete this action.\nSelect Allow.\nCopy the Bot User OAuth Token and enter it as the Access Token in your n8n credential.\nIf you're using this credential for the Slack Trigger, follow the steps in Slack Trigger configuration to finish setting up your app.\nRefer to the Slack API Quickstart for more information.\nSlack Trigger configuration\nTo use your Slack app with the Slack Trigger node:\nGo to Features > Event Subscriptions.\nTurn on the Enable Events control.\nIn n8n, copy the Webhook URL and enter it as the Request URL in your Slack app.\nOnce verified, select the bot events to subscribe to. Use the Trigger on field in n8n to filter these requests.\nTo use an event not in the list, add it as a bot event and select Any Event in the n8n node.\nRefer to Quickstart TABLE_PLACEHOLDER_0\nCommon issues\nToken expired"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sms77.md",
    "content": "seven credentials\nYou can use these credentials to authenticate the following nodes:\nseven\nPrerequisites\nCreate a seven developer account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to seven's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API key: Go to Account > Developer > API Keys to create an API key. Refer to API First Steps for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\snowflake.md",
    "content": "Snowflake credentials\nYou can use these credentials to authenticate the following nodes:\nSnowflake\nPrerequisites\nCreate a Snowflake account.\nSupported authentication methods\nDatabase connection\nRelated resources\nRefer to Snowflake's API documentation and SQL Command Reference for more information about the service.\nUsing database connection\nTo configure this credential, you'll need:\nAn Account name: Your account name is the string of characters located between https:// and snowflakecomputing.com in your Snowflake URL. For example, if the URL of your Snowflake account is  then the name of your account is abc.eu-central-1.\nA Database: Enter the name of the database the credential should connect to.\nA Warehouse: Enter the name of the default virtual warehouse to use for the session after connecting. n8n uses this warehouse for performing queries, loading data, and so on.\nA Username\nA Password\nA Schema: Enter the schema you want to use after connecting.\nA Role: Enter the security role you want to use after connecting.\nClient Session Keep Alive: By default, client connections typically time out three or four hours after the most recent query execution. Turning this setting on sets the clientSessionKeepAlive parameter to true: the server will keep the client's connection alive indefinitely, even if the connection doesn't execute any queries.\nRefer to Session Commands for more information on these settings."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\solarwindsipam.md",
    "content": "SolarWinds IPAM credentials\nSupported authentication methods\nUsername & Password\nRelated resources\nRefer to SolarWinds IPAM's API documentation for more information about the service.\nUsing Username & Password\nTo configure this credential, you'll need a SolarWinds IPAM account and:\nURL: The base URL of your SolarWinds IPAM server\nUsername: The username you use to access SolarWinds IPAM\nPassword: The password you use to access SolarWinds IPAM\nRefer to SolarWinds IPAM's API documentation for more information about authenticating to the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\solarwindsobservability.md",
    "content": "SolarWinds Observability SaaS credentials\nSupported authentication methods\nAPI Token\nRelated resources\nRefer to SolarWinds Observability SaaS's API documentation for more information about the service.\nUsing API Token\nTo configure this credential, you'll need a SolarWinds Observability SaaS account and:\nURL: The URL you use to access the SolarWinds Observability SaaS platform\nAPI Token: An API token found in the SolarWinds Observability SaaS platform under Settings > Api Tokens\nRefer to SolarWinds Observability SaaS's API documentation for more information about authenticating to the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\splunk.md",
    "content": "Splunk credentials\nYou can use these credentials to authenticate the following nodes:\nSplunk\nPrerequisites\nDownload and install Splunk Enterprise.\nEnable token authentication in Settings > Tokens.\nSupported authentication methods\nAPI auth token\nRelated resources\nRefer to Splunk's Enterprise API documentation for more information about the service.\nUsing API auth token\nTo configure this credential, you'll need:\nAn Auth Token: Once you've enabled token authentication, create an auth token in Settings > Tokens. Refer to Creating authentication tokens for more information.\nA Base URL: For your Splunk instance. This should include the protocol, domain, and port, for example:\nAllow Self-Signed Certificates: If turned on, n8n will connect even if SSL validation fails.\nRequired capabilities\nYour Splunk platform account and role must have certain capabilities to create authentication tokens:\nedit_tokens_own: Required if you want to create tokens for yourself.\nedit_tokens_all: Required if you want to create tokens for any user on the instance.\nRefer to Define roles on the Splunk platform with capabilities for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\spontit.md",
    "content": "Spontit credentials\nYou can use these credentials to authenticate the following nodes:\nSpontit\nPrerequisites\nCreate a Spontit account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Spontit's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Go to API to get a new secret key. Enter that key as your API Key.\nA Username: Enter your Spontit username. If you're unsure how to format it, open your Profile and copy the username from there."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\spotify.md",
    "content": "Spotify credentials\nYou can use these credentials to authenticate the following nodes:\nSpotify\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Spotify's Web API documentation for more information about the service.\nUsing OAuth2\nIf you're self-hosting n8n, you'll need a Spotify Developer account so you can create a Spotify app:\nOpen the Spotify developer dashboard.\nSelect Create an app.\nEnter an App name, like n8n integration.\nEnter an App description.\nCopy the OAuth Redirect URL from n8n and enter it as the Redirect URI in your Spotify app.\nCheck the box to agree to the Spotify Terms of Service and Branding Guidelines.\nSelect Create. The App overview page opens.\nCopy the Client ID and enter it in your n8n credential.\nCopy the Client Secret and enter it in your n8n credential.\nSelect Connect my account and follow the on-screen prompts to finish authorizing the credential.\nRefer to Spotify Apps for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\ssh.md",
    "content": "SSH credentials\nYou can use these credentials to authenticate the following nodes:\nSSH\nPrerequisites\nCreate a remote server with SSH enabled.\nCreate a user account that can ssh into the server using one of the following:\nTheir own password\nAn SSH private key\nSupported authentication methods\nPassword: Use this method if you have a user account that can ssh into the server using their own password.\nPrivate key: Use this method if you have a user account that uses an SSH key for the server or service.\nRelated resources\nSecure Shell (SSH) protocol is a method for securely sending commands over a network. Refer to Connecting to GitHub with SSH for an example of SSH setup.\nUsing password\nUse this method if you have a user account that can ssh into the server using their own password.\nTo configure this credential, you'll need to:\nEnter the IP address of the server you're connecting to as the Host.\nEnter the Port to use for the connection. SSH uses port 22 by default.\nEnter the Username for a user account with ssh access on the server.\nEnter the Password for that user account.\nUsing private key\nUse this method if you have a user account that uses an SSH key for the server or service.\nTo configure this credential, you'll need to:\nEnter the IP address of the server you're connecting to as the Host.\nEnter the Port to use for the connection. SSH uses port 22 by default.\nEnter the Username of the account that generated the private key.\nEnter the entire contents of your SSH Private Key.\nIf you created a Passphrase for the Private Key, enter the passphrase.\nIf you didn't create a passphrase for the key, leave blank."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\stackby.md",
    "content": "Stackby credentials\nYou can use these credentials to authenticate the following nodes:\nStackby\nPrerequisites\nCreate a Stackby account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Stackby's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Go to your Account Settings > API to create an API Key. Refer to API Key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\storyblok.md",
    "content": "Storyblok credentials\nYou can use these credentials to authenticate the following nodes:\nStoryblok\nPrerequisites\nCreate a Storyblok account.\nSupported authentication methods\nContent API key: For read-only access\nManagement API key: For full CRUD operations\nRelated resources\nRefer to Storyblok's Content v1 API documentation and Management API documentation for more information about the services.\nUsing Content API key\nTo configure this credential, you'll need:\nA Content API Key: Go to your Storyblok workspace's Settings > Access Tokens to get an API key. Choose an Access Level of either Public (version=published) or Preview (version-published and version=draft). Enter this access token as your API Key. Refer to How to retrieve and generate access tokens for more detailed instructions.\nRefer to Content v1 API Authentication for more information about supported operations with each Access Level.\nUsing Management API key\nTo configure this credential, you'll need:\nA Personal Access Token: Go to My Account > Personal access tokens to generate a new access token. Enter this access token as your Personal Access Token."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\strapi.md",
    "content": "Strapi credentials\nYou can use these credentials to authenticate the following nodes:\nStrapi\nPrerequisites\nCreate a Strapi admin account with:\nAccess to an existing Strapi project.\nAt least one collection type within that project.\nPublished data within that collection type.\nRefer to the Strapi developer Quick Start Guide for more information.\nSupported authentication methods\nAPI user account: Requires a user account with appropriate content permissions.\nAPI token: Requires an admin account.\nRelated resources\nRefer to Strapi's documentation for more information about the service.\nUsing API user account\nTo configure this credential, you'll need:\nA user Email: Must be for a user account, not an admin account. Refer to the more detailed instructions below.\nA user Password: Must be for a user account, not an admin account. Refer to the more detailed instructions below.\nThe URL: Use the public URL of your Strapi server, defined in ./config/server.js as the url parameter. Strapi recommends using an absolute URL.\nFor Strapi Cloud projects, use the URL of your Cloud project, for example:\nThe API Version: Select the version of the API you want your calls to use. Options include:\nVersion 3\nVersion 4\nIn Strapi, the configuration involves two steps:\nConfigure a role.\nCreate a user account.\nRefer to the more detailed instructions below for each step.\nConfigure a role\nFor API access, use the Users & Permissions Plugin in Settings > Users & Permissions Plugin.\nRefer to Configuring Users & Permissions Plugin for more information on the plugin. Refer to Configuring end-user roles for more information on roles.\nFor the n8n credential, the user must have a role that grants them API permissions on the collection type. For the role, you can either:\nUpdate the default Authenticated role to include the permissions and assign the user to that role. Refer to Configuring role's permissions for more information.\nCreate a new role to include the permissions and assign the user to that role. Refer to Creating a new role for more information.\nFor either option, once you open the role:\nGo to the Permissions section.\nOpen the section for the relevant collection type.\nSelect the permissions for the collection type that the role should have. Options include:\ncreate (POST)\nfind and findone (GET)\nupdate (PUT)\ndelete (DELETE)\nRepeat for all relevant collection types.\nSave the role.\nRefer to Endpoints for more information on the permission options.\nCreate a user account\nNow that you have an appropriate role, create an end-user account and assign the role to it:\nGo to Content Manager > Collection Types > User.\nSelect Add new entry.\nFill in the user details. The n8n credential requires these fields, though your Strapi project may have more custom required fields:\nUsername: Required for all Strapi users.\nEmail: Enter in Strapi and use as the Email in the n8n credential.\nPassword: Enter in Strapi and use as the Password in the n8n credential.\nRole: Select the role you set up in the previous step.\nRefer to Managing end-user accounts for more information.\nUsing API token\nTo configure this credential, you'll need:\nAn API Token: Create an API token from Settings > Global Settings > API Tokens. Refer to Strapi's Creating a new API token documentation for more details and information on regenerating API tokens.\nThe URL: Use the public URL of your Strapi server, defined in ./config/server.js as the url parameter. Strapi recommends using an absolute URL.\nFor Strapi Cloud projects, use the URL of your Cloud project, for example:\nThe API Version: Select the version of the API you want your calls to use. Options include:\nVersion 3\nVersion 4"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\strava.md",
    "content": "Strava credentials\nYou can use these credentials to authenticate the following nodes:\nStrava\nStrava Trigger\nPrerequisites\nCreate a Strava account.\nCreate a Strava application in Settings > API. Refer to Using OAuth2 for more information.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Strava's API documentation for more information about the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you create a Strava app.\nA Client Secret: Generated when you create a Strava app.\nUse these settings for your Strava app:\nIn n8n, copy the OAuth Callback URL. Paste this URL into your Strava app's Authorization Callback Domain.\nRemove the protocol (https:// or http://) and the relative URL (/oauth2/callback or /rest/oauth2-credential/callback) from the Authorization Callback Domain. For example, if the OAuth Redirect URL was originally  the Authorization Callback Domain would be oauth.n8n.cloud.\nCopy the Client ID and Client Secret from your app and add them to your n8n credential.\nRefer to Authentication for more information about Strava's OAuth flow."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\stripe.md",
    "content": "Stripe credentials\nYou can use these credentials to authenticate the following nodes:\nStripe Trigger\nStripe\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Stripe's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a Stripe admin or developer account and:\nAn API Secret Key\nBefore you generate an API key, decide whether to generate it in live mode or test mode. Refer to Test mode and live mode for more information about the two modes.\nLive mode Secret key\nTo generate a Secret key in live mode:\nOpen the Stripe developer dashboard and select API Keys.\nIn the Standard Keys section, select Create secret key.\nEnter a Key name, like n8n integration.\nSelect Create. The new API key displays.\nCopy the key and enter it in your n8n credential as the Secret Key.\nRefer to Stripe's Create a secret API key for more information.\nTest mode Secret key\nTo use a Secret key in test mode, you must copy the existing one:\nGo to your Stripe test mode developer dashboard and select API Keys.\nIn the Standard Keys section, select Reveal test key for the Secret key.\nCopy the key and enter it in your n8n credential as the Secret Key.\nRefer to Stripe's Create a secret API key for more information.\nTest mode and live mode\nAll Stripe API requests happen within either test mode or live mode. Each mode has its own API key.\nUse test mode to access simulated test data and live mode to access actual account data. Objects in one mode aren‚Äôt accessible to the other.\nRefer to API keys | Test mode versus live mode for more information about what's available in each mode and guidance on when to use each.\nKey prefixes\nStripes' Secret keys always begin with sk_:\nLive keys begin with sk_live_.\nTest keys begin with sk_test_.\nn8n hasn't tested these credentials with Restricted keys (prefixed rk_)."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\supabase.md",
    "content": "Supabase credentials\nYou can use these credentials to authenticate the following nodes:\nSupabase\nSupabase Vector Store\nPrerequisites\nCreate a Supabase account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Supabase's API documentation for more information about the service.\nUsing access token\nTo configure this credential, you'll need:\nA Host\nA Service Role Secret\nTo generate your API Key:\nIn your Supabase account, go to the Dashboard and create or select a project for which you want to create an API key.\nGo to Project Settings > API to see the API Settings for your project.\nCopy the URL from the Project URL section and enter it as your n8n Host. Refer to API URL and keys for more detailed instruction.\nReveal and copy the Project API key for the service_role. Copy that key and enter it as your n8n Service Role Secret. Refer to Understanding API Keys for more information on the service_role privileges."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\surveymonkey.md",
    "content": "SurveyMonkey credentials\nYou can use these credentials to authenticate the following nodes:\nSurveyMonkey Trigger\nPrerequisites\nCreate a SurveyMonkey account.\nRegister an app from your Developer dashboard > My apps.\nRefer to Required app scopes for information on the scopes you must use.\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to SurveyMonkey's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nAn Access Token: Generated once you create an app.\nA Client ID: Generated once you create an app.\nA Client Secret: Generated once you create an app.\nOnce you've created your app and assigned appropriate scopes, go to Settings > Credentials. Copy the Access Token, Client ID, and Secret and add them to n8n.\nUsing OAuth\nTo configure this credential, you'll need:\nA Client ID: Generated once you create an app.\nA Client Secret: Generated once you create an app.\nOnce you've created your app and assigned appropriate scopes:\nGo to the app's Settings > Settings.\nFrom n8n, copy the OAuth Redirect URL.\nOverwrite the app's existing OAuth Redirect URL with that URL.\nSelect Submit Changes.\nBe sure the Scopes section contains the Required app scopes.\nFrom the app's Settings > Credentials, copy the Client ID and Client Secret and add them to your n8n credential. You can now select Connect my account from n8n.\nRequired app scopes\nOnce you create your app, go to Settings > Scopes. Select these scopes for your n8n credential to work:\nView Surveys\nView Collectors\nView Responses\nView Response Details\nCreate/Modify Webhooks\nView Webhooks\nSelect Update Scopes to save them."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\syncromsp.md",
    "content": "SyncroMSP credentials\nYou can use these credentials to authenticate the following nodes:\nSyncroMSP\nPrerequisites\nCreate a SyncroMSP account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to SyncroMSP's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Called an API token in SyncroMSP. To create an API token, go to your user menu > Profile/Password > API Tokens and select the option to Create New Token. Select Custom Permissions to enter a name for your token and adjust the permissions to match your requirements.\nYour Subdomain: Enter your SyncroMSP subdomain. This is visible in the URL of your SyncroMSP, located between https:// and .syncromsp.com. If your full URL is  you'd enter n8n-instance as the subdomain.\nRefer to API Tokens for more information on creating new tokens."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sysdig.md",
    "content": "Sysdig Management credentials\nPrerequisites\nCreate a Sysdig account or configure a local instance.\nSupported authentication methods\nAccess Key\nRelated resources\nRefer to Sysdig's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more.\nUsing API access key\nTo configure this credential, you'll need:\nAn Access Key\nRefer to the Sysdig Agent Access Keys documentation for instructions on obtaining the Access Key from the application."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\taiga.md",
    "content": "Taiga credentials\nYou can use these credentials to authenticate the following nodes:\nTaiga\nTaiga Trigger\nPrerequisites\nCreate a Taiga account.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Taiga's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA Username: Enter your username or user email address. Refer to Normal login for more information.\nA Password: Enter your password.\nThe Environment: Choose between Cloud or Self-Hosted. For Self-Hosted instances, you'll also need to add:\nThe URL: Enter your Taiga URL."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\tapfiliate.md",
    "content": "Tapfiliate credentials\nYou can use these credentials to authenticate the following nodes:\nTapfiliate\nPrerequisites\nCreate a Tapfiliate account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Tapfiliate's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Get your API Key from your Profile Settings > API Key.\nRefer to Your API key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\telegram.md",
    "content": "Telegram credentials\nYou can use these credentials to authenticate the following nodes:\nTelegram\nTelegram Trigger\nPrerequisites\nCreate a Telegram account.\nSupported authentication methods\nAPI bot access token\nRelated resources\nRefer to Telegram's Bot API documentation for more information about the service.\nRefer to the Telegram Bot Features documentation for more information on creating and working with bots.\nUsing API bot access token\nTo configure this credential, you'll need:\nA bot Access Token\nTo generate your access token:\nStart a chat with the BotFather.\nEnter the /newbot command to create a new bot.\nThe BotFather will ask you for a name and username for your new bot:\nThe name is the bot's name displayed in contact details and elsewhere. You can change the bot name later.\nThe username is a short name used in search, mentions, and t.me links. Use these guidelines when creating your username:\nMust be between 5 and 32 characters long.\nNot case sensitive.\nMay only include Latin characters, numbers, and underscores.\nMust end in bot, like tetris_bot or TetrisBot.\nYou can't change the username later.\nCopy the bot token the BotFather generates and add it as the Access Token in n8n.\nRefer to the BotFather Create a new bot documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\thehive.md",
    "content": "TheHive credentials\nYou can use these credentials to authenticate the following nodes:\nTheHive\nPrerequisites\nInstall TheHive on your server.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to TheHive 3's API documentation and TheHive 4's API documentation for more information about the services.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Create an API key from Organization > Create API Key. Refer to API Authentication for more information.\nYour URL: The URL of your TheHive server.\nAn API Version: Choose between:\nTheHive 3 (api v0)\nTheHive 4 (api v1)\nFor TheHive 5, use TheHive 5 credentials instead.\nIgnore SSL Issues: When turned on, n8n will connect even if SSL certificate validation fails."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\thehive5.md",
    "content": "TheHive 5 credentials\nYou can use these credentials to authenticate the following nodes with TheHive 5.\nTheHive 5\nPrerequisites\nInstall TheHive 5 on your server.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to TheHive's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Users with orgAdmin and superAdmin accounts can generate API keys:\norgAdmin account: Go to Organization > Create API Key for the user you wish to generate a key for.\nsuperAdmin account: Go to Users > Create API Key for the user you wish to generate a key for.\nRefer to API Authentication for more information.\nA URL: The URL of your TheHive server.\nIgnore SSL Issues: When turned on, n8n will connect even if SSL certificate validation fails."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\timescaledb.md",
    "content": "TimescaleDB credentials\nYou can use these credentials to authenticate the following nodes:\nTimescaleDB\nPrerequisites\nAn available instance of TimescaleDB.\nSupported authentication methods\nDatabase connection\nRelated resources\nRefer to the Timescale documentation for more information about the service.\nUsing database connection\nTo configure this credential, you'll need:\nThe Host: The fully qualified server name or IP address of your TimescaleDB server.\nThe Database: The name of the database to connect to.\nA User: The user name you want to log in with.\nA Password: Enter the password for the database user you are connecting to.\nIgnore SSL Issues: If turned on, n8n will connect even if SSL certificate validation fails and you won't see the SSL selector.\nSSL: This setting controls the ssl-mode connection string for the connection. Options include:\nAllow: Sets the ssl-mode parameter to allow. First try a non-SSL connection; if that fails, try an SSL connection.\nDisable: Sets the ssl-mode parameter to disable. Only try a non-SSL connection.\nRequire: Sets the ssl-mode parameter to require, which is the default for TimescaleDB connection strings. Only try an SSL connection. If a root CA file is present, verify that a trusted certificate authority (CA) issued the server certificate.\nPort: The port number of the TimescaleDB server.\nRefer to the Timescale connection settings documentation for more information about the non-SSL fields. Refer to Connect with a stricter SSL for more information about the SSL options."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\todoist.md",
    "content": "Todoist credentials\nYou can use these credentials to authenticate the following nodes:\nTodoist\nSupported authentication methods\nAPI key\nOAuth2\nRelated resources\nRefer to Todoist's REST API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a Todoist account and:\nAn API Key\nTo get your API Key:\nIn Todoist, open your Integration settings.\nSelect the Developer tab.\nCopy your API token and enter it as the API Key in your n8n credential.\nRefer to Find your API token for more information.\nUsing OAuth2\nIf you're self-hosting n8n, you'll need a Todoist account and:\nA Client ID\nA Client Secret\nGet both by creating an application:\nOpen the Todoist App Management Console.\nSelect Create a new app.\nEnter an App name for your app, like n8n integration.\nSelect Create app.\nCopy the n8n OAuth Redirect URL and enter it as the OAuth redirect URL in Todoist.\nCopy the Client ID from Todoist and enter it in your n8n credential.\nCopy the Client Secret from Todoist and enter it in your n8n credential.\nConfigure the rest of your Todoist app as it makes sense for your use case.\nRefer to the Todoist Authorization Guide for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\toggl.md",
    "content": "Toggl credentials\nYou can use these credentials to authenticate the following nodes:\nToggl Trigger\nPrerequisites\nCreate a Toggl account.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Toggl's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA Username: Enter your user email address.\nA Password: Enter your user password.\nRefer to Authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\totp.md",
    "content": "TOTP credentials\nYou can use these credentials to authenticate the following nodes:\nTOTP\nPrerequisites\nGenerate a TOTP Secret and Label.\nSupported authentication methods\nSecret and label\nRelated resources\nTime-based One-time Password (TOTP) is an algorithm that generates a one-time password (OTP) using the current time. Refer to Google Authenticator | Key URI format for more information.\nUsing secret and label\nTo configure this credential, you'll need:\nA Secret: The secret key encoded in the QR code during authenticator setup. It's an arbitrary key value encoded in Base32, for example: BVDRSBXQB2ZEL5HE. Refer to Google Authenticator Secret for more information.\nA Label: The identifier for the account. It contains an account name as a URI-encoded string. You can include prefixes to identify the provider or service managing the account. If you use prefixes, use a literal or url-encoded colon to separate the issuer prefix and the account name, for example: GitHub:john-doe. Refer to Google Authenticator Label for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\travisci.md",
    "content": "Travis CI credentials\nYou can use these credentials to authenticate the following nodes:\nTravis CI\nPrerequisites\nCreate a Travis CI account.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Travis CI's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API Token: Get your API token from Account Settings > API Token or generate one through the Travis CI command line client ."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\trellixepo.md",
    "content": "Trellix ePO credentials\nPrerequisites\nCreate a Trellix ePolicy Orchestrator account.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Trellix ePO's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing basic auth\nTo configure this credential, you'll need:\nA Username to connect as.\nA Password for that user account.\nn8n uses These fields to build the -u parameter in the format of -u username:pw. Refer to Web API basics for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\trello.md",
    "content": "Trello credentials\nYou can use these credentials to authenticate the following nodes:\nTrello\nTrello Trigger\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Trello's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need a Trello account and:\nAn API Key\nAn API Token\nTo generate both the API Key and API Token, create a Trello Power-Up:\nOpen the Trello Power-Up Admin Portal.\nSelect New.\nEnter a Name for your Power-Up, like n8n integration.\nSelect the Workspace the Power-Up should have access to.\nLeave the iframe connector URL blank.\nEnter appropriate contact information.\nSelect Create.\nThis should open the Power-Up to the API Key page. (If it doesn't, open that page.)\nSelect Generate a new API Key.\nCopy the API key from Trello and enter it in your n8n credential.\nIn your Trello API key page, enter your n8n base URL as an Allowed origin.\nIn Capabilities make sure to select the necessary options.\nSelect the Token link next to your Trello API Key.\nWhen prompted, select Allow to grant all the permissions asked for.\nCopy the Trello Token and enter it as the n8n API Token.\nRefer to Trello's API Introduction for more information on API keys and tokens. Refer to Trello's Power-Up Admin Portal for more information on creating Power-Ups."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\twake.md",
    "content": "Twake credentials\nYou can use these credentials to authenticate the following nodes:\nTwake\nPrerequisites\nCreate a Twake account.\nSupported authentication methods\nCloud API key\nServer API key\nRelated resources\nRefer to Twake's documentation for more information about the service.\nUsing Cloud API key\nTo configure this credential, you'll need:\nA Workspace Key: Generated when you install the n8n application to your Twake Cloud environment and select Configure. Refer to How to connect n8n to Twake for more detailed instructions.\nUsing Server API key\nTo configure this credential, you'll need:\nA Host URL: The URL of your Twake self-hosted instance.\nA Public ID: Generated when you create an app.\nA Private API Key: Generated when you create an app.\nTo generate your Public ID and Private API Key, create a Twake application:\nGo to Workspace Settings > Applications and connectors > Access your applications and connectors > Create an application.\nEnter appropriate details.\nOnce you've created your app, view its API Details.\nCopy the Public identifier and add it as the n8n Public ID.\nCopy the Private key and add it as the n8n Private API Key.\nRefer to API settings for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\twilio.md",
    "content": "Twilio credentials\nYou can use these credentials to authenticate the following nodes:\nTwilio\nTwilio trigger\nSupported authentication methods\nAuth token: Twilio recommends this method for local testing only.\nAPI key: Twilio recommends this method for production.\nRelated resources\nRefer to Twilio's API documentation for more information about the service.\nUsing Auth Token\nTo configure this credential, you'll need a Twilio account and:\nYour Twilio Account SID\nYour Twilio Auth Token\nTo set up the credential:\nIn n8n, select Auth Token as the Auth Type.\nIn Twilio, go to Console Dashboard > Account Info.\nCopy your Account SID and enter this in your n8n credential. This acts as a username.\nCop your Auth Token and enter this in your n8n credential. This acts as a password.\nRefer to Auth Tokens and How to Change Them for more information.\nUsing API key\nTo configure this credential, you'll need a Twilio account and:\nYour Twilio Account SID\nAn API Key SID: Generated when you create an API key.\nAn API Key Secret: Generated when you create an API key.\nTo set up the credential:\nIn n8n, select API Key as the Auth Type.\nIn Twilio, go to Console Dashboard > Account Info.\nCopy your Account SID and enter it in your n8n credential.\nIn Twilio, go to your account's API keys & tokens page.\nSelect Create API Key.\nEnter a Friendly name for your API key, like n8n integration.\nSelect your Key type. n8n works with either Main or Standard. Refer to Selecting an API key type for more information.\nSelect Create API Key to finish creating the key.\nOn the Copy secret key page, copy the SID displayed with the key and enter it in your n8n credential API Key SID.\nOn the Copy secret key page, copy the Secret displayed with the key and enter it in your n8n credential API Key Secret.\nRefer to Create an API key for more detailed instructions.\nSelecting an API key type\nWhen you create a Twilio API key, you must select a key type. The n8n credential works with Main and Standard key types.\nHere are more details on the different API key types:\nMain: This key type gives you the same level of access as using your Account SID and Auth Token in API requests.\nStandard: This key type gives you access to all the functionality in Twilio's APIs except the API key resources and Account resources.\nRestricted: This key type is in beta. n8n hasn't tested the credential against this key type; if you try it, let us know if you run into any issues.\nRefer to Types of API keys for more information on the key types."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\twist.md",
    "content": "Twist credentials\nYou can use these credentials to authenticate the following nodes:\nTwist\nPrerequisites\nCreate a Twist account.\nCreate a general integration and configure a valid OAuth Redirect URL. Refer to Using OAuth2 for more information.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Twist's API documentation for more information about authenticating with the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated once you create a general integration.\nA Client Secret: Generated once you create a general integration.\nTo generate your Client ID and Client Secret, create a general integration.\nUse these settings for your integration's OAuth Authentication:\nCopy the OAuth Redirect URL from n8n and enter it as the OAuth 2 redirect URL in Twist.\nSelect Update OAuth settings to save those changes.\nCopy the Client ID and Client Secret from Twist and enter them in the appropriate fields in n8n.\nLocal environment redirect URL\nTwist doesn't accept a localhost callback URL. These steps should allow you to configure the OAuth credentials for the local environment:\nUse ngrok to expose the local server running on port 5678 to the internet. In your terminal, run the following command:\nRun the following command in a new terminal. Replace  with the URL that you get from the previous step.\nUse the generated URL as your OAuth 2 redirect URL in Twist."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\twitter.md",
    "content": "X (formerly Twitter) credentials\nYou can use these credentials to authenticate the following nodes:\nX (formerly Twitter)\nPrerequisites\nCreate an X developer account.\nCreate a Twitter app or use the default project and app created when you sign up for the developer portal. Refer to each supported authentication method below for more details on the app's configuration.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to X's API documentation for more information about the service. Refer to X's API authentication documentation for more information about authenticating with the service.\nRefer to Application-only Authentication for more information about app-only authentication.\nUsing OAuth2\nUse this method if you're using n8n version 0.236.0 or later.\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nTo generate your Client ID and Client Secret:\nIn the Twitter developer portal, open your project.\nOn the project's Overview tab, find the Apps section and select Add App.\nGive your app a Name and select Next.\nGo to the App Settings.\nIn the User authentication settings, select Set Up.\nSet the App permissions. Choose Read and write and Direct message if you want to use all functions of the n8n X node.\nIn the Type of app section, select Web App, Automated App or Bot.\nIn n8n, copy the OAuth Redirect URL.\nIn your X app, find the App Info section and paste that URL in as the Callback URI / Redirect URL.\nAdd a Website URL.\nSave your changes.\nCopy the Client ID and Client Secret displayed in X and add them to the corresponding fields in your n8n credential.\nRefer to X's OAuth 2.0 Authentication documentation for more information on working with this authentication method.\nX rate limits\nX has time-based rate limits per endpoint based on your developer access plan level. X calculates app rate limits and user rate limits independently. Refer to Rate limits for the access plan level rate limits and guidance on avoiding hitting them.\nUse the guidance below for calculating rate limits:\nIf you're using the deprecated OAuth method, user rate limits apply. You'll have one limit per time window for each set of users' access tokens.\nIf you're Using OAuth2, app rate limits apply. You'll have a limit per time window for requests made by your app.\nX calculates user rate limits and app rate limits independently.\nRefer to X's Rate limits and authentication methods for more information about these rate limit types."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\typeform.md",
    "content": "Typeform credentials\nYou can use these credentials to authenticate the following nodes:\nTypeform Trigger\nSupported authentication methods\nAPI token\nOAuth2\nRelated resources\nRefer to Typeform's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need a Typeform account and:\nA personal Access Token\nTo get your personal access token:\nLog into your Typeform account.\nSelect your profile avatar in the upper right and go to Account > Your settings > Personal Tokens.\nSelect Generate a new token.\nGive your token a Name, like n8n integration.\nFor Scopes, select Custom scopes. Select these scopes:\nForms: Read\nWebhooks: Read, Write\nSelect Generate token.\nCopy the token and enter it in your n8n credential.\nRefer to Typeform's Personal access token documentation for more information.\nUsing OAuth2\nTo configure this credential, you'll need a Typeform account and:\nA Client ID: Generated when you register an app.\nA Client Secret: Generated when you register an app.\nTo get your Client ID and Client Secret, register a new Typeform app:\nLog into your Typeform account.\nIn the upper left, select the dropdown for your organization and select Developer apps.\nSelect Register a new app.\nEnter an App Name that makes sense, like n8n OAuth2 integration.\nEnter your n8n base URL as the App website, for example\nFrom n8n, copy the OAuth Redirect URL. Enter this in Typeform as the Redirect URI(s).\nSelect Register app.\nCopy the Client Secret and enter it in your n8n credential.\nIn Typeform, select Got it to close the Client Secret modal.\nThe Developer apps panel displays your new app. Copy the Client ID and enter it in your n8n credential.\nOnce you enter both the Client ID and Client Secret in n8n, select Connect my account and follow the on-screen prompts to finish authorizing the app.\nRefer to Create applications that integrate with Typeform's APIs for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\unleashedsoftware.md",
    "content": "Unleashed Software credentials\nYou can use these credentials to authenticate the following nodes:\nUnleashed Software\nPrerequisites\nCreate an Unleashed Software account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Unleashed's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API ID: Go to Integrations > Unleashed API Access to find your API ID.\nAn API Key: Go to Integrations > Unleashed API Access to find your API Key.\nRefer to Unleashed API Access for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\uplead.md",
    "content": "UpLead credentials\nYou can use these credentials to authenticate the following nodes:\nUpLead\nPrerequisites\nCreate an UpLead account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to UpLead's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Go to your Account > Profiles to Generate New API Key. Refer to How can I generate an API key? for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\uproc.md",
    "content": "uProc credentials\nYou can use these credentials to authenticate the following nodes:\nuProc\nPrerequisites\nCreate a uProc account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to uProc's API documentation for more information about the service.\nUsing API Key\nTo configure this credential, you'll need:\nAn Email address: Enter the email address you use to log in to uProc. This is also displayed in Settings > Integrations > API Credentials.\nAn API Key: Go to Settings > Integrations > API Credentials. Copy the API Key (real) from the API Credentials section and enter it in your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\uptimerobot.md",
    "content": "UptimeRobot credentials\nYou can use these credentials to authenticate the following nodes:\nUptimeRobot\nPrerequisites\nCreate an UptimeRobot account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to UptimeRobot's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Get your API Key from My Settings > API Settings. Create a Main API Key and enter this key in your n8n credential.\nAPI key types\nUptimeRobot supports three API key types:\nAccount-specific (also known as main): Pulls data for multiple monitors.\nMonitor-specific: Pulls data for a single monitor.\nRead-only: Only runs GET API calls.\nTo complete all of the operations in the UptimeRobot node, use the Main or Account-specific API key type. Refer to API authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\urlscanio.md",
    "content": "urlscan.io credentials\nYou can use these credentials to authenticate the following nodes:\nurlscan.io\nPrerequisites\nCreate an urlscan.io account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to urlscan.io's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Get your API key from Settings & API > API Keys."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\venafitlsprotectcloud.md",
    "content": "Venafi TLS Protect Cloud credentials\nYou can use these credentials to authenticate the following nodes:\nVenafi TLS Protect Cloud node\nVenafi TLS Protect Cloud Trigger node\nPrerequisites\nCreate a Venafi TLS Protect Cloud account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Venafi TLS Protect Cloud's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Region: Select the region that matches your business needs. Choose EU if you're located in the European Union. Otherwise, choose US.\nAn API Key: Go to your avatar > Preferences > API Keys to get your API key. You can also use VCert to get your API key. Refer to Obtaining an API Key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\venafitlsprotectdatacenter.md",
    "content": "Venafi TLS Protect Datacenter credentials\nYou can use these credentials to authenticate the following nodes:\nVenafi TLS Protect Datacenter node\nPrerequisites\nCreate a Venafi TLS Protect Datacenter account.\nSet the expiration and refresh time for tokens. Refer to Setting up token authentication for more information.\nCreate an API integration in API > Integrations. Refer to Integrating other systems with Venafi products for detailed instructions.\nTake note of the Client ID for your integration.\nChoose the scopes needed for the operations you want to perform within n8n. Refer to the scopes table in Integrating other systems with Venafi products for more details on available scopes.\nSupported authentication methods\nAPI integration\nRelated resources\nRefer to Venafi's API integration documentation for more information about the service.\nUsing API integration\nTo configure this credential, you'll need:\nA Domain: Enter your Venafi TLS Protect Datacenter domain.\nA Client ID: Enter the Client ID from your API integration. Refer to the information and links in Prerequisites for more information on creating an API integration.\nA Username: Enter your username.\nA Password: Enter your password.\nAllow Self-Signed Certificates: If turned on, the credential will allow self-signed certificates."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\vero.md",
    "content": "Vero credentials\nYou can use these credentials to authenticate the following nodes:\nVero\nPrerequisites\nCreate a Vero account.\nSupported authentication methods\nAPI auth token\nRelated resources\nRefer to Vero's API documentation for more information about the service.\nUsing API auth token\nTo configure this credential, you'll need:\nAn Auth Token: Get your auth token from your Vero account settings. Refer to API authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\virustotal.md",
    "content": "VirusTotal credentials\nPrerequisites\nCreate a VirusTotal account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to VirusTotal's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nAn API Token: Go to your user account menu > API key to get your API key. Enter this as the API Token in your n8n credential. Refer to API authentication for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\vonage.md",
    "content": "Vonage credentials\nYou can use these credentials to authenticate the following nodes:\nVonage\nPrerequisites\nCreate a Vonage developer account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Vonage's SMS API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key\nAn API Secret\nGet your API Key and API Secret from your developer dashboard user account > Settings > API Settings. Refer to Retrieve your account information for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\weaviate.md",
    "content": "Weaviate credentials\nYou can use these credentials to authenticate the following nodes:\nWeaviate Vector Store\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Weaviate's connection documentationfor more information on how to connect to Weaviate.\nUsing API key\nConnection type: Weaviate Cloud\nCreate your Weaviate Cloud Database and follow these instructions get the following parameter values from your Weaviate Cloud Database:\nWeaviate Cloud Endpoint\nWeaviate Api Key\nNote: Weaviate provides a free sandbox option for testing.\nConnection type: Custom Connection\nFor this Connection Type, you need to deploy Weaviate on your own server, configured so n8n can access it. Refer to Weaviate's authentication documentation for information on creating and using API keys.\nYou can then provide the arguments for your custom connection:\nWeaviate Api Key: Your Weaviate API key.\nCustom Connection HTTP Host: The domain name or IP address of your Weaviate instance to use for HTTP API calls.\nCustom Connection HTTP Port: The port your Weaviate instance is running on for HTTP API calls. By default, this is 8080.\nCustom Connection HTTP Secure: Whether to connect to the Weaviate through HTTPS for HTTP API calls.\nCustom Connection gRPC Host: The hostname or IP address of your Weaviate instance to use for gRPC.\nCustom Connection gRPC Port: The gRPC API port for your Weaviate instance. By default, this is 50051.\nCustom Connection gRPC Secure: Whether to connect to the Weaviate through HTTPS for gRPC.\nFor community support, refer to Weaviate Forums."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\webflow.md",
    "content": "Webflow credentials\nYou can use these credentials to authenticate the following nodes:\nWebflow\nWebflow Trigger\nPrerequisites\nCreate a Webflow account.\nCreate a site: Required for API access token authentication only.\nSupported authentication methods\nAPI access token\nOAuth2\nRelated resources\nRefer to Webflow's API documentation for more information about the service.\nUsing API access token\nTo configure this credential, you'll need:\nA Site Access Token: Access tokens are site-specific. Go to your site's Site Settings > Apps & integrations > API access and select Generate API token. Refer to Get a Site Token for more information.\nUsing OAuth2\nIf you need to configure OAuth2 from scratch, register an application in your workspace.\nUse these settings for your application:\nCopy the OAuth callback URL from n8n and add it as a Redirect URI in your application.\nOnce you've created your application, copy the Client ID and Client Secret and enter them in your n8n credential.\nIf you are using the Webflow Data API V1 (deprecated), enable the Legacy toggle. Otherwise, leave this inactive.\nRefer to OAuth for more information on Webflow's OAuth web flow."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\webhook.md",
    "content": "Webhook credentials\nYou can use these credentials to authenticate the following nodes:\nWebhook\nPrerequisites\nYou must use the authentication method required by the app or service you want to query.\nSupported authentication methods\nBasic auth\nHeader auth\nJWT auth\nNone\nUsing JWT auth\nJWT Auth is a method of authentication that uses JSON Web Tokens (JWT) to digitally sign data. This authentication method uses the JWT credential and can use either a Passphrase or PEM Key as key type. Refer to JWT credential for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\wekan.md",
    "content": "Wekan credentials\nYou can use these credentials to authenticate the following nodes:\nWekan\nPrerequisites\nInstall Wekan on your server.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to Wekan's API documentation for more information about authenticating with the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA Username: Enter your Wekan username.\nA Password: Enter your Wekan password.\nA URL: Enter your Wekan domain."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\whatsapp.md",
    "content": "WhatsApp Business Cloud credentials\nYou can use these credentials to authenticate the following nodes:\nWhatsApp Business Cloud\nWhatsApp Trigger\nRequirements\nTo create credentials for WhatsApp, you need the following Meta assets:\nA Meta developer account: A developer account allows you to create and manage Meta apps, including WhatsApp integrations.\n??? note \"Set up a Meta developer account\"\nVisit the Facebook Developers site.\nClick Getting Started in the upper-right corner (if the link says My Apps, you've already set up a developer account).\nAgree to terms and conditions.\nProvide a phone number for verification.\nSelect your occupation or role.\nA Meta business portfolio: WhatsApp messaging services require a Meta business portfolio, formerly called a Business Manager account. The UI may still show either option.\n??? note \"Set up a Meta business portfolio\"\nVisit the Facebook Business site.\nSelect Create an account.\nIf you already have a Facebook Business account and portfolio, but want a new portfolio, open the business portfolio selector in the left-side menu and select Create a business portfolio.\nEnter a Business portfolio name.\nEnter your name.\nEnter a business email.\nSelect Submit or Create.\nA Meta business app configured with WhatsApp: Once you have a developer account, you will create a Meta business app.\n??? note \"Set up a Meta business app with WhatsApp\"\nVisit the Meta for Developers Apps dashboard\nSelect Create app.\nIn Add products to your app, select Set up in the WhatsApp tile. Refer to Add the WhatsApp Product for more detail.\nThis opens the WhatsApp Quickstart page. Select your business portfolio.\nSelect Continue.\nIn the left-side menu, go to App settings > Basic.\nSet the Privacy Policy URL and Terms of Service URL for the app.\nChange the App Mode to Live.\nSupported authentication methods\nAPI key: Use for the WhatsApp Business Cloud node.\nOAuth2: Use for the WhatsApp Trigger node.\nRelated resources\nRefer to WhatsApp's API documentation for more information about the service.\nMeta classifies users who create WhatsApp business apps as Tech Providers; refer to Meta's Get Started for Tech Providers for more information.\nUsing API key\nYou need WhatsApp API key credentials to use the WhatsApp Business Cloud node.\nTo configure this credential, you'll need:\nAn API Access Token\nA Business Account ID\nTo generate an access token, follow these steps:\nVisit the Meta for Developers Apps dashboard.\nSelect your Meta app.\nIn the left-side menu, select WhatsApp > API Setup.\nSelect Generate access token and confirm the access you want to grant.\nCopy the Access token and add it to n8n as the Access Token.\nCopy the WhatsApp Business Account ID and add it to n8n as the Business Account ID.\nRefer to Test Business Messaging on WhatsApp for more information on the above steps.\nFully verifying and launching your app will take further configuration. Refer to Meta's Get Started for Tech Providers Steps 5 and beyond for more information. Refer to App Review for more information on the Meta App Review process.\nUsing OAuth2\nYou need WhatsApp OAuth2 credentials to use the WhatsApp Trigger node.\nTo configure this credential, you'll need:\nA Client ID\nA Client Secret\nTo retrieve these items, follow these steps:\nVisit the Meta for Developers Apps dashboard.\nSelect your Meta app.\nIn the left-side menu, select App settings > Basic.\nCopy the App ID and enter it as the Client ID within the n8n credential.\nCopy the App Secret and enter it as the Client Secret within the n8n credential.\nFully verifying and launching your app will take further configuration. Refer to Meta's Get Started for Tech Providers Steps 5 and beyond for more information. Refer to App Review for more information on the Meta App Review process."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\wise.md",
    "content": "Wise credentials\nYou can use these credentials to authenticate the following nodes:\nWise\nWise Trigger\nPrerequisites\nCreate a Wise account.\nSupported authentication methods\nAPI token\nRelated resources\nRefer to Wise's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nAn API Token: Go to your user menu > Settings > API tokens to generate an API token. Enter the generated API key in your n8n credential. Refer to Getting started with the API for more information.\nYour Environment: Select the environment that best matches your Wise account environment.\nIf you're using a Wise test sandbox account, select Test.\nOtherwise, select Live.\nPrivate Key (Optional): For live endpoints requiring Strong Customer Authentication (SCA), generate a public and private key. Enter the private key here. Refer to Add a private key for more information.\nIf you're using a Test environment, you'll only need to enter a Private Key if you've enabled Strong Customer Authentication on the public keys management page.\nAdd a private key\nWise protects some live endpoints and operations with Strong Customer Authentication (SCA). Refer to Strong Customer Authentication & 2FA for details.\nIf you make a request to an endpoint that requires SCA, Wise returns a 403 Forbidden HTTP status code. The error returned will look like this:\nThis request requires Strong Customer Authentication (SCA). Please add a key pair to your account and n8n credentials. See\nTo use endpoints requiring SCA, generate an RSA key pair and add the relevant key information to both Wise and n8n:\nGenerate an RSA key pair:\nAdd the content of the public key public.pem to your Wise user menu > Settings > API tokens > Manage public keys.\nAdd the content of the private key private.pem in n8n to the Private Key (Optional).\nRefer to Personal Token SCA for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\wolframalpha.md",
    "content": "WolframAlpha credentials\nYou can use these credentials to authenticate the following nodes:\n* Wolfram Alpha\n## Supported authentication methods\n- API key\n## Related resources\nRefer to Wolfram Alpha's Simple API documentation for more information about the service.\n## Using API key\nTo configure this credential, you'll need a registered Wolfram ID and:\n- An **App ID**\nTo get an App ID:\n1. Open the Wolfram Alpha Developer Portal and go to **API Access**.\n2. Select **Get an App ID**.\n3. Enter a **Name** for your application, like n8n integration.\n4. Enter a **Description** for your application.\n5. Select **Simple API** as the **API**.\n6. Select **Submit**.\n6. Copy the generated **App ID** and enter it in your n8n credential.\nRefer to **Getting Started** in the Wolfram Alpha Simple API documentation for more information.\nResolve Forbidden connection error\nIf you enter your App ID and get an error that the credential is Forbidden, make sure that you have verified your email address for your Wolfram ID:\nGo to your Wolfram ID Details.\nIf you don't see the Verified label underneath your Email address, select the link to Send a verification email.\nYou must open the link in that email to verify your email address.\nIt may take several minutes for the verification to populate to the API, but once it does, retrying the n8n credential should succeed."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\woocommerce.md",
    "content": "WooCommerce credentials\nYou can use these credentials to authenticate the following nodes:\nWooCommerce\nWooCommerce Trigger\nPrerequisites\nInstall the WooCommerce plugin on your WordPress website.\nIn WordPress, go to Settings > Permalinks and set your WordPress permalinks to use something other than Plain.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to WooCommerce's REST API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Consumer Key: Created when you generate an API key.\nA Consumer Secret: Created when you generate an API key.\nA WooCommerce URL\nTo generate an API key and set up your credential:\nGo to WooCommerce > Settings > Advanced > Rest API > Add key.\nSelect Read/Write from the Permissions dropdown.\nCopy the generated Consumer Key and Consumer Secret and enter them into your n8n credentials.\nEnter your WordPress site URL as the WooCommerce URL.\nBy default, n8n passes your credential details in the Authorization header. If you need to pass them as query string parameters instead, turn on Include Credentials in Query.\nRefer to Generate Keys for more information.\nResolve \"Consumer key is missing\" error\nWhen you try to connect your credentials, you may receive an error like this: Consumer key is missing.\nThis occurs when the server can't parse the Authorization header details when authenticating over SSL.\nTo resolve it, turn on the Include Credentials in Query toggle to pass the consumer key/secret as query string parameters instead and retry the credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\wordpress.md",
    "content": "WordPress credentials\nYou can use these credentials to authenticate the following nodes:\nWordPress\nPrerequisites\nCreate a WordPress account or deploy WordPress on a server.\nSupported authentication methods\nBasic auth\nRelated resources\nRefer to WordPress's API documentation for more information about the service.\nUsing basic auth\nTo configure this credential, you'll need:\nYour WordPress Username\nA WordPress application Password\nYour WordPress URL\nDecide whether to Ignore SSL Issues\nUsing this credential involves three steps:\nEnable two-step authentication.\nCreate an application password.\nSet up the credential.\nRefer to the detailed instructions below for each step.\nEnable two-step authentication\nTo generate an application password, you must first enable Two-Step Authentication in WordPress. If you've already done this, skip to the next section.\nOpen your WordPress profile.\nSelect Security from the left menu.\nSelect Two-Step Authentication. The Two-Step Authentication page opens.\nIf Two-Step Authentication isn't enabled, you must enable it.\nChoose whether to enable it using an authenticator app or SMS codes and follow the on-screen instructions.\nRefer to WordPress's Enable Two-Step Authentication for detailed instructions.\nCreate an application password\nWith Two-Step Authentication enabled, you can now generate an application password:\nFrom the WordPress Security > Two-Step Authentication page, select + Add new application password in the Application passwords section.\nEnter an Application name, like n8n integration.\nSelect Generate Password.\nCopy the password it generates. You'll use this in your n8n credential.\nSet up the credential\nCongratulations! You're now ready to set up your n8n credential:\nEnter your WordPress Username in your n8n credential.\nEnter the application password you copied above as the Password in your n8n credential.\nEnter the URL of your WordPress site as the WordPress URL.\nOptional: Use the Ignore SSL Issues to choose whether you want the n8n credential to connect even if SSL certificate validation fails (turned on) or whether to respect SSL certificate validation (turned off)."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\workable.md",
    "content": "Workable credentials\nYou can use these credentials to authenticate the following nodes:\nWorkable Trigger\nPrerequisites\nCreate a Workable account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Workable's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Subdomain: Your Workable subdomain is the part of your Workable domain between https:// and .workable.com. So if the full domain is  the subdomain is n8n. The subdomain is also displayed on your Workable Company Profile page.\nAn Access Token: Go to your profile > Integrations > Apps and select Generate API token. Refer to Generate a new token for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\wufoo.md",
    "content": "Wufoo credentials\nYou can use these credentials to authenticate the following nodes:\nWufoo Trigger\nPrerequisites\nCreate a Wufoo account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Wufoo's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: Get your API key from the Wufoo Form Manager. To the right of a form, select More > API Information. Refer to Using API Information and Webhooks for more information.\nA Subdomain: Your subdomain is the part of your Wufoo URL that comes after https:// and before wufoo.com. So if the full domain is  the subdomain is n8n. Admins can view the subdomain in the Account Manager. Refer to Your Subdomain for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\xai.md",
    "content": "xAI credentials\nYou can use these credentials to authenticate the following nodes:\nChat xAI Grok\nPrerequisites\nCreate an xAI account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to xAI's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nAn API Key: You can create a new API key on the xAI Console API Keys page.\nRefer to the The Hitchhiker's Guide to Grok | xAI for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\xata.md",
    "content": "Xata credentials\nYou can use these credentials to authenticate the following nodes:\nXata\nPrerequisites\nCreate a Xata database or an account on an existing database.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Xata's documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nThe Database Endpoint: The Workspace API requires that you identify the database you're requesting information from using this format:  Refer to Workspace API for more information.\n{workspace-display-name}: The workspace display name is an optional identifier you can include in your Database Endpoint. The API ignores it, but including it can make it easier to figure out which workspace this database is in if you're saving multiple credentials.\n{workspace-id}: The unique ID of the workspace, 6 alphanumeric characters.\n{region}: The hosting region for the database. This value must match the database region configuration.\n{dbname}: The name of the database you're interacting with.\nA Branch: Enter the name of the GitHub branch for your database.\nAn API Key: To generate an API key, go to Account Settings and select + Add a key. Refer to Generate an API Key for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\xero.md",
    "content": "Xero credentials\nYou can use these credentials to authenticate the following nodes:\nXero\nPrerequisites\nCreate a Xero account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Zero's API documentation for more information about the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you create a new app for a custom connection.\nA Client Secret: Generated when you create a new app for a custom connection.\nTo generate your Client ID and Client Secret, create an OAuth2 custom connection app in your Xero developer portal My Apps.\nUse these settings for your app:\nSelect Web app as the Integration Type.\nFor the Company or Application URL, enter the URL of your n8n server or reverse proxy address. For cloud users, for example, this is:\nCopy the OAuth Redirect URL from n8n and add it as an OAuth 2.0 redirect URI in your app.\nSelect appropriate scopes for your app. Refer to OAuth2 Scopes for more information.\nTo use all functionality in the Xero node, add the accounting.contacts and accounting.transactions scopes.\nRefer to Xero's OAuth Custom Connections documentation for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\yourls.md",
    "content": "Yourls credentials\nYou can use these credentials to authenticate the following nodes:\nYourls\nPrerequisites\nInstall Yourls on your server.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Yourl's documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA Signature token: Go to Tools > Secure passwordless API call to get your Signature token. Refer to Yourl's Passworldess API documentation for more information.\nA URL: Enter the URL of your Yourls instance."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zabbix.md",
    "content": "Zabbix credentials\nPrerequisites\nCreate a Zabbix Cloud account or self-host your own Zabbix server.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Zabbix's API documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing API key\nTo configure this credential, you'll need:\nan API Token: An API key for your Zabbix user.\nthe URL: The URL of your Zabbix server. Don't include /zabbix as part of the URL.\nRefer to Zabbix's API documentation for more information about authenticating to the service."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zammad.md",
    "content": "Zammad credentials\nYou can use these credentials to authenticate the following nodes:\nZammad\nPrerequisites\nCreate a hosted Zammad account or set up your own Zammad instance.\nFor token authentication, enable API Token Access in Settings > System > API. Refer to Setting up a Zammad for more information.\nSupported authentication methods\nBasic auth\nToken auth: Zammad recommends using this authentication method.\nRelated resources\nRefer to Zammad's API Authentication documentation for more information about authenticating with the service.\nUsing basic auth\nTo configure this credential, you'll need:\nA Base URL: Enter the URL of your Zammad instance.\nAn Email address: Enter the email address you use to log in to Zammad.\nA Password: Enter your Zammad password.\nIgnore SSL Issues: When turned on, n8n will connect even if SSL certificate validation fails.\nUsing token auth\nTo configure this credential, you'll need:\nA Base URL: Enter the URL of your Zammad instance.\nAn Access Token: Once API Token Access is enabled for the Zammad instance, any user with the user_preferences.access_token permission can generate an Access Token by going to your avatar > Profile > Token Access and Create a new token.\nThe access token permissions depend on what actions you'd like to complete with this credential. For all functionality within the Zammad node, select:\nadmin.group\nadmin.organization\nadmin.user\nticket.agent\nticket.customer\nIgnore SSL Issues: When turned on, n8n will connect even if SSL certificate validation fails."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zendesk.md",
    "content": "Zendesk credentials\nYou can use these credentials to authenticate the following nodes:\nZendesk\nZendesk Trigger\nPrerequisites\nCreate a Zendesk account.\nFor API token authentication, enable token access to the API in Admin Center under Apps and integrations > APIs > Zendesk APIs.\nSupported authentication methods\nAPI token\nOAuth2\nRelated resources\nRefer to Zendesk's API documentation for more information about the service.\nUsing API token\nTo configure this credential, you'll need:\nYour Subdomain: Your Zendesk subdomain is the portion of the URL between https:// and .zendesk.com. For example, if the Zendesk URL is  the subdomain is n8n-example.\nAn Email address: Enter the email address you use to log in to Zendesk.\nAn API Token: Generate an API token in Apps and integrations > APIs > Zendesk API. Refer to API token for more information.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you create a new OAuth client.\nA Client Secret: Generated when you create a new OAuth client.\nYour Subdomain: Your Zendesk subdomain is the portion of the URL between https:// and .zendesk.com. For example, if the Zendesk URL is  the subdomain is n8n-example.\nTo create a new OAuth client, go to Apps and integrations > APIs > Zendesk API > OAuth Clients.\nUse these settings:\nCopy the OAuth Redirect URL from n8n and enter it as a Redirect URL in the OAuth client.\nCopy the Unique identifier for the Zendesk client and enter this as your n8n Client ID.\nCopy the Secret from Zendesk and enter this as your n8n Client Secret\nRefer to Registering your application with Zendesk for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zep.md",
    "content": "Zep credentials\nYou can use these credentials to authenticate the following nodes:\nZep\nZep Vector Store\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Zep's Cloud SDK documentation and Open Source SDK documentation for more information about the service. Refer to Zep's REST API documentation for information about the API.\nUsing API key\nTo configure this credential, you'll need a Zep server with at least one project and:\nAn API URL\nAn API Key\nSetup depends on whether you're using Zep Cloud or self-hosted Zep Open Source.\nZep Cloud setup\nFollow these instructions if you're using Zep Cloud:\nIn Zep, open the Project Settings.\nIn the Project Keys section, select Add Key.\nEnter a Key Name, like n8n integration.\nSelect Create.\nCopy the key and enter it in your n8n integration as the API Key.\nTurn on the Cloud toggle.\nSelf-hosted Zep Open Source setup\nFollow these instructions if you're self-hosting Zep Open Source:\nEnter the JWT token for your Zep server as the API Key in n8n.\nIf you haven't generated a JWT token for your Zep server before, refer to Zep's Configuring Authentication for instructions.\nMake sure the Cloud toggle is off.\nEnter the URL for your Zep server as the API URL."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zoho.md",
    "content": "Zoho credentials\nYou can use these credentials to authenticate the following nodes:\nZoho CRM\nPrerequisites\nCreate a Zoho account.\nSupported authentication methods\nOAuth2\nRelated resources\nRefer to Zoho's CRM API documentation for more information about the service.\nUsing OAuth2\nTo configure this credential, you'll need:\nAn Access Token URL: Zoho provides region-specific access token URLs. Select the region that best fits your Zoho data center:\nAU: Select this option for Australia data center.\nCN: Select this option for Canada data center.\nEU: Select this option for the European Union data center.\nIN: Select this option for the India data center.\nUS: Select this option for the United States data center.\nRefer to Multi DC for more information about selecting a data center.\nIf you need to configure OAuth2 from scratch, register an application with Zoho.\nUse these settings for your application:\nSelect Server-based Applications as the Client Type.\nCopy the OAuth Callback URL from n8n and enter it in the Zoho Authorized Redirect URIs field.\nCopy the Client ID and Client Secret from the application and enter them in your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zoom.md",
    "content": "Zoom credentials\nYou can use these credentials to authenticate the following nodes:\nZoom\nPrerequisites\nCreate a Zoom account. Your account must have one of the following permissions:\nAccount owner\nAccount admin\nZoom for developers role\nSupported authentication methods\nAPI JWT token\nOAuth2\nRelated resources\nRefer to Zoom's API documentation for more information about the service.\nUsing API JWT token\nThis authentication method has been fully deprecated by Zoom. Don't create new credentials with it.\nTo configure this credential, you'll need:\nA JWT token: To create a JWT token, create a new JWT app in the Zoom App Marketplace.\nUsing OAuth2\nTo configure this credential, you'll need:\nA Client ID: Generated when you create an OAuth app on the Zoom App Marketplace.\nA Client Secret: Generated when you create an OAuth app.\nTo generate your Client ID and Client Secret, create an OAuth app.\nUse these settings for your OAuth app:\nSelect User-managed app for Select how the app is managed.\nCopy the OAuth Callback URL from n8n and enter it as an OAuth Redirect URL in Zoom.\nIf your n8n credential displays a Whitelist URL, also enter that URL as a an OAuth Redirect URL.\nEnter Scopes for the scopes you plan to use. For all functionality in the Zoom node, select:\nmeeting:read\nmeeting:write\nRefer to OAuth scopes | Meeting scopes for more information on meeting scopes.\nCopy the Client ID and Client Secret provided in the Zoom app and enter them in your n8n credential."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zscalerzia.md",
    "content": "Zscaler ZIA credentials\nPrerequisites\nCreate an admin account on a Zscaler Internet Access (ZIA) cloud instance.\nSupported authentication methods\nBasic auth and API key combo\nRelated resources\nRefer to Zscaler ZIA's documentation for more information about the service.\nThis is a credential-only node. Refer to Custom API operations to learn more. View example workflows and related content on n8n's website.\nUsing basic auth and API key combo\nTo configure this credential, you'll need:\nA Base URL: Enter the base URL of your Zscaler ZIA cloud name. To get your base URL, log in to the ZIA Admin Portal and go to Administration > Cloud Service API Security. The base URL is displayed in both the Cloud Service API Key tab and the OAuth 2.0 Authorization Servers tab.\nA Username: Enter your ZIA admin username.\nA Password: Enter your ZIA admin password.\nAn Api Key: Get an API key by creating one from Administration > Cloud Service API Security > Cloud Service API Key.\nRefer to About Cloud Service API Key for more detailed instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\zulip.md",
    "content": "Zulip credentials\nYou can use these credentials to authenticate the following nodes:\nZulip\nPrerequisites\nCreate a Zulip account.\nSupported authentication methods\nAPI key\nRelated resources\nRefer to Zulip's API documentation for more information about the service.\nUsing API key\nTo configure this credential, you'll need:\nA URL: Enter the URL of your Zulip domain.\nAn Email address: Enter the email address you use to log in to Zulip.\nAn API Key: Get your API key in the Gear cog > Personal Settings > Account & privacy > API Key. Refer to API Keys for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\google\\index.md",
    "content": "Google credentials\nThis section contains:\nOAuth2 single service: Create an OAuth2 credential for a specific service node, such as the Gmail node.\nOAuth2 generic: Create an OAuth2 credential for use with custom operations.\nService Account: Create a Service Account credential for some specific service nodes.\nGoogle PaLM and Gemini: Get a Google Gemini/Google PaLM API key.\nOAuth2 and Service Account\nThere are two authentication methods available for Google services nodes:\nOAuth2: Recommended because it's more widely available and easier to set up.\nService Account: Refer to the Google documentation: Understanding service accounts for guidance on when you need a service account.\nCompatible nodes\nOnce configured, you can use your credentials to authenticate the following nodes. Most nodes are compatible with OAuth2 authentication. Support for Service Account authentication is limited.\nTABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\google\\oauth-generic.md",
    "content": "Google: OAuth2 generic\nThis document contains instructions for creating a generic OAuth2 Google credential for use with custom operations.\nPrerequisites\nCreate a Google Cloud account.\nSet up OAuth\nThere are five steps to connecting your n8n credential to Google services:\nCreate a Google Cloud Console project.\nEnable APIs.\nConfigure your OAuth consent screen.\nCreate your Google OAuth client credentials.\nFinish your n8n credential.\nCreate a Google Cloud Console project\nFirst, create a Google Cloud Console project. If you already have a project, jump to the next section:\nEnable APIs\nWith your project created, enable the APIs you'll need access to:\nConfigure your OAuth consent screen\nIf you haven't used OAuth in your Google Cloud project before, you'll need to configure the OAuth consent screen:\nAccess your Google Cloud Console - Library. Make sure you're in the correct project.\n!The project dropdown in the Google Cloud top navigation\nCheck the project dropdown in the Google Cloud top navigation\nOpen the left navigation menu and go to APIs & Services > OAuth consent screen.\nSelect Get started to begin configuring OAuth consent.\nEnter an App name and User support email to include on the Oauth screen.\nFor the Audience, select Internal for user access within your organization's Google workspace or External for any user with a Google account. Refer to Google's User type documentation for more information on user types.\nSelect the Email addresses Google should use to contact you about changes to your project.\nRead and accept the Google's User Data Policy and select Create.\nIn the left-hand menu, select Branding.\nIn the Authorized domains section, select Add domain:\nIf you're using n8n's Cloud service, add n8n.cloud\nIf you're self-hosting, add the domain of your n8n instance.\nSelect Save at the bottom of the page.\nCreate your Google OAuth client credentials\nNext, create the OAuth client credentials in Google:\nIn the APIs & Services section, select Credentials.\nSelect + Create credentials > OAuth client ID.\nIn the Application type dropdown, select Web application.\nGoogle automatically generates a Name. Update the Name to something you'll recognize in your console.\nFrom your n8n credential, copy the OAuth Redirect URL. Paste it into the Authorized redirect URIs in Google Console.\nSelect Create.\nFinish your n8n credential\nWith the Google project and credentials fully configured, finish the n8n credential:\nFrom Google's OAuth client created modal, copy the Client ID. Enter this in your n8n credential.\nFrom the same Google modal, copy the Client Secret. Enter this in your n8n credential.\nYou must provide the scopes for this credential. Refer to Scopes for more information. Enter multiple scopes in a space-separated list, for example:\nIn n8n, select Sign in with Google to complete your Google authentication.\nSave your new credentials.\nVideo\nThe following video demonstrates the steps described above:\nScopes\nGoogle services have one or more possible access scopes. A scope limits what a user can do. Refer to OAuth 2.0 Scopes for Google APIs for a list of scopes for all services.\nn8n doesn't support all scopes. When creating a generic Google OAuth2 API credential, you can enter scopes from the Supported scopes list below. If you enter a scope that n8n doesn't already support, it won't work.\n??? Details \"Supported scopes\"\nTABLE_PLACEHOLDER_0\nTroubleshooting\nGoogle hasn't verified this app\nGoogle Cloud app becoming unauthorized"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\google\\oauth-single-service.md",
    "content": "Google: OAuth2 single service\nThis document contains instructions for creating a Google credential for a single service. They're also available as a video.\nPrerequisites\nCreate a Google Cloud account.\nSet up OAuth\nThere are five steps to connecting your n8n credential to Google services:\nCreate a Google Cloud Console project.\nEnable APIs.\nConfigure your OAuth consent screen.\nCreate your Google OAuth client credentials.\nFinish your n8n credential.\nCreate a Google Cloud Console project\nFirst, create a Google Cloud Console project. If you already have a project, jump to the next section:\nEnable APIs\nWith your project created, enable the APIs you'll need access to:\nConfigure your OAuth consent screen\nIf you haven't used OAuth in your Google Cloud project before, you'll need to configure the OAuth consent screen:\nAccess your Google Cloud Console - Library. Make sure you're in the correct project.\n!The project dropdown in the Google Cloud top navigation\nCheck the project dropdown in the Google Cloud top navigation\nOpen the left navigation menu and go to APIs & Services > OAuth consent screen.\nSelect Get started to begin configuring OAuth consent.\nEnter an App name and User support email to include on the Oauth screen.\nFor the Audience, select Internal for user access within your organization's Google workspace or External for any user with a Google account. Refer to Google's User type documentation for more information on user types.\nSelect the Email addresses Google should use to contact you about changes to your project.\nRead and accept the Google's User Data Policy and select Create.\nIn the left-hand menu, select Branding.\nIn the Authorized domains section, select Add domain:\nIf you're using n8n's Cloud service, add n8n.cloud\nIf you're self-hosting, add the domain of your n8n instance.\nSelect Save at the bottom of the page.\nCreate your Google OAuth client credentials\nNext, create the OAuth client credentials in Google:\nIn the APIs & Services section, select Credentials.\nSelect + Create credentials > OAuth client ID.\nIn the Application type dropdown, select Web application.\nGoogle automatically generates a Name. Update the Name to something you'll recognize in your console.\nFrom your n8n credential, copy the OAuth Redirect URL. Paste it into the Authorized redirect URIs in Google Console.\nSelect Create.\nFinish your n8n credential\nWith the Google project and credentials fully configured, finish the n8n credential:\nFrom Google's OAuth client created modal, copy the Client ID. Enter this in your n8n credential.\nFrom the same Google modal, copy the Client Secret. Enter this in your n8n credential.\nIn n8n, select Sign in with Google to complete your Google authentication.\nSave your new credentials.\nVideo\nTroubleshooting\nGoogle hasn't verified this app\nGoogle Cloud app becoming unauthorized"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\google\\service-account.md",
    "content": "Google: Service Account\nUsing service accounts is more complex than OAuth2. Before you begin:\nCheck if your node is compatible with Service Account.\nMake sure you need to use Service Account. For most use cases, OAuth2 is a better option.\nRead the Google documentation on Creating and managing service accounts.\nPrerequisites\nCreate a Google Cloud account.\nSet up Service Account\nThere are four steps to connecting your n8n credential to a Google Service Account:\nCreate a Google Cloud Console project.\nEnable APIs.\nSet up Google Cloud Service Account.\nFinish your n8n credential.\nCreate a Google Cloud Console project\nFirst, create a Google Cloud Console project. If you already have a project, jump to the next section:\nEnable APIs\nWith your project created, enable the APIs you'll need access to:\nSet up Google Cloud Service Account\nAccess your Google Cloud Console - Library. Make sure you're in the correct project.\n!The project dropdown in the Google Cloud top navigation\nCheck the project dropdown in the Google Cloud top navigation\nSelect the hamburger menu > APIs & Services > Credentials. Google takes you to your Credentials page.\nSelect + CREATE CREDENTIALS > Service account.\nEnter a name in Service account name and an ID in Service account ID. Refer to Creating a service account for more information.\nSelect CREATE AND CONTINUE.\nBased on your use-case, you may want to Select a role and Grant users access to this service account  using the corresponding sections.\nSelect DONE.\nSelect your newly created service account under the Service Accounts section. Open the KEYS tab.\nSelect ADD KEY > Create new key.\nIn the modal that appears, select JSON, then select CREATE. Google saves the file to your computer.\nFinish your n8n credential\nWith the Google project and credentials fully configured, finish the n8n credential:\nOpen the downloaded JSON file.\nCopy the client_email and enter it in your n8n credential as the Service Account Email.\nCopy the private_key. Don't include the surrounding \" marks. Enter this as the Private Key in your n8n credential.\nOptional: Choose if you want to Impersonate a User (turned on).\nTo use this option, you must Enable domain-wide delegation for the service account as a Google Workspace super admin.\nEnter the Email of the user you want to impersonate.\nIf you plan to use this credential with the HTTP Request node, turn on Set up for use in HTTP Request node.\nWith this setting turned on, you'll need to add Scope(s) for the node. n8n prepopulates some scopes. Refer to OAuth 2.0 Scopes for Google APIs for more information.\nSave your credentials.\nVideo\nThe following video demonstrates the steps described above.\nTroubleshooting\nService Account can't access Google Drive files\nA Service Account can't access Google Drive files and folders that weren't shared with its associated user email.\nAccess your Google Cloud Console and copy your Service Account email.\nAccess your Google Drive and go to the designated file or folder.\nRight-click on the file or folder and select Share.\nPaste your Service Account email into Add People and groups.\nSelect Editor for read-write access or Viewer for read-only access.\nEnable domain-wide delegation\nTo impersonate a user with a service account, you must enable domain-wide delegation for the service account.\nTo delegate domain-wide authority to a service account, you must be a super administrator for the Google Workspace domain. Then:\nFrom your Google Workspace domain's Admin console, select the hamburger menu, then select Security > Access and data control > API Controls.\nIn the Domain wide delegation pane, select Manage Domain Wide Delegation.\nSelect Add new.\nIn the Client ID field, enter the service account's Client ID. To get the Client ID:\nOpen your Google Cloud Console project, then open the Service Accounts page.\nCopy the OAuth 2 Client ID and use this as the Client ID for the Domain Wide Delegation.\nIn the OAuth scopes field, enter a list of comma-separate scopes to grant your application access. For example, if your application needs domain-wide full access to the Google Drive API and the Google Calendar API, enter:\nSelect Authorize.\nIt can take from 5 minutes up to 24 hours before you can impersonate all users in your Workspace."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\imap\\gmail.md",
    "content": "Gmail IMAP credentials\nFollow these steps to configure the IMAP credentials with a Gmail account.\nPrerequisites\nTo follow these instructions, you must first:\nEnable 2-step Verification on your Gmail account.\nGenerate an app password.\nEnable 2-step Verification\nGenerate an app password\nSet up the credential\nTo set up the IMAP credential with a Gmail account, use these settings:\nEnter your Gmail email address as the User.\nEnter the app password you generated above as the Password.\nEnter imap.gmail.com as the Host.\nFor the Port, keep the default port number of 993. Check with your email administrator if this port doesn't work.\nTurn on the SSL/TLS toggle.\nCheck with your email administrator about whether to Allow Self-Signed Certificates.\nRefer to Add Gmail to another client for more information. You may need to Enable IMAP if you're using a personal Google account before June 2024."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\imap\\index.md",
    "content": "IMAP credentials\nYou can use these credentials to authenticate the following nodes:\nIMAP Email\nPrerequisites\nCreate an email account on a service with IMAP support.\nSupported authentication methods\nUser account\nRelated resources\nInternet Message Access Protocol (IMAP) is a standard protocol for receiving email. Most email providers offer instructions on setting up their service with IMAP; refer to your provider's IMAP instructions.\nUsing user account\nTo configure this credential, you'll need:\nA User name: The email address you're retrieving email for.\nA Password: Either the password you use to check email or an app password. Your provider will tell you whether to use your own password or to generate an app password.\nA Host: The IMAP host address for your email provider, often formatted as imap..com. Check with your provider.\nA Port number: The default is port 993. Use this port unless your provider or email administrator tells you to use something different.\nChoose whether to use SSL/TLS and whether to Allow Self-Signed Certificates.\nProvider instructions\nRefer to the quickstart guides for these common email providers.\nGmail\nRefer to Gmail.\nOutlook.com\nRefer to Outlook.com.\nYahoo\nRefer to Yahoo.\nMy provider isn't listed\nIf your email provider isn't listed here, search for their IMAP settings or IMAP instructions."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\imap\\outlook.md",
    "content": "Outlook.com IMAP credentials\nFollow these steps to configure the IMAP credentials with an Outlook.com account.\nSet up the credentials\nTo set up the IMAP credential with Outlook.com account, use these settings:\nEnter your Outlook.com email address as the User.\nEnter your Outlook.com password as the Password.\nEnter outlook.office365.com as the Host.\nFor the Port, keep the default port number of 993.\nTurn on the SSL/TLS toggle.\nCheck with your email administrator about whether to Allow Self-Signed Certificates.\nRefer to Microsoft's POP, IMAP, and SMTP settings for Outlook.com documentation for more information.\nConnection errors\nYou may receive a connection error if you configured your Outlook.com account as IMAP in multiple email clients. Microsoft is working on a fix for this. For now, try this workaround:\nGo to account.live.com/activity and sign in using the email address and password of the affected account.\nUnder Recent activity, find the Session Type event that matches the most recent time you received the connection error. Select it to expand the details.\nSelect This was me to approve the IMAP connection.\nRetest your n8n credential.\nRefer to What is the Recent activity page? for more information on using this page.\nThe source for these instructions is Outlook.com IMAP connection errors. Refer to that documentation for more information.\nUse an app password"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\imap\\yahoo.md",
    "content": "Yahoo IMAP credentials\nFollow these steps to configure the IMAP credentials with a Yahoo account.\nPrerequisites\nTo follow these instructions, you must first generate an app password:\nSet up the credential\nTo set up the IMAP credential with a Yahoo Mail account, use these settings:\nEnter your Yahoo email address as the User.\nEnter the app password you generated above as the Password.\nEnter imap.mail.yahoo.com as the Host.\nKeep the default Port number of 993. Check with your email administrator if this port doesn't work.\nTurn on the SSL/TLS toggle.\nCheck with your email administrator about whether to Allow Self-Signed Certificates.\nRefer to Set up IMAP for Yahoo mail account for more information."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sendemail\\gmail.md",
    "content": "Gmail Send Email credentials\nFollow these steps to configure the Send Email credentials with a Gmail account.\nPrerequisites\nTo follow these instructions, you must first:\nEnable 2-step Verification on your Gmail account.\nGenerate an app password.\nEnable 2-step Verification\nGenerate an app password\nSet up the credential\nTo set up the Send Email credential to use Gmail:\nEnter your Gmail email address as the User.\nEnter the app password you generated above as the Password.\nEnter smtp.gmail.com as the Host.\nFor the Port:\nKeep the default 465 for SSL or if you're unsure what to use.\nEnter 587 for TLS.\nTurn on the SSL/TLS toggle.\nRefer to the Outgoing Mail (SMTP) Server settings in Read Gmail messages on other email clients using POP for more information. If the settings above don't work for you, check with your email administrator."
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sendemail\\index.md",
    "content": "Send Email credentials\nYou can use these credentials to authenticate the following nodes:\nSend Email\nPrerequisites\nCreate an email account on a service that supports SMTP.\nSome email providers require that you enable or set up outgoing SMTP or generate an app password. Refer to your provider's documentation to see if there are other required steps.\nSupported authentication methods\nSMTP account\nRelated resources\nSimple Message Transfer Protocol (SMTP) is a standard protocol for sending and receiving email. Most email providers offer instructions on setting up their service with SMTP. Refer to your provider's SMTP instructions.\nUsing SMTP account\nTo configure this credential, you'll need:\nA User email address\nA Password: This may be the user's password or an app password. Refer to the documentation for your email provider.\nThe Host: The SMTP host address for your email provider, often formatted as smtp..com. Check with your provider.\nA Port number: The default is port 465, commonly used for SSL. Other common ports are 587 for TLS or 25 for no encryption. Check with your provider.\nSSL/TLS: When turned on, SMTP will use SSL/TLS.\nDisable STARTTLS: When SSL/TLS is disabled, the SMTP server can still try to upgrade the TCP connection using STARTTLS. Turning this on prevents that behaviour.\nClient Host Name: If needed by your provider, add a client host name. This name identifies the client to the server.\nProvider instructions\nRefer to the quickstart guides for these common email providers.\nGmail\nRefer to Gmail.\nOutlook.com\nRefer to Outlook.com.\nYahoo\nRefer to Yahoo.\nMy provider isn't listed\nIf your email provider isn't listed here, search for SMTP settings to find their instructions. (These instructions may also be included with IMAP settings or POP settings.)"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sendemail\\outlook.md",
    "content": "Outlook.com Send Email credentials\nFollow these steps to configure the Send Email credentials with an Outlook.com account.\nSet up the credential\nTo configure the Send Email credential to use an Outlook.com account:\nEnter your Outlook.com email address as the User.\nEnter your Outlook.com password as the Password.\nEnter smtp-mail.outlook.com as the Host.\nEnter 587 for the Port.\nTurn on the SSL/TLS toggle.\nRefer to Microsoft's POP, IMAP, and SMTP settings for Outlook.com documentation for more information. If the settings above don't work for you, check with your email administrator.\nUse an app password"
  },
  {
    "file_path": "integrations\\builtin\\credentials\\sendemail\\yahoo.md",
    "content": "Yahoo Send Email credentials\nFollow these steps to configure the Send Email credentials with a Yahoo account.\nPrerequisites\nTo follow these instructions, you must first generate an app password:\nSet up the credential\nTo configure the Send Email credential to use Yahoo Mail:\nEnter your Yahoo email address as the User.\nEnter the app password you generated above as the Password.\nEnter smtp.mail.yahoo.com as the Host.\nFor the Port:\nKeep the default 465 for SSL or if you're unsure what to use.\nEnter 587 for TLS.\nTurn on the SSL/TLS toggle.\nRefer to IMAP server settings for Yahoo Mail for more information. If the settings above don't work for you, check with your email administrator."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\index.md",
    "content": "Triggers library\nThis section provides information about n8n's Triggers."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.activecampaigntrigger.md",
    "content": "ActiveCampaign Trigger node\nActiveCampaign is a cloud software platform for small-to-mid-sized business. The company offers software for customer experience automation, which combines the email marketing, marketing automation, sales automation, and CRM categories."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.acuityschedulingtrigger.md",
    "content": "Acuity Scheduling Trigger node\nAcuity Scheduling is a cloud-based appointment scheduling software solution that enables business owners to manage their appointments online. It has the capability to automatically sync calendars according to users' time zones and can send regular alerts and reminders to users regarding their appointment schedules."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.affinitytrigger.md",
    "content": "Affinity Trigger node\nAffinity is a powerful relationship intelligence platform enabling teams to leverage their network to close the next big deal."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.airtabletrigger.md",
    "content": "Airtable Trigger node\nAirtable is a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet. The fields in an Airtable table are similar to cells in a spreadsheet, but have types such as 'checkbox', 'phone number', and 'drop-down list', and can reference file attachments like images.\nOn this page, you'll find a list of events the Airtable Trigger node can respond to and links to more resources.\nEvents\nNew Airtable event\nRelated resources\nn8n provides an app node for Airtable. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Airtable's documentation for details about their API.\nNode parameters\nUse these parameters to configure your node.\nPoll Times\nn8n's Airtable node uses polling for check for updates on configured Airtable resources. The Poll Times parameter configures the querying frequency:\nEvery Minute\nEvery Hour\nEvery Day\nEvery Week\nEvery Month\nEvery X: Check for updates every given number of minutes or hours.\nCustom: Customize the polling interval by providing a cron expression.\nUse the Add Poll Time button to add more polling intervals.\nBase\nThe Airtable base you want to check for updates on. You can provide your base's URL or base ID.\nTable\nThe Airtable table within the Airtable base that you want to check for updates on. You can provide the table's URL or table ID.\nTrigger Field\nA created or last modified field in your table. The Airtable Trigger node uses this to determine what updates occurred since the previous check.\nDownload Attachments\nWhether to download attachments from the table. When enabled, the Download Fields parameter defines the attachment fields.\nDownload Fields\nWhen you enable the Download Attachments toggle, this field defines which table fields to download. Field names are case sensitive. Use a comma to separate multiple field names.\nAdditional Fields\nUse the Add Field button to add the following parameters:\nFields: A comma-separated list of fields to include in the output. If you don't specify anything here, the output will contain only the Trigger Field.\nFormula: An Airtable formula to further filter the results. You can use this to add further constraints to the events that trigger the workflow. Note that formula values aren't taken into account for manual executions, only for production polling.\nView ID: The name or ID of a table view. When defined, only returns records available in the given view."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.amqptrigger.md",
    "content": "AMQP Trigger node\nAMQP is an open standard application layer protocol for message-oriented middleware. The defining features of AMQP are message orientation, queuing, routing, reliability and security. This node supports AMQP 1.0 compatible message brokers."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.asanatrigger.md",
    "content": "Asana Trigger node\nAsana is a web and mobile application designed to help teams organize, track, and manage their work."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.autopilottrigger.md",
    "content": "Autopilot Trigger node\nAutopilot is a visual marketing software that allows you to automate and personalize your marketing across the entire customer journey.\nEvents\nContact added\nContact added to a list\nContact entered to a segment\nContact left a segment\nContact removed from a list\nContact unsubscribed\nContact updated"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.awssnstrigger.md",
    "content": "AWS SNS Trigger node\nAWS SNS is a notification service provided as part of Amazon Web Services. It provides a low-cost infrastructure for the mass delivery of messages, predominantly to mobile users."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.bitbuckettrigger.md",
    "content": "Bitbucket Trigger node\nBitbucket is a web-based version control repository hosting service owned by Atlassian, for source code and development projects that use either Mercurial or Git revision control systems."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.boxtrigger.md",
    "content": "Box Trigger node\nBox is a cloud computing company which provides file sharing, collaborating, and other tools for working with files uploaded to its servers.\nFind your Box Target ID\nTo get your Target ID in Box:\nOpen the file/folder that you would like to monitor.\nCopy the string of characters after folder/ in your URL. This is the target ID. For example, if the URL is  then 12345 is the target ID.\nPaste it in the Target ID field in n8n."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.brevotrigger.md",
    "content": "Brevo Trigger node\nBrevo is a digital marketing platform to help users grow their business."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.calendlytrigger.md",
    "content": "Calendly Trigger node\nCalendly is an automated scheduling software that's designed to help find meeting times."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.caltrigger.md",
    "content": "Cal Trigger node\nCal is the event-juggling scheduler for everyone. Focus on meeting, not making meetings."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.chargebeetrigger.md",
    "content": "Chargebee Trigger node\nChargebee is a billing platform for subscription based SaaS and eCommerce businesses. Chargebee integrates with payment gateways to let you automate recurring payment collection along with invoicing, taxes, accounting, email notifications, SaaS Metrics and customer management.\nAdd webhook URL in Chargebee\nTo add a Webhook URL in Chargebee:\nOpen your Chargebee dashboard.\nGo to Settings > Configure Chargebee.\nScroll down and select Webhooks.\nSelect the Add Webhook button.\nEnter the Webhook Name and the Webhook URL.\nSelect Create."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.ciscowebextrigger.md",
    "content": "Webex by Cisco Trigger node\nWebex by Cisco is a web conferencing and videoconferencing application."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.clickuptrigger.md",
    "content": "ClickUp Trigger node\nClickUp is a cloud-based collaboration and project management tool suitable for businesses of all sizes and industries. Features include communication and collaboration tools, task assignments and statuses, alerts and a task toolbar."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.clockifytrigger.md",
    "content": "Clockify Trigger node\nClockify is a free time tracker and timesheet app for tracking work hours across projects.\nThis node uses the workflow timezone setting to specify the range of time entries starting time. Configure the timezone in your Workflow Settings if you want this trigger node to retrieve the right time entries."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.convertkittrigger.md",
    "content": "ConvertKit Trigger node\nConvertKit is a fully featured email marketing platform. Use ConvertKit to build an email list, send email broadcasts, automate sequences, create segments, and build landing pages."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.coppertrigger.md",
    "content": "Copper Trigger node\nCopper is a CRM that focuses on strong integration with Google Workspace. It's mainly targeted towards small and medium-sized businesses."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.crowddevtrigger.md",
    "content": "crowd.dev Trigger node\nUse the crowd.dev Trigger node to respond to events in crowd.dev and integrate crowd.dev with other applications. n8n has built-in support for a wide range of crowd.dev events, including new activities and new members.\nOn this page, you'll find a list of events the crowd.dev Trigger node can respond to and links to more resources.\nEvents\nNew Activity\nNew Member\nRelated resources\nn8n provides an app node for crowd.dev. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to crowd.dev's documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.customeriotrigger.md",
    "content": "Customer.io Trigger node\nCustomer.io enables users to send newsletters to selected segments of customers using their website data. You can send targeted emails, push notifications, and SMS to lower churn, create stronger relationships, and drive subscriptions."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.emeliatrigger.md",
    "content": "Emelia Trigger node\nEmelia is a cold-mailing tool.\nEvents\nEmail Bounced\nEmail Opened\nEmail Replied\nEmail Sent\nLink Clicked\nUnsubscribed Contact"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.eventbritetrigger.md",
    "content": "Eventbrite Trigger node\nEventbrite is an event management and ticketing website. The service allows users to browse, create, and promote local events."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebookleadadstrigger.md",
    "content": "Facebook Lead Ads Trigger node\nUse the Facebook Lead Ads Trigger node to respond to events in Facebook Lead Ads and integrate Facebook Lead Ads with other applications. n8n has built-in support for responding to new leads.\nOn this page, you'll find a list of events the Facebook Lead Ads Trigger node can respond to, and links to more resources.\nEvents\nNew lead\nRelated resources\nView example workflows and related content on n8n's website.\nRefer to Facebook Lead Ads' documentation for details about their API.\nCommon issues\nHere are some common errors and issues with the Facebook Lead Ads Trigger node and steps to resolve or troubleshoot them.\nWorkflow only works in testing or production\nFacebook Lead Ads only allows you to register a single webhook per app. This means that every time you switch from using the testing URL to the production URL (and vice versa), Facebook Lead Ads overwrites the registered webhook URL.\nYou may have trouble with this if you try to test a workflow that's also active in production. Facebook Lead Ads will only send events to one of the two webhook URLs, so the other will never receive event notifications.\nTo work around this, you can disable your workflow when testing:\nGo to your workflow page.\nToggle the Active switch in the top panel to disable the workflow temporarily.\nTest your workflow using the test webhook URL.\nWhen you finish testing, toggle the Inactive toggle to enable the workflow again. The production webhook URL should resume working."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.figmatrigger.md",
    "content": "Figma Trigger (Beta) node\nFigma is a prototyping tool which is primarily web-based, with more offline features enabled by desktop applications for macOS and Windows.\nEvents\nFile Commented: Triggers when someone comments on a file.\nFile Deleted: Triggers when someone deletes an individual file, but not when someone deletes an entire folder with all files.\nFile Updated: Triggers when someone saves or deletes a file. A save occurs when someone closes a file within 30 seconds after making changes.\nFile Version Updated: Triggers when someone creates a named version in the version history of a file.\nLibrary Publish: Triggers when someone publishes a library file."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.flowtrigger.md",
    "content": "Flow Trigger node\nFlow is modern task and project management software for teams. It brings together tasks, projects, timelines, and conversations, and integrates with a lot of tools."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.formiotrigger.md",
    "content": "Form.io Trigger node\nForm.io is an enterprise class combined form and API data management platform for building complex form-based business process applications."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.formstacktrigger.md",
    "content": "Formstack Trigger node\nFormstack is a workplace productivity platform that helps organizations streamline digital work through no-code online forms, documents, and signatures."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.getresponsetrigger.md",
    "content": "GetResponse Trigger node\nGetResponse is an online platform that offers email marketing software, landing page creator, webinar hosting, and much more.\nEvents\nReceive notifications when a customer is subscribed to a list\nReceive notifications when a customer is unsubscribed from a list\nReceive notifications when an email is opened\nReceive notifications when an email is clicked\nReceive notifications when a survey is submitted"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.githubtrigger.md",
    "content": "GitHub Trigger node\nGitHub provides hosting for software development and version control using Git. It offers the distributed version control and source code management (SCM) functionality of Git, access control and several collaboration features such as bug tracking, feature requests, task management, and wikis for every project."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.gitlabtrigger.md",
    "content": "GitLab Trigger node\nGitLab is a web-based DevOps lifecycle tool that provides a Git-repository manager providing wiki, issue-tracking, and continuous integration/continuous installation pipeline features."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.googlebusinessprofiletrigger.md",
    "content": "Google Business Profile Trigger node\nUse the Google Business Profile Trigger node to respond to events in Google Business Profile and integrate Google Business Profile with other applications. n8n has built-in support for responding to new reviews.\nOn this page, you'll find a list of events the Google Business Profile Trigger node can respond to and links to more resources.\nEvents\nReview Added\nRelated resources\nn8n provides an app node for Google Business Profile. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Google Business Profile's documentation for details about their API."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.googlecalendartrigger.md",
    "content": "Google Calendar Trigger node\nGoogle Calendar is a time-management and scheduling calendar service developed by Google.\nEvents\nEvent Cancelled\nEvent Created\nEvent Ended\nEvent Started\nEvent Updated\nRelated resources\nn8n provides an app node for Google Calendar. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Google Calendar's documentation for details about their API."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.gumroadtrigger.md",
    "content": "Gumroad Trigger node\nGumroad is an online platform that enables creators to sell products directly to consumers."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.helpscouttrigger.md",
    "content": "Help Scout Trigger node\nHelp Scout is a help desk software that provides an email-based customer support platform, knowledge base tool, and an embeddable search/contact widget for customer service professionals."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.hubspottrigger.md",
    "content": "HubSpot Trigger node\nHubSpot provides tools for social media marketing, content management, web analytics, landing pages, customer support, and search engine optimization.\nEvents\nCompany\nCreated\nDeleted\nProperty changed\nContact\nCreated\nDeleted\nPrivacy deleted\nProperty changed\nConversation\nCreated\nDeleted\nNew message\nPrivacy deletion\nProperty changed\nDeal\nCreated\nDeleted\nProperty changed\nTicket\nCreated\nDeleted\nProperty changed\nRelated resources\nn8n provides an app node for HubSpot. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to HubSpot's documentation for details about their API."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.invoiceninjatrigger.md",
    "content": "Invoice Ninja Trigger node\nInvoice Ninja is a free open-source online invoicing app for freelancers & businesses. It offers invoicing, payments, expense tracking, & time-tasks."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.jiratrigger.md",
    "content": "Jira Trigger node\nJira is a proprietary issue tracking product developed by Atlassian that allows bug tracking and agile project management."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.jotformtrigger.md",
    "content": "JotForm Trigger node\nJotForm is an online form building service. JotForm's software creates forms with a drag and drop creation tool and an option to encrypt user data."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.kafkatrigger.md",
    "content": "Kafka Trigger node\nKafka is an open-source distributed event streaming platform that one can use for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.keaptrigger.md",
    "content": "Keap Trigger node\nKeap is an e-mail marketing and sales platform for small businesses, including products to manage and optimize the customer lifecycle, customer relationship management, marketing automation, lead capture, and e-commerce."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.kobotoolboxtrigger.md",
    "content": "KoboToolbox Trigger node\nKoboToolbox is a field survey and data collection tool to design interactive forms to be completed offline from mobile devices. It's available both as a free cloud solution or as a self-hosted version.\nThis node starts a workflow upon new submissions of a specified form. The trigger node handles the creation/deletion of the hook, so you don't need to do any setup in KoboToolbox.\nIt works the same way as the Get Submission operation in the KoboToolbox node, including supporting the same reformatting options."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.lemlisttrigger.md",
    "content": "Lemlist Trigger node\nLemlist is an email outreach platform that allows you to automatically generate personalized images and videos and send personalized cold emails.\nEvents\n*\nAircall Created\nAircall Done\nAircall Ended\nAircall Interested\nAircall Not Interested\nApi Done\nApi Failed\nApi Interested\nApi Not Interested\nAttracted\nConnection Issue\nContacted\nCustom Domain Errors\nEmails Bounced\nEmails Clicked\nEmails Failed\nEmails Interested\nEmails Not Interested\nEmails Opened\nEmails Replied\nEmails Send Failed\nEmails Sent\nEmails Unsubscribed\nHooked\nInterested\nLemwarm Paused\nLinkedIn Interested\nLinkedIn Invite Accepted\nLinkedIn Invite Done\nLinkedIn Invite Failed\nLinkedIn Not Interested\nLinkedIn Replied\nLinkedIn Send Failed\nLinkedIn Sent\nLinkedIn Visit Done\nLinkedIn Visit Failed\nLinkedIn Voice Note Done\nLinkedIn Voice Note Failed\nManual Interested\nManual Not Interested\nNot Interested\nOpportunities Done\nPaused\nResumed\nSend Limit Reached\nSkipped\nWarmed"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.lineartrigger.md",
    "content": "Linear Trigger node\nLinear is a SaaS issue tracking tool.\nEvents\nComment Reaction\nCycle\nIssue\nIssue Comment\nIssue Label\nProject"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.lonescaletrigger.md",
    "content": "LoneScale Trigger node\nUse the LoneScale Trigger node to respond to workflow events in LoneScale and integrate LoneScale with other applications.\nOn this page, you'll find a list of operations the LoneScale node supports, and links to more resources.\nEvents\nOn new LoneScale event\nRelated resources\nn8n provides an app node for LoneScale. You can find the node docs here.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.mailchimptrigger.md",
    "content": "Mailchimp Trigger node\nMailchimp is an integrated marketing platform that allows business owners to automate their email campaigns and track user engagement."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.mailerlitetrigger.md",
    "content": "MailerLite Trigger node\nMailerLite is an email marketing solution that provides you with a user-friendly content editor, simplified subscriber management, and campaign reports with the most important statistics.\nOn this page, you'll find a list of events the MailerLite Trigger node can respond to and links to more resources.\nEvents\nCampaign Sent\nSubscriber Added to Group\nSubscriber Automation Completed\nSubscriber Automation Triggered\nSubscriber Bounced\nSubscriber Created\nSubscriber Complained\nSubscriber Removed from Group\nSubscriber Unsubscribe\nSubscriber Updated"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.mailjettrigger.md",
    "content": "Mailjet Trigger node\nMailjet is a cloud-based email sending and tracking system. The platform allows professionals to send both marketing emails and transactional emails. It includes tools for designing emails, sending massive volumes and tracking these messages."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.mautictrigger.md",
    "content": "Mautic Trigger node\nMautic is an open-source marketing automation software that helps online businesses automate their repetitive marketing tasks such as lead generation, contact scoring, contact segmentation, and marketing campaigns.\nRelated resources\nn8n provides an app node for Mautic. You can find the node docs here.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.microsoftonedrivetrigger.md",
    "content": "Microsoft OneDrive Trigger node\nUse the Microsoft OneDrive Trigger node to respond to events in Microsoft OneDrive and integrate Microsoft OneDrive with other applications. n8n has built-in support for file and folder events in OneDrive.\nOn this page, you'll find a list of events the Microsoft OneDrive Trigger node can respond to and links to more resources.\nEvents\nOn File Created\nOn File Updated\nOn Folder Created\nOn Folder Updates\nRelated resources\nn8n provides an app node for Microsoft OneDrive. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Microsoft's OneDrive API documentation for more information about the service."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.microsoftoutlooktrigger.md",
    "content": "Microsoft Outlook Trigger node\nUse the Microsoft Outlook Trigger node to respond to events in Microsoft Outlook and integrate Microsoft Outlook with other applications.\nOn this page, you'll find a list of events the Microsoft Outlook Trigger node can respond to, and links to more resources.\nEvents\nMessage Received\nRelated resources\nn8n provides an app node for Microsoft Outlook. You can find the node docs here.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.microsoftteamstrigger.md",
    "content": "Microsoft Teams Trigger node\nUse the Microsoft Teams Trigger node to respond to events in Microsoft Teams and integrate Microsoft Teams with other applications.\nOn this page, you'll find a list of events the Microsoft Teams Trigger node can respond to and links to more resources.\nEvents\nNew Channel\nNew Channel Message\nNew Chat\nNew Chat Message\nNew Team Member\nRelated resources\nn8n provides an app node for Microsoft Teams. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to the Microsoft Teams documentation for details about their API."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.mqtttrigger.md",
    "content": "MQTT Trigger node\nMQTT is an open OASIS and ISO standard lightweight, publish-subscribe network protocol that transports messages between devices."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.netlifytrigger.md",
    "content": "Netlify Trigger node\nNetlify offers hosting and serverless backend services for web applications and static websites.\nRelated resources\nn8n provides an app node for Netlify. You can find the node docs here.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.notiontrigger.md",
    "content": "Notion Trigger node\nNotion is an all-in-one workspace for your notes, tasks, wikis, and databases.\nEvents\nPage added to database\nPage updated in database\nRelated resources\nn8n provides an app node for Notion. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Notion's documentation for details about their API."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.onfleettrigger.md",
    "content": "Onfleet Trigger node\nOnfleet is a logistics platform offering a last-mile delivery solution.\nEvents\nTrigger a workflow on:\nSMS recipient opt out\nSMS recipient response missed\nTask arrival\nTask assigned\nTask cloned\nTask completed\nTask created\nTask delayed\nTask ETA\nTask failed\nTask started\nTask unassigned\nTask updated\nWorker created\nWorker deleted\nWorker duty"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.paypaltrigger.md",
    "content": "PayPal Trigger node\nPayPal is a digital payment service that supports online fund transfers that customers can use when shopping online."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.pipedrivetrigger.md",
    "content": "Pipedrive Trigger node\nPipedrive is a cloud-based sales software company that aims to improve the productivity of businesses through the use of their software."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.postgrestrigger.md",
    "content": "Postgres Trigger node\nUse the Postgres Trigger node to respond to events in Postgres and integrate Postgres with other applications. n8n has built-in support responding to insert, update, and delete events.\nEvents\nYou can configure how the node listens for events.\nSelect Listen and Create Trigger Rule, then choose the events to listen for:\nInsert\nUpdate\nDelete\nSelect Listen to Channel, then enter a channel name that the node should monitor.\nRelated resources\nn8n provides an app node for Postgres. You can find the node docs here.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.postmarktrigger.md",
    "content": "Postmark Trigger node\nPostmark helps deliver and track application email. You can track statistics such as the number of emails sent or processed, opens, bounces and, spam complaints."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.pushcuttrigger.md",
    "content": "Pushcut Trigger node\nPushcut is an app for iOS that lets you create smart notifications to kick off shortcuts, URLs, and online automation.\nConfigure a Pushcut action\nFollow these steps to configure your Pushcut Trigger node with your Pushcut app.\nIn your Pushcut app, select a notification from the Notifications screen.\nSelect the Add Action button.\nEnter an action name in the Label field.\nSelect the Server tab.\nSelect the Integration tab.\nSelect Integration Trigger.\nIn n8n, enter a name for the action and select Execute step.\nSelect this action under the Select Integration Trigger screen in your Pushcut app.\nSelect Done in the top right to save the action."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.rabbitmqtrigger.md",
    "content": "RabbitMQ Trigger node\nRabbitMQ is an open-source message broker that accepts and forwards messages.\nRelated resources\nn8n provides an app node for RabbitMQ. You can find the node docs here.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.redistrigger.md",
    "content": "Redis Trigger node\nRedis is an open-source, in-memory data structure store, used as a database, cache and message broker.\nUse the Redis Trigger node to subscribe to a Redis channel. The workflow starts whenever the channel receives a new message."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.salesforcetrigger.md",
    "content": "Salesforce Trigger node\nUse the Salesforce Trigger node to respond to events in Salesforce and integrate Salesforce with other applications. n8n has built-in support for a wide range of Salesforce events.\nOn this page, you'll find a list of events the Salesforce Trigger node can respond to, and links to more resources.\nEvents\nOn Account Created\nOn Account Updated\nOn Attachment Created\nOn Attachment Updated\nOn Case Created\nOn Case Updated\nOn Contact Created\nOn Contact Updated\nOn Custom Object Created\nOn Custom Object Updated\nOn Lead Created\nOn Lead Updated\nOn Opportunity Created\nOn Opportunity Updated\nOn Task Created\nOn Task Updated\nOn User Created\nOn User Updated\nRelated resources\nn8n provides an app node for Salesforce. You can find the node docs here.\nView example workflows and related content on n8n's website."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.seatabletrigger.md",
    "content": "SeaTable Trigger node\nSeaTable is a collaborative database application with a spreadsheet interface."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.shopifytrigger.md",
    "content": "Shopify Trigger node\nShopify is an e-commerce platform that allows users to set up an online store and sell their products."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.slacktrigger.md",
    "content": "Slack Trigger node\nUse the Slack Trigger node to respond to events in Slack and integrate Slack with other applications. n8n has built-in support for a wide range of Slack events, including new messages, reactions, and new channels.\nOn this page, you'll find a list of events the Slack Trigger node can respond to and links to more resources.\nEvents\nAny Event: The node triggers on any event in Slack.\nBot / App Mention: The node triggers when your bot or app is mentioned in a channel the app is in.\nFile Made Public: The node triggers when a file is made public.\nFile Shared: The node triggers when a file is shared in a channel the app is in.\nNew Message Posted to Channel: The node triggers when a new message is posted to a channel the app is in.\nNew Public Channel Created: The node triggers when a new public channel is created.\nNew User: The node triggers when a new user is added to Slack.\nReaction Added: The node triggers when a reaction is added to a message the app is added to.\nParameters\nOnce you've set the events to trigger on, use the remaining parameters to further define the node's behavior:\nWatch Whole Workspace: Whether the node should watch for the selected Events in all channels in the workspace (turned on) or not (turned off, default).\nChannel to Watch: Select the channel your node should watch for the selected Events. This parameter only appears if you don't turn on Watch Whole Workspace. You can select a channel:\nFrom list: The node uses your credential to look up a list of channels in the workspace so you can select the channel you want.\nBy ID: Enter the ID of a channel you want to watch. Slack displays the channel ID at the bottom of the channel details with a one-click copy button.\nBy URL: Enter the URL of the channel you want to watch, formatted as\nDownload Files: Whether to download files and use them in the node's output (turned on) or not (turned off, default). Use this parameter with the File Made Public and File Shared events.\nOptions\nYou can further refine the node's behavior when you Add Options:\nResolve IDs: Whether to resolve the IDs to their respective names and return them (turned on) or not (turned off, default).\nUsernames or IDs to ignore: Select usernames or enter a comma-separated string of encoded user IDs to ignore events from. Choose from the list, or specify IDs using an expression.\nRelated resources\nn8n provides an app node for Slack. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Slack's documentation for details about their API.\nRequired scopes\nTo use this node, you need to create an application in Slack and enable event subscriptions. Refer to Slack credentials TABLE_PLACEHOLDER_0\nCommon issues\nHere are some common errors and issues with the Slack Trigger node and steps to resolve or troubleshoot them.\nWorkflow only works in testing or production\nSlack only allows you to register a single webhook per app. This means that you can't switch from using the testing URL to the production URL (and vice versa) without reconfiguring the registered webhook URL.\nYou may have trouble with this if you try to test a workflow that's also active in production. Slack will only send events to one of the two webhook URLs, so the other will never receive event notifications.\nTo work around this, you can disable your workflow when testing:\nGo to your workflow page.\nToggle the Active switch in the top panel to disable the workflow temporarily.\nEdit the Request URL in your the Slack Trigger configuration to use the testing webhook URL instead of the production webhook URL.\nTest your workflow using the test webhook URL.\nWhen you finish testing, edit the Request URL in your the Slack Trigger configuration to use the production webhook URL instead of the testing webhook URL.\nToggle the Inactive toggle to enable the workflow again. The production webhook URL should resume working.\nToken expired"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.stravatrigger.md",
    "content": "Strava Trigger node\nStrava is an internet service for tracking human exercise which incorporates social network features.\nEvents\n[All]\n[All]\nCreated\nDeleted\nUpdated\nActivity\n[All]\nCreated\nDeleted\nUpdated\nAthlete\n[All]\nCreated\nDeleted\nUpdated"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.stripetrigger.md",
    "content": "Stripe Trigger node\nStripe is a suite of payment APIs that powers commerce for online businesses."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.surveymonkeytrigger.md",
    "content": "SurveyMonkey Trigger node\nSurveyMonkey is an online cloud-based SaaS survey platform that also provides a suite of paid back-end programs."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.taigatrigger.md",
    "content": "Taiga Trigger node\nTaiga is a free and open-source project management platform for startups, agile developers, and designers."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.thehive5trigger.md",
    "content": "TheHive 5 Trigger node\nUse the TheHive 5 Trigger node to respond to events in TheHive and integrate TheHive with other applications. n8n has built-in support for a wide range of TheHive events, including alerts, cases, comments, pages, and tasks.\nOn this page, you'll find a list of events the TheHive5 Trigger node can respond to and links to more resources.\nEvents\nAlert\nCreated\nDeleted\nUpdated\nCase\nCreated\nDeleted\nUpdated\nComment\nCreated\nDeleted\nUpdated\nObservable\nCreated\nDeleted\nUpdated\nPage\nCreated\nDeleted\nUpdated\nTask\nCreated\nDeleted\nUpdated\nTask log\nCreated\nDeleted\nUpdated\nRelated resources\nn8n provides an app node for TheHive 5. You can find the node docs here.\nRefer to TheHive's documentation for more information about the service.\nConfigure a webhook in TheHive\nTo configure the webhook for your TheHive instance:\nCopy the testing and production webhook URLs from TheHive Trigger node.\nAdd the following lines to the application.conf file. This is TheHive configuration file:\nReplace TESTING_WEBHOOK_URL and PRODUCTION_WEBHOOK_URL with the URLs you copied in the previous step.\nReplace TESTING_WEBHOOK_NAME and PRODUCTION_WEBHOOK_NAME with your preferred endpoint names.\nReplace ORGANIZATION_NAME with your organization name.\nExecute the following cURL command to enable notifications:"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.thehivetrigger.md",
    "content": "TheHive Trigger node\nOn this page, you'll find a list of events the TheHive Trigger node can respond to and links to more resources.\nEvents\nAlert\nCreated\nDeleted\nUpdated\nCase\nCreated\nDeleted\nUpdated\nLog\nCreated\nDeleted\nUpdated\nObservable\nCreated\nDeleted\nUpdated\nTask\nCreated\nDeleted\nUpdated\nRelated resources\nn8n provides an app node for TheHive. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to TheHive's documentation for more information about the service:\nVersion 3\nVersion 4\nConfigure a webhook in TheHive\nTo configure the webhook for your TheHive instance:\nCopy the testing and production webhook URLs from TheHive Trigger node.\nAdd the following lines to the application.conf file. This is TheHive configuration file:\nReplace TESTING_WEBHOOK_URL and PRODUCTION_WEBHOOK_URL with the URLs you copied in the previous step.\nReplace TESTING_WEBHOOK_NAME and PRODUCTION_WEBHOOK_NAME with your preferred endpoint names.\nReplace ORGANIZATION_NAME with your organization name.\nExecute the following cURL command to enable notifications:"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.toggltrigger.md",
    "content": "Toggl Trigger node\nToggl is a time tracking app that offers online time tracking and reporting services through their website along with mobile and desktop applications."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.trellotrigger.md",
    "content": "Trello Trigger node\nTrello is a web-based Kanban-style list-making application which is a subsidiary of Atlassian. Users can create their task boards with different columns and move the tasks between them.\nFind the Model ID\nThe model ID is the ID of any model in Trello. Depending on the use-case, it could be the User ID, List ID, and so on.\nFor this specific example, the List ID would be the Model ID:\nOpen the Trello board that contains the list.\nIf the list doesn't have any cards, add a card to the list.\nOpen the card, add .json at the end of the URL, and press enter.\nIn the JSON file, you will see a field called idList.\nCopy idListand paste it in the Model ID field in n8n."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.twiliotrigger.md",
    "content": "Twilio Trigger node\nUse the Twilio Trigger node to respond to events in Twilio and integrate Twilio with other applications. n8n has built-in support for a wide range of Twilio events, including new SMS and calls.\nOn this page, you'll find a list of events the Twilio Trigger node can respond to and links to more resources.\nEvents\nOn New SMS\nOn New Call\nRelated resources\nn8n provides an app node for Twilio. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Twilio's documentation for details about their API."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.typeformtrigger.md",
    "content": "Typeform Trigger node\nTypeform is an online software as a service company that specializes in online form building and online surveys. Its main software creates dynamic forms based on user needs."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.venafitlsprotectcloudtrigger.md",
    "content": "Venafi TLS Protect Cloud Trigger node\nVenafi is a cybersecurity company providing services for machine identity management. They offer solutions to manage and protect identities for a wide range of machine types, delivering global visibility, lifecycle automation, and actionable intelligence.\nUse the n8n Venafi TLS Protect Cloud Trigger node to start a workflow in n8n in response to events in the cloud-based Venafi TLS Protect service."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.webflowtrigger.md",
    "content": "Webflow Trigger node\nWebflow is an application that allows you to build responsive websites with browser-based visual editing software."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.whatsapptrigger.md",
    "content": "WhatsApp Trigger node\nUse the WhatsApp Trigger node to respond to events in WhatsApp and integrate WhatsApp with other applications. n8n has built-in support for a wide range of WhatsApp events, including account, message, and phone number events.\nOn this page, you'll find a list of events the WhatsApp Trigger node can respond to, and links to more resources.\nEvents\nAccount Review Update\nAccount Update\nBusiness Capability Update\nMessage Template Quality Update\nMessage Template Status Update\nMessages\nPhone Number Name Update\nPhone Number Quality Update\nSecurity\nTemplate Category Update\nRelated resources\nn8n provides an app node for WhatsApp. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to WhatsApp's documentation for details about their API.\nCommon issues\nHere are some common errors and issues with the WhatsApp Trigger node and steps to resolve or troubleshoot them.\nWorkflow only works in testing or production\nWhatsApp only allows you to register a single webhook per app. This means that every time you switch from using the testing URL to the production URL (and vice versa), WhatsApp overwrites the registered webhook URL.\nYou may have trouble with this if you try to test a workflow that's also active in production. WhatsApp will only send events to one of the two webhook URLs, so the other will never receive event notifications.\nTo work around this, you can disable your workflow when testing:\nGo to your workflow page.\nToggle the Active switch in the top panel to disable the workflow temporarily.\nTest your workflow using the test webhook URL.\nWhen you finish testing, toggle the Inactive toggle to enable the workflow again. The production webhook URL should resume working."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.wisetrigger.md",
    "content": "Wise Trigger node\nWise allows you to transfer money abroad with low-cost money transfers, receive money with international account details, and track transactions on your phone.\nEvents\nTriggered every time a balance account is credited\nTriggered every time a balance account is credited or debited\nTriggered every time a transfer's list of active cases is updated\nTriggered every time a transfer's status is updated"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.woocommercetrigger.md",
    "content": "WooCommerce Trigger node\nWooCommerce is a customizable, open-source e-commerce plugin for WordPress.\nEvents\ncoupon.created\ncoupon.updated\ncoupon.deleted\ncustomer.created\ncustomer.updated\ncustomer.deleted\norder.created\norder.updated\norder.deleted\nproduct.created\nproduct.updated\nproduct.deleted"
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.workabletrigger.md",
    "content": "Workable Trigger node\nUse the Workable Trigger node to respond to events in the Workable recruiting platform and integrate Workable with other applications. n8n has built-in support for a wide range of Workable events, including candidate created and moved.\nOn this page, you'll find a list of events the Workable Trigger node can respond to and links to more resources.\nEvents\nCandidate Created\nCandidate Moved\nRelated resources\nView example workflows and related content on n8n's website.\nRefer to Workable's API documentation for details about using the service."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.wufootrigger.md",
    "content": "Wufoo Trigger node\nWufoo is an online form builder that helps you create custom HTML forms without writing code."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.zendesktrigger.md",
    "content": "Zendesk Trigger node\nZendesk is a support ticketing system, designed to help track, prioritize, and solve customer support interactions. More than just a help desk, Zendesk Support helps nurture customer relationships with personalized, responsive support across any channel."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\ad-account.md",
    "content": "Facebook Trigger Ad Account object\nUse this object to receive updates on certain ads changes in an Ad Account. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Ad Account as the Object.\nField Names or IDs: By default, the node will trigger on all the available Ad Account events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in. Options include:\nIn Process Ad Objects: Notifies you when a campaign, ad set, or ad exits the IN_PROCESS status. Refer to Meta's Post-processing for Ad Creation and Edits for more information.\nWith Issues Ad Objects: Notifies you when a campaign, ad set, or ad under the ad account receives the WITH_ISSUES status.\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRelated resources\nRefer to Webhooks for Ad Accounts and Meta's Ad Account Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\application.md",
    "content": "Facebook Trigger Application object\nUse this object to receive updates sent to a specific app. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Application as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in. Options include:\nAdd Account\nAds Rules Engine\nAsync Requests\nAsync Sessions\nGroup Install\nOe Reseller Onboarding Request Created\nPlugin Comment\nPlugin Comment Reply\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRelated resources\nRefer to Meta's Application Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\certificate-transparency.md",
    "content": "Facebook Trigger Certificate Transparency object\nUse this object to receive updates about newly issued certificates for any domains that you have subscribed for certificate alerts or phishing alerts. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Certificate Transparency as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in. Options include:\nCertificate: Notifies you when someone issues a new certificate for your subscribed domains. You'll need to subscribe your domain for certificate alerts.\nPhishing: Notifies you when someone issues a new certificate that may be phishing one of your legitimate subscribed domains.\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nFor these alerts, you'll need to subscribe your domain to the relevant alerts:\nRefer to Certificate Alerts for Certificate Alerts subscriptions.\nRefer to Phishing Alerts for Phishing Alerts subscriptions.\nRelated resources\nRefer to Webhooks for Certificate Transparency and Meta's Certificate Transparency Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\group.md",
    "content": "Facebook Trigger Group object\nUse this object to receive updates about activities and events in a group. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Group as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in.\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRelated resources\nRefer to Meta's Groups Workplace API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\index.md",
    "content": "Facebook Trigger node\nFacebook is a social networking site to connect and share with family and friends online.\nUse the Facebook Trigger node to trigger a workflow when events occur in Facebook.\nObjects\nAd Account: Get updates for certain ads changes.\nApplication: Get updates sent to the application.\nCertificate Transparency: Get updates when new security certificates are generated for your subscribed domains, including new certificates and potential phishing attempts.\nActivity and events in a Group\nInstagram: Get updates when someone comments on the Media objects of your app users; @mentions your app users; or when Stories of your app users expire.\nLink: Get updates about the links for rich previews by an external provider\nPage updates\nPermissions: Updates when granting or revoking permissions\nUser profile updates\nWhatsApp Business Account\nWorkplace Security\nFor each Object, use the Field Names or IDs dropdown to select more details on what data to receive. Refer to the linked pages for more details.\nRelated resources\nView example workflows and related content on n8n's website.\nRefer to Meta's Graph API documentation for details about their API."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\instagram.md",
    "content": "Facebook Trigger Instagram object\nUse this object to receive updates when someone comments on the Media objects of your app users; @mentions your app users; or when Stories of your app users expire. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Instagram as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in. Options include:\nComments: Notifies you when anyone comments on an IG Media owned by your app's Instagram user.\nMessaging Handover\nMentions: Notifies you whenever an Instagram user @mentions an Instagram Business or Creator Account in a comment or caption.\nMessages: Notifies you when anyone messages your app's Instagram user.\nMessaging Seen: Notifies you when someone sees a message sent by your app's Instagram user.\nStandby\nStory Insights: Notifies you one hour after a story expires with metrics describing interactions on a story.\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRelated resources\nRefer to Webhooks for Instagram and Meta's Instagram Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\link.md",
    "content": "Facebook Trigger Link object\nUse this object to receive updates about links for rich previews by an external provider. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Link as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in.\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRelated resources\nRefer to Meta's Links Workplace API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\page.md",
    "content": "Facebook Trigger Page object\nUse this object to receive updates when updates to your page profile fields or profile settings occur or someone mentions your page. Refer to Facebook Trigger for more information on the trigger itself.\nPrerequisites\nThis Object requires some configuration in your app and page before you can use the trigger:\nAt least one page admin needs to grant the manage_pages permission to your app.\nThe page admin needs to have at least moderator privileges. If they don't, they won't receive all content.\nYou'll also need to add the app to your page, and you may need to go to the Graph API explorer and execute this call with your app token:\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Page as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in. Options include individual profile fields, as well as:\nFeed: Describes most changes to a page's feed, including posts, likes, shares, and so on.\nLeadgen: Notifies you when a page's lead generation settings change.\nLive Videos: Notifies you when a page's live video status changes.\nMention: Notifies you when new mentions in pages, comments, and so on occur.\nMerchant Review: Notifies you when a page's merchant review settings change.\nPage Change Proposal: Notifies you when Facebook suggests proposed changes for your Facebook Page.\nPage Upcoming Change: Notifies you about upcoming changes that will occur on your Facebook Page. Facebook has suggested these changes and they may have a deadline to accept or reject before automatically taking effect.\nProduct Review: Notifies you when a page's product review settings change.\nRatings: Notifies you when a page's ratings change, including new ratings or when a user comments on or reacts to a rating.\nVideos: Notifies you when the encoding status of a video on a page changes.\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRelated resources\nRefer to Webhooks for Pages and Meta's Page Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\permissions.md",
    "content": "Facebook Trigger Permissions object\nUse this object to receive updates when a user grants or revokes a permission for your app. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Permissions as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in.\nIn Options, choose whether to turn on the toggle to Include Values. When turned on, the node includes the new values for the changes.\nRelated resources\nRefer to Meta's Permissions Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\user.md",
    "content": "Facebook Trigger User object\nUse this object to receive updates when changes to a user's profile occur. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect User as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in.\nIn Options, choose whether to turn on the toggle to Include Values. When turned on, the node includes the new values for the changes.\nRelated resources\nRefer to Meta's User Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\whatsapp.md",
    "content": "Facebook Trigger WhatsApp Business Account object\nUse this object to receive updates when your WhatsApp Business Account (WABA) changes. Refer to Facebook Trigger for more information on the trigger itself.\nPrerequisites\nThis Object requires some configuration in your app and WhatsApp account before you can use the trigger:\nSubscribe your app under your WhatsApp business account. You must subscribe an app owned by your business. Apps shared with your business can't receive webhook notifications.\nIf you are working as a Solution Partner, make sure your app has completed App Review and requested the whatsapp_business_management permission.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect WhatsApp Business Account as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in. Options include:\nMessage Template Status Update\nPhone Number Name Update\nPhone Number Quality Update\nAccount Review Update\nAccount Update\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRefer to Webhooks for WhatsApp Business Accounts and Meta's WhatsApp Business Account Graph API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.facebooktrigger\\workplace-security.md",
    "content": "Facebook Trigger Workplace Security object\nUse this object to receive updates when Workplace security events occur, like adding or removing admins, users joining or leaving a Workplace, and more. Refer to Facebook Trigger for more information on the trigger itself.\nTrigger configuration\nTo configure the trigger with this Object:\nSelect the Credential to connect with. Select an existing or create a new Facebook App credential.\nEnter the APP ID of the app connected to your credential. Refer to the Facebook App credential documentation for more information.\nSelect Workplace Security as the Object.\nField Names or IDs: By default, the node will trigger on all the available events using the * wildcard filter. If you'd like to limit the events, use the X to remove the star and use the dropdown or an expression to select the updates you're interested in.\nIn Options, turn on the toggle to Include Values. This Object type fails without the option enabled.\nRelated resources\nRefer to Meta's Security Workplace API reference for more information."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.gmailtrigger\\common-issues.md",
    "content": "Gmail Trigger node common issues\nHere are some common errors and issues with the Gmail Trigger node and steps to resolve or troubleshoot them.\n401 unauthorized error\nThe full text of the error looks like this:\nThis error occurs when there's an issue with the credential you're using and its scopes or permissions.\nTo resolve:\nFor OAuth2 credentials, make sure you've enabled the Gmail API in APIs & Services > Library. Refer to Google OAuth2 Single Service - Enable APIs for more information.\nFor Service Account credentials:\nEnable domain-wide delegation.\nMake sure you add the Gmail API as part of the domain-wide delegation configuration."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.gmailtrigger\\index.md",
    "content": "Gmail Trigger node\nGmail is an email service developed by Google. The Gmail Trigger node can start a workflow based on events in Gmail.\nEvents\nMessage Received: The node triggers for new messages at the selected Poll Time.\nNode parameters\nConfigure the node with these parameters:\nCredential to connect with: Select or create a new Google credential to use for the trigger. Refer to Google credentials for more information on setting up a new credential.\nPoll Times: Select a poll Mode to set how often to trigger the poll. Your Mode selection will add or remove relevant fields. Refer to Poll Mode options to configure the parameters for each mode type.\nSimplify: Choose whether to return a simplified version of the response (turned on, default) or the raw data (turned off).\nThe simplified version returns email message IDs, labels, and email headers, including: From, To, CC, BCC, and Subject.\nNode filters\nUse these filters to further refine the node's behavior:\nInclude Spam and Trash: Select whether the node should trigger on new messages in the Spam and Trash folders (turned on) or not (turned off).\nLabel Names or IDs: Only trigger on messages with the selected labels added to them. Select the Label names you want to apply or enter an expression to specify IDs. The dropdown populates based on the Credential you selected.\nSearch: Enter Gmail search refine filters, like from:, to trigger the node on the filtered conditions only. Refer to Refine searches in Gmail for more information.\nRead Status: Choose whether to receive Unread and read emails, Unread emails only (default), or Read emails only.\nSender: Enter an email or a part of a sender name to trigger only on messages from that sender.\nRelated resources\nn8n provides an app node for Gmail. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Google's Gmail API documentation for details about their API.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.gmailtrigger\\poll-mode-options.md",
    "content": "Gmail Trigger node Poll Mode options\nUse the Gmail Trigger node's Poll Time parameter to set how often to trigger the poll. Your Mode selection will add or remove relevant fields.\nPoll mode options\nRefer to the sections below for details on using each Mode."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.googledrivetrigger\\common-issues.md",
    "content": "Google Drive Trigger node common issues\nHere are some common errors and issues with the Google Drive Trigger node and steps to resolve or troubleshoot them.\n401 unauthorized error\nThe full text of the error looks like this:\nThis error occurs when there's an issue with the credential you're using and its scopes or permissions.\nTo resolve:\nFor OAuth2 credentials, make sure you've enabled the Google Drive API in APIs & Services > Library. Refer to Google OAuth2 Single Service - Enable APIs for more information.\nFor Service Account credentials:\nEnable domain-wide delegation.\nMake sure you add the Google Drive API as part of the domain-wide delegation configuration.\nHandling more than one file change\nThe Google Drive Trigger node polls Google Drive for changes at a set interval (once every minute by default).\nIf multiple changes to the Watch For criteria occur during the polling interval, a single Google Drive Trigger event occurs containing the changes as items. To handle this, your workflow must account for times when the data might contain more than one item.\nYou can use an if node or a switch node to change your workflow's behavior depending on whether the data from the Google Drive Trigger node contains a single item or multiple items."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.googledrivetrigger\\index.md",
    "content": "Google Drive Trigger node\nGoogle Drive is a file storage and synchronization service developed by Google. It allows users to store files on their servers, synchronize files across devices, and share files.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.googlesheetstrigger\\common-issues.md",
    "content": "Google Sheets Trigger node common issues\nHere are some common errors and issues with the Google Sheets Trigger node and steps to resolve or troubleshoot them.\nStuck waiting for trigger event\nWhen testing the Google Sheets Trigger node with the Execute step or Execute workflow buttons, the execution may appear stuck and unable to stop listening for events. If this occurs, you may need to exit the workflow and open it again to reset the canvas.\nStuck listening events often occur due to issues with your network configuration outside of n8n. Specifically, this behavior often occurs when you run n8n behind a reverse proxy without configuring websocket proxying.\nTo resolve this issue, check your reverse proxy configuration (Nginx, Caddy, Apache HTTP Server, Traefik, etc.) to enable websocket support.\nDate and time columns are rendering as numbers\nGoogle Sheets can render dates and times a few different ways.\nThe serial number format, popularized by Lotus 1-2-3 and used my types of spreadsheet software, represents dates as a decimal number. The whole number component (the part left of the decimal) represents the number of days since December 30, 1899. The decimal portion (the part right of the decimal) represents time as a portion of a 24-hour period (for example, .5 represents noon).\nTo use a different format for date and time values, adjust the format in your Google Sheet Trigger node. This is available when Trigger On is set to Row Added:\nOpen the Google Sheet Trigger node on your canvas.\nSelect Add option.\nSelect DateTime Render.\nChange DateTime Render to Formatted String.\nThe Google Sheets Trigger node will now format date, time, datetime, and duration fields as strings according to their number format.\nThe number format depends on the spreadsheet's locale settings. You can change the local by opening the spreadsheet and selecting File > Settings. In the General tab, set Locale to your preferred locale. Select Save settings to adjust the value."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.googlesheetstrigger\\index.md",
    "content": "Google Sheets Trigger node\nGoogle Sheets is a web-based spreadsheet program that's part of Google's office software suite within its Google Drive service.\nEvents\nRow added\nRow updated\nRow added or updated\nRelated resources\nRefer to Google Sheet's API documentation for more information about the service.\nn8n provides an app node for Google Sheets. You can find the node docs here.\nView example workflows and related content on n8n's website.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.telegramtrigger\\common-issues.md",
    "content": "Telegram Trigger node common issues\nHere are some common errors and issues with the Telegram Trigger node and steps to resolve or troubleshoot them.\nStuck waiting for trigger event\nWhen testing the Telegram Trigger node with the Execute step or Execute workflow buttons, the execution may appear stuck and unable to stop listening for events. If this occurs, you may need to exit the workflow and open it again to reset the canvas.\nStuck listening events often occur due to issues with your network configuration outside of n8n. Specifically, this behavior often occurs when you run n8n behind a reverse proxy without configuring websocket proxying.\nTo resolve this issue, check your reverse proxy configuration (Nginx, Caddy, Apache HTTP Server, Traefik, etc.) to enable websocket support.\nBad request: bad webhook: An HTTPS URL must be provided for webhook\nThis error occurs when you run n8n behind a reverse proxy and there is a problem with your instance's webhook URL.\nWhen running n8n behind a reverse proxy, you must configure the WEBHOOK_URL environment variable with the public URL where your n8n instance is running. For Telegram, this URL must use HTTPS.\nTo fix this issue, configure TLS/SSL termination in your reverse proxy. Afterward, update your WEBHOOK_URL environment variable to use the HTTPS address.\nWorkflow only works in testing or production\nTelegram only allows you to register a single webhook per app. This means that every time you switch from using the testing URL to the production URL (and vice versa), Telegram overwrites the registered webhook URL.\nYou may have trouble with this if you try to test a workflow that's also active in production. The Telegram bot will only send events to one of the two webhook URLs, so the other will never receive event notifications.\nTo work around this, you can either disable your workflow when testing or create separate Telegram bots for testing and production.\nTo create a separate telegram bot for testing, repeat the process you completed to create your first bot. Reference Telegram's bot documentation and the Telegram bot API reference for more information.\nTo disable your workflow when testing, try the following:\nGo to your workflow page.\nToggle the Active switch in the top panel to disable the workflow temporarily.\nTest your workflow using the test webhook URL.\nWhen you finish testing, toggle the Inactive toggle to enable the workflow again. The production webhook URL should resume working."
  },
  {
    "file_path": "integrations\\builtin\\trigger-nodes\\n8n-nodes-base.telegramtrigger\\index.md",
    "content": "Telegram Trigger node\nTelegram is a cloud-based instant messaging and voice over IP service. Users can send messages and exchange photos, videos, stickers, audio, and files of any type. On this page, you'll find a list of events the Telegram Trigger node can respond to and links to more resources.\nEvents\n*: All updates except \"Chat Member\", \"Message Reaction\", and \"Message Reaction Count\" (default behavior of Telegram API as they produces a lot of calls of updates).\nBusiness Connection: Trigger when the bot is connected to or disconnected from a business account, or a user edited an existing connection with the bot.\nBusiness Message: Trigger on a new message from a connected business account.\nCallback Query: Trigger on new incoming callback query.\nChannel Post: Trigger on new incoming channel post of any kind ‚Äî including text, photo, sticker, and so on.\nChat Boost: Trigger when a chat boost is added or changed. The bot must be an administrator in the chat to receive these updates.\nChat Join Request: Trigger when a request to join the chat is sent. The bot must have the can_invite_users administrator right in the chat to receive these updates.\nChat Member: Trigger when a chat member's status is updated. The bot must be an administrator in the chat.\nChosen Inline Result: Trigger when the result of an inline query chosen by a user is sent. Please see Telegram's API documentation on feedback collection for details on how to enable these updates for your bot.\nDeleted Business Messages: Trigger when messages are deleted from a connected business account.\nEdited Business Message: Trigger on new version of a message from a connected business account.\nEdited Channel Post: Trigger on new version of a channel post that is known to the bot is edited.\nEdited Message: Trigger on new version of a channel post that is known to the bot is edited.\nInline Query: Trigger on new incoming inline query.\nMessage: Trigger on new incoming message of any kind ‚Äî text, photo, sticker, and so on.\nMessage Reaction: Trigger when a reaction to a message is changed by a user. The bot must be an administrator in the chat. The update isn't received for reactions set by bots.\nMessage Reaction Count: Trigger when reactions to a message with anonymous reactions are changed. The bot must be an administrator in the chat. The updates are grouped and can be sent with delay up to a few minutes.\nMy Chat Member: Trigger when the bot's chat member status is updated in a chat. For private chats, this update is received only when the bot is blocked or unblocked by the user.\nPoll: Trigger on new poll state. Bots only receive updates about stopped polls and polls which are sent by the bot.\nPoll Answer: Trigger when user changes their answer in a non-anonymous poll. Bots only receive new votes in polls that were sent by the bot itself.\nPre-Checkout Query: Trigger on new incoming pre-checkout query. Contains full information about checkout.\nPurchased Paid Media: Trigger when a user purchases paid media with a non-empty payload sent by the bot in a non-channel chat.\nRemoved Chat Boost: Trigger when a boost is removed from a chat. The bot must be an administrator in the chat to receive these updates.\nShipping Query: Trigger on new incoming shipping query. Only for invoices with flexible price.\nSome events may require additional permissions, see Telegram's API documentation for more information.\nOptions\nDownload Images/Files: Whether to download attached images or files to include in the output data.\nImage Size: When you enable Download Images/Files, this configures the size of image to download. Downloads large images by default.\nRestrict to Chat IDs: Only trigger for events with the listed chat IDs. You can include multiple chat IDs separated by commas.\nRestrict to User IDs: Only trigger for events with the listed user IDs. You can include multiple user IDs separated by commas.\nRelated resources\nn8n provides an app node for Telegram. You can find the node docs here.\nView example workflows and related content on n8n's website.\nRefer to Telegram's API documentation for details about their API.\nCommon issues\nFor common questions or issues and suggested solutions, refer to Common issues."
  },
  {
    "file_path": "integrations\\community-nodes\\blocklist.md",
    "content": "n8n community node blocklist\nn8n maintains a blocklist of community nodes. You can't install any node on this list.\nn8n may add community nodes to the blocklist for a range of reasons, including:\nThe node is intentionally malicious\nIt's low quality (low enough to be harmful)\nIf you are a community node creator whose node is on the blocklist, and you believe this is a mistake, contact hello@n8n.io."
  },
  {
    "file_path": "integrations\\community-nodes\\risks.md",
    "content": "Risks when using community nodes\nInstalling community nodes from npm means you are installing unverified code from a public source into your n8n instance. This has some risks.\nRisks include:\nSystem security: community nodes have full access to the machine that n8n runs on, and can do anything, including malicious actions.\nData security: any community node that you use has access to data in your workflows.\nBreaking changes: node developers may introduce breaking changes in new versions of their nodes. A breaking change is an update that breaks previous functionality. Depending on the node versioning approach that a node developer chooses, upgrading to a version with a breaking change could cause all workflows using the node to break. Be careful when upgrading your nodes.\nReport bad community nodes\nYou can report bad community nodes to security@n8n.io\nDisable community nodes\nIf you are self-hosting n8n, you can disable community nodes by setting N8N_COMMUNITY_PACKAGES_ENABLED to false. On n8n cloud, visit the Cloud Admin Panel and disable community nodes from there. See troubleshooting for more information."
  },
  {
    "file_path": "integrations\\community-nodes\\troubleshooting.md",
    "content": "Troubleshooting and errors\nError: Missing packages\nn8n installs community nodes directly onto the hard disk. The files must be available at startup for n8n to load them. If the packages aren't available at startup, you get an error warning of missing packages.\nIf running n8n using Docker: depending on your Docker setup, you may lose the packages when you recreate your container or upgrade your n8n version. You must either:\nPersist the contents of the ~/.n8n/nodes directory. This is the best option. If you follow the Docker installation guide, the setup steps include persisting this directory.\nSet the N8N_REINSTALL_MISSING_PACKAGES environment variable to true.\nThe second option might increase startup time and may cause health checks to fail.\nPrevent loading community nodes on n8n cloud\nIf your n8n cloud instance crashes and fails to start, you can prevent installed community nodes from loading on instance startup. Visit the Cloud Admin Panel > Manage and toggle Disable all community nodes to true. This toggle is only visible when you allow community node installation."
  },
  {
    "file_path": "integrations\\community-nodes\\usage.md",
    "content": "Using community nodes\nTo use community nodes, you first need to install them.\nAdding community nodes to your workflow\nAfter installing a community node, you can use it like any other node. n8n displays the node in search results in the Nodes panel. n8n marks community nodes with a Package !Package icon{.off-glb} icon in the nodes panel.\nCommunity nodes with duplicate names\nIt's possible for several community nodes to have the same name. If you use two nodes with the same name in your workflow, they'll look the same, unless they have different icons."
  },
  {
    "file_path": "integrations\\community-nodes\\installation\\gui-install.md",
    "content": "Install community nodes from npm in the n8n app\nInstall a community node\nTo install a community node from npm:\nGo to Settings > Community Nodes.\nSelect Install.\nFind the node you want to install:\nSelect Browse. n8n takes you to an npm search results page, showing all npm packages tagged with the keyword n8n-community-node-package.\nBrowse the list of results. You can filter the results or add more keywords.\nOnce you find the package you want, make a note of the package name. If you want to install a specific version, make a note of the version number as well.\nReturn to n8n.\nEnter the npm package name, and version number if required. For example, consider a community node designed to access a weather API called \"Storms.\" The package name is n8n-node-storms, and it has three major versions.\nTo install the latest version of a package called n8n-node-weather: enter n8n-nodes-storms in Enter npm package name.\nTo install version 2.3: enter n8n-node-storms@2.3 in Enter npm package name.\nAgree to the risks of using community nodes: select I understand the risks of installing unverified code from a public source.\nSelect Install. n8n installs the node, and returns to the Community Nodes list in Settings.\nUninstall a community node\nTo uninstall a community node:\nGo to Settings > Community nodes.\nOn the node you want to install, select Options !Three dots options menu{.off-glb}.\nSelect Uninstall package.\nSelect Uninstall Package in the confirmation modal.\nUpgrade a community node\nUpgrade to the latest version\nYou can upgrade community nodes to the latest version from the node list in Settings > community nodes.\nWhen a new version of a community node is available, n8n displays an Update button on the node. Click the button to upgrade to the latest version.\nUpgrade to a specific version\nTo upgrade to a specific version (a version other than the latest), uninstall the node, then reinstall it, making sure to specify the target version. Follow the Installation instructions for more guidance.\nDowngrade a community node\nIf there is a problem with a particular version of a community node, you may want to roll back to a previous version.\nTo do this, uninstall the community node, then reinstall it, targeting a specific node version. Follow the Installation instructions for more guidance."
  },
  {
    "file_path": "integrations\\community-nodes\\installation\\index.md",
    "content": "Install and manage community nodes\nThere are three ways to install community nodes:\nWithin n8n using the nodes panel (for verified community nodes only).\nWithin n8n using the GUI: Use this method to install community nodes from the npm registry.\nManually from the command line: use this method to install community nodes from npm if your n8n instance doesn't support installation through the in-app GUI."
  },
  {
    "file_path": "integrations\\community-nodes\\installation\\manual-install.md",
    "content": "Manually install community nodes from npm\nYou can manually install community nodes from the npm registry on self-hosted n8n.\nYou need to manually install community nodes in the following circumstances:\nYour n8n instance runs in queue mode.\nYou want to install private packages.\nInstall a community node\nAccess your Docker shell:\nCreate ~/.n8n/nodes if it doesn't already exist, and navigate into it:\nInstall the node:\nThen restart n8n.\nUninstall a community node\nAccess your Docker shell:\nRun npm uninstall:\nUpgrade a community node\nUpgrade to the latest version\nAccess your Docker shell:\nRun npm update:\nUpgrade or downgrade to a specific version\nAccess your Docker shell:\nRun npm uninstall to remove the current version:\nRun npm install with the version specified:"
  },
  {
    "file_path": "integrations\\community-nodes\\installation\\verified-install.md",
    "content": "Install verified community nodes in the n8n app\nInstall a community node\nTo install a verified community node:\nGo to the Canvas and open the nodes panel (either by selecting '+' or pressing ++tab++).\nSearch for the node that you're looking for. If there is a matching verified community node, you will see a More from the community section at the bottom of the nodes panel.\nSelect the node you want to install. This takes you to a detailed view of the node, showing all the supported actions.\nSelect install. This will install the node for your instance and enable all members to use it in their workflows.\nYou can now add the node to your workflows.\nUninstall a community node\nTo uninstall a community node:\nGo to Settings > Community nodes.\nOn the node you want to install, select Options !Three dots options menu{.off-glb}.\nSelect Uninstall package.\nSelect Uninstall Package in the confirmation modal."
  },
  {
    "file_path": "integrations\\creating-nodes\\overview.md",
    "content": "Creating nodes\nLearn how to build your own custom nodes.\nThis section includes:\nGuidance on planning your build, including which style to use.\nTutorials for different node building styles.\nInstructions for testing your node, including how to use the n8n node linter and troubleshooting support.\nHow to share your node with the community, submit it for verification by n8n, or use it as a private node.\nReference material, including UI elements and information on the individual files that make up a node.\nPrerequisites\nThis section assumes the following:\nSome familiarity with JavaScript and TypeScript.\nAbility to manage your own development environment, including git.\nKnowledge of npm, including creating and submitting packages.\nFamiliarity with n8n, including a good understanding of data structures and item linking."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\declarative-style-node.md",
    "content": "Build a declarative-style node\nThis tutorial walks through building a declarative-style node. Before you begin, make sure this is the node style you need. Refer to Choose your node building approach for more information.\nPrerequisites\nYou need the following installed on your development machine:\nYou need some understanding of:\nJavaScript/TypeScript\nREST APIs\ngit\nBuild your node\nIn this section, you'll clone n8n's node starter repository, and build a node that integrates the NASA API. You'll create a node that uses two of NASA's services: APOD (Astronomy Picture of the Day) and Mars Rover Photos. To keep the code examples short, the node won't implement every available option for the Mars Rover Photos endpoint.\nStep 1: Set up the project\nn8n provides a starter repository for node development. Using the starter ensures you have all necessary dependencies. It also provides a linter.\nClone the repository and navigate into the directory:\nGenerate a new repository from the template repository.\nClone your new repository:\nThe starter contains example nodes and credentials. Delete the following directories and files:\nnodes/ExampleNode\nnodes/HTTPBin\ncredentials/ExampleCredentials.credentials.ts\ncredentials/HttpBinApi.credentials.ts\nNow create the following directories and files:\nnodes/NasaPics\nnodes/NasaPics/NasaPics.node.json\nnodes/NasaPics/NasaPics.node.ts\ncredentials/NasaPicsApi.credentials.ts\nThese are the key files required for any node. Refer to Node file structure for more information on required files and recommended organization.\nNow install the project dependencies:\nStep 2: Add an icon\nSave the NASA SVG logo from here as nasapics.svg in nodes/NasaPics/.\nStep 3: Create the node\nEvery node must have a base file. Refer to Node base file for detailed information about base file parameters.\nIn this example, the file is NasaPics.node.ts. To keep this tutorial short, you'll place all the node functionality in this one file. When building more complex nodes, you should consider splitting out your functionality into modules. Refer to Node file structure for more information.\nStep 3.1: Imports\nStart by adding the import statements:\nStep 3.2: Create the main class\nThe node must export an interface that implements INodeType. This interface must include a description interface, which in turn contains the properties array.\nStep 3.3: Add node details\nAll nodes need some basic parameters, such as their display name, icon, and the basic information for making a request using the node. Add the following to the description:\nn8n uses some of the properties set in description to render the node in the Editor UI. These properties are displayName, icon, description, and subtitle.\nStep 3.4: Add resources\nThe resource object defines the API resource that the node uses. In this tutorial, you're creating a node to access two of NASA's API endpoints: planetary/apod and mars-photos. This means you need to define two resource options in NasaPics.node.ts. Update the properties array with the resource object:\ntype controls which UI element n8n displays for the resource, and tells n8n what type of data to expect from the user. options results in n8n adding a dropdown that allows users to choose one option. Refer to Node UI elements for more information.\nStep 3.5: Add operations\nThe operations object defines the available operations on a resource.\nIn a declarative-style node, the operations object includes routing (within the options array). This sets up the details of the API call.\nAdd the following to the properties array, after the resource object:\nThis code creates two operations: one to get today's APOD image, and another to send a get request for photos from one of the Mars Rovers. The object named roverName requires the user to choose which Rover they want photos from. The routing object in the Mars Rover operation references this to create the URL for the API call.\nStep 3.6: Optional fields\nMost APIs, including the NASA API that you're using in this example, have optional fields you can use to refine your query.\nTo avoid overwhelming users, n8n displays these under Additional Fields in the UI.\nFor this tutorial, you'll add one additional field, to allow users to pick a date to use with the APOD endpoint. Add the following to the properties array:\nStep 4: Set up authentication\nThe NASA API requires users to authenticate with an API key.\nAdd the following to nasaPicsApi.credentials.ts:\nFor more information about credentials files and options, refer to Credentials file.\nStep 5: Add node metadata\nMetadata about your node goes in the JSON file at the root of your node. n8n refers to this as the codex file. In this example, the file is NasaPics.node.json.\nAdd the following code to the JSON file:\nFor more information on these parameters, refer to Node codex files.\nStep 6: Update the npm package details\nYour npm package details are in the package.json at the root of the project. It's essential to include the n8n object with links to the credentials and base node file. Update this file to include the following information:\nYou need to update the package.json to include your own information, such as your name and repository URL. For more information on npm package.json files, refer to npm's package.json documentation.\nTest your node\nNext steps\nDeploy your node.\nView an example of a declarative node: n8n's Brevo node. Note that the main node is declarative, while the trigger node is in programmatic style.\nLearn about node versioning."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\index.md",
    "content": "Build a node\nThis section provides tutorials on building nodes. It covers:\nTutorial: Build a declarative-style node\nReference material on file structure, parameter definitions for base, codex, and credentials files, node UI elements, and more.\nComing soon:\nMore tutorials\nRevised guidance on standards"
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\node-development-environment.md",
    "content": "Set up your development environment\nThis document lists the essential dependencies for developing a node, as well as guidance on setting up your editor.\nRequirements\nTo build and test a node, you need:\nNode.js and npm. Minimum version Node 18.17.0. You can find instructions on how to install both using nvm (Node Version Manager) for Linux, Mac, and WSL (Windows Subsystem for Linux) here. For Windows users, refer to Microsoft's guide to Install NodeJS on Windows.\nA local instance of n8n. You can install n8n with npm install n8n -g, then follow the steps in Run your node locally to test your node.\nYou should also have git installed. This allows you to clone and use the n8n-node-starter.\nEditor setup\nn8n recommends using VS Code as your editor.\nInstall these extensions:\nESLint\nEditorConfig\nPrettier\nBy using VS Code and these extensions, you get access to the n8n node linter's warnings as you code."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\programmatic-style-node.md",
    "content": "Build a programmatic-style node\nThis tutorial walks through building a programmatic-style node. Before you begin, make sure this is the node style you need. Refer to Choose your node building approach for more information.\nPrerequisites\nYou need the following installed on your development machine:\nYou need some understanding of:\nJavaScript/TypeScript\nREST APIs\ngit\nExpressions in n8n\nBuild your node\nIn this section, you'll clone n8n's node starter repository, and build a node that integrates the SendGrid. You'll create a node that implements one piece of SendGrid functionality: create a contact.\nStep 1: Set up the project\nn8n provides a starter repository for node development. Using the starter ensures you have all necessary dependencies. It also provides a linter.\nClone the repository and navigate into the directory:\nGenerate a new repository from the template repository.\nClone your new repository:\nThe starter contains example nodes and credentials. Delete the following directories and files:\nnodes/ExampleNode\nnodes/HTTPBin\ncredentials/ExampleCredentials.credentials.ts\ncredentials/HttpBinApi.credentials.ts\nNow create the following directories and files:\nnodes/FriendGrid\nnodes/FriendGrid/FriendGrid.node.json\nnodes/FriendGrid/FriendGrid.node.ts\ncredentials/FriendGridApi.credentials.ts\nThese are the key files required for any node. Refer to Node file structure for more information on required files and recommended organization.\nNow install the project dependencies:\nStep 2: Add an icon\nSave the SendGrid SVG logo from here as friendGrid.svg in nodes/FriendGrid/.\nStep 3: Define the node in the base file\nEvery node must have a base file. Refer to Node base file for detailed information about base file parameters.\nIn this example, the file is FriendGrid.node.ts. To keep this tutorial short, you'll place all the node functionality in this one file. When building more complex nodes, you should consider splitting out your functionality into modules. Refer to Node file structure for more information.\nStep 3.1: Imports\nStart by adding the import statements:\nStep 3.2: Create the main class\nThe node must export an interface that implements INodeType. This interface must include a description interface, which in turn contains the properties array.\nStep 3.3: Add node details\nAll programmatic nodes need some basic parameters, such as their display name and icon. Add the following to the description:\nn8n uses some of the properties set in description to render the node in the Editor UI. These properties are displayName, icon, and description.\nStep 3.4: Add the resource\nThe resource object defines the API resource that the node uses. In this tutorial, you're creating a node to access one of SendGrid's API endpoints: /v3/marketing/contacts. This means you need to define a resource for this endpoint. Update the properties array with the resource object:\ntype controls which UI element n8n displays for the resource, and tells n8n what type of data to expect from the user. options results in n8n adding a dropdown that allows users to choose one option. Refer to Node UI elements for more information.\nStep 3.5: Add operations\nThe operations object defines what you can do with a resource. It usually relates to REST API verbs (GET, POST, and so on). In this tutorial, there's one operation: create a contact. It has one required field, the email address for the contact the user creates.\nAdd the following to the properties array, after the resource object:\nStep 3.6: Add optional fields\nMost APIs, including the SendGrid API that you're using in this example, have optional fields you can use to refine your query.\nTo avoid overwhelming users, n8n displays these under Additional Fields in the UI.\nFor this tutorial, you'll add two additional fields, to allow users to enter the contact's first name and last name. Add the following to the properties array:\nStep 4: Add the execute method\nYou've set up the node UI and basic information. It's time to map the node UI to API requests, and make the node actually do something.\nThe execute method runs every time the node runs. In this method, you have access to the input items and to the parameters that the user set in the UI, including the credentials.\nAdd the following the execute method in the FriendGrid.node.ts:\nNote the following lines of this code:\nUsers can provide data in two ways:\nEntered directly in the node fields\nBy mapping data from earlier nodes in the workflow\ngetInputData(), and the subsequent loop, allows the node to handle situations where data comes from a previous node. This includes supporting multiple inputs. This means that if, for example, the previous node outputs contact information for five people, your FriendGrid node can create five contacts.\nStep 5: Set up authentication\nThe SendGrid API requires users to authenticate with an API key.\nAdd the following to FriendGridApi.credentials.ts\nFor more information about credentials files and options, refer to Credentials file.\nStep 6: Add node metadata\nMetadata about your node goes in the JSON file at the root of your node. n8n refers to this as the codex file. In this example, the file is FriendGrid.node.json.\nAdd the following code to the JSON file:\nFor more information on these parameters, refer to Node codex files.\nStep 7: Update the npm package details\nYour npm package details are in the package.json at the root of the project. It's essential to include the n8n object with links to the credentials and base node file. Update this file to include the following information:\nYou need to update the package.json to include your own information, such as your name and repository URL. For more information on npm package.json files, refer to npm's package.json documentation.\nTest your node\nNext steps\nDeploy your node.\nView an example of a programmatic node: n8n's Mattermost node. This is an example of a more complex programmatic node structure.\nLearn about node versioning.\nMake sure you understand key concepts: item linking and data structures."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\code-standards.md",
    "content": "Code standards\nFollowing defined code standards when building your node makes your code more readable and maintainable, and helps avoid errors. This document provides guidance on good code practices for node building. It focuses on code details. For UI standards and UX guidance, refer to Node UI design.\nUse the linter\nThe n8n node linter provides automatic checking for many of the node-building standards. You should ensure your node passes the linter's checks before publishing it. Refer to the n8n node linter documentation for more information.\nUse the starter\nThe n8n node starter project includes a recommended setup, dependencies (including the linter), and examples to help you get started. Begin new projects with the starter.\nWrite in TypeScript\nAll n8n code is TypeScript. Writing your nodes in TypeScript can speed up development and reduce bugs.\nDetailed guidelines for writing a node\nThese guidelines apply to any node you build.\nResources and operations\nIf your node can perform several operations, call the parameter that sets the operation Operation. If your node can do these operations on more than one resource, create a Resource parameter. The following code sample shows a basic resource and operations setup:\nReuse internal parameter names\nAll resource and operation fields in an n8n node have two settings: a display name, set using the name parameter, and an internal name, set using the value parameter. Reusing the internal name for fields allows n8n to preserve user-entered data if a user switches operations.\nFor example: you're building a node with a resource named 'Order'. This resource has several operations, including Get, Edit, and Delete. Each of these operations uses an order ID to perform the operation on the specified order. You need to display an ID field for the user. This field has a display label, and an internal name. By using the same internal name (set in value) for the operation ID field on each resource, a user can enter the ID with the Get operation selected, and not lose it if they switch to Edit.\nWhen reusing the internal name, you must ensure that only one field is visible to the user at a time. You can control this using displayOptions.\nDetailed guidelines for writing a programmatic-style node\nThese guidelines apply when building nodes using the programmatic node-building style. They aren't relevant when using the declarative style. For more information on different node-building styles, refer to Choose your node building approach.\nDon't change incoming data\nNever change the incoming data a node receives (data accessible with this.getInputData()) as all nodes share it. If you need to add, change, or delete data, clone the incoming data and return the new data. If you don't do this, sibling nodes that execute after the current one will operate on the altered data and process incorrect data.\nIt's not necessary to always clone all the data. For example, if a node changes the binary data but not the JSON data, you can create a new item that reuses the reference to the JSON item.\nUse the built in request library\nSome third-party services have their own libraries on npm, which make it easier to create an integration. The problem with these packages is that you add another dependency (plus all the dependencies of the dependencies). This adds more and more code, which has to be loaded, can introduce security vulnerabilities, bugs, and so on. Instead, use the built-in module:\nThis uses the npm package Axios.\nRefer to HTTP helpers for more information, and for migration instructions for the removed this.helpers.request."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\credentials-files.md",
    "content": "Credentials file\nThe credentials file defines the authorization methods for the node. The settings in this file affect what n8n displays in the Credentials modal, and must reflect the authentication requirements of the service you're connecting to.\nIn the credentials file, you can use all the n8n UI elements. n8n encrypts the data that's stored using credentials using an encryption key.\nStructure of the credentials file\nThe credentials file follows this basic structure:\nImport statements\nCreate a class for the credentials\nWithin the class, define the properties that control authentication for the node.\nOutline structure\nParameters\nname\nString. The internal name of the object. Used to reference it from other places in the node.\ndisplayName\nString. The name n8n uses in the GUI.\ndocumentationUrl\nString. URL to your credentials documentation.\nproperties\nEach object contains:\ndisplayName: the name n8n uses in the GUI.\nname: the internal name of the object. Used to reference it from other places in the node.\ntype: the data type expected, such as string.\ndefault: the URL that n8n should use to test credentials.\nauthenticate\nauthenticate: Object. Contains objects that tell n8n how to inject the authentication data as part of the API request.\ntype\nString. If you're using an authentication method that sends data in the header, body, or query string, set this to 'generic'.\nproperties\nObject. Defines the authentication methods. Options are:\nbody: Object. Sends authentication data in the request body. Can contain nested objects.\nheader: Object. Send authentication data in the request header.\nqs: Object. Stands for \"query string.\" Send authentication data in the request query string.\nauth: Object. Used for Basic Auth. Requires username and password as the key names.\ntest\nProvide a request object containing a URL and authentication type that n8n can use to test the credential."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\error-handling.md",
    "content": "Error handling in n8n nodes\nProper error handling is crucial for creating robust n8n nodes that provide clear feedback to users when things go wrong. n8n provides two specialized error classes to handle different types of failures in node implementations:\nNodeApiError: For API-related errors and external service failures\nNodeOperationError: For operational errors, validation failures, and configuration issues\nNodeApiError\nUse NodeApiError when dealing with external API calls and HTTP requests. This error class is specifically designed to handle API response errors and provides enhanced features for parsing and presenting API-related failures such as:\nHTTP request failures\nexternal API errors\nauthentication/authorization failures\nrate limiting errors\nservice unavailable errors\nInitialize new NodeApiError instances using the following pattern:\nCommon usage patterns\nFor basic API request failures, catch the error and wrap it in NodeApiError:\nHandle specific HTTP status codes with custom messages:\nNodeOperationError\nUse NodeOperationError for:\noperational errors\nvalidation failures\nconfiguration issues that aren't related to external API calls\ninput validation errors\nmissing required parameters\ndata transformation errors\nworkflow logic errors\nInitialize new NodeOperationError instances using the following pattern:\nCommon usage patterns\nUse NodeOperationError for validating user inputs:\nWhen processing multiple items, include the item index for better error context:"
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\http-helpers.md",
    "content": "HTTP request helper for node builders\nn8n provides a flexible helper for making HTTP requests, which abstracts away most of the complexity.\nUsage\nCall the helper inside the execute function.\noptions is an object:\nurl is required. The other fields are optional. The default method is GET.\nSome notes about the possible fields:\nbody: you can use a regular JavaScript object for JSON payload, a buffer for file uploads, an instance of FormData for multipart/form-data, and URLSearchParams for application/x-www-form-urlencoded.\nheaders: a key-value pair.\nIf body is an instance of FormData then n8n adds content-type: multipart/form-data automatically.\nIf body is an instance of URLSearchParams, then n8n adds content-type: application/x-www-form-urlencoded.\nTo override this behavior, set a content-type header.\narrayFormat: if your query string contains an array of data, such as const qs = {IDs: [15,17]}, the value of arrayFormat defines how n8n formats it.\nindices (default):  { a: ['b', 'c'] } as a[0]=b&a[1]=c\nbrackets: { a: ['b', 'c'] } as a[]=b&a[]=c\nrepeat: { a: ['b', 'c'] } as a=b&a=c\ncomma: { a: ['b', 'c'] } as a=b,c\nauth: Used for Basic auth. Provide username and password. n8n recommends omitting this, and using helpers.httpRequestWithAuthentication(...) instead.\ndisableFollowRedirect: By default, n8n follows redirects. You can set this to true to prevent this from happening.\nskipSslCertificateValidation: Used for calling HTTPS services without proper certificate\nreturnFullResponse: Instead of returning just the body, returns an object with more data in the following format: {body: body, headers: object, statusCode: 200, statusMessage: 'OK'}\nencoding: n8n can detect the content type, but you can specify arrayBuffer to receive a Buffer you can read from and interact with.\nExample\nFor an example, refer to the Mattermost node.\nDeprecation of the previous helper\nThe previous helper implementation using this.helpers.request(options) used and exposed the request-promise library. This was removed in version 1.\nTo minimize incompatibility, n8n made a transparent conversion to another library called Axios.\nIf you are having issues, please report them in the Community Forums or on GitHub.\nMigration guide to the new helper\nThe new helper is much more robust, library agnostic, and easier to use.\nNew nodes should all use the new helper. You should strongly consider migrating existing custom nodes to the new helper. These are the main considerations when migrating:\nAccepts url. Doesn't accept uri.\nencoding: null now must be encoding: arrayBuffer.\nrejectUnauthorized: false is now skipSslCertificateValidation: true\nUse body according to content-type headers to clarify the payload.\nresolveWithFullResponse is now returnFullResponse and has similar behavior"
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\index.md",
    "content": "Node building reference\nThis section contains reference information, including details about:\nNode UI elements\nOrganizing your node files\nKey parameters in your node's base file and credentials file.\nUX guidelines and verification guidelines for submitting your node for verification by n8n."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-codex-files.md",
    "content": "Node codex files\nThe codex file contains metadata about your node. This file is the JSON file at the root of your node. For example, the HttpBin.node.json file in the n8n starter.\nThe codex filename must match the node base filename. For example, given a node base file named MyNode.node.ts, the codex would be named MyNode.node.json.\nTABLE_PLACEHOLDER_0\nNode categories\nYou can define one or more categories in your node configuration JSON. This helps n8n put the node in the correct category in the nodes panel.\nChoose from these categories:\nData & Storage\nFinance & Accounting\nMarketing & Content\nProductivity\nMiscellaneous\nSales\nDevelopment\nAnalytics\nCommunication\nUtility\nYou must match the syntax. For example, Data & Storage not data and storage."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-file-structure.md",
    "content": "Node file structure\nFollowing best practices and standards in your node structure makes your node easier to maintain. It's helpful if other people need to work with the code.\nThe file and directory structure of your node depends on:\nYour node's complexity.\nWhether you use node versioning.\nHow many nodes you include in the npm package.\nRequired files and directories\nYour node must include:\nA package.json file at the root of the project. This is required for any npm module.\nA nodes directory, containing the code for your node:\nThis directory must contain the base file, in the format .node.ts. For example, MyNode.node.ts.\nn8n recommends including a codex file, containing metadata for your node. The codex filename must match the node base filename. For example, given a node base file named MyNode.node.ts, the codex name is MyNode.node.json.\nThe nodes directory can contain other files and subdirectories, including directories for versions, and node code split across more than one file to create a modular structure.\nA credentials directory, containing your credentials code. This code lives in a single credentials file. The filename format is .credentials.ts. For example, MyNode.credentials.ts.\nModular structure\nYou can choose whether to place all your node's functionality in one file, or split it out into a base file and other modules, which the base file then imports. Unless your node is very simple, it's a best practice to split it out.\nA basic pattern is to separate out operations. Refer to the HttpBin starter node for an example of this.\nFor more complex nodes, n8n recommends a directory structure. Refer to the Airtable node or Microsoft Outlook node as examples.\nactions: a directory containing sub-directories that represent resources.\nEach sub-directory should contain two types of files:\nAn index file with resource description (named either .resource.ts or index.ts)\nFiles for operations .operation.ts. These files should have two exports: description of the operation and an execute function.\nmethods: an optional directory dynamic parameters' functions.\ntransport: a directory containing the communication implementation.\nVersioning\nIf your node has more than one version, and you're using full versioning, this makes the file structure more complex. You need a directory for each version, along with a base file that sets the default version. Refer to Node versioning for more information on working with versions, including types of versioning.\nDecide how many nodes to include in a package\nThere are two possible setups when building a node:\nOne node in one npm package.\nMore than one node in a single npm package.\nn8n supports both approaches. If you include more than one node, each node should have its own directory in the nodes directory.\nA best-practice example for programmatic nodes\nn8n's built-in Airtable node implements a modular structure and versioning, following recommended patterns."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-versioning.md",
    "content": "Node versioning\nn8n supports node versioning. You can make changes to existing nodes without breaking the existing behavior by introducing a new version.\nBe aware of how n8n decides which node version to load:\nIf a user builds and saves a workflow using version 1, n8n continues to use version 1 in that workflow, even if you create and publish a version 2 of the node.\nWhen a user creates a new workflow and browses for nodes, n8n always loads the latest version of the node.\nLight versioning\nThis is available for all node types.\nOne node can contain more than one version, allowing small version increments without code duplication. To use this feature:\nChange the main version parameter to an array, and add your version numbers, including your existing version.\nYou can then access the version parameter with @version in your displayOptions in any object (to control which versions n8n displays the object with). You can also query the version from a function using const nodeVersion = this.getNode().typeVersion;.\nAs an example, say you want to add versioning to the NasaPics node from the Declarative node tutorial, then configure a resource so that n8n only displays it in version 2 of the node. In your base NasaPics.node.ts file:\nFull versioning\nThis isn't available for declarative-style nodes.\nAs an example, refer to the Mattermost node.\nFull versioning summary:\nThe base node file should extend NodeVersionedType instead of INodeType.\nThe base node file should contain a description including the defaultVersion (usually the latest), other basic node metadata such as name, and a list of versions. It shouldn't contain any node functionality.\nn8n recommends using v1, v2, and so on, for version folder names."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\ui-elements.md",
    "content": "Node user interface elements\nn8n provides a set of predefined UI components (based on a JSON file) that allows users to input all sorts of data types. The following UI elements are available in n8n.\nString\nBasic configuration:\n!String\nString field for inputting passwords:\n!Password\nString field with more than one row:\n!Multiple rows\nSupport drag and drop for data keys\nUsers can drag and drop data values to map them to fields. Dragging and dropping creates an expression to load the data value. n8n supports this automatically.\nYou need to add an extra configuration option to support dragging and dropping data keys:\nrequiresDataPath: 'single': for fields that require a single string.\nrequiresDataPath: 'multiple': for fields that can accept a comma-separated list of string.\nThe Compare Datasets node code has examples.\nNumber\nNumber field with decimal points:\n!Decimal\nCollection\nUse the collection type when you need to display optional fields.\n!Collection\nDateTime\nThe dateTime type provides a date picker.\n!DateTime\nBoolean\nThe boolean type adds a toggle for entering true or false.\n!Boolean\nColor\nThe color type provides a color selector.\n!Color\nOptions\nThe options type adds an options list. Users can select a single value.\n!Options\nMulti-options\nThe multiOptions type adds an options list. Users can select more than one value.\n!Multi-options\nFilter\nUse this component to evaluate, match, or filter incoming data.\nThis is the code from n8n's own If node. It shows a filter component working with a collection component where users can configure the filter's behavior.\n!Filter\nAssignment collection (drag and drop)\nUse the drag and drop component when you want users to pre-fill name and value parameters with a single drag interaction.\nYou can see an example in n8n's Edit Fields (Set) node:\n!A gif showing the drag and drop action, as well as changing a field to fixed\nFixed collection\nUse the fixedCollection type to group fields that are semantically related.\n!Fixed collection\nResource locator\n!Resource locator\nThe resource locator element helps users find a specific resource in an external service, such as a card or label in Trello.\nThe following options are available:\nID\nURL\nList: allows users to select or search from a prepopulated list. This option requires more coding, as you must populate the list, and handle searching if you choose to support it.\nYou can choose which types to include.\nExample:\nRefer to the following for live examples:\nRefer to CardDescription.ts and Trello.node.ts  in n8n's Trello node for an example of a list with search that includes searchFilterRequired: true.\nRefer to GoogleDrive.node.ts for an example where users can browse the list or search.\nResource mapper\nIf your node performs insert, update, or upsert operations, you need to send data from the node in a format supported by the service you're integrating with. A common pattern is to use a Set node before the node that sends data, to convert the data to match the schema of the service you're connecting to. The resource mapper UI component provides a way to get data into the required format directly within the node, rather than using a Set node. The resource mapper component can also validate input data against the schema provided in the node, and cast input data into the expected type.\nRefer to the Postgres node (version 2) for a live example using a database schema.\nRefer to the Google Sheets node (version 2) for a live example using a schema-less service.\nResource mapper type options interface\nThe typeOptions section must implement the following interface:\nResource mapper method\nThis method contains your node-specific logic for fetching the data schema. Every node must implement its own logic for fetching the schema, and setting up each UI field according to the schema.\nIt must return a value that implements the ResourceMapperFields interface:\nRefer to the Postgres resource mapping method and Google Sheets resource mapping method for live examples.\nJSON\n!JSON\nHTML\nThe HTML editor allows users to create HTML templates in their workflows. The editor supports standard HTML, CSS in"
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\ux-guidelines.md",
    "content": "UX guidelines for community nodes\nYour node's UI must conform to these guidelines to be a verified community node candidate.\nCredentials\nAPI key and sensitive credentials should always be password fields.\nOAuth\nAlways include the OAuth credential if available.\nNode structure\nOperations to include\nTry to include CRUD operations for each resource type.\nTry to include common operations in nodes for each resource. n8n uses some CRUD operations to keep the experience consistent and allow users to perform basic operations on the resource. The suggested operations are:\nCreate\nCreate or Update (Upsert)\nDelete\nGet\nGet Many: also used when some filtering or search is available\nUpdate\nNotes:\nThese operations can apply to the resource itself or an entity inside of the resource (for example, a row inside a Google Sheet). When operating on an entity inside of the resource, you must specify the name of the entity in the operations name.\nThe naming could change depending on the node and the resource. Check the following guidelines for details.\nResource Locator\nUse a Resource Locator component whenever possible. This provides a much better UX for users. The Resource Locator Component is most often useful when you have to select a single item.\nThe default option for the Resource Locator Component should be From list (if available).\nConsistency with other nodes\nMaintain UX consistency: n8n tries to keep its UX consistent. This means following existing UX patterns, in particular, those used in the latest new or overhauled nodes.\nCheck similar nodes: For example, if you're working on a database node, it's worth checking the Postgres node.\nSorting options\nYou can enhance certain \"Get Many\" operations by providing users with sorting options.\nAdd sorting in a dedicated collection (below the \"Options\" collection). Follow the example of Airtable Record:Search.\nNode functionality\nDeleting operations output\nWhen deleting an item (like a record or a row), return an array with a single object: {\"deleted\": true}. This is a confirmation for the user that the deletion was successful and the item will trigger the following node.\nSimplifying output fields\nNormal nodes: 'Simplify' parameter\nWhen an endpoint returns data with more than 10 fields, add the \"Simplify\" boolean parameter to return a simplified version of the output with max 10 fields.\nOne of the main issues with n8n can be the size of data and the Simplify parameter limits that problem by reducing data size.\nSelect the most useful fields to output in the simplified node and sort them to have the most used ones at the top.\nIn the Simplify mode, it's often best to flatten nested fields\nDisplay Name: Simplify\nDescription: Whether to return a simplified version of the response instead of the raw data\nAI tool nodes: ‚ÄòOutput‚Äô parameter\nWhen an endpoint returns data with more than 10 fields, add the 'Output' option parameter with 3 modes.\nIn AI tool nodes, allow the user to be more granular and select the fields to output. The rationale is that tools may run out of context window and they can get confused by too many fields, so it's better to pass only the ones they need.\nOptions:\nSimplified: Works the same as the \"Simplify\" parameter described above.\nRaw: Returns all the available fields.\nSelected fields: Shows a multi-option parameter for selecting the fields to add to the output and send to the AI agent. By default, this option always returns the ID of the record/entity.\nCopy\nText Case\nUse Title Case for the node name, parameters display names (labels), dropdown titles. Title Case is when you capitalize the first letter of each word, except for certain small words, such as articles and short prepositions.\nUse Sentence case for node action names, node descriptions, parameters descriptions (tooltips), hints, dropdown descriptions.\nTerminology\nUse the third-party service terminology: Try to use the same terminology as the service you're interfacing with (for example, Notion 'blocks', not Notion 'paragraphs').\nUse the terminology used in the UI: Stick to the terminology used in the user interface of the service, rather than that used in the APIs or technical documentation (for example, in Trello you \"archive\" cards, but in the API they show up as \"closed\". In this case, you might want to use \"archive\").\nNo tech jargon: Don't use technical jargon where simple words will do. For example, use \"field\" instead of \"key\".\nConsistent naming: Choose one term for something and stick to it. For example, don't mix \"directory\" and \"folder\".\nPlaceholders\nIt's often helpful to insert examples of content in parameters placeholders. These should start with \"e.g.\" and use camel case for the demo content in fields.\nPlaceholder examples to copy:\nimage: e.g.\nvideo: e.g.\nsearch term: e.g. automation\nemail: e.g. nathan@example.com\nTwitter user (or similar): e.g. n8n\nName and last name: e.g. Nathan Smith\nFirst name: e.g. Nathan\nLast name: e.g. Smith\nOperations name, action, and description\nName: This is the name displayed in the select when the node is open on the canvas. It must use title case and doesn't have to include the resource (for example, \"Delete\").\nAction: This is the name of the operation displayed in the panel where the user selects the node. It must be in sentence case and must include the resource (for example, \"Delete record\").\nDescription: This is the sub-text displayed below the name in the select when the node is open on the canvas. It must use sentence case and must include the resource. It can add a bit of information and use alternative words than the basic resource/operation (for example, \"Retrieve a list of users\").\nIf the operation acts on an entity that's not the Resource (for example, a row in a Google Sheet), specify that in the operation name (for example, \"Delete Row\").\nAs a general rule, is important to understand what the object of an operation is. Sometimes, the object of an Operation is the resource itself (for example, Sheet:Delete to delete a Sheet).\nIn other cases, the object of the operation isn't the resource, but something contained inside the resource (for example, Table:Delete rows, here the resource is the table, but what you are operating on are the rows inside of it).\nNaming name\nThis is the name displayed in the select when the node is open on the canvas.\nParameter: name\nCase: Title Case\nNaming guidelines:\nDon't repeat the resource (if the resource selection is above): The resource is often displayed above the operation, so it's not necessary to repeat it in the operation (this is the case if the object of the operation is the resource itself).\nFor example: Sheet:Delete ‚Üí No need to repeat Sheet in Delete, because n8n displays Sheet in the field above and what you're deleting is the Sheet.\nSpecify the resource if there's no resource selection above: In some nodes, you won't have a resource selection (because there's only one resource). In these cases, specify the resource in the operation.\nFor example: Delete Records ‚Üí In Airtable, there's no resource selection, so it's better to specify that the Delete operation will delete records.\nSpecify the object of the operation if it's not the resource: Sometimes, the object of the operation isn't the resource. In these cases, specify the object in the operation as well.\nFor example: Table:Get Columns ‚Üí Specify Columns because the resource is Table, while the object of the operation is Columns.\nNaming action\nThis is the name of the operation displayed in the panel where the user selects the node.\n* Parameter: action\n* Case: Sentence case\nNaming guidelines:\nOmit articles: To keep the text shorter, get rid of articles (a, an, the‚Ä¶).\ncorrect: Update row in sheet\nincorrect: Update a row in a sheet\nRepeat the resource: In this case, it's okay to repeat the resource. Even if the resource is visible in the list, the user might not notice and it's useful to repeat it in the operation label.\nSpecify the object of the operation if it is not the resource: Same as for the operation name. In this case, you don't need to repeat the resource.\nFor example: Append Rows ‚Üí You have to specify Rows because rows are what you're actually appending to. Don't add the resource (Sheet) since you aren't appending to the resource.\nNaming description\nThis is the subtext displayed below the name in the selection when the node is open on the canvas.\nParameter: description\nCase: Sentence case\nNaming guidelines:\nIf possible, add more information than that specified in the operation name\nUse alternative wording to help users better understand what the operation is doing. Some people might not understand the text used in the operation (maybe English isn't their native language), and using alternative working could help them.\nVocabulary\nn8n uses a general vocabulary and some context-specific vocabulary for groups of similar applications (for example, databases or spreadsheets).\nThe general vocabulary takes inspiration from CRUD operations:\nClear\nDelete all the contents of the resource (empty the resource).\nDescription: Delete all the s inside the\nCreate\nCreate a new instance of the resource.\nDescription: Create a new\nCreate or Update\nCreate or update an existing instance of the resource.\nDescription: Create a new  or update an existing one (upsert)\nDelete\nYou can use \"Delete\" in two different ways:\nDelete a resource:\nDescription: Delete a  permanently (use \"permanently\" only if that's the case)\nDelete something inside of the resource (for example, a row):\nIn this case, always specify the object of the operation: for example, Delete Rows or Delete Records.\nDescription: Delete a  permanently\nGet\nYou can use \"Get\" in two different ways:\nGet a resource:\nDescription: Retrieve a\nGet an item inside of the resource (for example, records):\nIn this case, always specify the object of the operation: for example, Get Row or Get Record.\nDescription: Retrieve a  from the/a\nGet Many\nYou can use \"Get Many\" in two different ways:\nGet a list of resources (without filtering):\nDescription: Retrieve a list of s\nGet a list of items inside of the resource (for example, records):\nIn this case, always specify the object of the operation: for example, Get Many Rows or Get Many Records.\nYou can omit Many: Get Many Rows can be Get Rows.\nDescription: List all s in the/a\nInsert or Append\nAdd something inside of a resource.\nUse insert for database nodes.\nDescription: Insert (s) in a\nInsert or Update or Append or Update\nAdd or update something inside of a resource.\nUse insert for database nodes.\nDescription: Insert (s) or update an existing one(s) (upsert)\nUpdate\nYou can use \"Update\" in two different ways:\nUpdate a resource:\nDescription: Update one or more s\nUpdate something inside of a resource (for example, a row):\nIn this case, always specify the object of the operation: for example, Update Rows or Update Records.\nDescription: Update (s) inside a\nReferring to parameter and field name\nWhen you need to refer to parameter names or field names in copy, wrap them in single quotation marks (for example, \"Please fill the 'name' parameter).\nBoolean description\nStart the description of boolean components with 'Whether...'\nErrors\nGeneral philosophy\nErrors are sources of pain for users. For this reason, n8n always wants to tell the user:\nWhat happened: a description of the error and what went wrong.\nHow to solve the problem: or at least how to get unstuck and continue using n8n without problems. n8n doesn't want users to remain blocked, so use this as an opportunity to guide them to success.\nError structure in the Output panel\nError Message - What happened\nThis message explains to the user what happened, and the current issue that prevents the execution completing.\nIf you have the displayName of the parameter that triggered the error, include it in the error message or description (or both).\nItem index: if you have the ID of the item that triggered the error, append [Item X] to the error message. For example, The ID of the release in the parameter ‚ÄúRelease ID‚Äù for could not be found [item 2].\nAvoid using words like \"error\", \"problem\", \"failure\", \"mistake\".\nError Description - How to solve or get unstuck\nThe description explains to users how to solve the problem, what to change in the node configuration (if that's the case), or how to get unstuck. Here, you should guide them to the next step and unblock them.\nAvoid using words like \"error\", \"problem\", \"failure\", \"mistake\"."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\verification-guidelines.md",
    "content": "Community node verification guidelines\nPackage source verification\nVerify that your npm package repository URL matches the expected GitHub (or other platform) repository.\nConfirm that the package author / maintainer matches between npm and the repository.\nConfirm that the git link in npm works and that the repository is public.\nMake sure your package has proper documentation (README, usage examples, etc.).\nMake sure your package license is MIT.\nNo external dependencies\nEnsure that your package does¬†not¬†include any external dependencies to keep it lightweight and easy to maintain.\nProper documentation\nProvide clear documentation, whether it‚Äôs a¬†README¬†on GitHub or links to relevant¬†API documentation.\nInclude usage instructions, example workflows, and any necessary authentication details.\nNo access to environment variables or file system\nThe code¬†must not¬†interact with environment variables or attempt to read/write files.\nPass all necessary data through node parameters.\nFollow n8n best practices\nMaintain a clear and consistent coding style.\nUse¬†TypeScript¬†and follow n8n's¬†node development guidelines.\nEnsure proper error handling and validation.\nMake sure the linter passes (in other words, make sure running npx @n8n/scan-community-package n8n-nodes-PACKAGE passes).\nUse English language only\nBoth the node interface and all documentation must be in English only.\nThis includes parameter names, descriptions, help text, error messages and README content."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-base-files\\declarative-style-parameters.md",
    "content": "Declarative-style parameters\nThese are the parameters available for node base file of declarative-style nodes.\nThis document gives short code snippets to help understand the code structure and concepts. For a full walk-through of building a node, including real-world code examples, refer to Build a declarative-style node.\nRefer to Standard parameters for parameters available to all nodes.\nmethods and loadOptions\nObject TABLE_PLACEHOLDER_0\nIf you have one version of your node, this can be a number. If you want to support more than one version, turn this into an array, containing numbers for each node version.\nn8n supports two methods of node versioning, but declarative-style nodes must use the light versioning approach. Refer to Node versioning for more information."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-base-files\\index.md",
    "content": "Node base file\nThe node base file contains the core code of your node. All nodes must have a base file. The contents of this file are different depending on whether you're building a declarative-style or programmatic-style node. For guidance on which style to use, refer to Choose your node building approach.\nThese documents give short code snippets to help understand the code structure and concepts. For full walk-throughs of building a node, including real-world code examples, refer to Build a declarative-style node or Build a programmatic-style node.\nYou can also explore the n8n-nodes-starter and n8n's own nodes for a wider range of examples. The starter contains basic examples that you can build on. The n8n Mattermost node is a good example of a more complex programmatic-style node, including versioning.\nFor all nodes, refer to the:\nStructure of the node base file\nStandard parameters\nFor declarative-style nodes, refer to the:\nDeclarative-style parameters\nFor programmatic-style nodes, refer to the:\nProgrammatic-style parameters\nProgrammatic-style execute() method"
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-base-files\\programmatic-style-execute-method.md",
    "content": "Programmatic-style execute() method\nThe main difference between the declarative and programmatic styles is how they handle incoming data and build API requests. The programmatic style requires an execute() method, which reads incoming data and parameters, then builds a request. The declarative style handles requests using the routing key in the operations object.\nThe execute() method creates and returns an instance of INodeExecutionData."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-base-files\\programmatic-style-parameters.md",
    "content": "Programmatic-style parameters\nThese are the parameters available for node base file of programmatic-style nodes.\nThis document gives short code snippets to help understand the code structure and concepts. For a full walk-through of building a node, including real-world code examples, refer to Build a programmatic-style node.\nProgrammatic-style nodes also use the execute() method. Refer to Programmatic-style execute method for more information.\nRefer to Standard parameters for parameters available to all nodes.\ndefaultVersion\nNumber TABLE_PLACEHOLDER_0\nUse version when using the light versioning approach.\nIf you have one version of your node, this can be a number. If you want to support multiple versions, turn this into an array, containing numbers for each node version.\nn8n support two methods of node versioning. Programmatic-style nodes can use either. Refer to Node versioning for more information."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-base-files\\standard-parameters.md",
    "content": "Standard parameters\nThese are the standard parameters for the node base file. They're the same for all node types.\ndisplayName\nString TABLE_PLACEHOLDER_0\nThis contains the resource and operations objects that define node behaviors, as well as objects to set up mandatory and optional fields that can receive user input.\nResource objects\nA resource object includes the following parameters:\ndisplayName: String. This should always be Resource.\nname: String. This should always be resource.\ntype: String. Tells n8n which UI element to use, and what input type to expect. For example, options results in n8n adding a dropdown that allows users to choose one option. Refer to Node UI elements for more information.\nnoDataExpression: Boolean. Prevents using an expression for the parameter. Must always be true for resource.\nOperations objects\nThe operations object defines the available operations on a resource.\ndisplayName: String. This should always be Options.\nname: String. This should always be option.\ntype: String. Tells n8n which UI element to use, and what input type to expect. For example, dateTime results in n8n adding a date picker. Refer to Node UI elements for more information.\nnoDataExpression: Boolean. Prevents using an expression for the parameter. Must always be true for operation.\noptions: Array of objects. Each objects describes an operation's behavior, such as its routing, the REST verb it uses, and so on. An options object includes:\nname. String.\nvalue. String.\naction: String. This parameter combines the resource and operation. You should always include it, as n8n will use it in future versions. For example, given a resource called \"Card\" and an operation \"Get all\", your action is \"Get all cards\".\ndescription: String.\nrouting: Object containing request details.\nAdditional fields objects\nThese objects define optional parameters. n8n displays them under Additional Fields in the GUI. Users can choose which parameters to set.\nThe objects must include:\nFor more information about UI element types, refer to UI elements."
  },
  {
    "file_path": "integrations\\creating-nodes\\build\\reference\\node-base-files\\structure.md",
    "content": "Structure of the node base file\nThe node base file follows this basic structure:\nAdd import statements.\nCreate a class for the node.\nWithin the node class, create a description object, which defines the node.\nA programmatic-style node also has an execute() method, which reads incoming data and parameters, then builds a request. The declarative style handles this using the routing key in the properties object, within descriptions.\nOutline structure for a declarative-style node\nThis code snippet gives an outline of the node structure.\nRefer to Standard parameters for information on parameters available to all node types. Refer to Declarative-style parameters for the parameters available for declarative-style nodes.\nOutline structure for a programmatic-style node\nThis code snippet gives an outline of the node structure.\nRefer to Standard parameters for information on parameters available to all node types. Refer to Programmatic-style parameters and Programmatic-style execute method for more information on working with programmatic-style nodes."
  },
  {
    "file_path": "integrations\\creating-nodes\\deploy\\index.md",
    "content": "Deploy a node\nThis section contains details on how to deploy and share your node.\nYou can choose to:\nSubmit your node to the community node repository. This makes it available for everyone to use, and allows you to install and use it like any other community node. This is the only way to use custom nodes on cloud.\nInstall the node into your n8n instance as a private node."
  },
  {
    "file_path": "integrations\\creating-nodes\\deploy\\install-private-nodes.md",
    "content": "Install private nodes\nYou can build your own nodes and install them in your n8n instance without publishing them on npm. This is useful for nodes that you create for internal use only at your company.\nInstall your node in a Docker n8n instance\nIf you're running n8n using Docker, you need to create a Docker image with the node installed in n8n.\nCreate a Dockerfile and paste the code from this Dockerfile.\nYour Dockerfile should look like this:\nCompile your custom node code (npm run build if you are using nodes starter). Copy the node and credential folders from within the dist folder into your container's ~/.n8n/custom/ directory. This makes them available to Docker.\nDownload the docker-entrypoint.sh file, and place it in the same directory as your Dockerfile.\nBuild your Docker image:\nYou can now use your node in Docker.\nInstall your node in a global n8n instance\nIf you've installed n8n globally, make sure that you install your node inside n8n. n8n will find the module and load it automatically."
  },
  {
    "file_path": "integrations\\creating-nodes\\plan\\choose-node-method.md",
    "content": "Choose your node building approach\nn8n has two node-building styles, declarative and programmatic.\nYou should use the declarative style for most nodes. This style:\nUses a JSON-based syntax, making it simpler to write, with less risk of introducing bugs.\nIs more future-proof.\nSupports integration with REST APIs.\nThe programmatic style is more verbose. You must use the programmatic style for:\nTrigger nodes\nAny node that isn't REST-based. This includes nodes that need to call a GraphQL API and nodes that use external dependencies.\nAny node that needs to transform incoming data.\nFull versioning. Refer to Node versioning for more information on types of versioning.\nData handling differences\nThe main difference between the declarative and programmatic styles is how they handle incoming data and build API requests. The programmatic style requires an execute() method, which reads incoming data and parameters, then builds a request. The declarative style handles this using the routing key in the operations object. Refer to Node base file for more information on node parameters and the execute() method.\nSyntax differences\nTo understand the difference between the declarative and programmatic styles, compare the two code snippets below. This example creates a simplified version of the SendGrid integration, called \"FriendGrid.\" The following code snippets aren't complete: they emphasize the differences in the node building styles.\nIn programmatic style:\nIn declarative style:"
  },
  {
    "file_path": "integrations\\creating-nodes\\plan\\index.md",
    "content": "Plan a node\nThis section provides guidance on designing your node, including key technical decisions such as choosing your node building style.\nWhen building a node, there are design choices you need to make before you start:\nWhich node type you need to build.\nWhich node building style to use.\nYour UI design and UX principles\nYour node's file structure."
  },
  {
    "file_path": "integrations\\creating-nodes\\plan\\node-types.md",
    "content": "Node types: Trigger and Action\nThere are two node types you can build for n8n: trigger nodes and action nodes.\nBoth types provide integrations with external services.\nTrigger nodes\nTrigger nodes start a workflow and supply the initial data. A workflow can contain multiple trigger nodes but with each execution, only one of them will execute, depending on the triggering event.\nThere are three types of trigger nodes in n8n:\nTABLE_PLACEHOLDER_0\nAction nodes\nAction nodes perform operations as part of your workflow. These can include manipulating data, and triggering events in other systems."
  },
  {
    "file_path": "integrations\\creating-nodes\\plan\\node-ui-design.md",
    "content": "Design your node's user interface\nMost nodes are a GUI (graphical user interface) representation of an API. Designing the interface means finding a user-friendly way to represent API endpoints and parameters. Directly translating an entire API into form fields in a node may not result in a good user experience.\nThis document provides design guidance and standards to follow. These guidelines are the same as those used by n8n. This helps provide a smooth and consistent user experience for users mixing community and built-in nodes.\nDesign guidance\nAll node's use n8n's node UI elements, so you don't need to consider style details such as colors, borders, and so on. However, it's still useful to go through a basic design process:\nReview the documentation for the API you're integrating. Ask yourself:\nWhat can you leave out?\nWhat can you simplify?\nWhich parts of the API are confusing? How can you help users understand them?\nUse a wireframe tool to try out your field layout. If you find your node has a lot of fields and is getting confusing, consider n8n's guidance on showing and hiding fields.\nStandards\nUI text style\nTABLE_PLACEHOLDER_0* Tooltips: callouts that appear when the user hovers over the tooltip icon !\"Screenshot of the tooltip icon. The icon is a ? in a grey circle\". Use tooltips for extra information that the user might need.\n* You don't have to provide a tooltip for every field. Only add one if it contains useful information.\n* When writing tooltips, think about what the user needs. Don't just copy-paste API parameter descriptions. If the description doesn't make sense, or has errors, improve it.\n* Placeholder text: n8n can display placeholder text in a field where the user hasn't entered a value. This can help the user know what's expected in that field.\nInfo boxes, hints, and tooltips can contain links to more information.\nErrors\nMake it clear which fields are required.\nAdd validation rules to fields if possible. For example, check for valid email patterns if the field expects an email.\nWhen displaying errors, make sure only the main error message displays in the red error title. More information should go in Details.\nRefer to Node Error Handling for more information.\nToggles\nTooltips for binary states should start with something like Whether to . . . .\nYou may need a list rather than a toggle:\nUse toggles when it's clear what happens in a false state. For example, Simplify Output?. The alternative (don't simplify output) is clear.\nUse a dropdown list with named options when you need more clarity. For example, Append?. What happens if you don't append is unclear (it could be that nothing happens, or information is overwritten, or discarded).\nLists\nSet default values for lists whenever possible. The default should be the most-used option.\nSort list options alphabetically.\nYou can include list option descriptions. Only add descriptions if they provide useful information.\nIf there is an option like All, use the word All, not shorthand like *.\nTrigger node inputs\nWhen a trigger node has a parameter for specifying which events to trigger on:\nName the parameter Trigger on.\nDon't include a tooltip.\nSubtitles\nSet subtitles based on the values of the main parameters. For example:\nIDs\nWhen performing an operation on a specific record, such as \"update a task comment\" you need a way to specify which record you want to change.\nWherever possible, provide two ways to specify a record:\nBy choosing from a pre-populated list. You can generate this list using the loadOptions parameter. Refer to Base files for more information.\nBy entering an ID.\nName the field  name or ID. For example, Workspace Name or ID. Add a tooltip saying \"Choose a name from the list, or specify an ID using an expression.\" Link to n8n's Expressions documentation.\nBuild your node so that it can handle users providing more information than required. For example:\nIf you need a relative path, handle the user pasting in the absolute path.\nIf the user needs to get an ID from a URL, handle the user pasting in the entire URL.\nDates and timestamps\nn8n uses ISO timestamp strings for dates and times. Make sure that any date or timestamp field you add supports all ISO 8601 formats.\nJSON\nYou should support two ways of specifying the content of a text input that expects JSON:\nTyping JSON directly into the text input: you need to parse the resulting string into a JSON object.\nUsing an expression that returns JSON.\nNode icons\nCommon patterns and exceptions\nThis section provides guidance on handling common design patterns, including some edge cases and exceptions to the main standards.\nSimplify responses\nAPIs can return a lot of data that isn't useful. Consider adding a toggle that allows users to choose to simplify the response data:\nName: Simplify Response\nDescription: Whether to return a simplified version of the response instead of the raw data\nUpsert operations\nThis should always be a separate operation with:\nName: Create or Update\nDescription: Create a new record, or update the current one if it already exists (upsert)\nBoolean operators\nn8n doesn't have good support for combining boolean operators, such as AND and OR, in the GUI. Whenever possible, provide options for all ANDs or all ORs.\nFor example, you have a field called Must match to test if values match. Include options to test for Any and All, as separate options.\nSource keys or binary properties\nBinary data is file data, such as spreadsheets or images. In n8n, you need a named key to reference the data. Don't use the terms \"binary data\" or \"binary property\" for this field. Instead, use a more descriptive name: Input data field name / Output data field name."
  },
  {
    "file_path": "integrations\\creating-nodes\\test\\index.md",
    "content": "Test a node\nThis section contains information about testing your node.\nThere are two ways to test your node:\nManually, by running it on your own machine within a local n8n instance.\nAutomatically, using the linter.\nYou should use both methods before publishing your node."
  },
  {
    "file_path": "integrations\\creating-nodes\\test\\node-linter.md",
    "content": "n8n node linter\nn8n's node linter, eslint-plugin-n8n-nodes-base, statically analyzes (\"lints\") the source code of n8n nodes and credentials in the official repository and in community packages. The linter detects issues and automatically fixes them to help you follow best practices.\neslint-plugin-n8n-nodes-base contains a collection of rules for node files (.node.ts), resource description files (Description.ts), credential files (*.credentials.ts), and the package.json of a community package.\nSetup\nIf using the n8n node starter: Run npm install in the starter project to install all dependencies. Once the installation finishes, the linter is available to you.\nIf using VS Code, install the ESLint VS Code extension. For other IDEs, refer to their ESLint integrations.\nUsage\nYou can use the linter in a community package or in the main n8n repository.\nLinting\nIn a community package, the linter runs automatically after installing dependencies and before publishing the package to npm. In the main n8n repository, the linter runs automatically using GitHub Actions whenever you push to your pull request.\nIn both cases, VS Code lints in the background as you work on your project. Hover over a detected issue to see a full description of the linting and a link to further information.\nYou can also run the linter manually:\nRun npm run lint to lint and view detected issues in your console.\nRun npm run lintfix to lint and automatically fix issues. The linter fixes violations of rules marked as automatically fixable.\nBoth commands can run in the root directory of your community package, or in /packages/nodes-base/ in the main repository.\nExceptions\nInstead of fixing a rule violation, you can also make an exception for it, so the linter doesn't flag it.\nTo make a lint exception from VS Code: hover over the issue and click on Quick fix (or cmd+. in macOS) and select Disable {rule} for this line. Only disable rules for a line where you have good reason to. If you think the linter is incorrectly reporting an issue, please report it in the linter repository.\nTo add a lint exception to a single file, add a code comment. In particular, TSLint rules may not show up in VS Code and may need to be turned off using code comments. Refer to the TSLint documentation for more guidance."
  },
  {
    "file_path": "integrations\\creating-nodes\\test\\troubleshooting-node-development.md",
    "content": "Troubleshooting\nCredentials\nError message: 'Credentials of type \"*\" aren't known'\nCheck that the name in the credentials array matches the name used in the property name of the credentials' class.\n!Troubleshooting credentials\nEditor UI\nError message: 'There was a problem loading init data: API-Server can not be reached. It's probably down'\nCheck that the names of the node file, node folder, and class match the path added to packages/nodes-base/package.json.\nCheck that the names used in the displayOptions property are names used by UI elements in the node.\nNode icon doesn't show up in the Add Node menu and the Editor UI\nCheck that the icon is in the same folder as the node.\nCheck that it's either in PNG or SVG format.\nWhen the icon property references the icon file, check that it includes the logo extension (.png or .svg) and that it prefixes it with file:. For example, file:friendGrid.png or file:friendGrid.svg.\nNode icon doesn't fit\nIf you use an SVG file, make sure the canvas size is square. You can find instructions to change the canvas size of an SVG file using GIMP here.\nIf you use a PNG file, make sure that it's 60x60 pixels.\nNode doesn't show up in the Add Node menu\nCheck that you registered the node in the package.json file in your project.\nChanges to the description properties don't show in the UI on refreshing\nEvery time you change the description properties, you have to stop the current n8n process (ctrl + c) and run it again. You may also need to re-run npm link.\nLinter incorrectly warning about file name case\nThe node linter has rules for file names, including what case they should be. Windows users may encounter an issue when renaming files that causes the linter to continue giving warnings, even after you rename the files. This is due to a known Windows issue with changing case when renaming files."
  },
  {
    "file_path": "manage-cloud\\ai-assistant.md",
    "content": "AI Assistant\nThe n8n AI Assistant helps you build, debug, and optimize your workflows seamlessly. From answering questions about n8n to providing help with coding and expressions, the AI Assistant can streamline your workflow-building process and support you as you navigate n8n's capabilities.\nCurrent capabilities\nThe AI Assistant offers a range of tools to support you:\nDebug helper: Identify and troubleshoot node execution issues in your workflows to keep them running without issues.\nAnswer n8n questions: Get instant answers to your n8n-related questions, whether they're about specific features or general functionality.\nCoding support: Receive guidance on coding, including SQL and JSON, to optimize your nodes and data processing.\nExpression assistance: Learn how to create and refine expressions to get the most out of your workflows.\nCredential setup tips: Find out how to set up and manage node credentials securely and efficiently.\nTips for getting the most out of the Assistant\nEngage in a conversation: The AI Assistant can collaborate with you step-by-step. If a suggestion isn't what you need, let it know! The more context you provide, the better the recommendations will be.\nAsk specific questions: For the best results, ask focused questions (for example, \"How do I set up credentials for Google Sheets?\"). The assistant works best with clear queries.\nIterate on suggestions: Don't hesitate to build on the assistant's responses. Try different approaches and keep refining based on the assistant's feedback to get closer to your ideal solution.\nThings to try out:\nDebug any error you're seeing\nAsk how to setup credentials\n\"Explain what this workflow does.\"\n\"I need your help to write code: [Explain your code here]\"\n\"How can I build X in n8n?\"\nFAQs\nWhat context does the Assistant have?\nThe AI Assistant has access to all elements displayed on your n8n screen, excluding actual input and output data values (like customer information). To learn more about what data n8n shares with the Assistant, refer to AI in n8n.\nWho can use the Assistant?\nAny user on a Cloud plan can use the assistant.\nHow does the Assistant work?\nThe underlying logic of the assistant is build with the advanced AI capabilities of n8n. It uses a combination of different agents, specialized in different areas of n8n, RAG to gather knowledge from the docs and the community forum, and custom prompts, memory and context."
  },
  {
    "file_path": "manage-cloud\\change-ownership-or-username.md",
    "content": "Change instance ownership\nYou can change the ownership of an instance by navigating to the Settings page in the owner's account and editing the Email field. After making the changes, scroll down and press Save.\nNote that for the change to be effective, the new email address can't be linked to any other n8n account.\nChanging emails will change the owner of the instance, the email you log in with, and the email your invoices and general communication gets sent to.\nIf the workspace is deactivated, there will be no Settings page and no possibility to change the email address or the owner info.\nChange instance username\nIt's not currently possible to change usernames.\nIf you want your instance to have a different name you will need to create a new account and transfer your work into it. The import/export documentation explains how you can transfer your work to a new n8n instance."
  },
  {
    "file_path": "manage-cloud\\cloud-admin-dashboard.md",
    "content": "Cloud admin dashboard\nInstance owners can access the admin dashboard to manage their Cloud instance. This is where you can upgrade your n8n version and set the timezone.\nAccess the dashboard from the app\nLog in to n8n\nSelect Admin Dashboard. n8n opens the dashboard.\nAccess the dashboard if the app is offline\nIf your instance is down, you can still access the admin dashboard. When you log in to the app, n8n will ask you if you want a magic link to access your dashboard. Select Send magic link, then check your email for the link."
  },
  {
    "file_path": "manage-cloud\\cloud-data-management.md",
    "content": "Cloud data management\nThere are two concerns when managing data on Cloud:\nMemory usage: complex workflows processing large amounts of data can exceed n8n's memory limits. If this happens, the instance can crash and become inaccessible.\nData storage: depending on your execution settings and volume, your n8n database can grow in size and run out of storage.\nTo avoid these issues, n8n recommends that you build your workflows with memory efficiency in mind, and don't save unnecessary data\nMemory limits on each Cloud plan\nCurrent plans:\nTrial: 320MiB RAM, 10 millicore CPU burstable\nStarter: 320MiB RAM, 10 millicore CPU burstable\nPro-1 (10k executions): 640MiB RAM, 20 millicore CPU burstable\nPro-2 (50k executions): 1280MiB RAM, 80 millicore CPU burstable\nEnterprise: 4096MiB RAM, 80 millicore CPU burstable\nLegacy plans:\nStart: 320MiB RAM, 10 millicore CPU burstable\nPower: 1280MiB RAM, 80 millicore CPU burstable\nn8n gives each instance up to 100GB of data storage.\nHow to reduce memory consumption in your workflow\nThe way you build workflows affects how much data they consume when executed. Although these guidelines aren't applicable to all cases, they provide a baseline of best practices to avoid exceeding instance memory.\nNote that n8n itself consumes memory to run. On average, the software alone uses around 180MiB RAM.\nInteractions with the UI also consume memory. Playing around with the workflow UI while it performs heavy executions could also push the memory capacity over the limit.\nHow to manage execution data on Cloud\nExecution data includes node data, parameters, variables, execution context, and binary data references. It's text-based.\nBinary data is non-textual data that n8n can't represent as plain text. This is files and media such as images, documents, audio files, and videos. It's much larger than textual data.\nIf a workflow consumes a large amounts of data and is past testing stage, it's a good option to stop saving the successful executions.\nThere are two ways you can control how much execution data n8n stores in the database:\nIn the admin dashboard:\nFrom your workspace or editor, navigate to Admin Panel.\nSelect Manage.\nIn Executions to Save deselect the executions you don't want to log.\nIn your workflow settings:\nSelect the Options !Options menu{.off-glb} menu.\nSelect Settings. n8n opens the Workflow settings modal.\nChange Save successful production executions to Do not save.\nCloud data pruning and out of memory incident prevention\nAutomatic data pruning\nn8n automatically prunes execution logs after a certain time or once you reach the max storage limit, whichever comes first. The pruning always happens from oldest to newest and the limits depend on your Could plan:\nStart and Starter plans: max 2500 executions saved and 7 days execution log retention;\nPro and Power plans: max 25000 executions saved and 30 days execution log retention;\nEnterprise plan: max 50000 executions saved and unlimited execution log retention time.\nManual data pruning\nHeavier executions and use cases can exceed database capacity despite the automatic pruning practices. In cases like this, n8n will manually prune data to protect instance stability.\nAn alert system warns n8n if an instance is at 85% disk capacity.\nn8n prunes execution data. n8n does this by running a backup of the instance (workflows, users, credentials and execution data) and restoring it without execution data.\nDue to the human steps in this process, the alert system isn't perfect. If warnings are triggered after hours or if data consumption rates are high, there might not be time to prune the data before the remaining disk space fills up."
  },
  {
    "file_path": "manage-cloud\\cloud-free-trial.md",
    "content": "Cloud free trial\nWhen you create a new n8n cloud trial, you have 14 days to try all the features of the Pro plan, including:\nGlobal variables\nInsights dashboard\nExecution search\n5 days workflow history to rollback\nThe trial gives you Pro plan features with limits of 1000 executions, 5 active workflows, and the same computing power as the Starter plan.\nUpgrade to a paid account\nYou can upgrade to a paid n8n account at any time. To upgrade:\nLog in to your account.\nClick the Upgrade button in the upper-right corner.\nSelect your plan and whether to pay annually or by the month.\nSelect a payment method.\nTrial expiration\nIf you don't upgrade by the end of your trial, the trial will automatically expire and your workspace will be deleted.\nCancelling your trial\nYou don't need to cancel your trial. Your trial will automatically expire at the end of the trial period and no charges will occur. All your data will be deleted soon after.\nEnterprise trial\nYou can contact the sales team if you want to test the Enterprise plan, which includes features such as:\nSSO SAML and LDAP\nDifferent environments\nExternal secret store integration\nLog streaming\nVersion control using Git\nClick the Contact button on the n8n website."
  },
  {
    "file_path": "manage-cloud\\cloud-ip.md",
    "content": "Cloud IP addresses\nOutbound traffic may appear to originate from any of:\n20.79.227.226/32\n20.113.47.122/32\n20.218.202.73/32\n98.67.233.91/32\n4.182.111.50/32\n4.182.129.20/32\n4.182.88.118/32\n4.182.212.136/32\n98.67.244.108/32\n72.144.128.145/32\n72.144.83.147/32\n72.144.69.38/32\n72.144.111.50/32\n4.182.128.108/32\n4.182.190.144/32\n4.182.191.184/32\n98.67.233.200/32\n20.52.126.0/28\n20.218.238.112/28\n4.182.64.64/28\n20.218.174.0/28\n4.184.78.240/28\n20.79.32.32/28\n51.116.119.64/28"
  },
  {
    "file_path": "manage-cloud\\concurrency.md",
    "content": "Cloud concurrency\nToo many concurrent executions can cause performance degradation and unresponsiveness. To prevent this and improve instance stability, n8n sets concurrency limits for production executions in regular mode.\nAny executions beyond the limits queue for later processing. These executions remain in the queue until concurrency capacity frees up, and are then processed in FIFO order.\nConcurrency limits\nn8n limits the number of concurrent executions for Cloud instances according to their plan:\nStarter and Trial: 5\nPro (10k workflow executions, 15 active workflows): 20\nPro (50k workflow executions, 50 active workflows): 50\nEnterprise (in regular mode): 200\nYou can view the number of active executions and your plan's concurrency limit at the top of a project's or workflow's executions tab.\nDetails\nSome other details about concurrency to keep in mind:\nConcurrency control applies only to production executions: those started from a webhook or trigger node. It doesn't apply to any other kinds, such as manual executions, sub-workflow executions, or error executions.\nTest evaluations do not count towards concurrency limits. Your test evaluation concurrency limit is equal to, but separate from, your plan's regular concurrency limit.\nYou can't retry queued executions. Cancelling or deleting a queued execution also removes it from the queue.\nOn instance startup, n8n resumes queued executions up to the concurrency limit and re-enqueues the rest.\nComparison to queue mode\nConcurrency in queue mode is a separate mechanism from concurrency in regular mode. In queue mode, the concurrency settings determine how many jobs each worker can run in parallel. In regular mode, concurrency limits apply to the entire instance."
  },
  {
    "file_path": "manage-cloud\\download-workflows.md",
    "content": "Download workflows\nn8n Cloud instance owners can download workflows from the most recent backup.\nYou can do this with the Cloud admin dashboard.\nHow to download workflows\nLog in to n8n.\nSelect Admin Dashboard to open the dashboard.\nIn the Manage section, select the Export tab.\nSelect Download Workflows.\nAccessing workflows after your free trial\nYou have 90 days to download your workflows after your free trial ends. After that, all workflows will be permanently deleted and are unrecoverable."
  },
  {
    "file_path": "manage-cloud\\overview.md",
    "content": "n8n Cloud\nn8n Cloud is n8n's hosted solution. It provides:\nNo technical set up or maintenance for your n8n instance\nContinual uptime monitoring\nManaged OAuth for authentication\nOne-click upgrades to the newest n8n versions\nSign up for n8n Cloud"
  },
  {
    "file_path": "manage-cloud\\set-cloud-timezone.md",
    "content": "Set the Cloud instance timezone\nYou can change the timezone for your n8n instance. This affects the Schedule Trigger and Date & Time node. Users can configure the timezone for individual workflows in Workflow settings.\nOn your dashboard, select Manage.\nChange the Timezone dropdown to the timezone you want."
  },
  {
    "file_path": "manage-cloud\\update-cloud-version.md",
    "content": "Update your Cloud version\nn8n recommends regularly updating your Cloud version. Check the Release notes to learn more about changes.\nLog in to the n8n Cloud dashboard\nOn your dashboard, select Manage.\nUse the n8n version dropdown to select your preferred release version:\nLatest Stable: recommended for most users.\nLatest Beta: get the newest n8n. This may be unstable.\nSelect Save Changes to restart your n8n instance and perform the update.\nIn the confirmation modal, select Confirm.\nBest practices for updating\nAutomatic update\nn8n automatically updates outdated Cloud instances.\nIf you don't update you instance for 120 days, n8n emails you to warn you to update. After a further 30 days, n8n automatically updates your instance."
  },
  {
    "file_path": "privacy-security\\incident-response.md",
    "content": "Incident response\nn8n implements incident response best practices for identifying, documenting, resolving and communicating incidents.\nn8n publishes incident notifications to a status page at¬†n8n Status.\nn8n notifies customers of any data breaches according to the company's Data Processing Addendum."
  },
  {
    "file_path": "privacy-security\\index.md",
    "content": "Privacy and security at n8n\nn8n is committed to the privacy and security of your data. This section outlines how n8n handles and secures data. This isn't an exhaustive list of practices, but an overview of key policies and procedures.\nIf you have any questions related to data privacy, email¬†privacy@n8n.io.\nIf you have any security-related questions, or if you want to report a suspected vulnerability, email¬†security@n8n.io."
  },
  {
    "file_path": "privacy-security\\privacy.md",
    "content": "Privacy\nThis page describes n8n's data privacy practices.\nGDPR\nData processing agreement\nFor Cloud versions of n8n, n8n is considered both a Controller and a Processor as defined by the GDPR. As a Processor, n8n implements policies and practices that secure the personal data you send to the platform, and includes a¬†Data Processing Agreement as part of the company's standard¬†Terms of Service.\nThe n8n Data Processing Agreement includes the¬†Standard Contractual Clauses (SCCs). These clarify how n8n handles your data, and they update n8n's GDPR policies to cover the latest standards set by the European Commission.\nYou can find a list of n8n sub-processors¬†here.\nSubmitting a GDPR deletion request\nEmail privacy@n8n.io to request data deletion.\nSub-processors\nThis is a list of sub-processors authorized to process customer data for n8n's service. n8n audits each sub-processor's security controls and applicable regulations for the protection of personal data.\nTABLE_PLACEHOLDER_0\nSubscribe here to receive updates when n8n adds or changes a sub-processor.\nGDPR for self-hosted users\nData collection\nn8n collects selected usage and performance data to help diagnose problems and improve the platform. Read about how n8n stores and processes this information in the privacy policy.\nThe data gathered is different in self-hosted n8n and n8n Cloud.\nData collection in self-hosted n8n\nn8n takes care to keep self-hosted data anonymous and avoids collecting sensitive data.\nWhat n8n collects\nError codes and messages of failed executions (excluding any payload data, and not for custom nodes)\nError reports for app crashes and API issues\nThe graph of a workflow (types of nodes used and how they're connected)\nFrom node parameters:\nThe 'resource' and 'operation' that a node is set to (if applicable)\nFor HTTP request nodes, the domain, path, and method (with personal data anonymized)\nData around workflow executions:\nStatus\nThe user ID of the user who ran the execution\nThe first time a workflow loads data from an external source\nThe first successful production (non-manual) workflow execution\nThe domain of webhook calls, if specified (excluding subdomain).\nDetails on how the UI is used (for example, navigation, nodes panel searches)\nDiagnostic information:\nn8n version\nSelected settings:\nDB_TYPE\nN8N_VERSION_NOTIFICATIONS_ENABLED\nN8N_DISABLE_PRODUCTION_MAIN_PROCESS\nExecution variables\nOS, RAM, and CPUs\nAnonymous instance ID\nIP address\nWhat n8n doesn't collect\nn8n doesn't collect private or sensitive information, such as:\nPersonally identifiable information (except IP address)\nCredential information\nNode parameters (except 'resource' and 'operation')\nExecution data\nSensitive settings (for example, endpoints, ports, DB connections, username/password)\nError payloads\nHow collection works\nMost data is sent to n8n as events that generate it occur. Workflow execution counts and an instance pulse are sent periodically (every 6 hours).\nOpting out of telemetry\nTelemetry collection is enabled by default. To disable it you can configure the following environment variables.\nTo opt out of telemetry events:\nTo opt out of checking for new versions of n8n:\nTo disable the templates feature (prevents background health check calls):\nSee configuration for more info on how to set environment variables.\nData collection in n8n Cloud\nn8n Cloud collects everything listed in Data collection in self-hosted n8n.\nAdditionally, in n8n Cloud, n8n uses PostHog to track events and visualise usage, including using session recordings. Session recordings comprise the data seen by a user on screen, with the exception of credential values. n8n's product team uses this data to improve the product. All recordings are deleted after 21 days.\nAI in n8n\nTo provide enhanced assistance, n8n integrates AI-powered features that leverage Large Language Models (LLMs).\nHow n8n uses AI\nTo assist and improve user experience, n8n may send specific context data to LLMs. This context data is strictly limited to information about the current workflow. n8n does not send any values from credential fields or actual output data to AI services. The data will not be incorporated, used, or retained to train the models of the AI services. Any data will be deleted after 30 days.\nWhen n8n shares data\nData is only sent to AI services if workspaces have opted in to use the assistant. The Assistant is enabled by default for n8n Cloud users. When a workspace opts in to use the assistant, node-specific data is transmitted only during direct interactions and active sessions with the AI assistant, ensuring no unnecessary data sharing occurs.\nWhat n8n shares\nGeneral Workflow Information: This includes details about which nodes are present in your workflow, the number of items currently in the workflow, and whether the workflow is active.\nInput & Output Schemas of Nodes: This includes the schema of all nodes with incoming data and the output schema of a node in question. We do not send the actual data value of the schema.\nNode Configuration: This includes the operations, options, and settings chosen in the referenced node.\nCode and Expressions: This includes any code or expressions in the node in question to help with debugging potential issues and optimizations.\nWhat n8n doesn't share\nCredentials: Any values of the credential fields of your nodes.\nOutput Data: The actual data processed by your workflows.\nSensitive Information: Any personally identifiable information or other sensitive data that could compromise your privacy or security that you have not explicitly mentioned in node parameters or your code of a Code Node.\nDocumentation telemetry\nn8n's documentation (this website) uses cookies to recognize your repeated visits and preferences, as well as to measure the effectiveness of n8n's documentation and whether users find what they're searching for. With your consent, you're helping n8n to make our documentation better. You can control cookie consent using the cookie widget.\nRetention and deletion of personal identifiable data\nPID (personal identifiable data) is data that's personal to you and would identify you as an individual.\nn8n Cloud\nPID retention\nn8n only retains data for as long as necessary to provide the core service.\nFor n8n Cloud, n8n stores your workflow code, credentials, and other data indefinitely, until you choose to delete it or close your account. The platform stores execution data according to¬†the retention rules on your account.\nn8n deletes most internal application logs and logs tied to subprocessors within 90 days. The company retains a subset of logs for longer periods where required for security investigations.\nPID deletion\nIf you choose to delete your n8n account, n8n deletes all customer data and event data associated with your account. n8n deletes customer data in backups within 90 days.\nSelf-hosted\nSelf-hosted users should have their own PID policy and data deletion processes. Refer to What you can do for more information.\nPayment processor\nn8n uses¬†Paddle.com to process payments. When you sign up for a paid plan, Paddle transmits and stores the details of your payment method according to their security policy. n8n stores no information about your payment method."
  },
  {
    "file_path": "privacy-security\\what-you-can-do.md",
    "content": "What you can do\nIt's also your responsibility as a customer to ensure you are securing your code and data. This document lists some steps you can take.\nAll users\nReport security issues and terms of service violations to security@n8n.io.\nIf more than one person uses your n8n instance, set up User management and follow the Best practices.\nUse OAuth to connect integrations whenever possible.\nSelf-hosted users\nIf you self-host n8n, there are additional steps you can take:\nSet up a reverse proxy to handle TLS, ensuring data is encrypted in transit.\nEnsure data is encrypted at rest by using encrypted partitions, or encryption at the hardware level, and ensuring n8n and its database is written to that location.\nRun a Security audit.\nBe aware of the Risks when installing community nodes, or choose to disable them.\nMake sure users can't import external modules in the Code node. Refer to Environment variables TABLE_PLACEHOLDER_0* For maximum privacy, you can Isolate n8n.\nGDPR for self-hosted users"
  },
  {
    "file_path": "release-notes\\0-x.md",
    "content": "Release notes pre 1.0\nFeatures and bug fixes for n8n before the release of 1.0.0.\nYou can also view the Releases in the GitHub repository.\nSemantic versioning in n8n\nn8n uses semantic versioning. All version numbers are in the format MAJOR.MINOR.PATCH. Version numbers increment as follows:\nMAJOR version when making incompatible changes which can require user action.\nMINOR version when adding functionality in a backward-compatible manner.\nPATCH version when making backward-compatible bug fixes.\nn8n@0.237.0\nView the commits for this version.\nRelease date: 2023-08-17\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nContributors\nJordan Hall\nXavier Calland\nn8n@0.236.3\nView the commits for this version.\nRelease date: 2023-07-18\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nContributors\nRomain Dunand\nnoctarius aka Christoph Engelbert\nn8n@0.236.2\nView the commits for this version.\nRelease date: 2023-07-14\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.236.1\nView the commits for this version.\nRelease date: 2023-07-12\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.236.0\nView the commits for this version.\nRelease date: 2023-07-05\nThis release contains new nodes, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nNew nodes\n#### crowd.dev\nThis release includes a crowd.dev node and crowd.dev Trigger node. crowd.dev is a tool to help you understand who is engaging with your open source project.\ncrowd.dev node documentation.\nContributors\nAlberto Pasqualetto\nperseus-algol\nRomeo Balta\nZergRael\nn8n@0.234.1\nView the commits for this version.\nRelease date: 2023-07-05\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.235.0\nView the commits for this version.\nRelease date: 2023-06-28\nThis release contains new features, new nodes, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nContributors\nMarten Steketee\nSandra Ashipala\nn8n@0.234.0\nView the commits for this version.\nRelease date: 2023-06-22\nThis release contains new features, new nodes, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nNew nodes\n#### Debug Helper\nThe Debug Helper node can be used to trigger different error types or generate random datasets to help test n8n workflows.\nDebug Helper node documentation.\nn8n@0.233.1\nView the commits for this version.\nRelease date: 2023-06-19\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.233.0\nView the commits for this version.\nRelease date: 2023-06-14\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.232.0\nView the commits for this version.\nRelease date: 2023-06-07\nThis release contains new features, new nodes, node enhancements, and bug fixes.\nFor full release details, refer to Releases on GitHub.\nNew nodes\nThis release includes a new trigger node for Postgres, which allows you to listen to events, as well as listen to custom channels. Refer to Postgres Trigger for more information.\nn8n@0.231.3\nView the commits for this version.\nRelease date: 2023-06-17\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.231.2\nView the commits for this version.\nRelease date: 2023-06-14\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.231.1\nView the commits for this version.\nRelease date: 2023-06-06\nThis is a bug fix release.\nFor full release details, refer to Releases on GitHub.\nn8n@0.231.0\nView the commits for this version.\nRelease date: 2023-05-31\nThis release contains bug fixes and new features.\nFor full release details, refer to Releases on GitHub.\nNew features\nNotable new features.\nResource mapper UI component\nThis release includes a new UI component, the resource mapper. This component is useful for node creators. If your node does insert, update, or upsert operations, you need to send data from the node in a format supported by the service you're integrating with. Often it's necessary to use a Set node before a node that sends data, to get the data to match the schema of the service you're connecting to. The resource mapper UI component provides a way to get data into the required format directly within the node.\nRefer to Node user interface elements TABLE_PLACEHOLDER_0* Editor: add the HTML editor component for use in parameters. This means node builders can now use the HTML editor that n8n uses in the HTML node as a UI component.\n* Editor: append expressions in fixed values when mapping to string and JSON inputs.\n* Editor: continue to show mapping tooltip after dismiss.\n* Editor: roll out schema view.\nNode enhancements\nFTP Node: stream binary data for uploads and downloads.\nNotion Node: add support for image blocks.\nOpenAI Node: add Frequency Penalty and Presence Penalty to the node options for the text resource.\nSalesforce Node: add Has Opted Out Of Email field to lead resource options.\nSSH Node: stream binary data for uploads and downloads.\nWrite Binary File Node: stream binary data for writes.\nYouTube Node: switch upload operation over to streaming and resumable uploads API.\nBug fixes\nAdd paired item to the most used nodes.\nCore: fix OAuth2 client credentials not always working.\nCore: fix populating of node custom API call options.\nCore: fix value resolution in declarative node design.\nCore: prevent shared user details being saved alongside execution data.\nCore: revert custom API option injecting.\nEditor: add SMTP info translation link slot.\nEditor: change executions title to match menu.\nEditor: fix JSON field completions while typing.\nEditor: handling router errors when navigation is canceled by user.\nEditor: set max width for executions list.\nEditor: stop unsaved changes popup display when navigating away from an untouched workflow.\nEditor: fix workflow executions view.\nInvoice Ninja Node: fix line items not being correctly set for quotes and invoices.\nLinear Node: fix pagination issue for get all issues.\nMailchimp Trigger Node: fix webhook recreation.\nPrevent unnecessarily touching updatedAt when n8n starts.\nSchedule Trigger Node: change scheduler behaviour for intervals days and hours.\nSet Node: fix behaviour when selecting continueOnFail and pairedItem.\nn8n@0.213.0\nView the commits for this version.\nRelease date: 2023-01-27\nThis release introduces LDAP, and a new node for working with HTML in n8n. It also contains node enhancements and bug fixes.\nNew features\n#### LDAP\nThis release introduces support for LDAP on Self-hosted Enterprise and Cloud Enterprise plans. Refer to LDAP for more information on this feature.\nSimplify the Node Details View by moving authentication details to the Credentials modal.\nImprove workflow list performance.\nNew nodes\n#### HTML node\nn8n has a new HTML node. This replaces the HTML Extract node, and adds new functionality to generate HTML templates.\nNode enhancements\nGitLab node: add file resource and operations.\nJIRA Software node: introduce the resource locator component to improve UX.\nSend Email node: this node has been overhauled.\nBug fixes\nCore: don't crash express app on unhandled rejected promises.\nCore: handle missing binary metadata in download URLs.\nCore: upsert (update and insert) credentials and workflows in the import: commands.\nCore: validate numeric IDs in the public API.\nEditor: don't request workflow data twice when opening a workflow.\nEditor: execution list micro optimization.\nEditor: fix node authentication options ordering and hiding options based on node version.\nEditor: fix save modal appearing after duplicating a workflow.\nEditor: prevent workflow execution list infinite no network error.\nExtension being too eager and making calls when it shouldn't.\nGoogle Drive Node: use the correct MIME type on converted downloads.\nHelpScout Node: fix tag search not working when getting all conversations.\nNotion (Beta) Node: fix create database page with multiple relation IDs not working.\nUpdate Sign in with Google button to properly match design guidelines.\nContributors\nDevin Buhl\nSven Ziegler\nn8n@0.212.1\nView the commits for this version.\nRelease date: 2023-01-23\nThis release includes an overhaul of the Google Analytics node, and bug fixes.\nNode enhancements\nThis release includes an overhaul of the Google Analytics node. This brings the node's code and components in line with n8n's latest node building styles, and adds support for GA4 properties.\nBug fixes\nAdd schema to Postgres migrations.\nCore: fix execute-once incoming data handling.\nCore: fix expression extension miss-detection.\nCore: fix onWorkflowPostExecute not being called.\nCore: fix URL in error handling for the error Trigger.\nCore: make pinned data with webhook responding on last node manual-only.\nEditor: making parameter input components label configurable.\nEditor: remove infinite loading in not found workflow level execution.\nLinear Node: fix issue with single item not being returned.\nNotion (Beta) Node: fix create database page fails if relation parameter is empty/undefined.\nn8n@0.212.0\nView the commits for this version.\nRelease date: 2023-01-19\nThis release contains enhancements to the Item Lists node, and bug fixes.\nNew features\nThis release adds experimental support for more Prometheus metrics. Self-hosting users can configure Prometheus using environment variables.\nNode enhancements\nThe Item Lists node now supports a Summarize operation. This acts similarly to generating pivot tables in Excel, allowing you to aggregate and compare data.\nBug fixes\nCore: revert a lint rule @typescript-eslint/prefer-nullish-coalescing.\nEditor: allow special characters in node selector completion.\nGitLab Node: update the credential test endpoint.\nGmail Trigger Node: resolve an issue that was preventing filter by labels from working.\nHTTP Request Node: ensure node enforces the requirement for valid JSON input.\nHTTP Request Node: convert responses to text for all formats, including JSON.\nContributors\nSven Ziegler\nn8n@0.211.2\nView the commits for this version.\nRelease date: 2023-01-17\nThis release contains a bug fix for community nodes, and a new trigger node.\nNew nodes\n#### Google Sheets Trigger node\nThis release adds a new Google Sheets Trigger node. You can now start workflows in response to row changes or new rows in a Google Sheet.\nBug fixes\nFixes an issue that was preventing users from installing community nodes.\nn8n@0.211.1\nView the commits for this version.\nRelease date: 2023-01-16\nThis is a bug fix release. It resolves major issues with 0.211.0.\nNew features\nEditor: suppress validation errors for freshly added nodes.\nNode enhancements\nGoogle Ads node: update the API version to 11.\nGoogle Drive Trigger node: start using the resource locator component.\nBug fixes\nBuild CLI to fix Postgres and MySQL test runs.\nExtend date functions clobbering plus/minus.\nExtension deep comparen't quite working for some primitives.\nUpgrade jsonwebtoken to address CVE-2022-23540.\nn8n@0.211.0\nView the commits for this version.\nRelease date: 2023-01-13\nNew features\nAdd demo experiment to help users activate.\nEditor: Improvements to the Executions page.\nEditor: Remove prevent-ndv-auto-open feature flag.\nEditor: Update callout component design.\nAdd the expression extension framework.\nBug fixes\nCore: Fixes event message confirmations if no subscribers present.\nCore: Remove threads package, rewrite log writer worker.\nCore: Throw error in UI on expression referencing missing node but don't fail execution.\nDB revert command shouldn't run full migrations before each revert.\nEditor: Disable data pinning on multiple output node types.\nEditor: Don't overwrite window.onerror in production.\nEditor: Execution page bug fixes.\nEditor: Fixes event bus test.\nEditor: Hide data pinning discoverability tooltip in execution view.\nEditor: Mapping tooltip dismiss.\nEditor: Recover from unsaved finished execution.\nEditor: Setting NDV session ID.\nFirst/last being extended on proxy objects.\nHandle memory issues gracefully.\nPayPal Trigger Node: Omit verification in sandbox environment.\nReport app startup and database migration errors to Sentry.\nRun every database migration inside a transaction.\nUpgrade class-validator to address CVE-2019-18413.\nZoom Node: Add notice about deprecation of Zoom JWT app support.\nKnown issues\nYou may encounter errors when using the optional chaining operator in expressions. If this happens, avoid using the operator for now.\nn8n@0.210.2\nView the commits for this version.\nRelease date: 2023-01-09\nNew features\n#### Typeahead for expressions\nWhen using expressions, n8n will now offer you suggestions as you type.\n!\"Animated GIF showing typeahead in action\"\nBug fixes\nCore: fix crash of manual workflow executions for unsaved workflows.\nEditor: omit pairedItem from proxy completions.\nEditor: prevent refresh on submit in credential edit modal.\nGoogle Sheets Node: fix for auto-range detection.\nRead Binary File Node: don't crash the execution when the source file doesn't exist.\nRemove anonymous ID from tracking calls.\nStop OOM crashes in Execution Data pruning.\nUpdate links for user management and SMTP help.\nn8n@0.210.1\nView the commits for this version.\nRelease date: 2023-01-05\nThis is a bug fix release. It also contains a new feature to support user management without SMTP set up.\nNew features\nInvite link for users on self-hosted n8n\nIn earlier versions of self-hosted n8n, you needed SMTP set up on your n8n instance for user management to work. User management required SMTP to sent invitation emails.\n0.210.1 introduces an invite link, which you can copy and send to users manually. n8n still recommends setting up SMTP, as this is needed for password resets.\nBug fixes\nGoogle Sheets node: fix an issue that was causing append and update operations to fail for numeric values.\nResolve issues with external hooks.\nn8n@0.210.0\nView the commits for this version.\nRelease date: 2023-01-05\nThis release introduces two major new features: log streaming and security audits. It also contains node enhancements, bug fixes, and performance improvements.\nNew features\n#### Log streaming\nThis release introduces log streaming for users on Enterprise self-hosted plans and custom Cloud plans. Log streaming allows you to send events from n8n to your own logging tools. This allows you to manage your n8n monitoring in your own alerting and logging processes.\n#### Security audit\nThis release adds a security audit feature. You can now run a security audit on your n8n instance, to detect common security issues.\nCore: add support for Redis 6+ ACLs system using username in queue mode. Add the QUEUE_BULL_REDIS_USERNAME environment variable.\nNode enhancements\nCompare Datasets node: add an option for fuzzy compare.\nBug fixes\nApply credential overwrites recursively. This ensures that overwrites defined for a parent credential type also apply to all credentials extending it.\nCore: enable full manual execution of a workflow using the error trigger.\nCore: fix OAuth credential creation using the API.\nCore: fix an issue with workflow lastUpdated field.\nEditor: clear node creator and scrim on workspace reset.\nEditor: fix an infinite loop while loading executions that aren't on the current executions list.\nEditor: make node title non-editable in executions view.\nEditor: prevent scrim on executable triggers.\nEditor: support tabbing away from inline expression editor.\nFix executions bulk deletion.\nGoogle Sheets Node: fix exception when no Values to Send are set.\nRespond to Webhook Node: fix issue that caused the content-type header to be overwritten.\nSlack Node: add missing channels:read OAuth2 scope.\nPerformance improvements\nLazy-load public API dependencies to reduce baseline memory usage.\nLazy-load queue mode and analytics dependencies.\nContributors\nThomas S.\nn8n@0.209.4\nView the commits for this version.\nRelease date: 2022-12-28\nThis is primarily a bug fix release.\nBug fixes\nEditor: add sticky note without manual trigger.\nEditor: display default missing value in table view as undefined.\nEditor: fix displaying of some trigger nodes in the creator panel.\nEditor: fix trigger node type identification on add to canvas.\nEditor: add the usage and plans page to Desktop.\nNew features\nEditor: pressing = in an empty parameter input switches to expression mode.\nn8n@0.209.3\nView the commits for this version.\nRelease date: 2022-12-27\nThis is primarily a bug fix release.\nBug fixes\nCore: don't send credentials to browser console.\nCore: permit a workflow user who isn't the owner to use their own credentials.\nEditor: fix for loading executions that aren't on the current executions list.\nEditor: make the tertiary button on the Usage page transparent.\nEditor: update credential owner warning when sharing.\nNew features\nEditor: Improve UX for brace completion in the inline expressions editor.\nNode enhancements\nWebhook node: when test the node by selecting Listen For Test Event then dispatching a call to the webhook, n8n now only runs the Webhook node. Previously, n8n ran the entire workflow. You can still test the full workflow by selecting Execute Workflow, then dispatching a test call.\nn8n@0.209.2\nView the commits for this version.\nRelease date: 2022-12-23\nThis is a bug fix release.\nBug fixes\nEditor: ensure full tree on expression editor parse. This resolves an issue with the expressions editor cutting off results.\nFix automatic credential selection when credentials are shared.\nPerformance improvements\nImprovements to the workflows list performance.\nn8n@0.209.1\nView the commits for this version.\nRelease date: 2022-12-22\nThis is a bug fix release.\nBug fixes\nEditor: fix for executions preview scroll load bug and wrong execution being displayed.\nEditor: force parse on long expressions.\nEditor: restore trigger to the nodes panel.\nNodes: AWS DynamoDB Node Fix issue pagination and simplify issue.\nNodes: fix DynamoDB node type issues.\nResolve an issue with credentials and workflows not being matched correctly due to incorrect typing.\nRestore missing tags when retrieving a workflow.\nContributors\nNathan Apter\nn8n@0.209.0\nView the commits for this version.\nRelease date: 2022-12-21\nThis release introduces workflow sharing, and changes to licensing and payment plans.\nNew features\n#### Workflow sharing\nThis release introduces workflow sharing for users on some plans. With workflow sharing, users can invite other users on the same n8n instance to use and edit their workflows. Refer to Workflow sharing for details.\nBug fixes\nEditor: Correctly display trigger nodes without actions and with related regular node in the \"On App Events\" category.\nFix stickies resize.\nHide trigger tooltip for nodes with static test output.\nKeep expression when dropping mapped value.\nPrevent keyboard shortcuts in expression editor modal.\nRedirect home to workflows always.\nUpdate mapping GIFs.\nUpgrade amqplib to address CVE-2022-0686.\nView option for binary-data shouldn't download the file on Chrome/Edge.\nn8n@0.208.1\nView the commits for this version.\nRelease date: 2022-12-19\nThis is a bug fix release.\nBug fixes\nAlways retain original errors in the error chain on NodeOperationError.\nBinaryDataManager should store metadata when saving from buffer.\nEditor: fix for wrong execution data displayed in executions preview.\nPick up credential test functions from versioned nodes.\nn8n@0.208.0\nView the commits for this version.\nRelease date: 2022-12-16\nThis release introduces a new inline expressions editor, and a new node: OpenAI. It also contains updates and bug fixes.\nNew features\n#### Inline expression editor\nYou can now quickly write expressions inline in a node parameter. You can still choose to open the full expressions editor.\n!\"Screenshot of the inline expressions editors\"\nAdd workflow sharing telemetry.\nCore: allow for hiding page usage with environment variables (for upcoming feature)\nEditor: update UI copy for user management setup when sharing is disabled.\nEditor: hide credentials password values.\nEditor: set All workflows view as default view on the Workflows page.\nEditor: update UI copy for workflow overwriting message.\nNew nodes\n#### Open AI node\nThis release adds an integration with OpenAI. Refer to the OpenAI node documentation for details.\nNode enhancements\nSend Email node: add support for a \"Reply to\" email address.\nBug fixes\nCore: fix for Google and Microsoft generic OAuth2 credentials.\nCore: fix HTTP Digest Auth for responses without an opaque parameter.\nDisqus node: fix thread parameter for \"Get All Threads\" operation.\nDon't crash the server when Telemetry is blocked using DNS.\nEditor: allow mapping onto expression editor with selection range.\nEditor: don't show actions dialog for actionless triggers when selected using keyboard.\nEditor: fix an issue where some node actions wouldn't select default parameters correctly.\nEditor: fix typo in retry-button option \"Retry with original workflow\".\nUpdate permission for showing workflow caller policy.\nUpdate pnpm-lock to fix build.\nContributors\nDaemonxiao\nKirill\nRicardo Duarte\nn8n@0.207.1\nView the commits for this version.\nRelease date: 2022-12-13\nThis is a bug fix release. It resolves an issue with undo.\nn8n@0.207.0\nView the commits for this version.\nRelease date: 2022-12-12\nThis release adds support for undo/redo actions on the canvas, and includes bug fixes.\nNew features\n#### Undo/redo\nYou can now undo and redo actions on the canvas.\nUse ctrl/cmd + z to undo, ctrl/cmd + shift + z to redo.\nCurrently, n8n supports undo/redo for the following canvas actions:\n- Adding nodes\n- Deleting nodes\n- Adding connections\n- Deleting connections\n- Moving nodes\n- Moving connections\n- Import workflow (from file/from URL)\n- Copy/paste nodes\n- Renaming nodes\n- Duplicating nodes\n- Disabling/enabling nodes\nApp integration actions are now displayed in the nodes pane.\nAdd sharing permissions info for workflow sharees.\nHandle sharing features when the user skips instance owner setup.\nUpdate the credential test error message for credential sharees.\nBug fixes\nCore: remove nodeGetter.\nCore: Increase workflow reactivation max timeout to one day.\nCore: Resolve an issue listing executions with Postgres.\nCore: Remove foreign credentials when copying nodes or duplicating workflow.\nCore: upgrade sse-channel to mitigate CVE-2019-10744.\nCore: use license-sdk v1.6.1.\nEditor: avoid adding Manual Trigger node when webhook node is added.\nEditor: fix credential sharing issues handler when no matching ID or name.\nEditor: fix for broken tab navigation.\nEditor: schema view shows checkbox in case of empty data.\nEditor: Stop returning UNKNOWN ERROR in the response if an actual error message is available.\nEditor: update duplicate workflow action.\nMove Binary Data Node: stringify objects before encoding them in MoveBinaryData.\nSplit In Batches Node: fix issue with pairedItem.\nn8n@0.206.1\nView the commits for this version.\nRelease date: 2022-12-06\nThis is a bug fix release.\nBug fixes\nCore: make expression resolution improvements.\nEditor: schema unit test stub for Font Awesome icons.\nRemove unnecessary console message.\nn8n@0.206.0\nView the commits for this version.\nRelease date: 2022-12-06\nThis release contains bug fixes, node enhancements, and a new node input view: schema view.\nNew features\n#### Schema view\nSchema view is a new node input view. It helps you browse the structure of your data, using the first input item.\nCore: add workflow execution statistics.\nEditor: add the alert design system component.\nEditor: fix checkbox line hight and make checkbox label clickable.\nNodes: add a message for read-only nodes.\nNodes: add a prompt to overwrite changes when concurrent editing occurs.\nNode enhancements\nKoBo Toolbox node: add support for the media file API.\nBug fixes\nCore: fix linter error.\nCore: fix partial execution with pinned data on child node run.\nCore: OAuth2 scopes now save.\nEnable source-maps on WorkflowRunnerProcess in own mode.\nHandle error when workflow doesn'texist or is inaccessible.\nMake nodes.exclude and nodes.include work with lazy-loaded nodes.\nCode Node: restore pairedItem to required n8n item keys.\nExecute Workflow Node: update Execute Workflow node info notice text.\nGmail Trigger Node: trigger node missing some emails.\nLocal File Trigger Node: fix issue that causes a crash if the ignore field is empty.\nContributors\nMarcel\nYann Jouanique\nn8n@0.205.0\nView the commits for this version.\nRelease date: 2022-12-02\nThis release contains an overhaul of the expressions editor, node enhancements, and bug fixes.\nNew features\n#### Expressions editor usability overhaul\nThis release contains usability enhancements for the expressions editor. The editor now includes color signals to indicate when syntax is valid or invalid, and better error messages and tips.\nNode enhancements\nFacebook Graph APInode: update to support API version 15.\nGoogle Calendar node: introduce the resource locator component to help users retrieve calendar parameters.\nPostmark Trigger node: update credentials so they can be used with the HTTP Request node (for custom API calls).\nTodoist node: update to use API version 2.\nBug fixes\nCore: ensure executions list is properly filtered for all users.\nCore: fix $items().length in Execute Once mode.\nCore: mark binary data to be deleted when pruning executions.\nCore: OAuth2 scope saved to database fix.\nEditor: fix slots rendering of NodeCreator's NoResults component.\nEditor: JSON view values can be mapped like keys.\nAWS SNS Node: fix a pagination issue.\nGoogle Sheets Node: fix exception if no matching rows are found.\nGoogle Sheets Node: fix for append operation if no empty rows in sheet.\nMicrosoft Outlook Node: fix binary attachment upload.\nPipedrive Node: resolve properties not working.\nLazy load nodes for credentials testing.\nCredential overwrites should take precedence over credential default values.\nRemove background for resource ownership selector.\nUpdate padding for resource filters dropdown.\nUpdate size of select components in filters dropdown.\nUpdate workflow save button type and design and share button type.\nn8n@0.204.0\nView the commits for this version.\nRelease date: 2022-11-24\nThis release contains performance enhancements and bug fixes.\nNew features\nCore: lazy-load nodes and credentials to reduce baseline memory usage.\nCore: use longer stack traces when error reporting is enabled.\nDev: add credentials E2E test suite and page object.\nBug fixes\nCore: fix $items().length behavior in executeOnce mode.\nCore: fix for unused imports.\nCore: use CredentialsOverwrites when testing credentials.\nCore: disable workflow locking due to issues.\nEditor: fix for missing node connections in dev environment.\nEditor: fix missing resource locator component.\nEditor: prevent node-creator tabs from showing when toggled by CanvasAddButton.\nEditor: table view column limit tooltip.\nEditor: fix broken n8n-info-tip slots.\nIF Node: fix \"Is Empty\" and \"Is Not Empty\" operation failures for date objects.\nRemove redundant await in nodes API request functions without try/catch.\nSchedule Trigger Node: fixes inconsistent behavior with cron and weekly intervals.\nWorkflow activation shouldn't crash if one of the credential is invalid.\nn8n@0.203.1\nView the commits for this version.\nRelease date: 2022-11-18\nThis is a bug fix release. It resolves an issue with the Google Sheets node versioning.\nn8n@0.203.0\nView the commits for this version.\nRelease date: 2022-11-17\nThis release includes an overhaul of the Google Sheets node, as well as other new features, node enhancements, and bug fixes.\nNew features\nAdd duplicate workflow error handler.\nAdd workflow data reset action.\nAdd credential runtime checks and prevent tampering during a manual run.\nNode enhancements\nCompare Datasets: UI copy changes to improve usability.\nGoogle Sheets: n8n has overhauled this node, including improved lookup for document and sheet selection.\nNotion (beta) node: use the resource locator component for database and page parameters.\nBug fixes\nCore: deduplicate error handling in nodes.\nEditor: show back mapping hint when parameter is focused.\nEditor: add Stop execution button to execution preview.\nEditor: curb direct item access linting.\nEditor: fix expression editor variable selector filter.\nEditor: fix for execution retry dropdown not closing.\nEditor: fix for logging error on user logout.\nEditor: fix zero treated as missing value in resource locator.\nEditor: hide pin data in production executions.\nEditor: skip optional chaining operators in Code Node editor linting.\nEditor: update to Expression/Fixed toggle - keep expression when switching to Fixed.\nEditor: fix foreign credentials being shown for new nodes.\nEditor: store copy of workflow in workflowsById to prevent node data bugs.\nEditor: fix user redirect to signin bug.\nn8n@0.202.1\nView the commits for this version.\nRelease date: 2022-11-10\nThis is a bug fix release. It removes some error tracking.\nn8n@0.202.0\nView the commits for this version.\nRelease date: 2022-11-10\nThis release contains core product improvements and bug fixes.\nNew features\nAPI: report unhandled app crashes using Sentry.\nAPI: set up error tracking using Sentry.\nCore: Add ownership, sharing and credential details to GET /workflows in n8n's internal API.\nEditor: when building nodes, you can now add a property with type notice to your credentials properties.This was previously available in nodes but not credentials. Refer to Node UI elements for more information.\nBug fixes\nAPI: Don't use names for type ORM connections.\nCore: Fix manual execution of pinned trigger on main mode.\nCore: Streamline multiple pinned triggers behavior.\nEditor: Curb argument linting for $input.first() and $input.last()\nEditor: Fix duplicate bug when new workflow is open.\nEditor: Fix for incorrect execution saving indicator in executions view.\nEditor: Fix for OAuth authorization.\nEditor: Fix workflow activation from the Workflows view.\nEditor: Fix workflow back button navigation.\nEditor: Prevent adding of the start node when importing workflow in the demo mode.\nEditor: Show string numbers and null properly in JSON view.\nEditor: Switch CodeNodeEditor linter parser to esprima-next.\nEditor: Tweak dragged mapping state.\nEditor: Update workflow buttons spacings.\nEditor: Use base path in workflow preview component URL.\nHTTP Request Node: Show error cause in the output.\nHTTP Request Node: Use the data in Put Output in Field field.\nHubSpot Node: Add notice to HubSpot credentials about API Key Sunset.\nNotion Trigger (Beta) Node: Fix Notion trigger polling strategy.\nRaindrop Node: Update access token URL.\nSendInBlue Trigger Node: Fix typo in credential name.\nUpdate E2E testing ENV variables.\nContributors\nfeelgood-interface\nUgo Bataillard\nn8n@0.201.0\nView the commits for this version.\nRelease date: 2022-11-02\nThis release contains workflow and node enhancements, and bug fixes.\nNew features\nCore: reimplement blocking workflow updates on interim changes.\nEditor: block the UI in node details view when the workflow is listening for an event.\nPerformance improvements\nNode enhancements\nVenafi TLS Protect Cloud node: make issuing template depend on application.\nBug fixes\nCore: fix wokflow hashing for MySQL.\nCore: make deepCopy backward compatible.\nEditor: ensure displayOptions received the value from the resource locator component.\nEditor: disable the settings link in executions view for unsaved workflows.\nEditor: ensure forms reliably save.\nEditor: fix issues with interim updates in executions view.\nEditor: fix for node creator search.\nEditor: limit columns in table view to prevent the UI becoming unresponsive in the node details view.\nn8n@0.200.1\nView the commits for this version.\nRelease date: 2022-10-28\nThis is a bug fix release.\nBug fixes\nAPI: do not reset the auth cookie on every request to GET /login.\nAWS SNS Trigger node: add missing jsonParse import.\nCore: avoid callstack with circular dependencies.\nEditor: resolve issues with the executions list auto-refresh, and with saving new workflows.\nEditor: redirect the outdated /workflow path.\nEditor: remove a filter that prevented display of running executions.\nn8n@0.200.0\nView the commits for this version.\nRelease date: 2022-10-27\nThis release contains improvements to the editor, node enhancements and bug fixes.\nNew features\nCore, editor: introduce workflow caller policy.\nCore: block workflow update on interim change.\nEditor: add a read-only state for nodes.\nEditor: add execution previews using the new Executions tab in the node view.\nEditor: improvements to node panel search.\nNode enhancements\nAirtable Trigger node: add the resource locator component.\nHTTP Request node: add options for raw JSON headers and queries.\nInvoiceNinja node: add support for V5.\nWrite Binary File node: add option to append to a file.\nBug fixes\nAPI: validate executions and workflow filter parameters.\nCore: amend typing for jsonParse() options.\nCore: fix predefinedCredentialType in node graph item.\nCore: fix canvas node execution skipping parent nodes.\nCore: fix single node execution failing in main mode.\nCore: set JWT authentication token sameSite policy to lax.\nCore: update to imports in helpers.\nEditor: curb item method linting in single-item mode.\nEditor: stop rendering expressions as HTML.\nEmail Trigger node: backport V2 mark-seen-after processing to V1.\nEmail Trigger node: improve connection handling and credentials.\nHTTP Request node: fix sending previously selected credentials.\nTheHive node: small fixes.\nContributors\nBram Kn\nNicholas Penree\nn8n@0.199.0\nView the commits for this version.\nRelease date: 2022-10-21\nThis release includes new nodes, an improved workflow UI, performance improvements, and bug fixes.\nNew features\n#### New workflow experience\nThis release brings a collection of UI changes, aimed at improving the workflow experience for users. This includes:\n* Removing the Start node, and adding help to guide users to find a trigger node.\n* Improved node search.\n* New nodes: Manual Trigger and Execute Workflow Trigger.\nCore: block workflow updates on interim changes.\nCore: enable sending client credentials in the body of API calls.\nEditor: add automatic credential selection for new nodes.\nNew nodes\n#### Compare node\nThe Compare Datasets node helps you compare data from two input streams. You can find documentation for the new node here.\n#### Execute Workflow Trigger node\nThe Execute Workflow Trigger starts a workflow in response to another workflow. You can find documentation for the new node here.\n#### Manual Trigger node\nThe Manual Trigger allows you to start a workflow by clicking **Execute Workflow**, without any option to run it automatically. You can find documentation for the new node here.\n#### Schedule Trigger node\nThis release introduces the Schedule Trigger node, replacing the Cron node. You can find documentation for the new node here.\nNode enhancements\nHubSpot node: you can now use your HubSpot credentials in the HTTP Request node to make a custom API call.\nRundeck node: you can now use your Rundeck credentials in the HTTP Request node to make a custom API call.\nBug fixes\nEditor: fix a hover bug in the bottom menu.\nEditor: resolve performance issues when opening a node, or editing a code node, with a large amount of data.\nEditor: ensure workflows always stop when clicking the stop button.\nEditor: fix a bug that was causing text highlighting when mapping data in Firefox.\nEditor: ensure correct linting in the Code node editor.\nEditor: handle null values in table view.\nElasticsearch node: fix a pagination issue.\nGoogle Drive node: fix typo.\nHTTP Request node: avoid errors when a response doesn't provide a content type.\nn8n node: fix a bug that was preventing the resource locator component from returning all items.\nContributors\nAndLLA\nNicholas Penree\nvcrwr\nn8n@0.198.2\nView the commits for this version.\nRelease date: 2022-10-14\nThis release fixes a bug affecting scrolling through parameter lists.\nn8n@0.198.1\nView the commits for this version.\nRelease date: 2022-10-14\nThis is a bug fix release.\nBug fixes\nEditor: change the initial position of the Start node.\nEditor: align JSON view properties with their values.\nEditor: fix BASE_PATH for Vite dev mode.\nEditor: fix data pinning success source.\nContributor\nBram Kn\nn8n@0.198.0\nView the commits for this version.\nRelease date: 2022-10-14\nNew features\nEditor: update the expressions display.\nEditor: update the n8n-menu component.\nNew nodes\n#### Code node\nThis release introduces the Code node. This node replaces both the Function and Function Item nodes. Refer to the Code node documentation for more information.\n#### Venafi TLS Protect Cloud Trigger node\nStart a workflow in response to events in your Venafi Cloud service.\nNode enhancements\nCitrix ADC node: add Certificate Install operation.\nKafka node: add a Use key option for messages.\nMySQL node: use the resource locator component for table parameters, making it easier for users to browse and select their database fields from within n8n.\nBug fixes\nCore, Editor: prevent overlap between running and pinning data.\nCore: expression evaluation of processes now respects N8N_BLOCK_ENV_ACCESS_IN_NODE.\nEditor: ensure the Axios base URL still works when hosted in a subfolder.\nEditor: fixes for horizontal scrollbar rendering.\nEditor: ensure the menu closes promptly when loading a credentials page.\nEditor: menu UI fixes.\nBox node: fix an issue that was causing the Create Folder operation to show extra items.\nGSuite Admin node: resolve issue that was causing the User Update operation to fail.\nGitLab Trigger node: ensure this node activates reliably.\nHTTP Request node: ensure OAuth credentials work properly with predefined credentials.\nKoboToolbox node: fix the hook logs.\nSeaTable node: ensure link items show in response.\nZoom node: resolve an issue that was causing missing output items.\nContributors\nJakob Backlund\nYan Jouanique\nn8n@0.197.1\nView the commits for this version.\nRelease date: 2022-10-10\nThis is a bug fix release. It resolves an issue with display width on the resource locator UI component.\nn8n@0.197.0\nView the commits for this version.\nRelease date: 2022-10-10\nThis release includes six new nodes, focused around infrastructure management. It also adds support for drag and drop data mapping in the JSON input view, and includes bug fixes.\nNew features\nCore: improve light versioning support in declarative node design.\nEditor UI: data mapping for JSON view. You can now map data using drag and drop from JSON view, as well as table view.\nNew nodes\n#### AWS Certificate Manager\nA new integration with AWS Certificate Manager. You can find the documentation here.\n#### AWS Elastic Load Balancing\nManage your AWS load balancers from your workflow using the new AWS Elastic Load Balancing node. You can find the documentation here.\n#### Citrix ADC\nCitrix ADC is an application delivery and load balancing solution for monolithic and microservices-based applications. You can find the documentation here.\n#### Cloudflare\nCloudflare provides a range of services to manage and protect your websites. This new node allows you to manage zone certificates in Cloudflare from your workflows. You can find the documentation here.\n#### Venafi nodes\nThis release includes two new Venafi nodes, to integrate with their Protect TLS service.\nNode enhancements\nCrypto node: add SHA3 support.\nBug fixes\nCLI: cache generated assets in a user-writeable directory.\nCore: prevent excess runs when data is pinned in a trigger node.\nCore: ensure hook URLs always added correctly.\nEditor: a fix for an issue affecting linked items in combination with data pinning.\nEditor: resolve a bug with the binary data view.\nGitHub Trigger node: ensure trigger executes reliably.\nMicrosoft Excel node: fix pagination issue.\nMicrosoft ToDo node: fix pagination issue.\nContributors\nStratos Theodorou\nn8n@0.196.0\nView the commits for this version.\nRelease date: 2022-09-30\nThis release includes major new features:\nBetter item linking\nNew built-in variables and methods\nA redesigned main navigation\nNew nodes, as well as an overhaul of the HTTP Request node\nIt also contains bug fixes and node enhancements.\nNew features\n#### Improved item linking\nIntroducing improved support for item linking (paired items). Item linking is a key concept in the n8n data flow. Learn more in Data item linking.\n#### Overhauled built-in variables\nn8n's built-in methods and variables have been overhauled, introducing new variables, and providing greater consistency in behavior and naming.\n#### Redesigned main navigation\nWe've redesigned the main navigation (the left hand menu) to create a simpler user experience.\nOther new features\nImproved error text when loading options in a node.\nOn reset, share unshared credentials with the instance owner.\nNew nodes\n#### n8n node\nThe n8n node allows you to consume the n8n API in your workflows.\n#### WhatsApp Business Platform node\nThe WhatsApp Business Platform node allows you to use the WhatsApp Business Platform Cloud API in your workflows.\nNode enhancements\nHTTP Request node: a major overhaul. It's now much simpler to build a custom API request. Refer to the HTTP Request node documentation for more information.\nRabbitMQ Trigger node: now automatically reconnects on disconnect.\nSlack node: add the 'get many' operation for users.\nBug fixes\nBuild: add typing for SSE channel.\nBuild: fix lint issue.\nCLI: add git to all Docker images\nCLI: disable X-Powered-By: Express header.\nCLI: disable CORS on SSE connections in production.\nCore: remove commented out lines.\nCore: delete unused dependencies.\nCore: fix and harmonize documentation links for nodes.\nCore: remove the --forceExit flag from CLI tests.\nEditor: add missing event handler to accordion component.\nEditor: fix Storybook setup.\nEditor: ensure BASE_URL replacement works correctly on Windows.\nEditor: fix parameter input field focus.\nEditor: make lodash aliases work on case-sensitive file systems.\nEditor: fix an issue affecting copy-pasting workflows into pinned data in the code editor.\nEditor: ensure the run data pagination selector displays when appropriate.\nEditor: ensure the run selector can open.\nEditor: tidy up leftover i18n references in the node view.\nEditor: correct an i18n string.\nEditor: resolve slow loading times for node types, node creators, and push connections in the settings view.\nNodes: update descriptions in the Merge node\nNodes: ensure the card ID property displays for completed checklists in the Trello node.\nNodes: fix authentication for the new verions of WeKan.\nNodes: ensure form names list correctly in the Wufoo Trigger node.\nContributors\nCristobal Schlaubitz Garcia\nn8n@0.195.5\nView the commits for this version.\nRelease date: 2022-09-23\nThis is a bug fix release. It fixes an issue with extracting values in expressions.\nn8n@0.195.4\nView the commits for this version.\nRelease date: 2022-09-22\nThis release:\nAdds the ability to resize the main node panel.\nResolves an issue with resource locator in expressions.\nn8n@0.195.3\nView the commits for this version.\nRelease date: 2022-09-22\nThis is a bug fix release.\nEditor: fix an expressions bug affecting numbers and booleans.\nAdded support for setting the TDS version in Microsoft SQL credentials.\nn8n@0.195.2\nView the commits for this version.\nRelease date: 2022-09-22\nThis is a bug fix release. It resolves an issue with MySQL migrations.\nn8n@0.195.1\nView the commits for this version.\nRelease date: 2022-09-21\nThis is a bug fix release. It resolves an issue with Postgres migrations.\nn8n@0.195.0\nView the commits for this version.\nRelease date: 2022-09-21\nThis release introduces user management and credential sharing for n8n's Cloud platform. It also contains other enhancements and bug fixes.\nNew features\n#### User management and credential sharing for Cloud\nThis release adds support for n8n's existing user management functionality to Cloud, and introduces a new feature: credential sharing. Credential sharing is currently only available on Cloud.\nAlso in this release:\nAdded a resourceLocator parameter type for nodes, and started upgrading n8n's built-in nodes to use it. This new option helps users who need to specify the ID of a record or item in an external service. For example, when using the Trello node, you can now search for a specific card by ID, URL, or do a free text search for card titles. Node builders can learn more about working with this new UI element in n8n's UI elements documentation.\nCache npm dependencies to improve performance on self-hosted n8n\nBug fixes\nBox node: fix an issue that sometimes prevented response data from being returned.\nCLI: prevent n8n from crashing when it encounters an error in poll method.\nCore: prevent calls to constructor, to forbid arbitrary code execution.\nEditor: fix the output panel for Wait node executions.\nHTTP node: ensure instance doesn't crash when batching enabled.\nPublic API: corrections to the OAuth schema.\nXero node: fix an issue that was causing line amount types to be ignored when creating new invoices.\nContributors\nIkko Ashimine\nn8n@0.194.0\nView the commits for this version.\nRelease date: 2022-09-15\nThis release includes new nodes: a Gmail trigger, Google Cloud Storage, and Adalo. It also contains major overhauls of the Gmail and Merge nodes.\nNew features\nCLI: load all nodes and credentials code in isolation.\nCore, Editor UI: introduce support for node deprecation.\nEditor: implement HTML sanitization for Notification and Message components.\nEditor: display the input number on multi-input nodes.\nNew nodes\n#### Adalo\nAdalo is a low code app builder. Refer to n8n's Adalo node documentation for more information.\n#### Google Cloud Storage\nn8n now has a Google Cloud Storage node.\n#### Gmail Trigger\nn8n now has a Gmail Trigger node. This allows you to trigger workflows in response to a Gmail account receiving an email.\nNode enhancements\nGmail node: this release includes an overhaul of the Gmail node, with updated resources and operations.\nMerge node: a major overhaul. Merge mode's have new names, and have been simplified. Refer to the Merge node documentation to learn more.\nMongoDB node: updated the Mongo driver to 4.9.1.\nBug fixes\nCLI: core: address Dependabot warnings.\nCLI: avoid scanning unnecessary directories on Windows.\nCLI: load nodes and directories on Windows using the correct file path.\nCLI: ensure password reset triggers internal and external hooks.\nCLI: use absolute paths for loading custom nodes and credentials.\nCore: returnJsonArray helper no longer breaks nodes that return no data.\nCore: fix an issue with node renaming and expressions.\nCore: update OAuth endpoints to use the instance base URL.\nNodes: resolved an issue that was preventing versioned nodes from loading.\nPublic API: better error handling for bad requests.\nAWS nodes: fixed an issue with credentials testing.\nGoogleBigQuery node: fix for empty responses when creating records.\nHubSpot node: correct the node name on the canvas.\nContributors\nRhys Williams\nn8n@0.193.5\nView the commits for this version.\nRelease date: 2022-09-07\nThis is a bug fix release.\nBug fixes\nEditor: prevent editing in the Function nodes in executions view.\nEditor: ensure button widths are correct.\nEditor: fix a popup title.\nGmail node: fix an issue introduced due to incorrect automatic data formatting.\nn8n@0.193.4\nView the commits for this version.\nRelease date: 2022-09-06\nThis release contains new features that lay the groundwork for upcoming releases, and bug fixes.\nNew features\nIt's now possible to configure the stop time for workers.\nCLI: Added external hooks for when members are added or deleted.\nEditor: Use the i18n component for localization (replacing v-html)\nBug fixes\nCLI: include \"auth-excluded\" endpoints on the history middleware as well.\nCore: fix MySQL migration issue with table prefix.\nCorrect spelling.\nFix n8n-square-button import.\nAWS nodes: handle query string and body properly for AWS related requests.\nAWS Lambda node: fix JSON data being sent to AWS Lambda as string.\nBeeminder node: fix request ID not being sent when creating a new data point.\nGitHub node: fix binary data not being returned.\nGraphQL node: fix issue with return items.\nPostgres node: fix issue with Postgres insert and paired item.\nKafka Trigger node: fix Kafka trigger not working with default max requests value.\nMonicaCrm node: fix pagination when using return all.\nGmail node: fix bug related to paired items.\nRaindrop node: fix issue refreshing OAuth2 credentials.\nShopify node: fix pagination when empty fields are sent.\nContributors\nAaron Delasy\nruanjiefeng\nn8n@0.193.3\nView the commits for this version.\nRelease date: 2022-09-01\nThis release contains bug fixes and node enhancements.\nNode enhancements\nMongoDB node: add credential testing and two new operations.\nBug fixes\nCLI: only initialize the mailer if the connection can be verified.\nCore: fix an issue with disabled parent outputs in partial executions.\nNodes: remove duplicate wrapping of paired item data.\nn8n@0.193.2\nView the commits for this version.\nRelease date: 2022-09-01\nThis is a bug fix release. It resolves an issue that was causing errors with OAuth2 credentials.\nn8n@0.193.1\nView the commits for this version.\nRelease date: 2022-08-31\nThis is a bug fix release. It resolves an issue that was preventing column headings from displaying correctly in the editor.\nn8n@0.193.0\nView the commits for this version.\nRelease date: 2022-08-31\nThis release contains a new node, feature enhancements, and bug fixes.\nNew nodes\nThis release adds an integration for HighLevel, an all-in-one sales and marketing platform.\nEnhancements\nDocker: reduce the size of Alpine Docker images.\nEditor: improve mapping tooltip behavior.\nBug fixes\nCore: make digest auth work with query parameters.\nEditor: send data as query on DELETE requests.\nFix credentials_entity table migration for MySQL.\nImprove .npmignore to reduce the size of the published packages.\nContributors\npemontto\nTzachi Shirazi\nn8n@0.192.2\nView the commits for this version.\nRelease date: 2022-08-25\nThis is a bug fix release.\nBug fixes\nEditor: fix the feature flag check when PostHog is unavailable.\nEditor: fix for a mapping bug that occured when value is null.\nn8n@0.192.1\nView the commits for this version.\nRelease date: 2022-08-25\nThis is a bug fix release.\nBug fixes\nAccount for non-array types in pinData migration.\nn8n@0.192.0\nView the commits for this version.\nRelease date: 2022-08-24\nThis release contains new features and enhancements, as well as bug fixes.\nNew features\n#### Map nested fields\nn8n@0.187.0 saw the first release of data mapping, allowing you to drag and drop top level data from a node's **INPUT** panel into parameter fields. With this release, you can now drag and drop data from any level.\nCore and editor: support pairedItem for pinned data.\nCore and editor: integrate PostHog.\nCore: add a command to scripts making it easier to launch n8n with tunnel.\nCLI: notify external hooks about user profile and password changes.\nBug fixes\nCore: account for the enabled state in the first pinned trigger in a workflow.\nCore: fix pinned trigger execution.\nCLI: handle unparseable strings during JSON key migration.\nCLI: fix the excessive instantiation type error for flattened executions.\nCLI: initiate the nodes directory to ensure npm install succeeds.\nCLI: ensure tsc build errors also cause Turbeorepo builds to fail.\nNextcloud node: fix an issue with credential verification.\nFreshdesk node: fix an issue where the getAll operation required non-existant options.\nn8n@0.191.1\nView the commits for this version.\nRelease date: 2022-08-19\nThis is a bug fix release. It resolves an issue that was causing node connectors to disappear after a user renamed them.\nn8n@0.191.0\nView the commits for this version.\nRelease date: 2022-08-17\nThis release lays the groundwork for wider community nodes support. It also includes some bug fixes.\nNew features\nCommunity nodes are now enabled based on npm availability on the host system. This allows n8n to introduce community nodes to the Desktop edition in a future release.\nImproved in-app guidance on mapping data.\nBug fixes\nCLI: fix the community node tests on Postgres and MySQL.\nCore: fix an issue preventing child workflow executions from displaying.\nEditor: handle errors when opening settings and executions.\nEditor: improve expression and parameters performance.\nPublic API: fix executions pagination for n8n instances using Postgres and MySQL.\nn8n@0.190.0\nView the commits for this version.\nRelease date: 2022-08-10\nThis is a bug fix release.\nBug fixes\nCore: fix a crash caused by parallel calls to test webhooks.\nCore: fix an issue preventing static data being saved for poll triggers.\nPublic API: fix a pagination issue.\nGitHub Trigger: typo fix.\nContributors\nNathan Poirier\nn8n@0.189.1\nView the commits for this version.\nRelease date: 2022-08-05\nThis is a bug fix release.\nBug fixes\nFixed an issue with MySQL and MariaDB migrations.\nn8n@0.189.0\nView the commits for this version.\nRelease date: 2022-08-03\nThis release includes a new node, Sendinblue, as well as bug fixes.\nNew nodes\nSendinblue node and Sendinblue Trigger node: introducing n8n's Sendinblue integration.\nNode enhancements\nNocoDB node: add support for v0.90.0+\nBug fixes\nEditor: fix a label cut off.\nFix an issue with saving workflows when tags are disabled.\nEnsure support for community nodes on Windows.\nContributors\nmertmit\nNicholas Penree\nn8n@0.188.0\nView the commits for this version.\nRelease date: 2022-07-27\nThis release contains a new node for Metabase, bug fixes, and node and product enhancements.\nNew nodes\n#### Metabase\nThis release includes a new Metabase node. Metabase is a business data analysis tool.\nEnhancements\nThis release includes improvements to n8n's core pairedItems functionality.\nNode enhancements\nItem Lists node: add an operation to create arrays from input items.\nKafka Trigger node: add more option fields.\nBug fixes\nCore: add Windows support to import:credentials --separate.\nEditor: correct linking buttons color.\nEditor: ensure data pinning works as expected when pinData is null.\nEditor: fix a bug with spaces.\nEditor: resolve an issue with sticky note duplication and positioning.\nEditor: restore missing header colors.\nAWS DynamoDB node: fix for errors with expression attribute names.\nMautic node: fix an authentication issue.\nRocketchat node: fix an authentication issue.\nContributors\nNicholas Penree\nn8n@0.187.2\nView the commits for this version.\nRelease date: 2022-07-21\nThis is a bug fix release.\nEditor: fix for a console issue.\nEditor: fix a login issue for non-admin users.\nEditor: fix problems with the credentials modal that occured when no node is open.\nNocoDB node: fix for an authentication issue.\nn8n@0.187.1\nView the commits for this version.\nRelease date: 2022-07-20\nThis release fixes a bug that was preventing new nodes from reliably displaying in all browsers.\nn8n@0.187.0\nView the commits for this version.\nRelease date: 2022-07-20\nThis release includes several major new features, including:\nThe community nodes repository: a new way to build and share nodes.\nData pinning and data mapping: accelerate workflow development with better data manipulation functionality.\nNew features\n#### Community nodes repository\nThis release introduces the community node repository. This allows developers to build and share nodes as npm packages. Users can install community-built nodes directly in n8n.\n#### Data pinning\nData pinning allows you to freeze and edit data during workflow development. Data pinning means saving the output data of a node, and using the saved data instead of fetching fresh data in future workflow executions. This avoids repeated API calls when developing a workflow, reducing calls to external systems, and speeding up workflow development.\n#### Data mapping\nThis release introduces a drag and drop interface for data mapping, as a quick way to map data without using expressions.\n#### Simplify authentication setup for node creators\nThis release introduces a simpler way of handling authorization when building a node. All credentials should now contain an authenticate property that dictates how the credential is used in a request.\nn8n has also simplified authentication types: instead of specifying an authentication type and using the correct interface, you can now set the type as \"generic\", and use the IAuthenticateGeneric interface.\nYou can use this approach for any authentication method where data is sent in the header, body, or query string. This includes methods like bearer and basic auth. You can't use this approach for more complex authentication types that require multiple calls, or for methods that don't pass authentication data. This includes OAuth.\nFor an example of the new authentication syntax, refer to n8n's Asana node.\nOther new features\nAdded a preAuthentication method to credentials.\nAdded more credentials tests.\nIntroduce automatic fixing for paired item information in some scenarios.\nNode enhancements\nERPNext node: add credential tests, and add support for unauthorized certs.\nGoogle Drive node: add support for move to trash.\nMindee node: support new version.\nNotion node: support ignoring the Notion URL property if empty.\nShopify node: add OAuth support.\nBug fixes\nAPI: add missing node settings parameters.\nAPI: validate static data value for resource workflow.\nBaserow Node: fix an issue preventing table names from loading.\nEditor: hide the Execute previous node button when in read-only mode.\nEditor: hide tabs if there's only one branch.\nRoundup of link fixes in nodes.\nContributors\nFlorian Bachmann\nOlivier Aygalenq\nn8n@0.186.1\nView the commits for this version.\nRelease date: 2022-07-14\nThis is a bug fix release. It includes a fix for an issue with the Airtable node.\nn8n@0.186.0\nView the commits for this version.\nRelease date: 2022-07-13\nThis release contains bug fixes and node enhancements.\nNew features\nAdd item information to more node errors.\nUpdate multiple credentials with tests, and add support for custom operations.\nNode enhancements\nAWS DynamoDB node: improve error handling and add an optional GetAll Scan FilterExpression.\nCustomer.io node: add support for tracking region selection.\nElasticsearch node: add 'Source Excludes' and 'Source Includes' options to the Document: getAll operation. Add credential tests, index pipelines, and index refresh.\nFreshworks CRM node: add search and lookup functionality.\nJIRA node: add optional query authentication.\nPostgres node: improve handling of large numbers.\nRedis node: add push and pop operations.\nRename node: add regex replace.\nSpreadsheet file node: allow skipping headers when writing spreadsheets.\nBug fixes\nEditor: Fix an error that occured after repeated executions.\nEmailReadImap node: improve handling of network problems.\nGoogle Drive node: process input items using the list operation.\nTelegram node: fix for a bug affecting sending binary data (images, documents and so on).\nContributors\nBryce Sheehan\nh4ux\nmiguel-mconf\nNicholas Penree\npemontto\nYann Jouanique\nn8n@0.185.0\nView the commits for this version.\nRelease date: 2022-07-05\nThis release adds a new node, Google Ads. It also contains bug fixes and node enhancements, as well as a small addition to core.\nNew features\nCore: add the action parameter to INodePropertyOptions. This parameter is now available when building nodes.\nNew nodes\nGoogle Ads node: n8n now provides a Google Ads node, allowing you to get data from Google Ad campaigns.\nNode enhancements\nDeepL node: Add support for longer text fields, and add credentials tests.\nFacebook Graph API node: Add support for Facebook Graph API 14.\nJIRA node: Add support for the simplified option with rendered fields.\nWebflow Trigger node: Reduce the chance of webhook duplication. Add a credentials test.\nWordPress node: Add a post template option.\nBug fixes\nHubSpot node: Fix for search endpoints.\nKoboToolbox node: Improve attachment matching logic and GeoJSON Polygon format.\nOdoo node: Prevent possible issues with some custom fields.\nSticky note node: Fix an issue that was causing the main header to hide.\nTodoist node: Improve multi-item support.\nContributors\ncgobrech\npemontto\nYann Jouanique\nZapfmeister\nn8n@0.184.0\nView the commits for this version.\nRelease date: 2022-06-29\nThis release includes:\nNew core features\nEnhancements to the Clockify node.\nBug fixes.\nNew features\nYou can now access getBinaryDataBuffer in the pre-send method.\nn8n now exposes the item index being processed by a node.\nMigrated the expressions templating engine to n8n's fork of riot-tmpl.\nNode enhancements\nClockify node: added three new resources: Client, User, and Workspace. Also added support for custom API calls.\nBug fixes\nCore: fixed an error with logging circular links in JSON.\nEditor UI: now display the full text of long error messages.\nEditor UI: fix for an issue with credentials rendering when the node has no parameters.\nCortex node: fix an issue preventing all analyzers being returned.\nHTTP Request node: ensure all OAuth2 credentials work with this node.\nLinkedIn node: fix an issue with image preview.\nSalesforce node: fix an issue that was causing the lead status to not use the new name when name is updated.\nFixed an issue with required/optional parameters.\nContributors\npemontto\nn8n@0.183.0\nView the commits for this version.\nRelease date: 2022-06-21\nThis release contains node enhancements and bug fixes, as well as an improved trigger nodes panel.\nNew features\nEnhancements to the Trigger inputs panel: When using a trigger node, you will now see an INPUT view that gives guidance on how to load data into your trigger.\nNode enhancements\nHubSpot node: you can now assign a stage on ticket update.\nTodoist node: it's now possible to move tasks between sections.\nTwake node: updated icon, credential test added, and added support for custom operations.\nBug fixes\nCore: don't allow OPTIONS requests from any source.\nCore: GET /workflows/:id now returns tags.\nCore: ensure predefined credentials show up in the HTTP Request node.\nCore: return the correct error message on Axios error.\nCore: updates to the expressions allow-list and deny-list.\nContributors\nBryce Sheehan\nRahimli Rahim\nn8n@0.182.1\nView the commits for this version.\nRelease date: 2022-06-16\nThis is a bug fix release. It resolves an issue with restarting waiting executions.\nn8n@0.182.0\nView the commits for this version.\nRelease date: 2022-06-14\nThis release contains enhancements to the Twilio and Wise integrations, and adds support for a new grant type for OAuth2. It also includes some bug fixes.\nNew features\nAdded support for the client_credentials grant type for OAuth2.\nNode enhancements\nTwilio node: added the ability to make a voice call using TTS.\nWise node: added support for downloading statements as JSON, CSV, or PDF.\nBug fixes\nCore: fixes an issue that was causing parameters to get lost in some edge cases.\nCore: fixes an issue with combined expressions not resolving if one expression was invalid.\nCore: fixed an issue that was causing the public API to fail to build on Windows.\nEditor: ensure errors display correctly.\nHTTP Request node: better handling for requests that return null.\nPipedrive node: fixes a limits issue with the GetAll operation on the Lead resource.\nPostbin node: remove a false error.\nContributors\nAlbrecht Schmidt\nErick Friis\nJoLo\nShaun\nValentin Mocanu\nn8n@0.181.2\nView the commits for this version.\nRelease date: 2022-06-09\nThis is a bug fix release. It resolves an issue that was sometimes causing nodes to error when they didn't return data.\nn8n@0.181.1\nView the commits for this version.\nRelease date: 2022-06-09\nThis is a bug fix release. It fixes two issues with multi-input nodes.\nn8n@0.181.0\nView the commits for this version.\nRelease date: 2022-06-08\nThis release introduces the public API.\nNew feature highlights\n#### The n8n public API\nThis release introduces the n8n public REST API. Using n8n's public API, you can programmatically perform many of the same tasks as you can in the GUI. The API includes a built-in Swagger UI playground. Refer to the API documentation for more information.\nOther new features\nCore: you can now block user access to environment variables using the N8N_BLOCK_ENV_ACCESS_IN_NODE variable.\nBug fixes\nCore: properly resolve expressions in declarative style nodes.\nn8n@0.180.0\nView the commits for this version.\nRelease date: 2022-06-07\nThis release adds a new node for Cal.com, support for tags in workflow import and export, UI improvements, node enhancements, and bug fixes.\nNew features\n#### Tags in workflow import and export\nWhen importing or exporting a workflow, the JSON can now include workflow tags.\n#### Improved handling of activation errors\nn8n now supports running an error workflow in response to an activation error.\nNew nodes\n#### Cal.com trigger\nThis release adds a new trigger node for Cal.com. Refer to the Cal Trigger documentation for more guidance.\nNode enhancements\nGitHub node: add the Get All operation to the Organization resource.\nQuickBooks node: add a new optional field for tax items.\nBug fixes\nRestore support for window in expressions.\nFix to the user-management:reset command.\nResolve crashes in queue mode.\nCorrect delete button hover spacing.\nResolve a bug causing stuck loading states.\nEmailReadImap node: improve error handling.\nHubSpot node: fix contact loading.\nContributors\nMark Steve Samson\nSyed Ali Shahbaz\nn8n@0.179.0\nView the commits for this version.\nRelease date: 2022-05-30\nThis release features a new node for PostBin, as well as various node enhancements and bug fixes.\nNew nodes\n#### PostBin node\nPostBin serves as a wrapper for standard HTTP libraries which can be used to test arbitrary API/Webhooks by sending requests and providing more advanced ways to analyze the responses.\nNode enhancements\nRabbitMQ Trigger node: Made message acknowledgement and parallel processing configurable.\nServiceNow node: Added support for attachments.\nTodoist node: Added support for specifying the parent task when adding and listing tasks.\nBug fixes\nCore: Fixed migrations on non-public Postgres schema.\nCore: Mitigated possible XSS vulnerability when importing workflow templates.\nEditor UI: fixed erroneous hover state detection close to the sticky note button.\nEditor UI: fixed display behavior of credentials assigned to versioned nodes.\nDiscord node: Fixed rate limit handling.\nGmail node: Fixed sending attachments in filesystem data mode.\nGoogle Sheets node: Fixed an error preventing the Use Header Names as JSON Paths option from working as expected.\nNextcloud node: Updated the node so the list:folder operation works with Nextcloud version 24.\nYouTube node: Fixed problem with uploading large files.\nn8n@0.178.2\nView the commits for this version.\nRelease date: 2022-05-25\nThis is a bug fix release. It solves an issue with loading parameters when making custom operations calls.\nn8n@0.178.1\nView the commits for this version.\nRelease date: 2022-05-24\nThis is a bug fix release. It solves an issue with setting credentials in the HTTP Request node.\nn8n@0.178.0\nView the commits for this version.\nRelease date: 2022-05-24\nThis release adds support for reusing existing credentials in the HTTP Request node, making it easier to do custom operation with APIs where n8n already has an integration.\nThe release also includes improvements to the nodes view, giving better detail about incoming data, as well as some bug fixes.\nNew features\n#### Credential reuse for custom API operations\nn8n supplies hundreds of nodes, allowing you to create workflows that link multiple products. However, some nodes don't include all the possible operations supported by a product's API. You can work around this by making a custom API call using the HTTP Request node.\nOne of the most complex parts of setting up API calls is managing authentication. To simplify this, n8n now provides a way to use existential credential types (credentials associated with n8n nodes) in the HTTP Request node.\nFor more information, refer to Custom API operations.\n#### Node details view\nAn improved node view, showing more detail about node inputs.\nNode enhancements\nSalesforce Node: Add the Country field.\nBug fixes\nEditor UI: don't display the dividing line unless necessary.\nEditor UI: don't display the 'Welcome' sticky in template workflows.\nSlack Node: Fix the kick operation for the channel resource.\nn8n@0.177.0\nView the commits for this version.\nRelease date: 2022-05-17\nThis release contains node enhancements, an improved welcome experience, and bug fixes.\nNew features\n#### Improved welcome experience\nA new introductory video, automatically displayed for new users.\n#### Automatically convert Luxon dates to strings\nn8n now automatically converts Luxon DateTime objects to strings.\nNode enhancements\nGoogle Drive Node: Drive upload, delete, and share operations now support shared Drives.\nMicrosoft OneDrive: Add the rename operation for files and folders.\nTrello: Add support for operations relating to board members.\nBug fixes\ncore: Fix call to /executions-current with unsaved workflow.\ncore: Fix issue with fixedCollection having all default values.\nEdit Image Node: Fix font selection.\nGhost Node: Fix post tags and add credential tests.\nGoogle Calendar Node: Make it work with public calendars and clean up.\nKoBoToolbox Node: Fix query and sort + use question name in attachments.\nMailjet Trigger Node: Fix issue that node couldn't get activated.\nPipedrive Node: Fix resolve properties when using multi option field.\nContributors\nCristobal Schlaubitz Garcia\nYann Jouanique\nn8n@0.176.0\nView the commits for this version.\nRelease date: 2022-05-10\nThis release contains bug fixes and node enhancements.\nNode enhancements\nPipedrive node: adds support for filters to the Organization: Get All operation.\nPushover node: adds an HTML formatting option, and a credential test.\nUProc node: adds new tools.\nBug fixes\ncore: a fix for filtering the executions list by waiting status.\ncore: improved webhook error messages.\nEdit Image node: node now works correctly with the binary-data-mode 'filesystem'.\nContributors\nAlbert Kiskorov\nMiquel Colomer\nn8n@0.175.1\nView the commits for this version.\nRelease date: 2022-05-03\nThis is a bug fix release.\nBug fixes\nFixes a bug in the editor UI related to node versioning.\nn8n@0.175.0\nView the commits for this version.\nRelease date: 2022-05-02\nThis release adds support for node versioning, along with node enhancements and bug fixes.\nNew features\n#### Node versioning\n0.175.0 adds support for a lightweight method of node versioning. One node can contain multiple versions, allowing small version increments without code duplication. To use this feature, change the version parameter in your node to an array, and add your version numbers, including your existing version. You can then access the version parameter with @version in your displayOptions (to control which version n8n displays). You can also query the version in your execute function using const nodeVersion = this.getNode().typeVersion;.\nNode enhancements\nGoogle Sheets node: n8n now handles header names formatted as JSON paths.\nMicrosoft Dynamics CRM node: add support for regions other than North America.\nTelegram node: add support for querying chat administrators.\nBug fixes\ncore: fixed an issue that was causing n8n to apply authentication checks, even when user management was disabled.\ncore: n8n now skips credentials checks for disabled nodes.\neditor: fix a bug affecting touchscreen monitors.\nHubSpot node: fix for search operators.\nSendGrid node: fixed an issue with sending attachments.\nWise node: respect the time parameter on get: exchangeRate.\nContributors\nJack Rudenko\nMC Naveen\nvcrwr\nn8n@0.174.0\nView the commits for this version.\nRelease date: 2022-04-25\nNew features\n#### Sticky Notes\nThis release adds Sticky Notes, a new feature that allows you to annotate and comment on your workflows. Refer to the Sticky Notes for more information.\nEnhancements\ncore: allow external OAuth connection. This enhancement adds support for connecting OAuth apps without access to n8n.\nAll AWS nodes now support AWS temporary credentials.\nGoogle Sheets node: Added upsert support.\nMicrosoft Teams node: adds several enhancements:\nAn option to limit groups to \"member of\", rather than retrieving the whole directory.\nAn option to get all tasks from a plan instead of just a group member.\nAutocompletion for plans, buckets, labels, and members in update fields for tasks.\nMongoDB node: you can now parse dates using dot notation.\nBug fixes\nCalendly Trigger node: updated the logo.\nMicrosoft OneDrive node: fixed an issue that was preventing upload of files with special characters in the file name.\nQuickBooks node: fixed a pagination issue.\nContributors\nBasit Ali\nCody Stamps\nLuiz Eduardo de Oliveira\nOliver Trajceski\npemontto\nRyan Goggin\nn8n@0.173.1\nView the commits for this version.\nRelease date: 2022-04-19\nFixes a bug with the Discord node icon name.\nn8n@0.173.0\nView the commits for this version.\nRelease date: 2022-04-19\nNew nodes\n#### Markdown node\nMarkdown node: added a new Markdown node to convert between Markdown and HTML.\nEnhancements\neditor: you can now drag and drop nodes from the nodes panel onto the canvas.\nNode enhancements\nDiscord node: additional fields now available when sending a message to Discord.\nGoogleBigQuery: added support for service account authentication.\nGoogle Cloud Realtime Database node: you can now select a region.\nPagerDuty node: now supports more detail in incidents.\nSlack node: added support for blocks in Slack message update.\nBug fixes\ncore: make the email for user management case insensitive.\ncore: add rawBody for XML requests.\neditor: fix a glitch that caused dropdowns to break after adding expressions.\neditor: reset text input value when closed with Esc.\nDiscourse node: fix an issue that was causing incomplete results when getting posts. Added a credentials test.\nZendesk Trigger node: remove deprecated targets, replace with webhooks.\nZoho node: fix pagination issue.\nContributors\nFlorian Metz\nFrancesco Pongiluppi\nMark Steve Samson\nMike Quinlan\nn8n@0.172.0\nView the commits for this version.\nRelease date: 2022-04-11\nEnhancements\nChanges to the data output display in nodes.\nNode enhancements\nMagento 2 Node: Added credential tests.\nPayPal Node: Added credential tests and updated the API URL.\nBug fixes\ncore: Luxon now applies the correct timezone. Refer to Luxon for more information.\ncore: fixed an issue with localization that was preventing i18n files from loading.\nAction Network Node: Fix a pagination issue and add credentials test.\nContributors\nPaolo Rechia\nn8n@0.171.1\nView the commits for this version.\nRelease date: 2022-04-06\nThis is a small bug fix release.\nBug fixes\ncore: fix issue with current executions not displaying.\ncore: fix an issue causing n8n to falsely skip some authentication.\nWooCommerce Node: Fix a pagination issue with the GetAll operation.\nn8n@0.171.0\nView the commits for this version.\nRelease date: 2022-04-03\nThis release focuses on bug fixes and node enhancements, with one new feature, and one breaking change to the GraphQL node.\nBreaking change to GraphQL node\nThe GraphQL node now errors when the response includes an error. If you use this node, you can choose to:\nDo nothing: a GraphQL response containing an error will now cause the workflow to fail.\nUpdate your GraphQL node settings: set Continue on Fail to true to allow the workflow to continue even when the GraphQL response contains an error.\nNew features\nYou can now download binary data from individual nodes in your workflow.\nEnhanced nodes\nEmelia Node: Add Campaign > Duplicate functionality.\nFTP Node: Add option to recursively create directories on rename.\nMautic Node: Add credential test and allow trailing slash in host.\nMicrosoft Teams Node: Add chat message support.\nMocean Node: Add 'Delivery Report URL' option and credential tests.\nServiceNow Node: Add basicAuth support and fix getColumns loadOptions.\nStrava Node: Add 'Get Streams' operation.\nBug fixes\ncore: Fix crash on webhook when last node did not return data\nEmailReadImap Node: Fix issue that crashed process if node was configured wrong.\nGoogle Tasks Node: Fix 'Show Completed' option and hide title field where not needed.\nNocoDB Node: Fix pagination.\nSalesforce Node: Fix issue that 'status' did not get used for Case => Create & Update\nContributors\nCharles Lecalier\nd3no\nKetan Somvanshi\nLuis Cipriani\npemontto\nRhys Williams\nn8n@0.170.0\nView the commits for this version.\nRelease date: 2022-03-27\nThis release focuses on bug fixes and adding functionality to existing nodes.\nEnhanced nodes\nCrypto Node: Add Generate operation to generate random values.\nHTTP Request Node: Add support for OPTIONS method.\nJira Node: Add Simplify Output option to Issue > Get.\nReddit Node: Add possibility to query saved posts.\nZendesk Node: Add ticket status On-hold.\nBug fixes\ncore: Add logs and error catches for possible failures in queue mode.\nAWS Lambda Node: Fix Invocation Type > Continue Workflow.\nSupabase Node: Send token also using Authorization Bearer; fix Row > Get operation.\nXero Node: Fix some operations and add support for setting address and phone number.\nWise Node: Fix issue when executing a transfer.\nContributors\nFFTDB\nFred\nJasper Zonneveld\npemontto\nSergio\nTheFSilver\nValentin Mocanu\nYassine Fathi\nn8n@0.169.0\nView the commits for this version.\nRelease date: 2022-03-20\nThis release includes:\nNew functionality for existing nodes\nA new node for Linear\nBug fixes\nAnd a license change!\nNew license\nThis release changes n8n's license, from Apache 2.0 with Commons Clause to Sustainable Use License.\nThis change aims to clarify n8n's license terms, and n8n's position as a fair-code project.\nRead more about the new license in License.\nNew nodes\nLinear Node: Add Linear Node.\nEnhanced nodes\nHTTP Request Node: Allow Delete requests with body.\nKoBoToolbox Node: Add KoBoToolbox Regular and Trigger Node.\nMailjet Node: Add credential tests and support for sandbox, JSON parameters & variables.\nMattermost Node: Add support for Channel search.\nOther improvements\nAdd support for reading IDs from file with executeBatch command.\nBug fixes\nGitHub node: Fix credential tests and File List operation.\nTelegram node: Fix sending binary data when disable notification is set.\nContributors\nManuel\nMarcin Kozey\nMatthew Walther\nYann Jouanique\nn8n@0.168.2\nFor a comprehensive list of changes, view the commits for this version.\nRelease date: 2022-03-16\nThis release contains an important bug fix for 0.168.0. Users on 0.168.0 or 0.168.1 should upgrade to this.\nn8n@0.168.1\nFor a comprehensive list of changes, view the commits for this version.\nRelease date: 2022-03-15\nA bug fix for user management: fixed an issue with email templates that was preventing owners from inviting members.\nn8n@0.168.0\nFor a comprehensive list of changes, view the commits for this version.\nRelease date: 2022-03-14\nNew feature: user management\nUser management in n8n allows you to invite people to work in your self-hosted n8n instance. It includes:\nLogin and password management\nAdding and removing users\nTwo account types: owner and member\nCheck out the user management documentation for more information.\nn8n@0.167.0\nFor a comprehensive list of changes, view the commits for this version.\nRelease date: 2022-03-13\nHighlights\nLuxon and JMESPath\n0.167.0 adds support for two new libraries:\nLuxon: a JavaScript library for working with date and time\nJMESPath: a query language for JSON\nYou can use Luxon and JMESPath in the code editor and in expressions.\nNew expressions variables\nWe've added two new variables to simplify working with date and time in expressions:\n$now: a Luxon object containing the current timestamp. Equivalent to DateTime.now().\n$today: a Luxon object containing the current timestamp, rounded down to the day. Equivalent to DateTime.now().set({ hour: 0, minute: 0, second: 0, millisecond: 0 }).\nNegative operations in If and Switch nodes\nMade it easier to perform negative operations on strings.\nThis release adds one new operation for numbers:\nIs Not Empty\nAnd the following new operations for strings:\nNot Ends With\nRegex Not Match\nNot Starts With\nIs Not Empty\nAdditionally, Regex is now labelled Regex Match.\nNew node: Redis Trigger\nAdded a Redis Trigger node, so you can now start workflows based on a Redis event.\nRedis Trigger: Added a Redis Trigger node.\nCore functionality\nAdded support for Luxon and JMESPath.\nAdded two new expressions variables, $now and $today.\nAdded more negative operations for numbers and strings.\nAdded a link to the course from the help menu.\nNodes\nFacebook Graph API: Added suport for Facebook Graph API 13.\nHubSpot: Added suport for private app token authentication.\nMongoDB: Added the aggregate operation.\nRedis Trigger: Added a Redis Trigger node.\nRedis: Added support for publish operations.\nStrapi: Added support for Strapi 4.\nWordPress: Added status as an option to getAll post requests.\nBugfixes\nThe Google Calendar node now correctly applies timezones when creating, updating, and scheduling all day events.\nFixed a bug that occasionally caused n8n to crash, or shut down workflows unexpectedly.\nYou can now use long credential type names with Postgres.\nContributors\nLuiz Eduardo de Oliveira Fonseca\nVitaliy Fratkin\nsol\nvcrwr\nFFTDB\nn8n@0.166.0\nFor a comprehensive list of changes, view the commits for this version.\nRelease date: 2022-03-08\nNew nodes\nOdoo\nEnhanced nodes\nFunction: Added support for items without a JSON key.\nCore functionality\nAdded new environment variable N8N_HIRING_BANNER_ENABLED to enable/disable the hiring banner.\nFixed a bug preventing keyboard shortcuts from working as expected.\nFixed a bug causing tooltips to be hidden behind other elements.\nFixed a bug causing some credentials to be hidden from the credentials list.\nBug fixes\nBaserow: Fixed a bug preventing the Sorting option of the Get All operation from working as expected.\nHTTP Request: Fixed a bug causing Digest Authentication to fail in some scenarios.\nWise: Fixed a bug causing API requests requiring Strong Customer Authentication (SCA) to fail.\nContributors\npemontto\nn8n@0.165.0\nFor a comprehensive list of changes, view the commits for this version.\nRelease date: 2022-02-28\nNew nodes\nOnfleet\nEnhanced nodes\nAsana: Added Create operation to the Project resource.\nMautic: Added Edit Contact Points, Edit Do Not Contact List, Send Email operations to Contact resource. Also added new Segment Email resource.\nNotion (Beta): Added support for rollup fields to the Simplify Output option. Also added the Parent ID to the Get All operation of the Block resource.\nPipedrive: Added Marketing Status field to the Create operation of the Person resource, also added User ID field to the Create and Update operations of the Person resource.\nCore functionality\nAdded support for workflow templates.\nFixed a bug causing credentials tests to fail for versioned nodes.\nFixed a build problem by addind dependencies @types/lodash.set to the workflow package and @types/uuid to the core package.\nFixed an error causing some resources to ignore a non-standard N8N_PATH value.\nFixed an error preventing the placeholder text from being shown when entering credentials.\nImproved error handling for telemetry-related errors.\nBug fixes\nOrbit: Fixed a bug causing API requests to use an incorrect workspace identifier.\nTheHive:  Fixed a bug causing the Ignore SSL Issues option to be applied incorrectly.\nContributors\nalexwitkowski, I√±aki Breinbauer, lsemaj, Luiz Eduardo de Oliveira Fonseca, Rodrigo Correia, Santiago Botero Ruiz, Saurabh Kashyap, Ugo Bataillard\nn8n@0.164.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-02-20\nCore Functionality\nFixed a bug preventing webhooks from working as expected in some scenarios.\nn8n@0.164.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-02-20\nNew nodes\nGoogle Chat\nEnhanced nodes\nGrist: Added support for self-hosted Grist instances.\nTelegram Trigger: Added new Extra Large option to Image Size field.\nWebhook: Added new No Response Body option. Also added support for DELETE, PATCH and PUT methods.\nCore Functionality\nAdded new database indices to improve the performance when querying past executions.\nFixed a bug causing the base portion of a URL not to be prepended as expected in some scenarios.\nFixed a bug cuasing expressions to resolve incorrectly when referencing non-existent nodes or parameters.\nContributors\nJhalter5Stones, Valentina Lilova, thorstenfreitag\nn8n@0.163.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-02-13\nCore Functionality\nFixed a bug preventing OAuth2 authentication from working as expected in some scenarios.\nn8n@0.163.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-02-13\nNew nodes\nHaloPSA\nLinear Trigger\nZammad\nEnhanced nodes\nGitHub: Added Reference option to the Get operation of the File resource.\nTwilio: Added Status Callbacks option.\nuProc: Sanitized Data Webhook field description.\nCore Functionality\nAdded automatic sorting by relative position to the node list inside the expression editor.\nAdded new /workflows/demo page to allow read-only rendering of workflows inside an iframe.\nAdded optional /healthz health check endpoint to worker instances.\nFixed unwanted list autofill behaviour inside the expression editor.\nImproved the GitHub actions used by the nightly Docker image.\nBug fixes\nFunction: Fixed a bug leaving the code editor size unchanged after resizing the window.\nFunction Item: Fixed a bug leaving the code editor size unchanged after resizing the window.\nIF: Removed the empty sections left after removing a condition.\nItem Lists: Fixed an erroneous placeholder text.\nContributors\nI√±aki Breinbauer, Manuel, pemontto\nn8n@0.162.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-02-06\nEnhanced nodes\nGitHub: Added new List operation to File resource.\nCore Functionality\nAdded configurable debug logging for telemetry.\nAdded support for defining nodes through JSON. This functionality is in alpha state and breaking changes to the interface can take place in upcoming versions.\nAdded telemetry support to page events occuring before telemetry is initialized.\nFixed a bug preventing errors in sub-workflows from appearing in parent executions.\nFixed a bug where node versioning would not work as expected.\nFixed a bug where remote parameters would not load as expected.\nFixed a bug where unkown node types would not work as expected.\nPrevented the node details view from opening automatically after duplicating a node.\nRemoved dependency fibers which is incompatible with the current LTS version 16 of Node.js.\nBug fixes\nXML: Fixed a bug causing the node to alter incoming data.\nContributors\npemontto\nn8n@0.161.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-02-01\nCore Functionality\nAdded optional debug logging to health check functionality.\nn8n@0.161.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-01-30\nCore Functionality\nAdded default polling interval for trigger nodes using polling.\nAdded support for additional hints below parameter fields.\nFixed a bug preventing default values from being used when testing credentials.\nImproved the wording in the Save your Changes? dialog.\nBug fixes\nAirtable: Improved field description.\nAirtable Trigger: Improved field description.\nerpNext: Prevented the node from throwing an error when no data is found.\nGmail: Fixed a bug causing the BCC field to be ignored.\nMove Binary Data: Fixed a bug causing the binary data to JSON conversion to fail when using filesystem-based binary data handling.\nSlack: Fixed a typo in the Type field.\nContributors\nfabian wohlgemuth\nn8n@0.160.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-01-22\nNew nodes\nBambooHR\nCore Functionality\nFixed a bug preventing the binary data preview from using the full available height and width.\nFixed a build problem by pinning chokidar version 3.5.2.\nPrevent workflow activation when no trigger is presentand introduced a modal explaining production data handling.\nFixed Filter by tags placeholder text used in the Open Workflow modal.\nBug fixes\nHTTP Request: Fixed a bug causing custom headers from being ignored.\nMautic: Fixed a bug preventing all items from being returned in some situations.\nMicrosoft OneDrive: Fixed a bug preventing more than 200 items from being returned.\nSpotify: Fixed a bug causing the execution to fail if there are more than 1000 search results, also fixed a bug preventing the Get New Releases operation of the Album resource from working as expected.\nContributors\nfabian wohlgemuth\nn8n@0.159.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-01-18\nCore Functionality\nTemporarily removed debug logging for Axios requests.\nn8n@0.159.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-01-16\nNew nodes\nJenkins\nEnhanced nodes\nGraphQL: Added support for additional authentication methods Basic Auth, Digest Auth, OAuth1, OAuth2, and Query Auth.\nCore Functionality\nAdded support for executing workflows without an ID through the CLI.\nFixed a build problem.\nFixed a bug preventing the tag description from being shown on the canvas.\nImproved build performance by skipping the node-dev package during build.\nBug fixes\nBox: Fixed a bug causing some files to be corrupted during download.\nPhilips Hue: Fixed a bug preventing the node from connecting to Philips Hue.\nSalesforce: Fixed a bug preventing filters on date and datetime fields from working as expected.\nSupabase: Fixed an errorneous documentation link.\nContributors\nPhil Clifford\nn8n@0.158.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-01-09\nNew nodes\nMicrosoft Graph Security\nSyncroMSP\nSupabase\nEnhanced nodes\nEdit Image: Added Transparent operation.\nKafka: Added Use Schema Registry option.\nKafka Trigger: Added Use Schema Registry option.\nRedis: Added database field to credentials.\nSalesforce: Added Account Number field.\nCore Functionality\nAdded new external hook when active workflows finished initializing.\nFixed a bug preventing the personalisation survey from showing up.\nImproved telemetry.\nBug fixes\nEdit Image: Fixed a bug causing two items to be returned.\niCalendar: Fixed a bug preventing dates in January from working as expected.\nMerge: Fixed causing empty binary data to overwrite other binary data on merge.\nContributors\nRicardo Georgel, Pierre, Vahid Sebto\nn8n@0.157.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-01-03\nCore Functionality\nFixed a bug where not all nodes could use the new binary data handling.\nn8n@0.157.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2022-01-02\nEnhanced nodes\nFunction: The node now prevents unsupported data from being returned.\nFunction Item: The node now prevents unsupported data from being returned.\nHubSpot: Added Engagement resource with Create, Delete, Get, and Get All operations.\nNotion (Beta): Upgraded the Notion node: Added Search operation for the Database resource, Get operation for Database Page resource, Archive operation for the Page resource. Also added Simplify Output option and test for credential validity.\nWait: Added new Ignore Bots option.\nWebhook: Added new Ignore Bots option.\nCore Functionality\nFixed a bug where a wrong number suffix was used after duplicating nodes.\nBug fixes\nHTTP Request: Fixed a bug where using Digest Auth would fail.\nContributors\npemontto\nn8n@0.156.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-12-25\nEnhanced nodes\nGitLab Trigger: Added new trigger events: Confidential Issue, Confidential Comment, Deployment, Release.\nGoogle Drive: Added support for downloading and converting native Google files.\nKitemaker: Added Space ID field to Create operation of Work Item resource.\nRaindrop: Added Parse Metadata option to Create, Update operations of the Bookmark resource.\nCore Functionality\nAdded execution ID to workflow.postExecute hook\nAdded response body to UI for failed Axios requests\nAdded support for automatically removing new lines from Google Service Account credentials\nAdded support for disabling the UI using environment variable\nFixed a bug causing the wrong expression result to be shown for items from an output other than the first\nImproved binary data management\nIntroduced Monaco as new UI code editor\nContributors\nArpad Gabor, Leo Lou, Manuel\nn8n@0.155.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-12-19\nCore Functionality\nAdded support for internationalization (i18n). This functionality is currently in alpha status and breaking changes are to be expected.\nn8n@0.154.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-12-19\nEnhanced nodes\nPlivo: Added user agent to all API requests.\nCore Functionality\nAllow deletion of nodes from the canvas using the backspace key\nFixed an issue causing clicks in the value survey to impact the main view\nFixed an issue preventing the update panel from closing\nBug fixes\nTodoist: Fixed a bug where using the additional field Due Date Time on the Task resource would cause the Create operation to fail.\nContributors\nMohammed Huzaif, –õ–µ–±–µ–¥–µ–≤ –ò–≤–∞–Ω\nn8n@0.153.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-12-11\nNew nodes\nFigma Trigger (Beta)\nWorkable Trigger\nEnhanced nodes\nGoogle Contacts: Added Query option to Get All operation, also prevented the node from failing when no contacts are found.\nHTTP Request: Added support for query-based authentication.\nHome Assistant: Added support for loading possible options in the Domain, Service, and Entity ID fields.\nOne Simple API: Added support for Social Profile resources.\nPagerDuty: Write scope is now requested upon authentication against the PagerDuty OAuth2 API.\nCore Functionality\nAdded frontend for value surveys\nFixed an issue preventing the recommendation logic from working as expected after selecting a work area\nFixed an issue where a wrong exit code was sent when running n8n on an unsupported version of Node.js\nFixed an issue where node options would disappear on hovering when a node isn't selected\nFixed an issue where the execution id was missing when running n8n in queue mode\nFixed an issue where execution data was missing when waiting for a webhook in queue mode\nImproved error handling when the n8n port is already in use\nImproved diagnostic events\nRemoved toast notification on webhook deletion, added toast notification after node is copied\nRemoved default trigger tooltip for polling trigger nodes\nBug fixes\nAPITemplate.io: Fixed a bug where the Create operation on the Image resource would fail when the Download option isn't enabled.\nHubSpot: Fixed authentication for new HubSpot applications by using granular scopes when authenticating against the HubSpot OAuth2 API.\nHubSpot Trigger: Fixed authentication for new HubSpot applications by using granular scopes when authenticating against the HubSpot Developer API.\nJira Software: Fixed an issue where the Reporter field would not work as expected on Jira Server instances.\nSalesforce: Fixed a typo preventing the value in the amount field of from being saved.\nContributors\npemontto, Jascha L√ºlsdorf, Jonathan Bennetts\nn8n@0.152.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-12-04\nNew nodes\nGoogle Calendar Trigger\nEnhanced nodes\nTelegram Trigger: Added support for downloading images to channel_post updates.\nCore Functionality\nAdded a plus (+) connector to end nodes\nAllowed opening workflows and executions in a new window when using Ctrl + Click\nEnforced type checking for all node parameters\nFixed a build issue in the custom n8n docker image\nFixed a memory leak in the UI which could occur when renaming nodes or navigate to another workflow\nImproved stability of internal test workflows\nImproved expression security\nIntroduced redirect to a new page and UI error message when trying to open a deleted workflow\nIntroduced support for multiple arguments when logging\nUpdated the onboarding survey\nBug fixes\nGoogle BigQuery: Fixed a bug preventing pagination from working as expected when the Return All option is enabled.\nRabbitMQ Trigger: Added Trigger to the name of the trigger node.\nSalesforce: Fixed a typo affecting the Type field of the Opportunity resource.\nContributors\nZvonimir Erdelja, m2scared\nn8n@0.151.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-11-26\nNew nodes\nDHL\nGrafana\nCore Functionality\nFixed a bug causing connections between nodes to disappear when renaming a newly added node after drawing a connection to its endpoints.\nFixed a build issue by adding TypeScript definitions for validator.js to CLI package, also fixed a linting issue by removing an unused import.\nImproved the waiting state of trigger nodes to explain when an external event is required.\nLoops are now drawn below their source node.\nBug fixes\nEdit Image: Fixed an issue preventing the Composite operation from working correctly in some cases.\nContributors\nJonathan Bennetts\nn8n@0.150.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-11-19\nEnhanced nodes\nJira Software: Added Components as an additional field.\nCore Functionality\nFixed a build issue by pinning rudder-sdk-node version 1.0.6 in CLI package.\nFixed an issue preventing the n8n import:workflow --separate CLI command from finding workflows on Windows.\nFurther improved the expression security.\nMoved all nodes into separate directories in preparation for internationalization.\nRemoving default headers for PUT and PATCH operations when using Axios.\nRevamped the workflow canvas.\nBug fixes\nHTTP Request: Fixed an issue causing the wrong Content-Type header to be set when downloading a file.\nServiceNow: Fixed incorrect mapping of incident urgency and impact values.\nStart: Fixed an issue causing the node to be disabled in a new workflow.\nXero: Fixed an issue causing the node to only fetch the first page when querying the Xero API.\nn8n@0.149.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-11-13\nNew nodes\nOne Simple API\nEnhanced nodes\nEdit Image: Added Circle Primitive to Draw operation. Also added Composite operation.\nZendesk: Added check for API credentials validity.\nZulip: Added additional field Role to the Update operation of the User resource.\nCore Functionality\nFixed an issue causing an error message to be thrown when executing a workflow through the CLI.\nImproved expression security by limiting the available process properties.\nImproved the behaviour of internal tests executed through the CLI.\nUpdated the owner of the node user's home directory in the custom docker image.\nBug fixes\nGoogle Tasks: Fixed an issue where the Due Date field had no effect (Update operation) or was unavailable (Create operation).\nHTTP Request: Fixed an issue where the Content-Length header was not calculated and sent when using the a Body Content Type of Form-Data Multipart.\nStripe Trigger: Fixed an issue preventing the node from being activated when a previously created webhook no longer exists.\nToggl Trigger: Updated the API URL used by the node.\nContributors\nGeylaniBerk, Jonathan Bennetts\nn8n@0.148.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-11-05\nNew nodes\nDropcontact\nRespond to Webhook\nEnhanced nodes\nLemlist: Added additional fields to Create operation of Lead resource.\nSlack: Added User Group resource.\nTodoist: Added Update operation to Task resource.\nWait: Improved descriptions of available Respond options.\nWooCommerce: Added password field to Crate operation of Customer resource.\nCore Functionality\nAdded a hook after workflow creation.\nFixed a build issue with npm v7 by overriding unwanted behaviour through the .npmrc file.\nFixed an issue preventing unknown node types from being imported.\nFixed an issue with the UI falsely indicating a credential can't be selected when using SQLite and multiple credentials with the same name exist.\nBug fixes\nStripe: Fixed an issue where setting additional Metadata fields would not have the expected effect. Also fixed an issue where pagination would not work as expected.\nZendesk: Fixed an issue preventing the additional field External ID from being evaulated correctly.\nContributors\nmizzimizzi, nikozila, Pauline\nn8n@0.147.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-11-03\nCore Functionality\nFixed a build issue by moving the chokidar dependency to a regular dependency.\nn8n@0.147.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-11-03\nNew nodes\nLocal File Trigger\nCore Functionality\nImproved the database migration process to reduce memory footprint.\nFixed an issue with telemetry by adding an anonymous ID.\nn8n@0.146.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-10-29\nNew nodes\nMicrosoft Dynamics CRM\nEnhanced nodes\nAgile CRM: Added Filters to Get All operation of Contact and Company resources.\nDate & Time: Ensuring the return values are always of type string.\nIF: Added support for moment types to Date & Time condition.\nCore Functionality\nAdded name and ID of a workflow to its settings.\nAdded parameter inputs to be multi-line.\nFixed an issue with declaring proxies when Axios is used.\nFixed an issue with serializing arrays and special characters.\nFixed an issue with updating expressions after renaming a node.\nBug fixes\nHTTP Request: Fixed an issue with the Full Response option not taking effect when used with the Ignore Response Code option.\nContributors\nValentina Lilova, Oliver Trajceski\nn8n@0.145.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-10-22\nNew nodes\nAWS Textract\nGoogle Drive Trigger\nEnhanced nodes\nBitbucket Trigger: Added check for credentials validity. Removed deprecated User and Team resources, and added the Workspace resource.\nGitHub: Added check for API credentials validity.\nHome Assistant: Added check for credentials validity.\nJira Software: Added check for credentials validity.\nMicrosoft OneDrive: Added functionality to create folder hierarchy automatically upon subfolder creation.\nPipedrive: Added All Users option to Get All operation of Activity resource.\nSlack: Increase the Slack default query limit from 5 to 100 in order to reduce number of requests.\nTwitter: Added Tweet Mode additional field to the Search operation of Tweet resource.\nCore Functionality\nChanged vm2 library version from 3.9.3 to 3.9.5.\nFixed an issue with ignoring the response code.\nFixed an issue with overwriting credentials using environment variables.\nFixed an issue with using query strings combined with the x-www-form-urlencoded content type.\nIntroduced telemetry.\nBug fixes\nJira Software: Fixed an issue with the Expand option for the Issue resource. Also fixed an issue with using custom fields on Jira Server.\nSlack: Fixed an issue with pagination when loading more than 1,000 channels.\nStrapi: Fixed an issue using the Where option of the Get All operation.\nWooCommerce: Fixed an issue where a wrong postcode field name was used for the Order resource.\nContributors\npemontto, rdd2, robertodamiani, Rodrigo Correia\nn8n@0.144.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-10-15\nEnhanced nodes\nNextcloud: Added Share operation to the File and Folder resources.\nZendesk: Added support for deleting, listing, getting, and recovering suspended tickets. Added the query option for regular tickets. Added assignee emails, internal notes, and public replies options to the update ticket operation.\nCore Functionality\nImproved the autofill behaviour on Google Chrome when entering credentials.\nBug fixes\nAirtable: Fixed an issue with the sort field.\nCron: Set the version of the cron library to 1.7.2.\nContributors\nJonathan Bennetts\nn8n@0.143.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-10-14\nEnhanced nodes\nPipedrive: Added support for getting activities from deal ID.\nFacebook Graph API: Added support for Facebook Graph API versions 11 and 12.\nCore Functionality\nFixed a build issue affecting a number of AWS nodes.\nChanged workflows to use credential ids primarily (instead of names), allowing users to have different credentials with the same name.\nBug fixes\nFTP: Fixed error when opening FTP/SFTP credentials.\nContributors\nRodrigo Correia\nn8n@0.142.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-10-07\nNew nodes\nStop and Error\nCore Functionality\nFixed overlapping buttons when viewing on mobile.\nFixed issue with partial workflow executions when Wait node was last.\nFixed issue with broken non-JSON requests.\nNode errors now only displayed for executing nodes, not disconnected nodes.\nAutomatic save when executing new workflows with Webhook node.\nFixed an issue with how arrays were serialized for certain nodes.\nFixed an issue where executions could not be cancelled when running in Main mode.\nDuplicated workflows now open in a new window.\nBug fixes\nHTTP Request: Fixed 'Ignore response code' flag.\nRundeck: Fixed issue with async loading of credentials.\nSeaTable: Fixed issue when entering a Baser URI with a trailing slash.\nContributors\nG√ºnther, Tom Klingenberg\nn8n@0.141.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-10-01\nCore Functionality\nFixed issue with body formatting of x-form-www-urlencoded requests.\nn8n@0.141.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-09-30\nNew nodes\nGrist\nSeaTable\nSeaTable Trigger\nurlscan.io\nCore Functionality\nPerformance improvements in Editor UI\nImproved error reporting\nContributors\nAlex Hall, Tom Klingenberg\nn8n@0.140.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-09-29\nNew nodes\nSplunk\nEnhanced nodes\nTelegram: Added binary data support to the Send Animation, Send Audio, Send Document, Send Photo, Send Video, and Send Sticker operations.\nCore Functionality\nFixed startup behavior when running n8n in scaled mode (i.e. skipWebhoooksDeregistrationOnShutdown is enabled).\nFixed behavior around handling empty response bodies.\nFixed an issue with handling of refresh tokens.\nContributors\npemontto\nn8n@0.139.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-09-23\nCore Functionality\nBug fixes and improvements for Editor UI.\nn8n@0.139.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-09-22\nNew nodes\nElastic Security\nMisp\nNetlify\nNetlify Trigger\nEnhanced nodes\nHubSpot Trigger: Authentication method changed to OAuth2.\nWait: Added improved status messages for Wait behavior.\nCore Functionality\nUpdated node design to include support for versioned nodes.\nBug fixes\nSendGrid: Fixed issue with adding contacts to lists.\nContributors\nMat√≠as Aguirre\nn8n@0.138.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-09-15\nNew nodes\nItem Lists\nMagento 2\nEnhanced nodes\nBaserow: Added the following filter options: Contains, Contains Not, Date Before Date, Date After Date, Filename Contains, Is Empty, Is Not Empty, Link Row Has, Link Row Does Not Have, Single Select Equal, and Single Select Not Equal.\nPipedrive: Added support for Notes on Leads.\nWeKan: Added Sort field to the Card resource.\nCore Functionality\nGeneral UX improvements to the Editor UI.\nFixed an issue with the PayloadTooLargeError.\nBug fixes\nLemlist: Fixed issue where events were not sent in the correct property.\nNotion: Fixed issue listed unnamed databases.\nContributors\nbramknuever, Chris Magnuson\nn8n@0.137.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-09-05\nNew nodes\nFreshservice\nEnhanced nodes\nClockify: Added Task resource.\nHubSpot: Added dropdown selection for Properties and Properties with History filters for Get All Deals operations.\nMautic: Added Campaign Contact resource.\nMongoDB: Added ability to query documents by '_id'.\nMQTT: Added SSL/TLS support to authentication.\nMQTT Trigger: Added SSL/TLS support to authentication.\nSalesforce: Added File Extension option to the Document resource. Added Type field to Task resource.\nSms77: Added Voice Call resource. Added the following options to SMS resource: Debug, Delay, Foreign ID, Flash, Label, No Reload, Performance Tracking, TTL.\nZendesk: Added Organization resource. Added Get Organizations and Get Related Data operations to User resource.\nCore Functionality\nAdded execution ID to logs of queue processes.\nAdded description to operation errors.\nAdded ability for webhook processes to wake waiting executions.\nBug fixes\nHubSpot: Fixed issue with 'RequestAllItems' API.\nWordPress: Fixed issue with 'RequestAllItems' API only returning the first 10 items.\nContributors\nAndr√© Matthies, DeskYT, Frederic Alix, Jonathan Bennetts, Ketan Somvanshi, Luiz Eduardo de Oliveira Fonseca, TheFSilver\nn8n@0.136.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-30\nEnhanced nodes\nNotion: Added handling of Rich Text when simplifying data.\nCore Functionality\nGeneral UI design improvements.\nImproved errors messages during debugging of custom nodes.\nAll packages upgraded to TypeScript 4.3.5, improved linting and formatting.\nBug fixes\nFTP: Fixed issue where incorrect paths were displayed when using the node.\nWait: Fixed issue when receiving multiple files using On Webhook Call operation.\nWebhook: Fixed issue when receiving multiple files.\nn8n@0.135.3\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-27\nCore Functionality\nFixed Canvas UI inconsistencies when duplicating workflows.\nAdded log message during upgrade to indicate database migration has started.\nGeneral improvements to parameter labels and tooltips.\nContributors\nKyle Mohr\nn8n@0.135.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-26\nCore Functionality\nAdded expression support for credentials.\nFixed performance issues when loading credentials.\nn8n@0.135.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-23\nCore Functionality\nFixed an issue where if n8n was shutdown during database migration while upgrading versions, errors would result upon next startup.\nn8n@0.135.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-22\nNew nodes\nForm.io Trigger\nFormstack Trigger\nWait\nCore Functionality\nIn-node method for accessing binary data is now asynchronous and a helper function for this has been implemented.\nCredentials are now loaded from the database on-demand.\nWebhook UUIDs are automatically updated when duplicating a workflow.\nFixed an issue when referencing values before loops.\nBug fixes\nInterval: Fixed issue where entering too large a value (> 2147483647ms) resulted in an interval of 1sec being used rather than an error.\nContributors\nAniruddha Adhikary, lublak, parthibanbalaji\nn8n@0.134.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-15\nEnhanced nodes\nAWS DynamoDB: Added Scan option to Item > Get All operation.\nGoogle Drive: Added File Name option to File > Update operation.\nMautic: Added the following fields to Company resource: Address, Annual Revenue, Company Email, Custom Fields, Description, Fax, Industry, Number of Employees, Phone, Website.\nNotion: Added Timezone option when inserting Date fields.\nPipedrive: Added the following Filters options to the Deal > Get All operation: Predefined Filter, Stage ID, Status, and User ID.\nQuickBooks: Added the Transaction resource and Get Report operation.\nCore Functionality\nIntegrated Nodelinter in n8n.\nFix to add a trailing slash (/) to all webhook URLs for proper functionality.\nBug fixes\nAWS SES: Fixed issue where special characters in the message were not encoded.\nBaserow: Fixed issue where Create operation inserted null values.\nHubSpot: Fixed issue when sending context parameter.\nContributors\ncalvintwr, CFarcy, Jeremie Dokime, Michael Hirschler, Rodrigo Correia, sol\nn8n@0.133.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-08\nNew nodes\nMonica CRM\nEnhanced nodes\nHTTP Request: Added Follow All Redirects option.\nSalesforce: Added Record Type ID field.\nCore Functionality\nFixed UI lag when editing large workflows.\nBug fixes\nNextcloud: Fixed issue where List operation on an empty Folder returned an error.\nSpotify: Fixed issues with pagination and infinite executions.\nContributors\nJacob Burrell, –õ–µ–±–µ–¥–µ–≤ –ò–≤–∞–Ω\nn8n@0.132.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-02\nBug fixes\nInterval: Fixed issue with infinite executions.\nContributors\n–õ–µ–±–µ–¥–µ–≤ –ò–≤–∞–Ω\nn8n@0.132.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-02\nCore Functionality\nChanged TypeORM version to 0.2.34\nn8n@0.132.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-08-01\nNew nodes\nFreshworks CRM\nGoogle Perspective\nMarketstack\nNocoDB\nEnhanced nodes\nFacebook Trigger: Added Fields parameter.\nGmail: Added Sender Name parameter.\nHome Assistant: Added Event resource.\nPipedrive: Added Deal Product resource.\nSalesforce: Added Document resource with Upload operation.\nWooCommerce: Added Customer resource.\nCore Functionality\nFixed an issue for large internal values.\nContributors\nEd Linklater, Rodrigo Correia\nn8n@0.131.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-07-24\nNew nodes\nWebex by Cisco\nWebex by Cisco Trigger\nEnhanced nodes\nPipedrive: Added Lead resource. Added Search operation to Organization resource.\nTaiga Trigger: Added Resource and Operations filters.\nCore Functionality\nAdded Continue-on-fail support to all nodes.\nAdded new version notifications.\nAdded Refresh List for remote options lists.\nAdded $position expression variable to return the index of an item within a list.\nBug fixes\nSpreadsheet File: Fixed issue when saving dates.\nContributors\nAnthr@x, Felipe Cecagno\nn8n@0.130.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-07-18\nNew nodes\nAWS DynamoDB\nElasticsearch\nServiceNow\nEnhanced nodes\nKafka Trigger: Added Read Messages From Beginning option.\nSalesforce: Added Sandbox Environment Type for OAuth2 credentials.\nTaiga: Added Epic, Task, and User Story operations.\nTheHive: Added Custom Fields option to the available Additional Fields.\nCore Functionality\nFixed an issue where failed workflows were displayed as \"running\".\nFixes issues with uncaught errors.\nBug fixes\nNotion: Fixed issue when filtering field data type.\nContributors\nMichael Hirschler, Mika Luhta, Pierre Lanvin\nn8n@0.129.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-07-12\nNew nodes\nBaserow\nBug fixes\nSSH: Fixed issue with access rights when downloading files.\nContributors\nJ√©r√©mie Pardou-Piquemal\nn8n@0.128.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-07-11\nNew nodes\nHome Assistant\nStripe\nEnhanced nodes\nHTTP Request: Added support for arrays in Querystring. Any parameter appearing multiple times with the same name is grouped into an array.\nMautic: Added Contact Segment resource.\nTelegram: Added Delete operation to the Message resource.\nCore Functionality\nPerformance improvement for loading of historical executions (> 3mil) when using Postgres.\nFixed error handling for unending workflows and display of \"unknown\" workflow status.\nFixed format of Workflow ID when downloading from UI Editor to enable compatibility with importing from CLI.\nBug fixes\nMicrosoft SQL: Fixed an issue with sending the connectionTimeout parameter, and creating and updating data using columns with spaces.\nContributors\nKaito Udagawa, Rodrigo Correia\nn8n@0.127.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-07-04\nEnhanced nodes\nAirtable: Added Bulk Size option to all Operations.\nBox: Added Share operation to File and Folder resources.\nSalesforce: Added Last Name field to Update operation on Contact resource.\nZoho CRM: Added Account, Contact, Deal, Invoice, Product, Purchase, Quote, Sales Order, and Vendor resources.\nCore Functionality\nAdded a workflow testing framework using a new CLI command to execute all desired workflows. Run n8n executeBatch --help for details.\nAdded support to display binary video content in Editor UI.\nBug fixes\nGoogle Sheets: Fixed an issue with handling 0 value that resulted in empty cells.\nSSH: Fixed an issue with setting passphrases.\nContributors\nflybluewolf, Kaito Udagawa\nn8n@0.126.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-29\nCore Functionality\nFixed issues with keyboard shortcuts when a modal was open.\nBug fixes\nMicrosoft SQL: Fixed an issue with handling of Boolean values when inserting.\nPipedrive: Fixed an issue with the node icon.\nn8n@0.126.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-27\nNew nodes\nAction Network\nGoogle Docs\nEnhanced nodes\nAWS S3: Added Delete operation to the Bucket Resource.\nGoogle Analytics: Added Dimension Filters to the available Additional Fields.\nHTTP Request: Added Split Into Items option.\nMQTT: Added mqqts protocol for MQTT credentials.\nQuickBooks: Added Purchase resource with Get and Get All operations.\nCore Functionality\nTemplates from the n8n Workflows page can now be directly imported by appending /workflows/templates/ to your instance base URL. For example, localhost:5678/workflows/templates/1142.\nAdded new Editor UI shortcuts. See Keyboard Shortcuts for details.\nFixed an issue causing console errors when deleting a node from the canvas.\nBug fixes\nGhost: Fixed an issue with the Get All operation functionality.\nGoogle Analytics: Fixed an issue that caused an error when attempting to sort with no data present.\nMicrosoft SQL: Fixed an issue when escaping single quotes and mapping empty fields.\nNotion: Fixed an issue with pagination of databases and users.\nContributors\ncalvintwr, Jan Baykara\nn8n@0.125.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-20\nEnhanced nodes\nSpotify: Added Search operation to Album, Artist, Playlist, and Track resources, and Resume and Volume operations to Player resource.\nCore Functionality\nImplemented new design of the Nodes Panel, adding categories and subcategories, along with improved search. For full details, see the commits.\nBug fixes\nMySQL: Fixed an issue where n8n was unable to save data due to collation, resulting in workflows ending with Unknown status.\nContributors\nAmudhan Manivasagam, Carlos Alexandro Becker, Kaito Udagawa\nn8n@0.124.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-16\nCore Functionality\nImproved error log messages\nFixed an issue where the tags got removed when deactivating the workflow or updating settings\nRemoved the circular references for the error caused by the request library\nn8n@0.124.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-13\nEnhanced nodes\nGoogle Drive: Added APP Properties and Properties options to the Upload operation of the File resource\nHTTP Request: Added the functionality to log the request to the browser console for testing\nNotion: Added the Include Time parameter date field types\nSalesforce: Added Upsert operation to Account, Contact, Custom Object, Lead, and Opportunity resources\nTodoist: Added the Description option to the Task resource\nCore Functionality\nImplemented the functionality to display the error details in a toast message for trigger nodes\nImproved error handling by removing circular references from API errors\nBug fixes\nJira: Fixed an issues with the API version and fixed an issue with fetching the custom fields for the Issue resource\nContributors\nJean M, romaincolombo-daily, Thomas Jost, Vincent\nn8n@0.123.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-06\nCore Functionality\nFixed a build issue for missing node icons\nn8n@0.123.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-06\nNew nodes\nGit\nMicrosoft To Do\nEnhanced nodes\nPipedrive: Added a feature to fetch data from the Pipedrive API, added Search operation to the Deals resource, and added custom fields option\nSpotify: Added My Data resource\nCore Functionality\nFixed issues with NodeViewNew navigation handling\nFixed an issue with the view crashing with large requests\nBug fixes\nASW Transcribe: Fixed issues with options\nContributors\nRodrigo Correia, Sam Roquitte\nn8n@0.122.3\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-04\nCore Functionality\nFixed error messages for the text area field\nAdded the missing winston dependency\nFixed an issue with adding values using the Variable selector. The deleted values don't reappear\nFixed an issue with the Error Workflows not getting executed in the queue mode\nBug fixes\nNotion: Fixed an issue with parsing the last edited time\nn8n@0.122.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-31\nEnhanced nodes\nFunction: Added console.log support for writing to browser console\nFunction Item: Added console.log support for writing to browser console\nCore Functionality\nFixed an issue that enables clicks on tags\nFixed an issue with escaping workflow name\nFixed an issue with selecting variables in the Expression Editor\nn8n@0.122.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-30\nCore Functionality\nFixed an issue with the order in migration rollback\nn8n@0.122.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-30\nNew nodes\nAWS Transcribe\nSSH\nUptimeRobot\nEnhanced nodes\nDeepL: Added support for Free API\nFunction: Added the functionality to log console.log messages to the browser console\nFunction Item: Added the functionality to log console.log messages to the browser console\nCore Functionality\nChanged bcrypt library from @node-rs/bcrypt to bcryptjs\nFixed an issue with optional parameters that have the same name\nAdded the functionality to tag workflows\nFixed errors in the Expression Editor\nFixed an issue with nodes that only get connected to the second input. This solves the issue of copying and pasting the workflows where only one output of the IF node gets connected to a node\nBug fixes\nGoogle Drive: Fixed an issue with the Drive resource\nNotion: Fixed an issue with the filtering fields type and fixed an issue with the link option\nSwitch: Fixed an issue with the Expression mode\nContributors\nAlexander Mustafin\nn8n@0.121.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-06-01\nCore Functionality\nFixed an issue with copying the output values\nFixed issues with the Expression Editor\nMade improvements to the Expression Editor\nn8n@0.121.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-20\nNew nodes\nNotion\nNotion Trigger\nEnhanced nodes\nGraphQL: Added Header Auth authentication method\nTwilio: Added API Key authentication method\nBug fixes\nHubSpot: Fixed an issue with pagination for Deals resource\nKeap: Fixed an issue with the data type of the Order Title field\nOrbit: Fixed an issue with the activity type in Post operation\nSlack: Fixed an issue with the Get Profile operation\nStrava: Fixed an issue with the paging parameter\nContributors\nJacob Spizziri\nn8n@0.120.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-17\nNew nodes\niCalendar\nEnhanced nodes\nGoogle Cloud Firestore: Added the functionality for GeoPoint parsing and added ISO-8601 format for date validation\nIMAP Email: Added the Force reconnect option\nPaddle: Added the Use Sandbox environment API parameter\nSpotify: Added the Position parameter to the Add operation of the Playlist resource\nWooCommerce: Added the Include Credentials in Query parameter\nCore Functionality\nAdded await to hooks to fix issues with the Unknown status of the workflows\nChanged the data type of the credentials_entity field for MySQL database to fix issues with long credentials\nFixed an issue with the ordering of the executions when the list is auto-refreshed\nAdded the functionality that allows reading sibling parameters\nBug fixes\nClockify Trigger: Fixed an issue that occurred when the node returned an empty array\nGoogle Cloud Firestore: Fixed an issue with parsing empty document, and an issue with the detection of date\nHubSpot: Fixed an issue with the Return All option\nContributors\nDeskYT, Daniel Lazaro, DerEnderKeks, mdasmendel\nn8n@0.119.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-09\nEnhanced nodes\nAWS Comprehend: Added the Detect Entities operation\nAWS Lambda: Added the ability to list functions recursively if the number of functions exceeds 50\nGoogle Analytics: Added pagination to the Report resource\nMailjet: Added Reply To parameter\nRedis: Added the Increment operation\nSpreadsheet File: Added the Header Row option\nWebflow Trigger: Added Collection Item Created, Collection Item Updated, and Collection Item Deleted events\nCore Functionality\nImplemented timeout for subworkflows\nRemoved the deregistration webhooks functionality from the webhook process\nBug fixes\nGoogle Cloud Firestore: Fixed an issue with parsing null value\nGoogle Sheets: Fixed an issue with the Key Row parameter\nHubSpot: Fixed an issue with the authentication\nContributors\nNikita\nn8n@0.118.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-05\nCore Functionality\nFixed an issue with error workflows\nn8n@0.118.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-05-02\nNew nodes\nKitemaker\nMQTT\nEnhanced nodes\nCrateDB: Added query parameters. The Execute Query operation returns the result from all queries executed instead of just one of the results.\nERPNext: Added support for self-hosted ERPNext instances\nFTP: Added the functionality to delete folders\nGoogle Calendar: Added the Continue on Fail functionality\nGoogle Drive: Added the functionality to add file name when downloading files\nGmail: Added functionality to handle multiple binary properties\nMicrosoft Outlook: Added Is Read and Move option to the Message resource\nPostgres: Added query parameters. The Execute Query operation returns the result from all queries executed instead of just one of the results.\nQuestDB: Added query parameters. The Execute Query operation returns the result from all queries executed instead of just one of the results.\nQuickBase: Added option to use Field IDs\nTimescaleDB: Added query parameters. The Execute Query operation returns the result from all queries executed instead of just one of the results.\nTwist: Added Get, Get All, Delete, and Update operations to the Message Conversation resource. Added Archive, Unarchive, and Delete operations to the Channel resource. Added Thread and Comment resource\nCore Functionality\nImplemented the native fs/promise library where possible\nAdded the functionality to output logs to the console or a file\nWe have updated the minimum required version for Node.js to v14.15. For more details, check out the entry in the breaking changes page\nBug fixes\nGetResponse Trigger: Fixed an issue with error handling\nGitHub Trigger: Fixed an issue with error handling\nGitLab Trigger: Fixed an issue with error handling\nGoogle Sheets: Fixed an issue with the Lookup operation for returning empty rows\nOrbit: Fixed issues with the Post resource\nRedis: Fixed an issue with the node not returning an error\nXero: Fixed an issue with the Create operation for the Contact resource\nContributors\nGustavo Arjones, lublak, Colton Anglin, Mika Luhta\nn8n@0.117.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-04-24\nNew nodes\nMailcheck\nn8n Trigger\nWorkflow Trigger\nEnhanced nodes\nCrateDB: Added the Mode option that allows you to execute queries as transactions\nNextcloud: Added Delete, Get, Get All, and Update operation to the User resource\nPostgres: Added the Mode option that allows you to execute queries as transactions\nQuestDB: Added the Mode option that allows you to execute queries as transactions\nSalesforce: Added Owner option to the Case and Lead resources. Added custom fields to Create and Update operations of the Case resource\nSentry.io: Added Delete and Update operations to Project, Release, and Team resources\nTimescaleDB: Added the Mode option that allows you to execute queries as transactions\nZendesk Trigger: Added support to retrieve custom fields\nCore Functionality\nThe Activation Trigger node has been deprecated. It has been replaced by two new nodes - the n8n Trigger and the Workflow Trigger node. For more details, check out the entry in the breaking changes page\nAdded the functionality to open the New Credentials dropdown by default\nBug fixes\nGoogle Sheets: Fixed an issue with the Lookup operation for returning multiple empty rows\nIntercom: Fixed an issue with the User operation in the Company resource\nMautic: Fixed an issue with sending the lastActive parameter\nContributors\nBart Vollebregt, Ivan Timoshenko, Konstantin Nosov, lublak, Umair Kamran,\nn8n@0.116.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-04-20\nCore Functionality\nFixed a timeout issue with the workflows in the main process\nn8n@0.116.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-04-17\nNew nodes\nGoogle BigQuery\nWebflow\nEnhanced nodes\nDate & Time: Added Calculate a Date action that allows you to add or subtract time from a date\nGitLab: Added Get, Get All, Update, and Delete operations to the Release resource\nMicrosoft OneDrive: Added Delete operation to the Folder resource\nMonday: Added support for OAuth2 authentication\nMongoDB: Added Limit, Skip, and Sort options to the Find operation and added Upsert parameter to the Update operation. Added the functionality to close the connection after use\nMySQL: Added support for insert modifiers and added support for SSL\nRabbitMQ: Added the functionality to close the connection after use and added support for AMPQS\nCore Functionality\nChanged bcrypt library from bcryptjs to @node-rs/bcrypt\nImproved node error handling. Status codes and error messages in API responses have been standardized\nAdded global timeout setting for all HTTP requests (except HTTP Request node)\nImplemented timeout for workers and corrected timeout for sub workflows\nBug fixes\nAWS SQS: Fixed an issue with API version and casing\nIMAP: Fixed re-connection issue\nKeap: Fixed an issue with the Opt In Reason parameter\nSalesforce: Fixed an issue with loading custom fields\nContributors\nAllan Daemon, Anton Romanov, Bart Vollebregt, Cassiano Vailati, entrailz, Konstantin Nosov, LongYinan\nn8n@0.115.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-04-10\nNew nodes\nGoogle Slides\nEnhanced nodes\nGitHub: Added Release resource\nTheHive: Added support to fetch observable data types\nRabbitMQ: Added header parameters\nCore Functionality\nFixed an issue with expressions not being displayed in read-only mode\nFixed an issue that didn't allow editing JavaScript code in read-only mode\nAdded support for configuring the maximum payload size\nAdded support to dynamically add menu items\nBug fixes\nJira: Fixed an issue with loading issue types with classic project type\nRabbitMQ Trigger: Fixed an issue with the node reusing the same item\nSendGrid: Fixed an issue with the dynamic field generation\nContributors\nMika Luhta, Loran, stwonary\nn8n@0.114.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-04-03\nNew nodes\nAWS SQS\nCopper\nERPNext\nOura\nEnhanced nodes\nGoogle Drive: Added support for creating folders for shared drives\nGoogle Sheets: Added Create and Remove operation to the Sheet resource\nHarvest: Added Update operation to the Task resource\nJira: Added Reporter field to the Issue resource\nPostgres: Added support for type casting\nCore Functionality\nFixed an issue with the Redis connection to prevent memory leaks\nBug fixes\nBitwarden: Fixed an issue with the Update operation of the Group resource\nCortex: Fixed an issue where only the last item got returned\nInvoice Ninja: Fixed an issue with the Project parameter\nSalesforce: Fixed an issue with the Get All operation of the Custom Object resource\nContributors\nAgata M, Allan Daemon, Craig McElroy, mjysci\nn8n@0.113.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-03-26\nNew nodes\nActivation Trigger\nPlivo\nEnhanced nodes\nClickUp: Added Space Tag, Task List, and Task Tag resource\nGitHub: Added pagination to Get Issues and Get Repositories operations\nMattermost: Added Reaction resource and Post Ephemeral operation\nMove Binary Data: Added Encoding and Add BOM option to JSON to Binary mode and Strip BOM to Binary to JSON mode\nSendGrid: Added Mail resource\nSpotify: Added Library resource\nTelegram: Added Answer Inline Query operation to the Callback resource\nuProc: Added Get ASIN code by EAN code, Get EAN code by ASIN code, Get Email by Social Profile, Get Email by Full name and Company's domain, and Get Email by Full name and Company's name operations\nBug fixes\nClearbit: Fixed an issue with the autocomplete URI\nDropbox: Fixed an issue with the Dropbox credentials by adding the APP Access Type parameter in the credentials. For more details, check out the entry in the breaking changes page\nSpotify: Fixed an issue with the Delete operation of the Playlist resource\nThe variable selector now displays empty arrays\nFixed a permission issue with the Raspberry Pi Docker image\nn8n@0.112.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-03-19\nNew nodes\nDeepL\nEnhanced nodes\nTheHive: Added Mark as Read and Mark as Unread operations and added Ignore SSL Issues parameter to the credentials\nBug fixes\nAWS SES: Fixed an issue to map CC addresses correctly\nSalesforce: Fixed an issue with custom object for Get All operations and fixed an issue with the first name field for the Create and Update operations for the Lead resource\nStrava: Fixed an issue with the access tokens not getting refreshed\nTheHive: Fixed an issue with the case resolution status\nFixed an issue with importing separate decrypted credentials\nFixed issues with the sub-workflows not finishing\nFixed an issue with the sub-workflows running on the main process\nFixed concurrency issues with sub-workflows\nn8n@0.111.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-03-12\nNew nodes\nAutopilot\nAutopilot Trigger\nWise\nWise Trigger\nEnhanced nodes\nBox: Added Get operation to the Folder resource\nDropbox: Added Search operation to the File resource. All operations are now performed relative to the user's root directory. For more details, check out the entry in the breaking changes page\nFacebook Graph API: Added new API versions\nGoogle Drive: Added Update operation to the File resource\nHubSpot: Added the Deal Description option\nKafka: Added the SASL mechanism\nMonday.com: Added Move operation to Board Item resource\nMongoDB: Added Date field to the Insert and Update operations\nMicrsoft SQL: Added connection timeout parameter to credentials\nSalesforce: Added Mobile Phone field to the Lead resource\nSpotify: Added Create a Playlist operation to Playlist resource and Get New Releases to the Album resource\nBug fixes\nAirtable: Fixed a bug with updating and deleting records\nAdded the functionality to expose metrics to Prometheus. Read more about that here\nUpdated fallback values to match the value type\nAdded the functionality to display debugging information for pending workflows on exit\nFixed an issue with queue mode for the executions that shouldn't be saved\nFixed an issue with workflows crashing and displaying Unknown status in the execution list\nFixed an issue to prevent crashing while saving execution data when the data field has over 64KB in MySQL\nUpdated jws-rsa to version 1.12.1\nn8n@0.110.3\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-03-04\nBug fixes\nAPITemplate.io: Fixed an issue with the naming of the node\nn8n@0.110.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-03-04\nNew nodes\nAPITemplate.io\nBubble\nLemlist\nLemlist Trigger\nEnhanced nodes\nMicrosoft Teams: Added option to reply to a message\nBug fixes\nDropbox: Fixed an issue with parsing the response with the Upload operation\nGmail: Fixed an issue with the scope for the Service Account authentication method and fixed an issue with the label filter\nGoogle Drive: Fixed an issue with the missing Parent ID field for the Create operation and fixed an issue with the Permissions field\nHelpScout: Fixed an issue with sending tags when creating a conversation\nHTTP Request: Fixed an issue with the raw data and file response\nHubSpot: Fixed an issue with the OAuth2 credentials\nAdded support for Date & Time in the IF node and the Switch node\nFixed an issue with mouse selection when zooming in or out\nFixed an issue with current executing workflows when using queues for Postgres\nFixed naming and description for the N8N_SKIP_WEBHOOK_DEREGISTRATION_SHUTDOWN environment variable\nFixed an issue with auto-refresh of the execution list\nn8n@0.109.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-02-22\nNew nodes\nBitwarden\nEmelia\nEmelia Trigger\nGoToWebinar\nRaindrop\nEnhanced nodes\nAWS Rekognition: Added the Detect Text type to the Analyze operation for the Image resource\nGoogle Calendar: Added RRULE parameter to the Get All operation for the Event resource\nJira: Added User resource and operations\nReddit: Added the Search operation for the Post resource\nTelegram: Added the Send Location operation\nBug fixes\nRocketChat: Fixed error responses\nFixed the issue which caused the execution history of subworkflows (workflows started using the Execute Workflow node) not to be saved\nAdded an option to export the credential data in plain text format using the CLI\nn8n@0.108.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-02-15\nNew nodes\nDemio\nPostHog\nQuickBooks\nEnhanced nodes\nTrello: Added Create Checklist Item operation to the Checklist resource\nWebhook: Removed trailing slash in routes and updated logic to select dynamic webhook\nBug fixes\nGoogle Drive: Fixed an issue with returning the fields the user selects for the Folder and File resources\nTwitter: Fixed a typo in the description\nWebhook: Fixed logic for static route matching\nAdded the functionality to sort the values that you add in the IF node, Rename node, and the Set node\nAdded the functionality to optionally save execution data after each node\nAdded queue mode to scale workflow execution\nSeparated webhook from the core to scale webhook separately\nFixed an issue with current execution query for unsaved running workflows\nFixed an issue with the regex that detected node names\nn8n now generates a unified execution ID instead of two separate IDs for currently running and saved executions\nn8n@0.107.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-02-08\nNew nodes\nAWS Comprehend\nGetResponse Trigger\nPeekalink\nStackby\nEnhanced nodes\nAWS SES: Added Custom Verification Email resource\nMicrosoft Teams: Added Task resource\nTwitter: Added Delete operation to the Tweet resource\nBug fixes\nGoogle Drive: Fixed an issue with the Delete and Share operations\nFileMaker: Fixed an issue with the script list parsing\nUpdated Node.js version of Docker images to 14.15\nAdded a shortcut CTRL + scroll to zoom\nn8n@0.106.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-02-05\nNew nodes\nReddit\nTapfiliate\nEnhanced nodes\nAirtable Trigger: Added Download Attachment option\nHubSpot: Added Custom Properties option to the Create and Update operations of the Company resource\nMySQL: Added Connection Timeout parameter to the credentials\nTelegram: Added Pin Chat Message and Unpin Chat Message operations for the Message resource\nBug fixes\nTypeform: Fixed an issue with the OAuth2 authentication method\nAdded support for s and u flags for regex in the IF node and the Switch node\nn8n@0.105.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-02-01\nNew nodes\nDiscourse\nSecurityScorecard\nTimescaleDB\nEnhanced nodes\nAffinity: Added List and List Entry resource\nAsana: Added Project IDs option to the Create operation of the Task resource\nHubSpot Trigger: Added support for multiple subscriptions\nJira: Added Issue Attachment resource and added custom fields to Create and Update operations of the Issue resource\nTodoist: Added Section option\nBug fixes\nSIGNL4: Fixed an issue with the attachment functionality\nAdded variable $mode to check the mode in which the workflow is being executed\nn8n@0.104.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-01-27\nFixed an issue with the credentials parameters that have the same name\nn8n@0.104.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-01-26\nFixed a bug with expressions in credentials\nn8n@0.104.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-01-26\nNew nodes\nCompression\nEnhanced nodes\nGitHub: Added Invite operation to the User resource\nEmailReadImap: Increased the authentication timeout\nMautic: Added Custom Fields option to the Create and Update operations of the Contact resource. Also, the Mautic OAuth credentials have been updated. Now you don't have to enter the Authorization URL and the Access Token URL\nNextcloud: Added User resource\nSlack: Added Get Permalink and Delete operations to the Message resource\nWebhook: Added support for request parameters in webhook paths\nBug fixes\nGoogle Drive: Fixed the default value for the Send Notification Email option\nAdded support for expressions to credentials\nRemoved support for MongoDB as a database for n8n. For more details, check out the entry in the breaking changes page\nn8n@0.103.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-01-21\nBug fixes\nTrello: Fixed the icon\nn8n@0.103.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-01-21\nNew nodes\nSendGrid\nEnhanced nodes\nAMQP: Added Container ID, Reconnect, and Reconnect limit options\nAMQP Trigger: Added Container ID, Reconnect, and Reconnect Limit options\nGitHub: Added Review resource\nGoogle Drive: Added Drive resource\nTrello: Added Get All and Get Cards operation to the List resource\nBug fixes\nAWS Lambda: Fixed an issue with signature\nAWS SNS: Fixed an issue with signature\nFixed an issue with nodes not executing if two input gets passed and one of them didn't return any data\nThe code editor doesn'tget closed when clicked anywhere outside the editor\nAdded CLI commands to export and import credentials and workflows\nThe title in the browser tab now resets for new workflows\nn8n@0.102.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-01-15\nNew nodes\nBeeminder\nEnhanced nodes\nCrypto: Added hash type SHA384\nGoogle Books: Added support for user impersonation\nGoogle Drive: Added support for user impersonation\nGoogle Sheets: Added support for user impersonation\nGmail: Added support for user impersonation\nMicrosoft Outlook: Added support for a shared mailbox\nRabbitMQ: Added Exchange mode\nSalesforce: Added filters to all Get All operations\nSlack: Made changes to the properties As User and Ephemeral. For more details, check out the entry in the breaking changes page\nTypeform Trigger: The node now displays the recall information in the question in square brackets. For more details, check out the entry in the breaking changes page\nZendesk: Removed the Authorization URL and Access Token URL fields from the OAuth2 credentials. The node now uses the subdomain passed by a user to connect to Zendesk.\nBug fixes\nCoinGecko: Fixed an issue to process multiple input items correctly\nn8n@0.101.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2021-01-07\nNew nodes\nGoogle Analytics\nPhantomBuster\nEnhanced nodes\nAWS: Added support for custom endpoints\nGmail: Added an option to send messages formatted as HTML\nPhilips Hue: Added Room/Group name to Light name to make it easier to identify lights\nSlack: Added ephemeral message option\nTelegram: Removed the Bot resource as the endpoint is no longer supported\nBug fixes\nE-goi: Fixed the name of the node\nEdit Image: Fixed an issue with the Border operation\nHTTP Request: Fixed batch sizing to work when batchSize = 1\nPayPal: Fixed a typo in the Environment field\nSplit In Batches: Fixed a typo in the description\nTelegram: Fixed an issue with the Send Audio operation\nBased on your settings, vacuum runs on SQLite on startup\nUpdated Axios to version 0.21.1\nn8n@0.100.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-12-30\nNew nodes\nMicrosoft Outlook\nEnhanced nodes\nActiveCampaign: The node loads more options for the fields\nAsana: Added Subtask resource and Get All operation for the Task resource\nEdit Image: Added Multi Step operation\nHTTP Request: Added Use Querystring option\nIF: Added Ends With and Starts With operations\nJira: Added Issue Comment resource\nSwitch: Added Ends With and Starts With operations\nTelegram: Added File resource\nBug fixes\nBox Trigger: Fixed a typo in the description\nEdit Image: Fixed an issue with multiple composite operations\nHTTP Request: Fixed an issue with the binary data getting used by multiple nodes\nS3: Fixed an issue with uploading files\nStripe Trigger: Fixed an issue with the existing webhooks\nTelegram: Fixed an issue with the Send Audio operation\nBinary data stays visible if a node gets re-executed\nn8n@0.99.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-12-24\nFixed a bug that caused HTML to render in JSON view\nn8n@0.99.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-12-24\nNew nodes\ne-goi\nRabbitMQ\nRabbitMQ Trigger\nuProc\nEnhanced nodes\nActiveCampaign: Added the functionality to load the tags for a user\nFTP: Added Delete and Rename operation\nGoogle Cloud Firestore: The node now gives the Collection ID in response\nIterable: Added User List resource\nMessageBird: Added Balance resource\nTheHive Trigger: Added support for the TheHive3 webhook events, and added Log Updated and Log Deleted events\nBug fixes\nDropbox: Fixed an issue with the OAuth credentials\nGoogle Sheets: Fixed an issue with the parameters getting hidden for other operations\nAdded functionality to copy the data and the path from the output\nFixed an issue with the node getting selected after it was duplicated\nn8n@0.98.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-12-16\nNew nodes\nBrandfetch\nPushcut\nPushcut Trigger\nEnhanced nodes\nGoogle Sheets: Added Spreadsheet resource\nIF: Added Is Empty option\nSlack: Added Reaction and User resource, and Member operation to the Channel resource\nSpreadsheet File: Added the option Include Empty Cell to display empty cells\nWebhook: Added option to send a custom response body. The node can now also return string data\nBug fixes\nGitLab: Fixed an issue with GitLab OAuth credentials. You can now specify your GitLab server to configure the credentials\nMautic: Fixed an issue with the OAuth credentials\nIf a workflow is using the Error Trigger node, by default, the workflow will use itself as the Error Workflow\nFixed a bug that caused the Editor UI to display an incorrect (save) state upon activating or deactivating a workflow\nn8n@0.97.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-12-10\nNew nodes\nGhost\nNASA\nSnowflake\nTwist\nEnhanced nodes\nAutomizy: Added options to add and remove tabs for the Update operation of the Contact resource\nPipedrive: Added label field to Person, Organization, and Deal resources. Also added Update operation for the Organization resource\nBug fixes\nFixed a bug that caused OAuth1 requests to break\nFixed Docker user mount path\nn8n@0.96.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-12-03\nNew nodes\nCortex\nIterable\nKafka Trigger\nTheHive\nTheHive Trigger\nYourls\nEnhanced nodes\nHubSpot: Added Contact List resource and Search operation for the Deal resource\nGoogle Calendar: You can now add multiple attendees in the Attendees field\nSlack: The node now loads both private and public channels\nBug Fixes\nMQTT: Fixed an issue with the connection. The node now uses mqtt@4.2.1\nFixed a bug which caused the Trigger-Nodes to require data from the first output\nAdded configuration to load only specific nodes\nn8n@0.95.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-11-25\nBug Fixes\nAirtable Trigger: Fixed the icon of the node\nn8n@0.95.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-11-25\nNew nodes\nAirtable Trigger\nLingvaNex\nOpenThesaurus\nProfitWell\nQuick Base\nSpontit\nEnhanced nodes\nAirtable: The Application ID field has been renamed to Base ID, and the Table ID field has been renamed to Table. The List operation now downloads attachments automatically\nHarvest: Moved the account field from the credentials to the node parameters. For more details, check out the entry in the breaking changes page\nBug Fixes\nSlack: Fixed an issue with creating channels and inviting users to a channel\nn8n@0.94.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-11-20\nBug Fixes\nGraphQL: Fixed an issue with the variables\nWooCommerce Trigger: Fixed an issue with the webhook. The node now reuses a webhook if it already exists.\nn8n@0.94.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-11-19\nNew nodes\nGoogle Cloud Natural Language\nGoogle Firebase Cloud Firestore\nGoogle Firebase Realtime Database\nHumantic AI\nEnhanced nodes\nActiveCampaign: Added Contact List and List resource\nEdit Image: Added support for drawing, font selection, creating a new image, and added the Composite resource\nFTP: Added Private Key and Passphrase fields to the SFTP credentials and made the directory creation more robust\nIMAP: Increased the timeout\nMatrix: Added option to send notice, emote, and HTML messages\nSegment: Made changes to the properties traits and properties. For more details, check out the entry in the breaking changes page\nBug Fixes\nGraphQL: Fixed an issue with the variables\nMailchimp: Fixed an issue with the OAuth credentials. The credentials are now sent with the body instead of the header\nYouTube: Fixed a typo for the Unlisted option\nAdded horizontal scrolling\nn8n@0.93.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-11-11\nNew nodes\nGetResponse\nGotify\nLine\nStrapi\nEnhanced nodes\nAMQP: Connection is now closed after a message is sent\nAMQP Trigger: Added Message per Cycle option to retrieve the specified number of messages from the bus for every cycle\nHubSpot: Added Custom Properties for the Deal resource as Additional Fields\nJira: The node retrieves all the projects for the Project field instead of just 50\nMattermost: Improved the channel selection\nMicrosoft SQL: Added TLS parameter for the credentials\nPipedrive Trigger: Added OAuth authentication method. For more details, check out the entry in the breaking changes page\nSegment: Added Custom Traits option for the Traits field\nBug Fixes\nShopify Trigger: Fixed an issue with activating the workflow\nFor custom nodes, you can now set custom documentation URLs\nn8n@0.92.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-11-04\nNew nodes\nFacebook Trigger\nGoogle Books\nOrbit\nStoryblok\nEnhanced nodes\nGoogle Drive: Removed duplicate parameters\nTwitter: Added Direct Message resource\nBug Fixes\nGmail: Fixed an issue with the encoding for the subject field\nImproved the Editor UI for the save workflow functionality\nn8n@0.91.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-23\nNew nodes\nKafka\nMailerLite\nMailerLite Trigger\nPushbullet\nEnhanced nodes\nAirtable: Added Ignore Fields option for the Update operation\nAMQP Sender: Added Azure Service Bus support\nGoogle Calendar: Added Calendar resource and an option to add a conference link\nG Suite Admin: Added Group resource\nHTTP Request: Added Batch Size and Batch Interval option\nMautic: Added Company resource\nSalesforce: Added OAuth 2.0 JWT authentication method\nBug Fixes\nIF: Fixed an issue with undefined expression\nPaddle: Fixed an issue with the Return All parameter\nSwitch: Fixed an issue with undefined expression\nAdded CLI commands to deactivate the workflow\nAdded an option to get the full execution data from the server\nThe Editor UI gives an alert if you redirect without saving a workflow\nThe Editor UI now indicates if a workflow is saved or not\nImproved support for touch devices\nNode properties now load on demand\nUpdated the Node.js version for the Docker images\nn8n@0.90.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-23\nAdded a check for the Node.js version on startup. For more details, check out the entry in the breaking changes page\nBug Fixes\nGoogle Translate: Fixed an issue with the rendering of the image in n8n.io\nn8n@0.89.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-22\nBug Fixes\nStrava Trigger: Fixed a typo in the node name\nn8n@0.89.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-22\nRemoved debug messages\nn8n@0.89.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-22\nNew Nodes\nPushover\nStrava\nStrava Trigger\nGoogle Translate\nBug Fixes\nHTTP Request: Fixed an issue with the POST request method for the 'File' response format\nFixed issue with displaying non-active workflows as active\nFixed an issue related to multiple-webhooks\nn8n@0.88.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-16\nBug Fixes\nHTTP Request: Fixed an issue with the Form-Data Multipart and the RAW/Custom Body Content Types\nn8n@0.88.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-16\nEnhanced Fixes\nMatrix: Added support for specifying a Matrix Homeserver URL\nSalesforce: Added Custom Object resource and Custom Fields and Sort options\nBug Fixes\nAWS SES: Fixed an issue with the Send Template operation for the Email resource\nAWS SNS Trigger: Fixed an issue with the Subscriptions topic\nn8n@0.87.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-15\nBug Fixes\nGoogle Sheets: Fixed an issue with spaces in sheet names\nAutomizy: Fixed an issue with the default resource\nn8n@0.87.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-15\nBug Fixes\nGmail: Fixed an issue with the Message ID\nHTTP Request: Fixed an issue with the GET Request\nAdded HMAC-SHA512 signature method for OAuth 1.0\nn8n@0.87.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-14\nNew nodes\nAutomizy\nAWS Rekognition\nMatrix\nSendy\nVonage\nWeKan\nEnhanced nodes\nAWS SES: Added Send Template operation for the Email resource and added the Template resource\nClickUp: Added Time Entry and Time Entry Tag resources\nFunction: The Function field is now called the JavaScript Code field\nMailchimp: Added Campaign resource\nMindee: Added currency to the simplified response\nOneDrive: Added Share operation\nOpenWeatherMap: Added Language parameter\nPipedrive: Added additional parameters to the Get All operation for the Note resource\nSalesforce: Added Flow resource\nSpreadsheet File: Added Range option for the Read from file operation\nBug Fixes\nClickUp Trigger: Fixed issue with creating credentials\nPipedrive Trigger: Fixed issue with adding multiple webhooks to Pipedrive\nThe link.fish Scrape node has been removed from n8n. For more details, check out the entry in the breaking changes page\nn8n@0.86.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-06\nEnhanced nodes\nCoinGecko: Small fixes to the CoinGecko node\nn8n@0.86.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-10-05\nNew nodes\nClockify\nCoinGecko\nG Suite Admin\nMindee\nWufoo Trigger\nEnhanced nodes\nSlack: Added User Profile resource\nMattermost: Added Create and Invite operations for the User resource\nBug Fixes\nS3: Fixed issue with uploading files\nWebhook ID gets refreshed on node duplication\nn8n@0.85.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-30\nEnhanced nodes\nPostgres: Added Schema parameter for the Update operation\nBug Fixes\nJira: Fixed a bug with the Issue Type field\nPipedrive Trigger: Fixed issues with the credentials\nChanged the bcrypt library to bcrypt.js to make it compatible with Windows\nThe OAuth callback URLs are now generated in the backend\nn8n@0.84.4\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-23\nBug Fixes\nGoogle Sheets: Fixed issues with the update and append operations\nn8n@0.84.3\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-23\nFixed an issue with the build by setting jwks-rsa to an older version\nn8n@0.84.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-23\nFixed an issue with the OAuth window. The OAuth window now closes after authentication is complete\nn8n@0.84.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-23\nAdditional endpoints can be excluded from authentication checks. Multiple endpoints can be added separated by colons\nn8n@0.84.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-23\nEnhanced nodes\nTwitter: Added support for auto mention of users in reply tweets\nBug Fixes\nGoogle Sheets: Fixed issue with non-Latin sheet names\nHubSpot: Fixed naming of credentials\nMicrosoft: Fixed naming of credentials\nMandrill: Fixed attachments with JSON parameters\nExpressions now use short variables when selecting input data for the current node\nFixed issue with renaming credentials for active workflows\nn8n@0.83.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-18\nNew nodes\nLinkedIn\nTaiga\nTaiga Trigger\nEnhanced nodes\nActiveCampaign: Added multiple functions, read more here\nAirtable: Added typecast functionality\nAsana: Added OAuth2 support\nClickUp: Added OAuth2 support\nGoogle Drive: Added share operation\nIMAP Email: Added support for custom rules when checking emails\nSentry.io: Added support for self-hosted version\nTwitter: Added retweet, reply, and like operations\nWordPress: Added author field to the post resource\nBug Fixes\nAsana Trigger: Webhook validation has been deactivated\nPaddle: Fixed returnData format and coupon description\nThe ActiveCampaign node has breaking changes\nFixed issues with test-webhook registration\nn8n@0.82.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-14\nSpeed for basic authentication with hashed password has been improved\nn8n@0.82.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-14\nNew nodes\nMicrosoft Teams\nEnhanced nodes\nFreshdesk: Added Freshdesk contact resource\nHTTP Request: Run parallel requests in HTTP Request Node\nBug Fixes\nPhilips Hue: Added APP ID to Philips Hue node credentials\nPostmark Trigger: Fixed parameters for the node\nThe default space between nodes has been increased to two units\nExpression support has been added to the credentials\nPasswords for your n8n instance can now be hashed\nn8n@0.81.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-09\nNew nodes\nSentry.io\nEnhanced nodes\nAsana\nClickUp\nClockify\nGoogle Contacts\nSalesforce\nSegment\nTelegram\nTelegram Trigger\nn8n@0.80.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-09-02\nNew nodes\nCustomer.io\nMQTT Trigger\nS3\nEnhanced nodes\nAcuity Scheduling\nAWS S3\nClickUp\nFTP\nTelegram Trigger\nZendesk\nn8n@0.79.3\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-08-30\nThe bug that caused the workflows to not get activated correctly has been fixed\nn8n@0.79.2\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-08-28\nAdded missing rawBody for \"application/x-www-form-urlencoded\"\nn8n@0.79.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-08-28\nEnhanced nodes\nContentful\nHTTP Request\nPostgres\nWebhook\nRemoved Test-Webhook also in case checkExists fails\nHTTP Request node doesn'toverwrite accept header if it's already set\nAdd rawBody to every request so that n8n doesn'tgive an error if body is missing\nn8n@0.79.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-08-27\nNew nodes\nContentful\nConvertKit\nConvertKit Trigger\nPaddle\nEnhanced nodes\nAirtable\nCoda\nGmail\nHubSpot\nIMAP Email\nPostgres\nSalesforce\nSIGNL4\nTodoist\nTrello\nYouTube\nThe Todoist node has breaking changes\nAdded dynamic titles on workflow execution\nNodes will now display a link to associated credential documentation\nn8n@0.78.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-08-18\nNew nodes\nGmail\nGoogle Contacts\nUnleashed Software\nYouTube\nEnhanced nodes\nAMQP\nAMQP Trigger\nBitly\nFunction Item\nGoogle Sheets\nShopify\nTodoist\nEnhanced support for JWT based authentication\nAdded an option to execute a node once, using data of only the first item\nn8n@0.76.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-08-05\nNew nodes\nCustomer.io Trigger\nFTP\nMedium\nPhilips Hue\nTravisCI\nTwake\nEnhanced nodes\nCrateDB\nMove Binary Data\nNodes will now display a link to associated documentation\nn8n@0.75.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-07-26\nNew nodes\nBox\nBox Trigger\nCrateDB\nJira Trigger\nEnhanced nodes\nGitLab\nNextcloud\nPipedrive\nQuestDB\nWebhooks now support OPTIONS request\nn8n@0.74.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-07-15\nNew nodes\nHacker News\nQuestDB\nXero\nEnhanced nodes\nAffinity Trigger\nHTTP Request\nMailchimp\nMongoDB\nPipedrive\nPostgres\nUpLead\nWebhook\nWebhook URLs are now handled independently of the workflow ID by  instead of the older\nn8n@0.73.1\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-07-08\nEnhanced nodes\nMicrosoft SQL\nn8n@0.73.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-07-08\nNew nodes\nCircleCI\nMicrosoft SQL\nZoom\nEnhanced nodes\nPostmark Trigger\nSalesforce\nIt's now possible to set default values for credentials that get prefilled, and the user can't change.\nn8n@0.72.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-07-02\nEnhanced nodes\nDrift\nEventbrite Trigger\nFacebook Graph API\nPipedrive\nFixed credential issue for the Execute Workflow node\nn8n@0.71.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-06-25\nNew nodes\nGoogle Tasks\nSIGNL4\nSpotify\nEnhanced nodes\nHubSpot\nMailchimp\nTypeform\nWebflow\nZendesk\nAdded Postgres SSL support\nIt's now possible to deploy n8n under a subfolder\nn8n@0.70.0\nFor a comprehensive list of changes, check out the commits for this version.\nRelease date: 2020-06-13\nEnhanced nodes\nGitHub\nMautic Trigger\nMonday.com\nMongoDB\nFixed the issue with multiuser-setup"
  },
  {
    "file_path": "source-control-environments\\create-environments.md",
    "content": "Tutorial: Create environments with source control\nThis tutorial walks through the process of setting up environments end-to-end. You'll create two environments: development and production. It uses GitHub as the Git provider. The process is similar for other providers.\nn8n has built its environments feature on top of Git, a version control software. You link an n8n instance to a Git branch, and use a push-pull pattern to move work between environments. You should have some understanding of environments and Git. If you need more information on these topics, refer to:\nEnvironments in n8n: the purpose of environments, and how they work in n8n.\nGit and n8n: Git concepts and source control in n8n.\nChoose your source control pattern\nBefore setting up source control and environments, you need to plan your environments, and how they relate to Git branches. n8n supports different Branch patterns. For environments, you need to choose between two patterns: multi-instance, multi-branch, or multi-instance, single-branch. This tutorial covers both patterns.\nMultiple instances, multiple branches\n!Diagram\nMultiple instances, one branch\n!Diagram\nSet up your repository\nOnce you've chosen your pattern, you need to set up your GitHub repository.\n=== \"Multi-branch\"\n1. Create a new repository.\n* Make sure the repository is private, unless you want your workflows, tags, and variable and credential stubs exposed to the internet.\n* Create the new repository with a README so you can immediately create branches.\n1. Create one branch named production and another named development. Refer to Creating and deleting branches within your repository for guidance.\n=== \"Single-branch\"\nCreate a new repository.\n* Make sure the repository is private, unless you want your workflows, tags, and variable and credential stubs exposed to the internet.\n* Create the new repository with a README. This creates the main branch, which you'll connect to.\nConnect your n8n instances to your repository\nCreate two n8n instances, one for development, one for production.\nConfigure Git in n8n\nSet up a deploy key\nSet up SSH access by creating a deploy key for the repository using the SSH key from n8n. The key must have write access. Refer to GitHub | Managing deploy keys for guidance.\nConnect n8n and configure your instance\n=== \"Multi-branch\"\n1. In **Settings** > **Environments** in n8n, select **Connect**. n8n connects to your Git repository.\n1. Under **Instance settings**, choose which branch you want to use for the current n8n instance. Connect the production branch to the production instance, and the development branch to the development instance.\n1. Production instance only: select **Protected instance** to prevent users editing workflows in this instance.\n1. Select **Save settings**.\n=== \"Single-branch\"\n1. In **Settings** > **Environments** in n8n, select **Connect**.\n1. Under **Instance settings**, select the main branch.\n1. Production instance only: select **Protected instance** to prevent users editing workflows in this instance.\n1. Select **Save settings**.\nPush work from development\nIn your development instance, create a few workflows, tags, variables, and credentials.\nPull work to production\nYour work is now in GitHub. If you're using a multi-branch setup, it's on the development branch. If you chose the single-branch setup, it's on main.\n=== \"Multi-branch\"\n1. In GitHub, create a pull request to merge development into production.\n1. Merge the pull request.\n1. In your production instance, select **Pull** <span class=\"inline-image\">!Pull icon{.off-glb}</span> in the main menu.\n=== \"Single-branch\"\nIn your production instance, select **Pull** <span class=\"inline-image\">!Pull icon{.off-glb}</span> in the main menu.\nOptional: Use a GitHub Action to automate pulls\nIf you want to avoid logging in to your production instance to pull, you can use a GitHub Action and the n8n API to automatically pull every time you push new work to your production or main branch.\nNext steps\nLearn more about:\nEnvironments in n8n and Git and n8n\nSource control patterns"
  },
  {
    "file_path": "source-control-environments\\index.md",
    "content": "Source control and environments\nn8n uses Git-based source control to support environments. Linking your n8n instances to a Git repository lets you create multiple n8n environments, backed by Git branches.\nIn this section:\nUnderstand:\nEnvironments in n8n: The purpose of environments, and how they work in n8n.\nGit and n8n: How n8n uses Git.\nBranch patterns: The possible relationships between n8n instances and Git branches.\nSet up source control for environments: How to connect your n8n instance to Git.\nUsing:\nPush and pull: Send work to Git, and fetch work from Git to your instance.\nCopy work between environments: How to copy work between different n8n instances.\nTutorial: Create environments with source control: An end-to-end tutorial, setting up environments using n8n's recommended configurations.\nRelated sections:\nVariables: reusable values.\nExternal secrets: manage credentials with an external secrets vault."
  },
  {
    "file_path": "source-control-environments\\setup.md",
    "content": "Set up source control for environments\nLink a Git repository to an n8n instance and configure your source control.\nn8n uses source control to provide environments. Refer to Environments in n8n for more information.\nPrerequisites\nTo use source control with n8n, you need a Git repository that allows SSH access.\nThis document assumes you are familiar with Git and your Git provider.\nStep 1: Set up your repository and branches\nFor a new setup:\nCreate a new repository for use with n8n.\nCreate the branches you need. For example, if you plan to have different environments for test and production, set up a branch for each.\nTo help decide what branches you need for your use case, refer to Branch patterns.\nStep 2: Configure Git in n8n\nStep 3: Set up a deploy key\nSet up SSH access by creating a deploy key for the repository using the SSH key from n8n. The key must have write access.\nThe steps depend on your Git provider. Help links for common providers:\nGitHub TABLE_PLACEHOLDER_0\nStep 4: Connect n8n and configure your instance\nIn Settings > Environments in n8n, select Connect. n8n connects to your Git repository.\nUnder Instance settings, choose which branch you want to use for the current n8n instance.\nOptional: select Protected instance to prevent users editing workflows in this instance. This is useful for protecting production instances.\nOptional: choose a custom color for the instance. This will appear in the menu next to the source control push and pull buttons. It helps users know which instance they're in.\nSelect Save settings."
  },
  {
    "file_path": "source-control-environments\\understand\\environments.md",
    "content": "Environments in n8n\nn8n has built its environments feature on top of Git, a version control software. This document helps you understand:\nThe purpose of environments.\nHow environments work in n8n.\nEnvironments: What and why\nIn software development, the environment is all the infrastructure and tooling around the code, including the tools that run the software, and the specific configuration of those tools. For a more detailed introduction to environments in software development, refer to Codecademy TABLE_PLACEHOLDER_0\nHow you copy work between environments depends on your branch and n8n instance configuration:\nMultiple instances, one branch: you can push from one instance to the Git branch, then pull the work to another instance.\nMultiple instances, multiple branches: you need to create a pull request and merge in your Git provider. For example, if you have development, test, and production branches, each linked to their own instance, you need to merge the development branch into test to make the work from the development instance available on the test instance. Refer to Copy work between environments for more information, including steps to partially automate the process.\nFor detailed guidance on pushing and pulling work, refer to Push and pull.\nRefer to Set up source control to learn more about linking your n8n instance to Git, or follow the Tutorial: Create environments with source control to set up your environments using one of n8n's recommended configurations."
  },
  {
    "file_path": "source-control-environments\\understand\\git.md",
    "content": "Git and n8n\nn8n uses Git to provide source control. To use this feature, it helps to have some knowledge of basic Git concepts. n8n doesn't implement all Git functionality: you shouldn't view n8n's source control as full version control.\nThis page introduces the Git concepts and terminology used in n8n. It doesn't cover everything you need to set up and manage a repository. The person doing the Setup should have some familiarity with Git and with their Git hosting provider.\nGit overview\nGit is a tool for managing, tracking, and collaborating on multiple versions of documents. It's the basis for widely used platforms such as GitHub and GitLab.\nBranches: Multiple copies of a project\nGit uses branches to maintain multiple copies of a document alongside each other. Every branch has its own version. A common pattern is to have a main branch, and then everyone who wants to contribute to the project works on their own branch (copy). When they finish their work, their branch is merged back into the main branch.\n!Diagram\nLocal and remote: Moving work between your machine and a Git provider\nA common pattern when using Git is to install Git on your own computer, and use a Git provider such as GitHub to work with Git in the cloud. In effect, you have a Git repository (project) on GitHub, and work with copies of it on your local machine.\nn8n uses this pattern for source control: you'll work with your workflows on your n8n instance, but send them to your Git provider to store them.\nPush, pull, and commit\nn8n uses three key Git processes:\nPush: send work from your instance to Git. This saves a copy of your workflows and tags, as well as credential and variable stubs, to Git. You can choose which workflows you want to save.\nPull: get the workflows, tags, and variables from Git and load it into n8n. You will need to populate any credentials or variable stubs included in the refreshed items.\nCommit: a commit in n8n is a single occurrence of pushing work to Git. In n8n, commit and push happen at the same time.\nRefer to Push and pull for detailed information about how n8n interacts with Git."
  },
  {
    "file_path": "source-control-environments\\understand\\index.md",
    "content": "Understand source control and environments\nEnvironments in n8n: The purpose of environments, and how they work in n8n.\nGit in n8n: How n8n uses Git.\nBranch patterns: The possible relationships between n8n instances and Git branches."
  },
  {
    "file_path": "source-control-environments\\understand\\patterns.md",
    "content": "Branch patterns\nThe relationship between n8n instances and Git branches is flexible. You can create different setups depending on your needs.\nMultiple instances, multiple branches\nThis pattern involves having multiple n8n instances, each one linked to its own branch.\nYou can use this pattern for environments. For example, create two n8n instances, development and production. Link them to their own branches. Push work from your development instance to its branch, do a pull request to move work to the production branch, then pull to the production instance.\n!Diagram\nMultiple instances, one branch\nUse this pattern if you want the same workflows, tags, and variables everywhere, but want to use them in different n8n instances.\nYou can use this pattern for environments. For example, create two n8n instances, development and production. Link them both to the same branch. Push work from development, and pull it into production.\nThis pattern is also useful when testing a new version of n8n: you can create a new n8n instance with the new version, connect it to the Git branch and test it, while your production instance remains on the older version until you're confident it's safe to upgrade.\n!Diagram\nOne instance, multiple branches\nThe instance owner can change which Git branch connects to the instance. The full setup in this case is likely to be a Multiple instances, multiple branches pattern, but with one instance switching between branches.\nThis is useful to review work. For example, different users could work on their own instance and push to their own branch. The reviewer could work in a review instance, and switch between branches to load work from different users.\n!Diagram\nOne instance, one branch\nThis is the simplest pattern.\n!Diagram"
  },
  {
    "file_path": "source-control-environments\\using\\copy-work.md",
    "content": "Copy work between environments\nThe steps to send work from one n8n instance to another are different depending on whether you use a single Git branch or multiple branches.\nSingle branch\nIf you have a single Git branch the steps to copy work are:\nPush work from one instance to the Git branch.\nLog in to the other instance to pull the work from Git. You can automate pulls.\nMultiple branches\nIf you have more than one Git branch, you need to merge the branches in your Git provider to copy work between environments. You can't copy work directly between environments in n8n.\nA common pattern is:\nDo work in your developments instance.\nPush the work to the development branch in Git.\nMerge your development branch into your production branch.   Refer to the documentation for your Git provider for guidance on doing this:\nGitHub: Creating a pull request\nGitLab: Creating merge requests\nGit: Basic branching and merging\nIn your production n8n instance, pull the changes. You can automate pulls.\nAutomatically send changes to n8n\nYou can automate parts of the process of copying work, using the /source-control/pull API endpoint. Call the API after merging the changes:\nThis means you can use a GitHub Action or GitLab CI/CD to automatically pull changes to the production instance on merge."
  },
  {
    "file_path": "source-control-environments\\using\\index.md",
    "content": "Using source control and environments\nPush and pull: Send work to Git, and fetch work from Git to your instance. Understand what gets committed, and how n8n handles merge conflicts.\nCopy work between environments: How to copy work between different n8n instances."
  },
  {
    "file_path": "source-control-environments\\using\\push-pull.md",
    "content": "Push and pull\nIf your n8n instance connects to a Git repository, you need to keep your work in sync with Git.\nThis document assumes some familiarity with Git concepts and terminology. Refer to Git and n8n for an introduction to how n8n works with Git.\nFetch other people's work\nTo pull work from Git, select Pull !Pull icon{.off-glb} in the main menu.\nn8n may display a warning about overriding local changes. Select Pull and override to override your local work with the content in Git.\nWhen the changes include new variable or credential stubs, n8n notifies you that you need to populate the values for the items before using them.\nWorkflow and credential owner may change on pull\nWhen you pull from Git to an n8n instance, n8n tries to assign workflows and credentials to a matching user or project.\nIf the original owner is a user:\nIf the same owner is available on both instances (matching email), the owner remains the same. If the original owner isn't on the new instance, n8n sets the user performing the pull as the workflow owner.\nIf the original owner is a project:\nn8n tries to match the original project name to a project name on the new instance. If no matching project exists, n8n creates a new project with the name, assigns the current user as project owner, and imports the workflows and credentials to the project.\nPulling may cause brief service interruption\nIf you pull changes to an active workflow, n8n sets the workflow to inactive while pulling, then reactivates it. This may result in a few seconds of downtime for the workflow.\nSend your work to Git\nWhat gets committed\nn8n commits the following to Git:\nWorkflows, including their tags and the email address of the workflow owner. You can choose which workflows to push.\nCredential stubs (ID, name, type)\nVariable stubs (ID and name)\nProjects\nFolders\nMerge behaviors and conflicts\nn8n's implementation of source control is opinionated. It resolves merge conflicts for credentials and variables automatically. n8n can't detect conflicts on workflows.\nWorkflows\nYou have to explicitly tell n8n what to do about workflows when pushing or pulling. The Git repository acts as the source of truth.\nWhen pulling, you might get warned that your local copy of a workflow differs from Git, and if you accept, your local copy would be overridden. Be careful not to lose relevant changes when pulling.\nWhen you push, your local workflow will override what's in Git, so make sure that you have the most up to date version or you risk overriding recent changes.\nTo prevent the issue described above, you should immediately push your changes to a workflow once you finish working on it. Then it's safe to pull.\nTo avoid losing data:\nDesign your source control setup so that workflows flow in one direction. For example, make edits on a development instance, push to Git, then pull to production. Don't make edits on the production instance and push them.\nDon't push all workflows. Select the ones you need.\nBe cautious about manually editing files in the Git repository.\nCredentials, variables and workflow tags\nCredentials and variables can't have merge issues, as n8n chooses the version to keep.\nOn pull:\nIf the tag, variable or credential doesn't exist, n8n creates it.\nIf the tag, variable or credential already exists, n8n doesn't update it, unless:\nYou set the value of a variable using the API or externally. The new value overwrites any existing value.\nThe credential name has changed. n8n uses the version in Git.\nThe name of a tag has changed. n8n updates the tag name. Be careful when renaming tags as tag names are unique and this could cause database issues when it comes to uniqueness during the pull process.\nOn push:\nn8n overwrites the entire variables and tags files.\nIf a credential already exists, n8n overwrites it with the changes, but doesn't apply these changes to existing credentials on pull."
  },
  {
    "file_path": "try-it-out\\index.md",
    "content": "Try it out\nThe best way to learn n8n is by using our tutorials to get familiar with the user interface and the many different types of nodes and integrations available. Here is a selection of material to get you started:\nLooking for a quick introduction? Check out the \"First Workflow\" tutorial.\nInterested in what you could do with AI? Find out how to build an AI chat agent with n8n.\nPrefer to work through extensive examples? Maybe the courses are for you."
  },
  {
    "file_path": "try-it-out\\quickstart.md",
    "content": "The very quick quickstart\nThis quickstart gets you started using n8n as quickly as possible. Its allows you to try out the UI and introduces two key features: workflow templates and expressions. It doesn't include detailed explanations or explore concepts in-depth.\nIn this tutorial, you will:\nLoad a workflow from the workflow templates library\nAdd a node and configure it using expressions\nRun your first workflow\nStep one: Sign up for n8n\nThis quickstart uses n8n Cloud. A free trial is available for new users. If you haven't already done so, sign up for an account now.\nStep two: Open a workflow template\nn8n provides a quickstart template using training nodes. You can use this to work with fake data and avoid setting up credentials.\nGo to Templates | Very quick quickstart.\nSelect Use workflow to view the options for using the template.\nSelect Import template to  cloud workspace to load the template into your Cloud instance.\nThis workflow:\nGets example data from the Customer Datastore node.\nUses the Edit Fields node to extract only the desired data and assigns that data to variables. In this example, you map the customer name, ID, and description.\nThe individual pieces in an n8n workflow are called nodes. Double click a node to explore its settings and how it processes data.\nStep three: Run the workflow\nSelect Execute Workflow. This runs the workflow, loading the data from the Customer Datastore node, then transforming it with Edit Fields. You need this data available in the workflow so that you can work with it in the next step.\nStep four: Add a node\nAdd a third node to message each customer and tell them their description. Use the Customer Messenger node to send a message to fake recipients.\nSelect the Add node !Add node icon{.off-glb} connector on the Edit Fields node.\nSearch for Customer Messenger. n8n shows a list of nodes that match the search.\nSelect Customer Messenger (n8n training) to add the node to the canvas. n8n opens the node automatically.\nUse expressions to map in the Customer ID and create the Message:\nIn the INPUT panel select the Schema tab.\nDrag Edit Fields1 > customer_id into the Customer ID field in the node settings.\nHover over Message. Select the Expression tab, then select the expand button !Add node icon{.off-glb} to open the full expressions editor.\nCopy this expression into the editor:\nClose the expressions editor, then close the Customer Messenger node by clicking outside the node or selecting Back to canvas.\nSelect Execute Workflow. n8n runs the workflow.\nThe complete workflow should look like this:\nNext steps\nRead n8n's longer try it out tutorial for a more complex workflow, and an introduction to more features and n8n concepts.\nTake the text courses or video courses."
  },
  {
    "file_path": "try-it-out\\tutorial-first-workflow.md",
    "content": "Your first workflow\nThis guide will show you how to construct a workflow in n8n, explaining key concepts along the way. You will:\nCreate a workflow from scratch.\nUnderstand key concepts and skills, including:\nStarting workflows with trigger nodes\nConfiguring credentials\nProcessing data\nRepresenting logic in an n8n workflow\nUsing expressions\n!\"Screenshot of the completed workflow\"\nThis quickstart uses n8n Cloud, which is recommended for new users. A free trial is available - if you haven't already done so, sign up for an account now.\nStep one: Create a new workflow\nWhen you open n8n, you'll see either:\nA window with a welcome message and two large buttons: Choose Start from Scratch to create a new workflow.\nThe Workflows list on the Overview page. Select the Create Workflow to create a new workflow.\nStep two: Add a trigger node\nn8n provides two ways to start a workflow:\nManually, by selecting Execute Workflow.\nAutomatically, using a trigger node as the first node. The trigger node runs the workflow in response to an external event, or based on your settings.\nFor this tutorial, we'll use the Schedule trigger. This allows you to run the workflow on a schedule:\nSelect Add first step.\nSearch for Schedule. n8n shows a list of nodes that match the search.\nSelect Schedule Trigger to add the node to the canvas. n8n opens the node.\nFor Trigger Interval, select Weeks.\nFor Weeks Between Triggers, enter 1.\nEnter a time and day. For this example, select Monday in Trigger on Weekdays, select 9am in Trigger at Hour, and enter 0 in Trigger at Minute.\nClose the node details view to return to the canvas.\nStep three: Add the NASA node and set up credentials\nThe NASA node interacts with NASA's public APIs to fetch useful data. We will use the real-time data from the API to find solar events.\n??? explanation \"Credentials\"\nCredentials are private pieces of information issued by apps and services to authenticate you as a user and allow you to connect and share information between the app or service and the n8n node. The type of information required varies depending on the app/service concerned. You should be careful about sharing or revealing the credentials outside of n8n.\nSelect the Add node !Add node icon{.off-glb} connector on the Schedule Trigger node.\nSearch for NASA. n8n shows a list of nodes that match the search.\nSelect NASA to view a list of operations.\nSearch for and select Get a DONKI solar flare. This operation returns a report about recent solar flares. When you select the operation, n8n adds the node to the canvas and opens it.\nTo access the NASA APIs, you need to set up credentials:\nSelect the  Credential for NASA API dropdown.\nSelect Create new credential. n8n opens the credentials view.\nGo to NASA APIs and fill out the form from the Generate API Key link. The NASA site generates the key and emails it to the address you entered.\nCheck your email account for the API key. Copy the key, and paste it into API Key in n8n.\nSelect Save.\nClose the credentials screen. n8n returns to the node. The new credentials should be automatically selected in Credential for NASA API.\nBy default, DONKI Solar Flare provides data for the past 30 days. To limit it to just the last week, use Additional Fields:\nSelect Add field.\nSelect Start date.\nTo get a report starting from a week ago, you can use an expression: next to Start date, select the Expression tab, then select the expand button !Add node icon{.off-glb} to open the full expressions editor.\nIn the Expression field, enter the following expression:\nThis generates a date in the correct format, seven days before the current date.\n!image showing the expression above generating a date\n??? explanation \"Date and time formats in n8n...\"\nn8n uses Luxon to work with date and time, and also provides two variables for convenience: $now and $today. For more information, refer to Expressions > Luxon.\nClose the Edit Expression modal to return to the NASA node.\nYou can now check that the node is working and returning the expected date: select Execute step to run the node manually. n8n calls the NASA API and displays details of solar flares in the past seven days in the OUTPUT section.\nClose the NASA node to return to the workflow canvas.\nStep four: Add logic with the If node\nn8n supports complex logic in workflows. In this tutorial we will use the If node to create two branches that each generate a report from the NASA data. Solar flares have five possible classifications; we will add logic that sends a report with the lower classifications to one output, and the higher classifications to another.\nAdd the If node:\nSelect the Add node !Add node icon{.off-glb} connector on the NASA node.\nSearch for If. n8n shows a list of nodes that match the search.\nSelect If to add the node to the canvas. n8n opens the node.\nYou need to check the value of the classType property in the NASA data. To do this:\nDrag classType into Value 1.\nChange the comparison operation to String > Contains.\nIn Value 2, enter X. This is the highest classification of solar flare. In the next step, you will create two reports: one for X class solar flares, and one for all the smaller solar flares.\nYou can now check that the node is working and returning the expected date: select Execute step to run the node manually. n8n tests the data against the condition, and shows which results match true or false in the OUTPUT panel.\nOnce you are happy the node will return some events, you can close the node to return to the canvas.\nStep five: Output data from your workflow\nThe last step of the workflow is to send the two reports about solar flares. For this example, you'll send data to Postbin. Postbin is a service that receives data and displays it on a temporary web page.\nOn the If node, select the Add node !Add node icon{.off-glb} connector labeled true.\nSearch for PostBin. n8n shows a list of nodes that match the search.\nSelect PostBin.\nSelect Send a request. n8n adds the node to the canvas and opens it.\nGo to Postbin and select Create Bin. Leave the tab open so you can come back to it when testing the workflow.\nCopy the bin ID. It looks similar to 1651063625300-2016451240051.\nIn n8n, paste your Postbin ID into Bin ID.\nNow, configure the data to send to Postbin. Next to Bin Content, select the Expression tab (you will need to mouse-over the Bin Content for the tab to appear), then select the expand button !Add node icon{.off-glb} to open the full expressions editor.\nYou can now click and drag the correct field from the If Node output into the expressions editor to automatically create a reference for this label. In this case the input we want is 'classType'.\nOnce dropped into the expressions editor it will transform into this reference: {{$json[\"classType\"]}}. Add a message to it, so that the full expression is:\n!image showing the expression above generating output\nClose the expressions editor to return to the node.\nClose the Postbin node to return to the canvas.\nAdd another Postbin node, to handle the false output path from the If node:\nHover over the Postbin node, then select Node context menu !Node context menu icon{.off-glb} > Duplicate node to duplicate the first Postbin node.\nDrag the false connector from the If node to the left side of the new Postbin node.\nStep six: Test the workflow\nYou can now test the entire workflow. Select Execute Workflow. n8n runs the workflow, showing each stage in progress.\nGo back to your Postbin bin. Refresh the page to see the output.\nIf you want to use this workflow (in other words, if you want it to run once a week automatically), you need to activate it by selecting the Active toggle.\nCongratulations\nYou now have a fully functioning workflow that does something useful! It should look something like this:\nAlong the way you have discovered:\nHow to find the nodes you want and join them together\nHow to use expressions to manipulate data\nHow to create credentials and attach them to nodes\nHow to use logic in your workflows\nThere are plenty of things you could add to this (perhaps add some more credentials and a node to send you an email of the results), or maybe you have a specific project in mind. Whatever your next steps, the resources linked below should help.\nNext steps\nInterested in what you could do with AI? Find out how to build an AI chat agent with n8n.\nTake n8n's text courses or video courses.\nExplore more examples in workflow templates."
  },
  {
    "file_path": "user-management\\account-types.md",
    "content": "Account types\nThere are three account types: owner, admin, and member. The account type affects the user permissions and access.\nTABLE_PLACEHOLDER_0"
  },
  {
    "file_path": "user-management\\best-practices.md",
    "content": "Best practices for user management\nThis page contains advice on best practices relating to user management in n8n.\nAll platforms\nn8n recommends that owners create a member-level account for themselves. Owners can see all workflows, but there is no way to see who created a particular workflow, so there is a risk of overriding other people's work if you build and edit workflows as an owner.\nUsers must be careful not to edit the same workflow simultaneously. It's possible to do it, but the users will overwrite each other's changes.\nTo move workflows between accounts, export the workflow as JSON, then import it to the new account. Note that this action loses the workflow history.\nWebhook paths must be unique across the entire instance. This means each webhook path must be unique for all workflows and all users. By default, n8n generates a long random value for the webhook path, but users can edit this to their own custom path. If two users set the same path value:\nThe path works for the first workflow that's run or activated.\nOther workflows will error if they try to run with the same path.\nSelf-hosted\nIf you run n8n behind a reverse proxy, set the following environment variables so that n8n generates emails with the correct URL:\nN8N_HOST\nN8N_PORT\nN8N_PROTOCOL\nN8N_EDITOR_BASE_URL\nMore information on these variables is available in Environment variables."
  },
  {
    "file_path": "user-management\\cloud-setup.md",
    "content": "Set up user management on n8n Cloud\nTo access user management, upgrade to version 0.195.0 or newer.\nStep one: In-app setup\nStep two: Invite users"
  },
  {
    "file_path": "user-management\\index.md",
    "content": "User management\nUser management in n8n allows you to invite people to work in your n8n instance. It includes:\nLogin and password management\nAdding and removing users\nThree account types: Owner and Member (and Admin for Pro & Enterprise plans)\nSetup guides\nThis section contains most usage information for user management, and the Cloud setup guide. If you self-host n8n, there are extra steps to configure your n8n instance. Refer to the Self-hosted guide.\nThis section includes guides to configuring LDAP and SAML in n8n."
  },
  {
    "file_path": "user-management\\ldap.md",
    "content": "Lightweight Directory Access Protocol (LDAP)\nThis page tells you how to enable LDAP in n8n. It assumes you're familiar with LDAP, and have an existing LDAP server set up.\nLDAP allows users to sign in to n8n with their organization credentials, instead of an n8n login.\nEnable LDAP\nLog in to n8n as the instance owner.\nSelect Settings !Settings icon{.off-glb} > LDAP.\nToggle on Enable LDAP Login.\nComplete the fields with details from your LDAP server.\nSelect Test connection to check your connection setup, or Save connection to create the connection.\nAfter enabling LDAP, anyone on your LDAP server can sign in to the n8n instance, unless you exclude them using the User Filter setting.\nYou can still create non-LDAP users (email users) on the Settings > Users page.\nMerging n8n and LDAP accounts\nIf n8n finds matching accounts (matching emails) for email users and LDAP users, the user must sign in with their LDAP account. n8n instance owner accounts are excluded from this: n8n never converts owner accounts to LDAP users.\nLDAP user accounts in n8n\nOn first sign in, n8n creates a user account in n8n for the LDAP user.\nYou must manage user details on the LDAP server, not in n8n. If you update or delete a user on your LDAP server, the n8n account updates at the next scheduled sync, or when the user next tries to log in, whichever happens first.\nTurn LDAP off\nTo turn LDAP off:\nLog in to n8n as the instance owner.\nSelect Settings !Settings icon{.off-glb} > LDAP.\nToggle off Enable LDAP Login.\nIf you turn LDAP off, n8n converts existing LDAP users to email users on their next login. The users must reset their password."
  },
  {
    "file_path": "user-management\\manage-users.md",
    "content": "Manage users\nThe Settings > Users page shows all users, including ones with pending invitations.\nDelete a user\nSelect the menu icon by the user you want to delete.\nConfirm you want to delete them.\nIf they're an active user, choose whether to copy their workflow data and credentials to a new user, or permanently delete their workflows and credentials.\nResend an invitation to a pending user\nClick the menu icon by the user, then click Resend invite."
  },
  {
    "file_path": "user-management\\two-factor-auth.md",
    "content": "Two-factor authentication (2FA)\nTwo-factor authentication (2FA) adds a second authentication method on top of username and password. This increases account security. n8n supports 2FA using an authenticator app.\nEnable 2FA\nYou need an authenticator app on your phone.\nTo enable 2FA in n8n:\nGo to you Settings > Personal.\nSelect Enable 2FA. n8n opens a modal with a QR code.\nScan the QR code in your authenticator app.\nEnter the code from your app in Code from authenticator app.\nSelect Continue. n8n displays recovery codes.\nSave the recovery codes. You need these to regain access to your account if you lose your authenticator.\nDisable 2FA for your instance\nSelf-hosted users can configure their n8n instance to disable 2FA for all users by setting N8N_MFA_ENABLED to false. Note that n8n ignores this if existing users have 2FA enabled. Refer to Configuration methods for more information on configuring your n8n instance with environment variables."
  },
  {
    "file_path": "user-management\\rbac\\index.md",
    "content": "Role-based access control (RBAC)\nRBAC is a way of managing access to workflows and credentials based on user roles and projects. You group workflows into projects, and user access depends on the user's project role. This section provides guidance on using RBAC in n8n."
  },
  {
    "file_path": "user-management\\rbac\\projects.md",
    "content": "n8n uses projects to group workflows and credentials, and assigns roles to users in each project. This means that a single user can have different roles in different projects, giving them different levels of access.\nCreate a project\nInstance owners and instance admins can create projects.\nTo create a project:\nSelect !Plus icon Add project.\nFill out the project settings.\nSelect Save.\nAdd and remove users in a project\nProject admins can add and remove users.\nTo add a user to a project:\nSelect the project.\nSelect Project settings.\nUnder Project members, browse for users or search by username or email address.\nSelect the user you want to add.\nCheck the role type and change it if needed.\nSelect Save.\nTo remove a user from a project:\nSelect the project.\nSelect Project settings.\nIn the role type dropdown for the user you want to remove, select Remove access.\nSelect Save.\nDelete a project\nTo delete a project:\nSelect the project.\nSelect Project settings.\nSelect Delete project.\nChoose what to do with the workflows and credentials. You can select:\nTransfer its workflows and credentials to another project: n8n prompts you to choose a project to move the data to.\nDelete its workflows and credentials: n8n prompts you to confirm that you want to delete all the data in the project.\nMove workflows and credentials between projects or users\nWorkflow and credential owners can move workflows or credentials (changing ownership) to other users or projects they have access to.\nSelect Workflow menu !Workflow menu icon{.off-glb} or Credential menu !Workflow menu icon{.off-glb} > Move.\nSelect the project or user you want to move to.\nSelect Next.\nConfirm you understand the impact of the move: workflows may stop working if the credentials they need aren't available in the target project, and n8n removes any current individual sharing.\nSelect Confirm move to new project.\nUsing external secrets in projects\nTo use external secrets in a project, you must have an instance owner or instance admin as a member of the project."
  },
  {
    "file_path": "user-management\\rbac\\role-types.md",
    "content": "RBAC role types\nWithin projects, there are three user roles: Admin, Editor, and Viewer. These roles control what the user can do in a project. A user can have different roles within different projects.\nProject Admin\nA Project Admin role has the highest level of permissions. Project admins can:\nManage project settings: Change name, delete project.\nManage project members: Invite members and remove members, change members' roles.\nView, create, update, and delete any workflows, credentials, or executions within a project.\nProject Editor\nA Project Editor can view, create, update, and delete any workflows, credentials, or executions within a project.\nProject Viewer\nA Project Viewer is effectively a read-only role with access to all workflows, credentials, and executions within a project.\nViewers aren't able to manually execute any workflows that exist in a project.\nTABLE_PLACEHOLDER_0\nVariables and tags aren't affected by RBAC: they're global across the n8n instance."
  },
  {
    "file_path": "user-management\\saml\\managing.md",
    "content": "Manage users with SAML\nThere are some user management tasks that are affected by SAML.\nExempt users from SAML\nYou can allow users to log in without using SAML. To do this:\nGo to Settings > Users.\nSelect the menu icon by the user you want to exempt from SAML.\nSelect Allow Manual Login.\nDeleting users\nIf you remove a user from your IdP, they remain logged in to n8n. You need to manually remove them from n8n as well. Refer to Manage users for guidance on deleting users."
  },
  {
    "file_path": "user-management\\saml\\okta.md",
    "content": "Okta Workforce Identity SAML setup\nSet up SAML SSO in n8n with Okta.\nPrerequisites\nYou need an Okta Workforce Identity account, and the redirect URL and entity ID from n8n's SAML settings.\nOkta Workforce may enforce two factor authentication for users, depending on your Okta configuration.\nRead the Set up SAML guide first.\nSetup\nIn your Okta admin panel, select Applications > Applications.\nSelect Create App Integration. Okta opens the app creation modal.\nSelect SAML 2.0, then select Next.\nOn the General Settings tab, enter n8n as the App name.\nSelect Next .\nOn the Configure SAML tab, complete the following General fields:\nSingle sign-on URL: the Redirect URL from n8n.\nAudience URI (SP Entity ID): the Entity ID from n8n.\nDefault RelayState: leave this empty.\nName ID format: EmailAddress.\nApplication username: Okta username.\nUpdate application username on: Create and update.\nCreate Attribute Statements:\nTABLE_PLACEHOLDER_0\n1. Select Next. Okta may prompt you to complete a marketing form, or may take you directly to your new n8n Okta app.\n1. Assign the n8n app to people:\n1. On the n8n app dashboard in Okta, select Assignments.\n1. Select Assign > Assign to People. Okta displays a modal with a list of available people.\n1. Select Assign next to the person you want to add. Okta displays a prompt to confirm the username.\n1. Leave the username as email address. Select Save and Go Back.\n1. Select Done.\n1. Get the metadata XML: on the Sign On tab, copy the Metadata URL. Navigate to it, and copy the XML. Paste this into Identity Provider Settings in n8n.\n1. Select Save settings.\n1. Select Test settings. n8n opens a new tab. If you're not currently logged in, Okta prompts you to sign in. n8n then displays a success message confirming the attributes returned by Okta."
  },
  {
    "file_path": "user-management\\saml\\setup.md",
    "content": "Set up SAML\nThis page tells you how to enable SAML SSO (single sign-on) in n8n. It assumes you're familiar with SAML. If you're not, SAML Explained in Plain English can help you understand how SAML works, and its benefits.\nEnable SAML\nIn n8n, go to Settings > SSO.\nMake a note of the n8n Redirect URL and Entity ID.\nOptional: if your IdP allows you to set up SAML from imported metadata, navigate to the Entity ID URL and save the XML.\nOptional: if you are running n8n behind a load balancer make sure you have N8N_EDITOR_BASE_URL configured.\nSet up SAML with your IdP (identity provider). You need the redirect URL and entity ID. You may also need an email address and name for the IdP user.\nAfter completing setup in your IdP, load the metadata XML into n8n. You can use a metadata URL or raw XML:\nMetadata URL: Copy the metadata URL from your IdP into the Identity Provider Settings field in n8n.\nRaw XML: Download the metadata XML from your IdP, toggle Identiy Provider Settings to XML, then copy the raw XML into Identity Provider Settings.\nSelect Save settings.\nSelect Test settings to check your SAML setup is working.\nSet SAML 2.0 to Activated.\nGeneric IdP setup\nThe steps to configure the IdP vary depending on your chosen IdP. These are some common setup tasks:\nCreate an app for n8n in your IdP.\nMap n8n attributes to IdP attributes:\nTABLE_PLACEHOLDER_0| PingIdentity | PingOne SSO |"
  },
  {
    "file_path": "user-management\\saml\\troubleshooting.md",
    "content": "Troubleshooting SAML SSO\nIf you get an error when testing your SAML setup, check the following:\nDoes the app you created in your IdP support SAML?\nDid you enter the n8n redirect URL and entity ID in the correct fields in your IdP?\nIs the metadata XML correct? Check that the metadata you copied into n8n is formatted correctly.\nFor more support, use the forum, or contact your support representative if you have a paid support plan."
  },
  {
    "file_path": "workflows\\create.md",
    "content": "Create a workflow\nA workflow is a collection of nodes connected together to automate a process. You build workflows on the workflow canvas.\nCreate a workflow\nSelect the !universal create resource icon{.off-glb} button in the upper-left corner of the side menu. Select workflow.\nIf your n8n instance supports projects, you'll also need to choose whether to create the workflow inside your personal space or a specific project you have access to. If you're using the community version, you'll always create workflows inside your personal space.\nGet started by adding a trigger node: select Add first step...\nOr:\nSelect the  !universal create resource icon{.off-glb} create button in the upper-right corner from either the Overview page or a specific project. Select workflow.\nIf you're doing this from the Overview page, you'll create the workflow inside your personal space. If you're doing this from inside a project, you'll create the workflow inside that specific project.\nGet started by adding a trigger node: select Add first step...\nIf it's your first time building a workflow, you may want to use the quickstart guides to quickly try out n8n features.\nRun workflows manually\nYou may need to run your workflow manually when building and testing, or if your workflow doesn't have a trigger node.\nTo run manually, select Execute Workflow.\nRun workflows automatically\nAll new workflows are inactive by default.\nYou need to activate workflows that start with a trigger node or Webhook node so that they can run automatically. When a workflow is inactive, you must run it manually.\nTo activate or deactivate your workflow, open your workflow and toggle Inactive / Active.\nOnce a workflow is active, it runs whenever its trigger conditions are met."
  },
  {
    "file_path": "workflows\\export-import.md",
    "content": "Export and import workflows\nn8n saves workflows in JSON format. You can export your workflows as JSON files or import JSON files into your n8n library.\nYou can export and import workflows in several ways.\nCopy-Paste\nYou can copy and paste a workflow or parts of it by selecting the nodes you want to copy to the clipboard (Ctrl + c or cmd +c) and pasting it (Ctrl + v or cmd + v) into the Editor UI.\nTo select all nodes or a group of nodes, click and drag:\n!Select a group of nodes\nFrom the Editor UI menu\nFrom the top navigation bar, select the three dots in the upper right  to see the following options:\nImport & Export workflows menu\nDownload: Downloads your current workflow as a JSON file to your computer.\nImport from URL: Imports workflow JSON from a URL, for example, this workflow JSON file on GitHub.\nImport from File: Imports a workflow as a JSON file from your computer.\nFrom the command line\nExport: See the full list of commands  for exporting workflows or credentials.\nImport: See the full list of commands  for importing workflows or credentials."
  },
  {
    "file_path": "workflows\\history.md",
    "content": "Workflow history\nUse workflow history to view and restore previous versions of your workflows.\nUnderstand workflow history\nn8n creates a new version when you:\nSave your workflow.\nRestore an old version. n8n saves the latest version before restoring.\nPull from a Git repository using Source control. Note that n8n saves versions to the instance database, not to Git.\nView workflow history\nTo view a workflow's history:\nOpen the workflow.\nSelect Workflow history !Workflow history icon{.off-glb}. n8n opens a menu showing the saved workflow versions, and a canvas with a preview of the selected version.\nRestore or copy previous versions\nYou can restore a previous workflow version, or make a copy of it:\nOn the version you want to restore or copy, select Options !Options icon{.off-glb}.\nChoose what you want to do:\nRestore this version: replace your current workflow with the selected version.\nClone to new workflow: create a new workflow based on the selected version.\nOpen version in new tab: open a second tab displaying the selected version. Use this to compare versions.\nDownload: download the version as JSON."
  },
  {
    "file_path": "workflows\\index.md",
    "content": "Workflows\nA workflow is a collection of nodes connected together to automate a process.\nCreate a workflow.\nUse Workflow templates to help you get started.\nLearn about the key components of an automation in n8n.\nDebug using the Executions list.\nShare workflows between users.\nIf it's your first time building a workflow, you may want to use the quickstart guides to quickly try out n8n features."
  },
  {
    "file_path": "workflows\\settings.md",
    "content": "Workflow settings\nYou can customize workflow behavior for individual workflows using workflow settings.\nTo open the settings:\nOpen your workflow.\nSelect the Options !Options menu{.off-glb} menu.\nSelect Settings. n8n opens the Workflow settings modal.\nThe following settings are available:\nExecution order: choose the execution order for multi-branch workflows. v0 (legacy) executes the first node of each branch, then the second node of each branch, and so on. v1 (recommended) executes each branch in turn, completing one branch before starting another. n8n orders the branches based on their position on the canvas, from topmost to bottommost. If two branches are at the same height, the leftmost branch executes first.\nError Workflow: select a workflow to trigger if the current workflow fails. See Error workflows for more details.\nThis workflow can be called by: choose whether other workflow can call this workflow.\nTimezone: sets the timezone for the workflow to use. The default timezone is EDT (New York). The timezone setting is  important for the Schedule Trigger node.\nSave failed production executions: whether n8n should save failed executions for active workflows.\nSave successful production executions: whether n8n should save successful executions for active workflows.\nSave manual executions: whether n8n should save executions for workflows started by the user in the editor.\nSave execution progress: whether n8n should save execution data for each node. If set to Save, the workflow resumes from where it stopped in case of an error. This might increase latency.\nTimeout Workflow: toggle to enable setting a duration after which n8n should cancel the current workflow execution.\nTimeout After: Set the time in hours, minutes, and seconds after which the workflow should timeout. For n8n Cloud users n8n enforces a maximum available timeout for each plan."
  },
  {
    "file_path": "workflows\\sharing.md",
    "content": "Workflow sharing\nWorkflow sharing allows you to share workflows between users of the same n8n instance.\nUsers can share workflows they created. Instance owners, and users with the admin role, can view and share all workflows in the instance. Refer to Account types for more information about owners and admins.\nShare a workflow\nOpen the workflow you want to share.\nSelect Share.\nIn Add users, find and select the users you want to share with.\nSelect Save.\nView shared workflows\nYou can browse and search workflows on the Workflows list. The workflows in the list depend on the project:\nOverview lists all workflows you can access. This includes:\nYour own workflows.\nWorkflows shared with you.\nWorkflows in projects you're a member of.\nIf you log in as the instance owner or admin: all workflows in the instance.\nOther projects: all workflows in the project.\nWorkflow roles and permissions\nThere are two workflow roles: creator and editor. The creator is the user who created the workflow. Editors are other users with access to the workflow.\nYou can't change the workflow owner, except when deleting the user.\nPermissions\nTABLE_PLACEHOLDER_0\nNode editing restrictions with unshared credentials\nSharing in n8n works on the principle of least privilege. This means that if a user shares a workflow with you, but they don't share their credentials, you can't edit the nodes within the workflow that use those credentials. You can view and run the workflow, and edit nodes that don't use unshared credentials.\nRefer to Credential sharing for guidance on sharing credentials."
  },
  {
    "file_path": "workflows\\subworkflow-conversion.md",
    "content": "Sub-workflow conversion\nUse sub-workflow conversion to refactor your workflows into reusable parts. Expressions referencing other nodes are automatically updated and added as parameters in the Execute Workflow Trigger node.\nSee sub-workflows for a general introduction to the concept.\nSelecting nodes for a sub-workflow\nTo convert part of a workflow to a sub-workflow, you must select the nodes in the original workflow that you want to convert.\nDo this by selecting a group of valid nodes. The selection must be continuous and must connect to the rest of the workflow from at most one start node and one end node. The selection must fulfill these conditions:\nMust not include trigger nodes.\nOnly a single node in the selection can have incoming connections from nodes outside of the selection.\nThat node can have multiple incoming connections, but only a single input branch (which means it can't be a Merge node for example).\nThat node can't have incoming connections from other nodes in the selection.\nOnly a single node in the selection can have outgoing connections to nodes outside of the selection.\nThat node can have multiple outgoing connections, but only a single output branch (it can't be an If node for example).\nThat node can't have outgoing connections to other nodes in the selection.\nThe selection must include all nodes between the input and output nodes.\nHow to convert part of a workflow to a sub-workflow\nSelect the desired nodes on the canvas. Right-click the canvas background and select Convert to sub-workflow.\nThings to keep in mind\nMost sub-workflow conversions work without issues, but there are some caveats and limitations to keep in mind:\nYou must set type constraints for input and output manually: By default, sub-workflow input and output allow all types. You can set expected types in sub-workflow's Execute Sub-workflow Trigger node and Edit Fields (set) node (labeled as Return and only included if the sub-workflow has outputs).\nLimited support for AI nodes: When dealing with sub-nodes like AI tools, you must select them all and may need to duplicate any nodes shared with other AI Agents before conversion.\nUses v1 execution ordering: New workflows use v1 execution ordering regardless of the parent workflow's settings - you can change this back in the settings.\nAccessor functions like first(), last(), and all() require extra care: Expressions using these functions don't always translate cleanly to a sub-workflow context. n8n may transform them to try to preserve their functionality, but you should check that they work as intended in their new context.\nThe itemMatching function requires a fixed index: You can't use expressions for the index value when using the itemMatching function. You must pass it a fixed number."
  },
  {
    "file_path": "workflows\\tags.md",
    "content": "Tags\nWorkflow tags allow you to label your workflows. You can then filter workflows by tag.\nTags are global. This means when you create a tag, it's available to all users on your n8n instance.\nAdd a tag to a workflow\nTo add a tag to your workflow:\nIn your workflow, select + Add tag.\nSelect an existing tag, or enter a new tag name.\nOnce you select a tag and click away from the tag modal, n8n displays the tag next to the workflow name.\nYou can add more than one tag.\nFilter by tag\nWhen browsing the workflows on your instance, you can filter by tag.\nOn the Workflows page, select Filters.\nSelect Tags.\nSelect the tag or tags you want to filter by. n8n lists the workflows with that tag.\nManage tags\nYou can edit existing tags. Instance owners can delete tags.\nSelect Manage tags. This is available from Filters > Tags on the Workflows page, or in the + Add tag modal in your workflow.\nHover over the tag you want to change.\nSelect Edit !Add node icon{.off-glb} to rename it, or Delete !Add node icon{.off-glb} to delete it."
  },
  {
    "file_path": "workflows\\templates.md",
    "content": "Workflow templates\nWhen creating a new workflow, you can choose whether to start with an empty workflow, or use an existing template.\nTemplates provide:\nHelp getting started: n8n might already have a template that does what you need.\nExamples of what you can build\nBest practices for creating your own workflows\nAccess templates\nSelect !View templates icon{.off-glb} Templates to view the templates library.\nIf you use n8n's template library, this takes you to browse Workflows on the n8n website. If you use a custom library provided by your organization, you'll be able to search and browse the templates within the app.\nAdd your workflow to the n8n library\nSelf-hosted n8n: Use your own library"
  },
  {
    "file_path": "workflows\\workflow-id.md",
    "content": "Find your workflow ID\nYour workflow ID is available in:\nThe URL of the open workflow.\nThe workflow settings title."
  },
  {
    "file_path": "workflows\\components\\connections.md",
    "content": "Connections\nA connection establishes a link between nodes to route data through the workflow. A connection between two nodes passes data from one node's output to another node's input.\n!Example of creating and deleting a connection\nCreate a connection\nTo create a connection between two nodes, select the grey dot or Add node !Add node icon{.off-glb} on the right side of a node and slide the arrow to the grey rectangle on the left side of the following node.\nDelete a connection\nHover over the connection, then select Delete !Delete connector icon{.off-glb}."
  },
  {
    "file_path": "workflows\\components\\index.md",
    "content": "Workflow components\nThis section contains:\nNodes: integrations and operations.\nConnections: node connectors.\nSticky notes: document your workflows."
  },
  {
    "file_path": "workflows\\components\\nodes.md",
    "content": "Nodes\nNodes are the key building blocks of a workflow. They perform a range of actions, including:\nStarting the workflow.\nFetching and sending data.\nProcessing and manipulating data.\nn8n provides a collection of built-in nodes, as well as the ability to create your own nodes. Refer to:\nBuilt-in integrations to browse the node library.\nCommunity nodes for guidance on finding and installing community-created nodes.\nCreating nodes to start building your own nodes.\nAdd a node to your workflow\nAdd a node to an empty workflow\nSelect Add first step. n8n opens the nodes panel, where you can search or browse trigger nodes.\nSelect the trigger you want to use.\nAdd a node to an existing workflow\nSelect the Add node !Add node icon{.off-glb} connector. n8n opens the nodes panel, where you can search or browse all nodes.\nNode controls\nTo view node controls, hover over the node on the canvas:\nExecute step !Execute step icon{.off-glb}: Run the node.\nDeactivate !Deactivate node icon{.off-glb}: Deactivate the node.\nDelete !Delete node icon{.off-glb}: Delete the node.\nNode context menu !Node context menu icon{.off-glb}: Select node actions. Available actions:\nOpen node\nExecute step\nRename node\nDeactivate node\nPin node\nCopy node\nDuplicate node\nSelect all\nClear selection\nDelete node\nNode settings\nThe node settings under the Settings tab allow you to control node behaviors and add node notes.\nWhen active or set, they do the following:\nRequest Options: Select Add Option to view and select these options.\nBatching: Control how to batch large numbers of input items.\nIgnore SSL Issues: Download the response even if SSL validation isn't possible.\nProxy: Use this if you need to specify an HTTP proxy.\nTimeout: Set a timeout for the request in ms.\nAlways Output Data: The node returns an empty item even if the node returns no data during execution. Be careful setting this on IF nodes, as it could cause an infinite loop.\nExecute Once: The node executes once, with data from the first item it receives. It doesn't process any extra items.\nRetry On Fail: When an execution fails, the node reruns until it succeeds.\nOn Error:\nStop Workflow: Halts the entire workflow when an error occurs, preventing further node execution.\nContinue: Proceeds to the next node despite the error, using the last valid data.\nContinue (using error output): Continues workflow execution, passing error information to the next node for potential handling.\nYou can document your workflow using node notes:\nNotes: Note to save with the node.\nDisplay note in flow: If active, n8n displays the note in the workflow as a subtitle."
  },
  {
    "file_path": "workflows\\components\\sticky-notes.md",
    "content": "Sticky Notes\nSticky Notes allow you to annotate and comment on your workflows.\nn8n recommends using Sticky Notes heavily, especially on template workflows, to help other users understand your workflow.\n!Screenshot of a basic workflow with an example sticky note\nCreate a Sticky Note\nSticky Notes are a core node. To add a new Sticky Note:\nOpen the nodes panel.\nSearch for note.\nClick the Sticky Note node. n8n adds a new Sticky Note to the canvas.\nEdit a Sticky Note\nDouble click the Sticky Note you want to edit.\nWrite your note. This guide explains how to format your text with Markdown. n8n uses markdown-it, which implements the CommonMark specification.\nClick away from the note, or press Esc, to stop editing.\nChange the color\nTo change the Sticky Note color:\nHover over the Sticky Note\nSelect Change color !Change Sticky Note color icon{.off-glb}\nSticky Note positioning\nYou can:\nDrag a Sticky Note anywhere on the canvas.\nDrag Sticky Notes behind nodes. You can use this to visually group nodes.\nResize Sticky Notes by hovering over the edge of the note and dragging to resize.\nChange the color: select Options !Options icon{.off-glb} to open the color selector.\nWriting in Markdown\nSticky Notes support Markdown formatting. This section describes some common options.\nFor a more detailed guide, refer to CommonMark's help. n8n uses markdown-it, which implements the CommonMark specification.\nMake images full width\nYou can force images to be 100% width of the sticky note by appending #full-width to the filename:\nEmbed a YouTube video\nTo display a YouTube video in a note, use the @youtube directive with the video's ID. For this to work, the video's creator must allow embedding.\nFor example:\nTo embed your own video, copy the above syntax, replacing ZCuL2e4zC_4 with your video ID. The YouTube video ID is the string that follows v= in the YouTube URL."
  },
  {
    "file_path": "workflows\\executions\\all-executions.md",
    "content": "All executions\nTo view all executions from an n8n instance, navigate to the Overview page and then click into the Executions tab. This will show you all executions from the workflows you have access to.\nIf your n8n instance supports projects, you'll also be able to view the executions tab within projects you have access to. This will show you executions only from the workflows within the specified project.\nFilter executions\nYou can filter the executions list:\nSelect¬†the Executions tab either from within the Overview page or a specific project¬†to open the list.\nSelect Filters.\nEnter your filters. You can filter by:\nWorkflows: choose all workflows, or a specific workflow name.\nStatus: choose from Failed, Running, Success, or Waiting.\nExecution start: see executions that started in the given time.\nSaved custom data: this is data you create within the workflow using the Code node. Enter the key and value to filter. Refer to Custom executions data for information on adding custom data.\nRetry failed workflows\nIf your workflow execution fails, you can retry the execution. To retry a failed workflow:\nSelect¬†the Executions tab from within either the Overview page or a specific project¬†to open the list.\nOn the execution you want to retry, select Retry execution !Options menu icon{.off-glb}.\nLoad data from previous executions into your current workflow\nYou can load data from a previous workflow back into the canvas. Refer to Debug executions for more information."
  },
  {
    "file_path": "workflows\\executions\\custom-executions-data.md",
    "content": "Custom executions data\nYou can set custom data on your workflow using the Code node or the Execution Data node. n8n records this with each execution. You can then use this data when filtering the executions list, or fetch it in your workflows using the Code node.\nSet and access custom data using the Code node\nThis section describes how to set and access data using the Code node. Refer to Execution Data node for information on using the Execution Data node to set data. You can't retrieve custom data using the Execution Data node.\nSet custom executions data\nSet a single piece of extra data:\n=== \"JavaScript\"\n=== \"Python\"\nSet all extra data. This overwrites the whole custom data object for this execution:\n=== \"JavaScript\"\n=== \"Python\"\nThere are limitations:\nThey must be strings\nkey has a maximum length of 50 characters\nvalue has a maximum length of 255 characters\nn8n supports a maximum of 10 items of custom data\nAccess the custom data object during execution\nYou can retrieve the custom data object, or a specific value in it, during an execution:\n=== \"JavaScript\"\n=== \"Python\""
  },
  {
    "file_path": "workflows\\executions\\debug.md",
    "content": "Debug and re-run past executions\nYou can load data from a previous execution into your current workflow. This is useful for debugging data from failed production executions: you can see a failed execution, make changes to your workflow to fix it, then re-run it with the previous execution data.\nLoad data\nTo load data from a previous execution:\nIn your workflow, select the Executions tab to view the Executions list.\nSelect the execution you want to debug. n8n displays options depending on whether the workflow was successful or failed:\nFor failed executions: select Debug in editor.\nFor successful executions: select Copy to editor.\nn8n copies the execution data into your current workflow, and pins the data in the first node in the workflow."
  },
  {
    "file_path": "workflows\\executions\\dirty-nodes.md",
    "content": "Dirty nodes\nA dirty node is a node that executed successfully in the past, but whose output n8n now considers stale or unreliable. They're labeled like this to indicate that if the node executes again, the output may be different. It may also be the point where a partial execution starts from.\nHow to recognize dirty node data\nIn the canvas of the workflow editor, you can identify dirty notes by their different-colored border and a yellow triangle in place of the previous green tick symbol. For example:\n!\"Image of node displayed with yellow border\"\nIn the node editor view, the output panel also displays a yellow triangle on the output panel. If you hover over the triangle, a tooltip appears with more information about why n8n considers the data stale:\n!\"Image of node displayed with yellow border\"\nWhy n8n marks nodes dirty\nThere are several reasons why n8n might flag execution data as stale. For example:\nInserting or deleting a node: labels the first node that follows the inserted node dirty.\nModifying node parameters: labels the modified node dirty.\nAdding a connector: labels the destination node of the new connector dirty.\nDeactivating a node: labels the first node that follows the deactivated node dirty.\n??? explanation \"Other reasons n8n marks nodes dirty\"\n- Unpinning a node: labels the unpinned node dirty.\n- Modifying pinned data: labels the node that comes after the pinned data dirty.\n- If any of the above actions occur inside a loop, also labels the first node of the loop dirty.\nFor sub-nodes, also labels any executed parent nodes (up to and including the root) when:\n- Editing an executed sub-node\n- Adding a new sub-node\n- Disconnecting or deleting a sub-node\n- Deactivating a sub-node\n- Activating a sub-node\n-   When deleting a connected node in a workflow:\n!\"Image of node displayed with yellow border\"\n-   The next node in the sequence becomes dirty:\n!\"Image of node displayed with yellow border\"\nWhen using loops (with the Loop over Items node), when any node within the loop is dirty, the initial node of the loop is also considered dirty:\n!\"Image of node displayed with yellow border\"\nResolving dirty nodes\nExecuting a node again clears its dirty status. You can do this manually by triggering the whole workflow, or by running a partial execution with Execute step on the individual node or any node which follows it."
  },
  {
    "file_path": "workflows\\executions\\index.md",
    "content": "Executions\nAn execution is a single run of a workflow.\nExecution modes\nThere are two execution modes:\nManual: run workflows manually when testing. Select Execute Workflow to start a manual execution. You can do manual executions of active workflows, but n8n recommends keeping your workflow set to Inactive while developing and testing.\nProduction: a production workflow is one that runs automatically. To enable this, set the workflow to Active.\nExecution lists\nn8n provides two execution lists:\nWorkflow-level executions: this execution list shows the executions for a single workflow.\nAll executions: this list shows all executions for all your workflows.\nn8n supports adding custom data to executions."
  },
  {
    "file_path": "workflows\\executions\\manual-partial-and-production-executions.md",
    "content": "Manual, partial, and production executions\nThere are some important differences in how n8n executes workflows manually (by clicking the Execute Workflow button) and automatically (when the workflow is Active and triggered by an event or schedule).\nManual executions\nManual executions allow you to run workflows directly from the canvas to test your workflow logic. These executions are \"ad-hoc\": they run only when you manually select the Execute workflow button.\nManual executions make building workflows easier by allowing you to iteratively test as you go, following the flow logic and seeing data transformations. You can test conditional branching, data formatting changes, and loop behavior by providing different input items and modifying node options.\nPartial executions\nClicking the Execute workflow button at the bottom of the workflow in the Editor tab manually runs the entire workflow. You can also perform partial executions to run specific steps in your workflow. Partial executions are manual executions that only run a subset of your workflow nodes.\nTo perform a partial execution, select a node, open its detail view, and select Execute step. This executes the specific node and any preceding nodes required to fill in its input data. You can also temporarily disable specific nodes in the workflow chain to avoid interacting with those services while building.\nIn particular, partial executions are useful when updating the logic of a specific node since they allow you to re-execute the node with the same input data.\nTroubleshooting partial executions\nSome common issues you might come across when running partial executions include the following:\nThe destination node is not connected to any trigger. Partial executions need a trigger.\nThis error message appears when you try to perform a partial execution without connecting the workflow to a trigger. Manual executions, including partial executions, attempt to mimic production executions when possible. Part of this includes requiring a trigger node to describe when the workflow logic should execute.\nTo work around this, connect a trigger node to the workflow with the node you're trying to execute. Most often, a manual trigger is the simplest option.\nPlease execute the whole workflow, rather than just the node. (Existing execution data is too large.)\nThis error can appear when performing partial executions on workflows with large numbers of branches. Partial executions involve sending data and workflow logic to the n8n backend in a way that isn't required for full executions. This error occurs when your workflow exceeds the maximum size allowed for these messages.\nTo work around this, consider using the limit node to limit node output while running partial executions. Once the workflow is running as intended, you can disable or delete the limit node before enabling production execution.\nProduction executions\nProduction executions occur when a triggering event or schedule automatically runs a workflow.\nTo configure production executions, you must attach a trigger node (any trigger other than the manual trigger works) and switch workflow's toggle to Active. Once activated, the workflow automatically executes whenever the trigger condition occurs.\nThe execution flow for production executions doesn't display in the Editor tab of the workflow as with manual executions. Instead, you can see executions in the workflow's Executions tab according to your workflow settings. From there, you can explore and troubleshoot problems using the debug in editor feature."
  },
  {
    "file_path": "workflows\\executions\\single-workflow-executions.md",
    "content": "Workflow-level executions list\nThe Executions list in a workflow shows all executions for that workflow.\nView executions for a single workflow\nIn the workflow, select the Executions tab in the top menu. You can preview all executions of that workflow.\nFilter executions\nYou can filter the executions list.\nIn your workflow, select Executions.\nSelect Filters.\nEnter your filters. You can filter by:\nStatus: choose from Failed, Running, Success, or Waiting.\nExecution start: see executions that started in the given time.\nSaved custom data: this is data you create within the workflow using the Code node. Enter the key and value to filter. Refer to Custom executions data for information on adding custom data.\nRetry failed workflows\nIf your workflow execution fails, you can retry the execution. To retry a failed workflow:\nOpen the Executions list.\nFor the workflow execution you want to retry, select Refresh !Refresh icon{.off-glb}."
  }
]